---
'title': '会话设置'
'sidebar_label': '会话设置'
'slug': '/operations/settings/settings'
'toc_max_heading_level': 2
'description': '在 ``system.settings`` 表中找到的设置.'
---

import ExperimentalBadge from '@theme/badges/ExperimentalBadge';
import BetaBadge from '@theme/badges/BetaBadge';
import CloudAvailableBadge from '@theme/badges/CloudAvailableBadge';
import SettingsInfoBlock from '@theme/SettingsInfoBlock/SettingsInfoBlock';
import VersionHistory from '@theme/VersionHistory/VersionHistory';

<!-- Autogenerated -->
所有以下设置也可以在表 [system.settings](/docs/operations/system-tables/settings) 中找到。这些设置是从 [source](https://github.com/ClickHouse/ClickHouse/blob/master/src/Core/Settings.cpp) 自动生成的。

## add_http_cors_header {#add_http_cors_header} 



<SettingsInfoBlock type="Bool" default_value="0" />

写入添加 http CORS 头。
## additional_result_filter {#additional_result_filter} 

对 `SELECT` 查询结果应用的额外过滤表达式。
此设置不适用于任何子查询。

**示例**

```sql
INSERT INTO table_1 VALUES (1, 'a'), (2, 'bb'), (3, 'ccc'), (4, 'dddd');
SElECT * FROM table_1;
```
```response
┌─x─┬─y────┐
│ 1 │ a    │
│ 2 │ bb   │
│ 3 │ ccc  │
│ 4 │ dddd │
└───┴──────┘
```
```sql
SELECT *
FROM table_1
SETTINGS additional_result_filter = 'x != 2'
```
```response
┌─x─┬─y────┐
│ 1 │ a    │
│ 3 │ ccc  │
│ 4 │ dddd │
└───┴──────┘
```
## additional_table_filters {#additional_table_filters} 



<SettingsInfoBlock type="Map" default_value="{}" />

在从指定表读取后应用的额外过滤表达式。

**示例**

```sql
INSERT INTO table_1 VALUES (1, 'a'), (2, 'bb'), (3, 'ccc'), (4, 'dddd');
SELECT * FROM table_1;
```
```response
┌─x─┬─y────┐
│ 1 │ a    │
│ 2 │ bb   │
│ 3 │ ccc  │
│ 4 │ dddd │
└───┴──────┘
```
```sql
SELECT *
FROM table_1
SETTINGS additional_table_filters = {'table_1': 'x != 2'}
```
```response
┌─x─┬─y────┐
│ 1 │ a    │
│ 3 │ ccc  │
│ 4 │ dddd │
└───┴──────┘
```
## aggregate_functions_null_for_empty {#aggregate_functions_null_for_empty} 



<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用在查询中重写所有聚合函数，给它们添加 [-OrNull](/sql-reference/aggregate-functions/combinators#-ornull) 后缀。为兼容 SQL 标准启用此选项。
通过查询重写实现（类似于 [count_distinct_implementation](#count_distinct_implementation) 设置），以便在分布式查询中获得一致的结果。

可能的值：

- 0 — 禁用。
- 1 — 启用。

**示例**

考虑以下带有聚合函数的查询：
```sql
SELECT SUM(-1), MAX(0) FROM system.one WHERE 0;
```

当 `aggregate_functions_null_for_empty = 0` 时，它将生成：
```text
┌─SUM(-1)─┬─MAX(0)─┐
│       0 │      0 │
└─────────┴────────┘
```

当 `aggregate_functions_null_for_empty = 1` 时，结果将是：
```text
┌─SUMOrNull(-1)─┬─MAXOrNull(0)─┐
│          NULL │         NULL │
└───────────────┴──────────────┘
```
## aggregation_in_order_max_block_bytes {#aggregation_in_order_max_block_bytes} 



<SettingsInfoBlock type="UInt64" default_value="50000000" />

按主键聚合时，积累的最大块大小（以字节为单位）。较小的块大小允许在聚合的最终合并阶段进行更多并行化。
## aggregation_memory_efficient_merge_threads {#aggregation_memory_efficient_merge_threads} 



<SettingsInfoBlock type="UInt64" default_value="0" />

在内存高效模式下用于合并中间聚合结果的线程数。值越大，消耗的内存越多。0 表示 — 与 'max_threads' 相同。
## allow_aggregate_partitions_independently {#allow_aggregate_partitions_independently} 



<SettingsInfoBlock type="Bool" default_value="0" />

当分区键适合分组键时，启用在单独线程上独立聚合分区。当分区数量接近内核数量且分区大小大致相同是有益的。
## allow_archive_path_syntax {#allow_archive_path_syntax} 



<SettingsInfoBlock type="Bool" default_value="1" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.8"},{"label": "1"},{"label": "Added new setting to allow disabling archive path syntax."}]}, {"id": "row-2","items": [{"label": "24.5"},{"label": "1"},{"label": "Added new setting to allow disabling archive path syntax."}]}]}/>

文件/S3 引擎/表函数将解析路径，如 '::' 为 `<archive> :: <file>`，如果归档具有正确的扩展名。
## allow_asynchronous_read_from_io_pool_for_merge_tree {#allow_asynchronous_read_from_io_pool_for_merge_tree} 



<SettingsInfoBlock type="Bool" default_value="0" />

使用后台 I/O 池从 MergeTree 表读取。此设置可能会提高 I/O 限制查询的性能。
## allow_changing_replica_until_first_data_packet {#allow_changing_replica_until_first_data_packet} 



<SettingsInfoBlock type="Bool" default_value="0" />

如果启用，在用户请求中，我们可以在接收到第一个数据包之前开始新的连接，即使我们已经取得了一些进展（但进展未更新到 `receive_data_timeout` 超时），否则我们在第一次取得进展后禁用更改副本。
## allow_create_index_without_type {#allow_create_index_without_type} 



<SettingsInfoBlock type="Bool" default_value="0" />

允许没有 TYPE 的 CREATE INDEX 查询。查询将被忽略。用于 SQL 兼容性测试。
## allow_custom_error_code_in_throwif {#allow_custom_error_code_in_throwif} 



<SettingsInfoBlock type="Bool" default_value="0" />

在函数 throwIf() 中启用自定义错误代码。如果为 true，抛出的异常可能会具有意外的错误代码。
## allow_ddl {#allow_ddl} 



<SettingsInfoBlock type="Bool" default_value="1" />

如果设置为 true，则用户被允许执行 DDL 查询。
## allow_deprecated_database_ordinary {#allow_deprecated_database_ordinary} 



<SettingsInfoBlock type="Bool" default_value="0" />

允许创建具有已弃用普通引擎的数据库。
## allow_deprecated_error_prone_window_functions {#allow_deprecated_error_prone_window_functions} 



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.5"},{"label": "0"},{"label": "Allow usage of deprecated error prone window functions (neighbor, runningAccumulate, runningDifferenceStartingWithFirstValue, runningDifference)"}]}]}/>

允许使用已弃用的易出错窗口函数（neighbor, runningAccumulate, runningDifferenceStartingWithFirstValue, runningDifference）。
## allow_deprecated_snowflake_conversion_functions {#allow_deprecated_snowflake_conversion_functions} 



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.6"},{"label": "0"},{"label": "Disabled deprecated functions snowflakeToDateTime[64] and dateTime[64]ToSnowflake."}]}]}/>

函数 `snowflakeToDateTime`、`snowflakeToDateTime64`、`dateTimeToSnowflake` 和 `dateTime64ToSnowflake` 已弃用，默认情况下禁用。请使用函数 `snowflakeIDToDateTime`、`snowflakeIDToDateTime64`、`dateTimeToSnowflakeID` 和 `dateTime64ToSnowflakeID`。

要重新启用已弃用的函数（例如，在过渡期间），请将此设置设置为 `true`。
## allow_deprecated_syntax_for_merge_tree {#allow_deprecated_syntax_for_merge_tree} 



<SettingsInfoBlock type="Bool" default_value="0" />

允许使用已弃用的引擎定义语法创建 *MergeTree 表。
## allow_distributed_ddl {#allow_distributed_ddl} 



<SettingsInfoBlock type="Bool" default_value="1" />

如果设置为 true，则用户被允许执行分布式 DDL 查询。
## allow_drop_detached {#allow_drop_detached} 



<SettingsInfoBlock type="Bool" default_value="0" />

允许 ALTER TABLE ... DROP DETACHED PART[ITION] ... 查询。
## allow_execute_multiif_columnar {#allow_execute_multiif_columnar} 



<SettingsInfoBlock type="Bool" default_value="1" />

允许执行多重 IF 函数的列式操作。
## allow_experimental_analyzer {#allow_experimental_analyzer} 



<SettingsInfoBlock type="Bool" default_value="1" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.3"},{"label": "1"},{"label": "Enable analyzer and planner by default."}]}]}/>

允许使用新的查询分析器。
## allow_experimental_codecs {#allow_experimental_codecs} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />

如果设置为 true，允许指定实验性的压缩编解码器（但我们目前还没有这些，此选项无效）。
## allow_experimental_correlated_subqueries {#allow_experimental_correlated_subqueries} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.4"},{"label": "0"},{"label": "Added new setting to allow correlated subqueries execution."}]}]}/>

允许执行相关子查询。
## allow_experimental_database_glue_catalog {#allow_experimental_database_glue_catalog} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.3"},{"label": "0"},{"label": "Allow experimental database engine DataLakeCatalog with catalog_type = 'glue'"}]}]}/>

允许实验性数据库引擎 DataLakeCatalog，catalog_type = 'glue'。
## allow_experimental_database_hms_catalog {#allow_experimental_database_hms_catalog} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.5"},{"label": "0"},{"label": "Allow experimental database engine DataLakeCatalog with catalog_type = 'hive'"}]}]}/>

允许实验性数据库引擎 DataLakeCatalog，catalog_type = 'hms'。
## allow_experimental_database_iceberg {#allow_experimental_database_iceberg} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.12"},{"label": "0"},{"label": "New setting."}]}]}/>

允许实验性数据库引擎 DataLakeCatalog，catalog_type = 'iceberg'。
## allow_experimental_database_materialized_postgresql {#allow_experimental_database_materialized_postgresql} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />

允许创建数据库，使用引擎=MaterializedPostgreSQL(...)。
## allow_experimental_database_unity_catalog {#allow_experimental_database_unity_catalog} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.3"},{"label": "0"},{"label": "Allow experimental database engine DataLakeCatalog with catalog_type = 'unity'"}]}]}/>

允许实验性数据库引擎 DataLakeCatalog，catalog_type = 'unity'。
## allow_experimental_delta_kernel_rs {#allow_experimental_delta_kernel_rs} 

<BetaBadge/>



<SettingsInfoBlock type="Bool" default_value="1" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.5"},{"label": "1"},{"label": "New setting"}]}]}/>

允许实验性的 delta-kernel-rs 实现。
## allow_experimental_dynamic_type {#allow_experimental_dynamic_type} 



<SettingsInfoBlock type="Bool" default_value="1" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.3"},{"label": "1"},{"label": "Dynamic data type is production-ready"}]}, {"id": "row-2","items": [{"label": "24.5"},{"label": "0"},{"label": "Add new experimental Dynamic type"}]}]}/>

允许创建 [Dynamic](../../sql-reference/data-types/dynamic.md) 数据类型。
## allow_experimental_full_text_index {#allow_experimental_full_text_index} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.6"},{"label": "0"},{"label": "Enable experimental full-text index"}]}]}/>

如果设置为 true，允许使用实验性的全文索引。
## allow_experimental_funnel_functions {#allow_experimental_funnel_functions} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />

启用实验性的漏斗分析函数。
## allow_experimental_hash_functions {#allow_experimental_hash_functions} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />

启用实验性的哈希函数。
## allow_experimental_inverted_index {#allow_experimental_inverted_index} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />

如果设置为 true，允许使用实验性的倒排索引。
## allow_experimental_join_condition {#allow_experimental_join_condition} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.5"},{"label": "0"},{"label": "Support join with inequal conditions which involve columns from both left and right table. e.g. t1.y < t2.y."}]}]}/>

支持涉及左表和右表列的非相等条件的连接，例如 `t1.y < t2.y`。
## allow_experimental_join_right_table_sorting {#allow_experimental_join_right_table_sorting} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.9"},{"label": "0"},{"label": "If it is set to true, and the conditions of `join_to_sort_minimum_perkey_rows` and `join_to_sort_maximum_table_rows` are met, rerange the right table by key to improve the performance in left or inner hash join"}]}]}/>

如果设置为 true，并且满足 `join_to_sort_minimum_perkey_rows` 和 `join_to_sort_maximum_table_rows` 的条件，则通过键重新排列右表，以提高左或内部哈希连接的性能。
## allow_experimental_json_type {#allow_experimental_json_type} 



<SettingsInfoBlock type="Bool" default_value="1" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.3"},{"label": "1"},{"label": "JSON data type is production-ready"}]}, {"id": "row-2","items": [{"label": "24.8"},{"label": "0"},{"label": "Add new experimental JSON type"}]}]}/>

允许创建 [JSON](../../sql-reference/data-types/newjson.md) 数据类型。
## allow_experimental_kafka_offsets_storage_in_keeper {#allow_experimental_kafka_offsets_storage_in_keeper} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.8"},{"label": "0"},{"label": "Allow the usage of experimental Kafka storage engine that stores the committed offsets in ClickHouse Keeper"}]}]}/>

允许将与 Kafka 相关的偏移量存储在 ClickHouse Keeper 中的实验性功能。当启用时，可以将 ClickHouse Keeper 路径和副本名称指定给 Kafka 表引擎。因此，将使用一种新的存储引擎类型，该类型主要在 ClickHouse Keeper 中存储已提交的偏移量。
## allow_experimental_kusto_dialect {#allow_experimental_kusto_dialect} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.1"},{"label": "0"},{"label": "A new setting"}]}]}/>

启用 Kusto 查询语言（KQL） - SQL 的替代方案。
## allow_experimental_lightweight_update {#allow_experimental_lightweight_update} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.5"},{"label": "0"},{"label": "A new setting"}]}]}/>

允许使用轻量级更新。
## allow_experimental_live_view {#allow_experimental_live_view} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />

允许创建已弃用的 LIVE VIEW。

可能的值：

- 0 — 禁用实时视图的工作。
- 1 — 启用实时视图的工作。
## allow_experimental_materialized_postgresql_table {#allow_experimental_materialized_postgresql_table} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />

允许使用 MaterializedPostgreSQL 表引擎。默认情况下禁用，因为此功能是实验性的。
## allow_experimental_nlp_functions {#allow_experimental_nlp_functions} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />

启用实验性的自然语言处理函数。
## allow_experimental_object_type {#allow_experimental_object_type} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />

允许过时的 Object 数据类型。
## allow_experimental_parallel_reading_from_replicas {#allow_experimental_parallel_reading_from_replicas} 

<BetaBadge/>



<SettingsInfoBlock type="UInt64" default_value="0" />

在 SELECT 查询执行中，从每个分片的 `max_parallel_replicas` 个副本中读取。读取并行化并动态协调。0 - 禁用，1 - 启用，失败时静默禁用，2 - 启用，失败时抛出异常。
## allow_experimental_prql_dialect {#allow_experimental_prql_dialect} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.1"},{"label": "0"},{"label": "A new setting"}]}]}/>

启用 PRQL - SQL 的替代方案。
## allow_experimental_query_deduplication {#allow_experimental_query_deduplication} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />

基于分区 UUID 的 SELECT 查询的实验性数据去重。
## allow_experimental_statistics {#allow_experimental_statistics} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.6"},{"label": "0"},{"label": "The setting was renamed. The previous name is `allow_experimental_statistic`."}]}]}/>

允许定义具有 [统计信息](../../engines/table-engines/mergetree-family/mergetree.md/#table_engine-mergetree-creating-a-table) 的列以及 [操纵统计信息](../../engines/table-engines/mergetree-family/mergetree.md/#column-statistics)。
## allow_experimental_time_series_table {#allow_experimental_time_series_table} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.8"},{"label": "0"},{"label": "Added new setting to allow the TimeSeries table engine"}]}]}/>

允许创建具有 [TimeSeries](../../engines/table-engines/integrations/time-series.md) 表引擎的表。可能的值：
- 0 — 禁用 [TimeSeries](../../engines/table-engines/integrations/time-series.md) 表引擎。
- 1 — 启用 [TimeSeries](../../engines/table-engines/integrations/time-series.md) 表引擎。
## allow_experimental_ts_to_grid_aggregate_function {#allow_experimental_ts_to_grid_aggregate_function} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.1"},{"label": "0"},{"label": "Cloud only"}]}]}/>

实验性的 tsToGrid 聚合函数，用于 Prometheus 风格的时间序列重采样。仅限云。
## allow_experimental_variant_type {#allow_experimental_variant_type} 



<SettingsInfoBlock type="Bool" default_value="1" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.3"},{"label": "1"},{"label": "Variant data type is production-ready"}]}, {"id": "row-2","items": [{"label": "24.1"},{"label": "0"},{"label": "Add new experimental Variant type"}]}]}/>

允许创建 [Variant](../../sql-reference/data-types/variant.md) 数据类型。
## allow_experimental_vector_similarity_index {#allow_experimental_vector_similarity_index} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.8"},{"label": "0"},{"label": "Added new setting to allow experimental vector similarity indexes"}]}]}/>

允许实验性的向量相似度索引。
## allow_experimental_window_view {#allow_experimental_window_view} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />

启用窗口视图。尚不成熟。
## allow_general_join_planning {#allow_general_join_planning} 



<SettingsInfoBlock type="Bool" default_value="1" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.1"},{"label": "1"},{"label": "Allow more general join planning algorithm when hash join algorithm is enabled."}]}]}/>

允许使用更通用的连接规划算法，能够处理更复杂的条件，但仅适用于哈希连接。如果未启用哈希连接，则通常使用的连接规划算法将应用，而不受此设置值的影响。
## allow_get_client_http_header {#allow_get_client_http_header} 



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.3"},{"label": "0"},{"label": "Introduced a new function."}]}]}/>

允许使用 `getClientHTTPHeader` 函数，该函数可以获取当前 HTTP 请求头的值。由于出于安全原因，某些头（例如 `Cookie`）可能包含敏感信息，默认情况下不启用此功能。请注意，`X-ClickHouse-*` 和 `Authentication` 头始终受到限制，不能通过此功能获取。
## allow_hyperscan {#allow_hyperscan} 



<SettingsInfoBlock type="Bool" default_value="1" />

允许使用 Hyperscan 库的函数。禁用以避免潜在的长时间编译和过多的资源使用。
## allow_introspection_functions {#allow_introspection_functions} 



<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用用于查询分析的 [自省函数](../../sql-reference/functions/introspection.md)。

可能的值：

- 1 — 启用自省函数。
- 0 — 禁用自省函数。

**参见**

- [采样查询分析器](../../operations/optimizing-performance/sampling-query-profiler.md)
- 系统表 [trace_log](/operations/system-tables/trace_log)
## allow_materialized_view_with_bad_select {#allow_materialized_view_with_bad_select} 



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.4"},{"label": "0"},{"label": "Don't allow creating MVs referencing nonexistent columns or tables"}]}, {"id": "row-2","items": [{"label": "24.9"},{"label": "1"},{"label": "Support (but not enable yet) stricter validation in CREATE MATERIALIZED VIEW"}]}]}/>

允许 CREATE MATERIALIZED VIEW 使用 SELECT 查询，该查询引用不存在的表或列。它仍然必须在语法上有效。对可刷新的物化视图不适用。如果 MV 架构需要从 SELECT 查询推断（即，CREATE 没有列列表和 TO 表时），则不适用。可用于在源表创建 MV 之前。
## allow_named_collection_override_by_default {#allow_named_collection_override_by_default} 



<SettingsInfoBlock type="Bool" default_value="1" />

默认情况下允许命名集合字段覆盖。
## allow_non_metadata_alters {#allow_non_metadata_alters} 



<SettingsInfoBlock type="Bool" default_value="1" />

允许执行影响不仅是表元数据的变更，也影响磁盘上数据的变更的操作。
## allow_nonconst_timezone_arguments {#allow_nonconst_timezone_arguments} 



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "23.4"},{"label": "0"},{"label": "Allow non-const timezone arguments in certain time-related functions like toTimeZone(), fromUnixTimestamp*(), snowflakeToDateTime*()."}]}]}/>

允许在某些与时间相关的函数中使用非常量时区参数，例如 toTimeZone()、fromUnixTimestamp*()、snowflakeToDateTime*()。
## allow_nondeterministic_mutations {#allow_nondeterministic_mutations} 



<SettingsInfoBlock type="Bool" default_value="0" />

用户级设置，允许在复制表上进行变更，使用非确定性函数，例如 `dictGet`。

由于字典可能在节点之间不同步，因此默认情况下不允许从字典中提取值的变更作用于复制表。启用此设置后，允许这种行为，用户有责任确保使用的数据在所有节点之间是同步的。

**示例**

```xml
<profiles>
    <default>
        <allow_nondeterministic_mutations>1</allow_nondeterministic_mutations>

        <!-- ... -->
    </default>

    <!-- ... -->

</profiles>
```
## allow_nondeterministic_optimize_skip_unused_shards {#allow_nondeterministic_optimize_skip_unused_shards} 



<SettingsInfoBlock type="Bool" default_value="0" />

允许在分片键中使用非确定性函数（如 `rand` 或 `dictGet`，后者在更新时有一些注意事项）。

可能的值：

- 0 — 不允许。
- 1 — 允许。
## allow_not_comparable_types_in_comparison_functions {#allow_not_comparable_types_in_comparison_functions} 



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.1"},{"label": "0"},{"label": "Don't allow not comparable types in comparison functions by default"}]}]}/>

允许或限制在比较函数 `equal/less/greater/etc` 中使用不可比较类型（如 JSON/Object/AggregateFunction）。
## allow_not_comparable_types_in_order_by {#allow_not_comparable_types_in_order_by} 



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.1"},{"label": "0"},{"label": "Don't allow not comparable types in order by by default"}]}]}/>

允许或限制在 ORDER BY 键中使用不可比较类型（如 JSON/Object/AggregateFunction）。
## allow_prefetched_read_pool_for_local_filesystem {#allow_prefetched_read_pool_for_local_filesystem} 



<SettingsInfoBlock type="Bool" default_value="0" />

如果所有部分都位于本地文件系统中，则优先使用预提取线程池。
## allow_prefetched_read_pool_for_remote_filesystem {#allow_prefetched_read_pool_for_remote_filesystem} 



<SettingsInfoBlock type="Bool" default_value="1" />

如果所有部分都位于远程文件系统中，则优先使用预提取线程池。
## allow_push_predicate_ast_for_distributed_subqueries {#allow_push_predicate_ast_for_distributed_subqueries} 



<SettingsInfoBlock type="Bool" default_value="1" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.1"},{"label": "1"},{"label": "A new setting"}]}]}/>

允许在启用了分析师的分布式子查询中在 AST 级别推送谓词。
## allow_push_predicate_when_subquery_contains_with {#allow_push_predicate_when_subquery_contains_with} 



<SettingsInfoBlock type="Bool" default_value="1" />

允许在子查询包含 WITH 子句时推送谓词。
## allow_reorder_prewhere_conditions {#allow_reorder_prewhere_conditions} 



<SettingsInfoBlock type="Bool" default_value="1" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.10"},{"label": "1"},{"label": "New setting"}]}]}/>

在将条件从 WHERE 移动到 PREWHERE 时，允许重新排序以优化过滤。
## allow_settings_after_format_in_insert {#allow_settings_after_format_in_insert} 



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "22.4"},{"label": "0"},{"label": "Do not allow SETTINGS after FORMAT for INSERT queries because ClickHouse interpret SETTINGS as some values, which is misleading"}]}]}/>

控制在 INSERT 查询中 `FORMAT` 后是否允许出现 `SETTINGS`。不建议使用此选项，因为这可能会将 `SETTINGS` 的一部分解释为值。

示例：

```sql
INSERT INTO FUNCTION null('foo String') SETTINGS max_threads=1 VALUES ('bar');
```

但是，以下查询仅在 `allow_settings_after_format_in_insert` 为真时有效：

```sql
SET allow_settings_after_format_in_insert=1;
INSERT INTO FUNCTION null('foo String') VALUES ('bar') SETTINGS max_threads=1;
```

可能的值：

- 0 — 不允许。
- 1 — 允许。

:::note
仅在你的用例依赖于旧语法时使用此设置以保持向后兼容。
:::
## allow_simdjson {#allow_simdjson} 



<SettingsInfoBlock type="Bool" default_value="1" />

在 'JSON*' 函数中，如果可用 AVX2 指令，则允许使用 simdjson 库。如果禁用，将使用 rapidjson。
## allow_statistics_optimize {#allow_statistics_optimize} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.6"},{"label": "0"},{"label": "The setting was renamed. The previous name is `allow_statistic_optimize`."}]}]}/>

允许使用统计信息来优化查询。
## allow_suspicious_codecs {#allow_suspicious_codecs} 



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "20.5"},{"label": "0"},{"label": "Don't allow to specify meaningless compression codecs"}]}]}/>

如果设置为 true，则允许指定无意义的压缩编解码器。
## allow_suspicious_fixed_string_types {#allow_suspicious_fixed_string_types} 



<SettingsInfoBlock type="Bool" default_value="0" />

在 CREATE TABLE 语句中，允许创建长度 n > 256 的 FixedString(n) 类型的列。长度 >= 256 的 FixedString 是可疑的，并且可能表示误用。
## allow_suspicious_indices {#allow_suspicious_indices} 



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "23.4"},{"label": "0"},{"label": "If true, index can defined with identical expressions"}]}]}/>

拒绝主/副索引和具有相同表达式的排序键。
## allow_suspicious_low_cardinality_types {#allow_suspicious_low_cardinality_types} 



<SettingsInfoBlock type="Bool" default_value="0" />

允许或限制使用数据类型固定大小为 8 字节或更小的 [LowCardinality](../../sql-reference/data-types/lowcardinality.md)：数值数据类型和 `FixedString(8_bytes_or_less)`。

对于较小的固定值，使用 `LowCardinality` 通常效率低下，因为 ClickHouse 为每行存储数值索引。因此：

- 硬盘空间使用可能增加。
- 内存消耗可能更高，具体取决于字典大小。
- 由于额外的编码/解码操作，有些函数可能会缓慢。

由于上述所有原因，[MergeTree](../../engines/table-engines/mergetree-family/mergetree.md) 引擎表的合并时间可能增长。

可能的值：

- 1 — 不限制使用 `LowCardinality`。
- 0 — 限制使用 `LowCardinality`。
## allow_suspicious_primary_key {#allow_suspicious_primary_key} 



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.3"},{"label": "0"},{"label": "Forbid suspicious PRIMARY KEY/ORDER BY for MergeTree (i.e. SimpleAggregateFunction)"}]}]}/>

允许可疑的 `PRIMARY KEY`/`ORDER BY` 用于 MergeTree（即 SimpleAggregateFunction）。
## allow_suspicious_ttl_expressions {#allow_suspicious_ttl_expressions} 



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "23.12"},{"label": "0"},{"label": "It is a new setting, and in previous versions the behavior was equivalent to allowing."}]}]}/>

拒绝不依赖于任何表列的 TTL 表达式。这通常表示用户错误。
## allow_suspicious_types_in_group_by {#allow_suspicious_types_in_group_by} 



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.11"},{"label": "0"},{"label": "Don't allow Variant/Dynamic types in GROUP BY by default"}]}]}/>

允许或限制在 GROUP BY 键中使用 [Variant](../../sql-reference/data-types/variant.md) 和 [Dynamic](../../sql-reference/data-types/dynamic.md) 类型。
## allow_suspicious_types_in_order_by {#allow_suspicious_types_in_order_by} 



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.11"},{"label": "0"},{"label": "Don't allow Variant/Dynamic types in ORDER BY by default"}]}]}/>

允许或限制在 ORDER BY 键中使用 [Variant](../../sql-reference/data-types/variant.md) 和 [Dynamic](../../sql-reference/data-types/dynamic.md) 类型。
## allow_suspicious_variant_types {#allow_suspicious_variant_types} 



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.2"},{"label": "0"},{"label": "Don't allow creating Variant type with suspicious variants by default"}]}]}/>

在 CREATE TABLE 语句中，允许指定具有相似变体类型的 Variant 类型（例如，具有不同的数字或日期类型）。启用此设置可能在处理具有相似类型的值时引入一些歧义。
## allow_unrestricted_reads_from_keeper {#allow_unrestricted_reads_from_keeper} 



<SettingsInfoBlock type="Bool" default_value="0" />

允许不受限制地从 system.zookeeper 表读取（不带路径条件），这可能很方便，但对 zookeeper 来说不安全。
## alter_move_to_space_execute_async {#alter_move_to_space_execute_async} 



<SettingsInfoBlock type="Bool" default_value="0" />

异步执行 ALTER TABLE MOVE ... TO [DISK|VOLUME]。
## alter_partition_verbose_result {#alter_partition_verbose_result} 



<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用显示关于已成功应用于分区和部分操作的信息。
适用于 [ATTACH PARTITION|PART](/sql-reference/statements/alter/partition#attach-partitionpart) 和 [FREEZE PARTITION](/sql-reference/statements/alter/partition#freeze-partition)。

可能的值：

- 0 — 禁用详细信息。
- 1 — 启用详细信息。

**示例**

```sql
CREATE TABLE test(a Int64, d Date, s String) ENGINE = MergeTree PARTITION BY toYYYYMDECLARE(d) ORDER BY a;
INSERT INTO test VALUES(1, '2021-01-01', '');
INSERT INTO test VALUES(1, '2021-01-01', '');
ALTER TABLE test DETACH PARTITION ID '202101';

ALTER TABLE test ATTACH PARTITION ID '202101' SETTINGS alter_partition_verbose_result = 1;

┌─command_type─────┬─partition_id─┬─part_name────┬─old_part_name─┐
│ ATTACH PARTITION │ 202101       │ 202101_7_7_0 │ 202101_5_5_0  │
│ ATTACH PARTITION │ 202101       │ 202101_8_8_0 │ 202101_6_6_0  │
└──────────────────┴──────────────┴──────────────┴───────────────┘

ALTER TABLE test FREEZE SETTINGS alter_partition_verbose_result = 1;

┌─command_type─┬─partition_id─┬─part_name────┬─backup_name─┬─backup_path───────────────────┬─part_backup_path────────────────────────────────────────────┐
│ FREEZE ALL   │ 202101       │ 202101_7_7_0 │ 8           │ /var/lib/clickhouse/shadow/8/ │ /var/lib/clickhouse/shadow/8/data/default/test/202101_7_7_0 │
│ FREEZE ALL   │ 202101       │ 202101_8_8_0 │ 8           │ /var/lib/clickhouse/shadow/8/ │ /var/lib/clickhouse/shadow/8/data/default/test/202101_8_8_0 │
└──────────────┴──────────────┴──────────────┴─────────────┴───────────────────────────────┴─────────────────────────────────────────────────────────────┘
```
## alter_sync {#alter_sync} 



<SettingsInfoBlock type="UInt64" default_value="1" />

允许设置在执行 [ALTER](../../sql-reference/statements/alter/index.md)、[OPTIMIZE](../../sql-reference/statements/optimize.md) 或 [TRUNCATE](../../sql-reference/statements/truncate.md) 查询时等待操作在副本上执行。

可能的值：

- 0 — 不等待。
- 1 — 等待自己的执行。
- 2 — 等待所有人。

云默认值：`0`。

:::note
`alter_sync` 仅适用于 `Replicated` 表，对非 `Replicated` 表的 ALTER 操作无效。
:::
## alter_update_mode {#alter_update_mode} 



<SettingsInfoBlock type="AlterUpdateMode" default_value="heavy" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.5"},{"label": "heavy"},{"label": "A new setting"}]}]}/>

具有 `UPDATE` 命令的 `ALTER` 查询的模式。

可能的值：
- `heavy` - 运行常规变更。
- `lightweight` - 如果可能，运行轻量级更新，否则运行常规变更。
- `lightweight_force` - 如果可能，运行轻量级更新，否则抛出错误。
## analyze_index_with_space_filling_curves {#analyze_index_with_space_filling_curves} 



<SettingsInfoBlock type="Bool" default_value="1" />

如果表的索引中具有空间填充曲线，例如 `ORDER BY mortonEncode(x, y)` 或 `ORDER BY hilbertEncode(x, y)`，并且查询对其参数具有条件，例如 `x >= 10 AND x <= 20 AND y >= 20 AND y <= 30`，请使用空间填充曲线进行索引分析。
## analyzer_compatibility_join_using_top_level_identifier {#analyzer_compatibility_join_using_top_level_identifier} 



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.3"},{"label": "0"},{"label": "Force to resolve identifier in JOIN USING from projection"}]}]}/>

强制在 JOIN USING 中从投影中解析标识符（例如，在 `SELECT a + 1 AS b FROM t1 JOIN t2 USING (b)` 中，将通过 `t1.a + 1 = t2.b` 执行连接，而不是 `t1.b = t2.b`）。
## any_join_distinct_right_table_keys {#any_join_distinct_right_table_keys} 



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "19.14"},{"label": "0"},{"label": "Disable ANY RIGHT and ANY FULL JOINs by default to avoid inconsistency"}]}]}/>

启用在 `ANY INNER|LEFT JOIN` 操作中遗留的 ClickHouse 服务器行为。

:::note
仅在你的用例依赖于遗留 `JOIN` 行为时使用此设置以保持向后兼容。
:::

启用遗留行为时：

- `t1 ANY LEFT JOIN t2` 和 `t2 ANY RIGHT JOIN t1` 操作的结果不相等，因为 ClickHouse 使用了从左到右的多对一表键映射逻辑。
- `ANY INNER JOIN` 操作的结果包含来自左表的所有行，就像 `SEMI LEFT JOIN` 操作一样。

禁用遗留行为时：

- `t1 ANY LEFT JOIN t2` 和 `t2 ANY RIGHT JOIN t1` 操作的结果相等，因为 ClickHouse 使用了在 `ANY RIGHT JOIN` 操作中提供一对多键映射的逻辑。
- `ANY INNER JOIN` 操作的结果包含来自左右表的每个键的一行。

可能的值：

- 0 — 禁用遗留行为。
- 1 — 启用遗留行为。

另请参阅：

- [JOIN 严格性](/sql-reference/statements/select/join#settings)
## apply_deleted_mask {#apply_deleted_mask} 



<SettingsInfoBlock type="Bool" default_value="1" />

启用过滤掉使用轻量级 DELETE 删除的行。如果禁用，查询将能够读取这些行。这在调试和“恢复删除”场景中很有用。
## apply_mutations_on_fly {#apply_mutations_on_fly} 



<SettingsInfoBlock type="Bool" default_value="0" />

如果为真，则在 SELECT 时应用未在数据部分物化的变更（UPDATE 和 DELETE）。
## apply_patch_parts {#apply_patch_parts} 



<SettingsInfoBlock type="Bool" default_value="1" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.5"},{"label": "1"},{"label": "A new setting"}]}]}/>

如果为真，则在 SELECT 时应用补丁部分（表示轻量级更新）。
## apply_settings_from_server {#apply_settings_from_server} 



<SettingsInfoBlock type="Bool" default_value="1" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.2"},{"label": "1"},{"label": "Client-side code (e.g. INSERT input parsing and query output formatting) will use the same settings as the server, including settings from server config."}]}]}/>

客户端是否应该接受来自服务器的设置。

这仅影响在客户端侧执行的操作，特别是解析 INSERT 输入数据和格式化查询结果。大多数查询执行发生在服务器上，不受该设置的影响。

通常，此设置应在用户配置文件中设置（users.xml 或 `ALTER USER` 之类的查询），而不是通过客户端（客户端命令行参数、`SET` 查询或 `SELECT` 查询的 `SETTINGS` 部分）。通过客户端可以将其更改为 false，但不能更改为 true（因为如果用户配置文件设置了 `apply_settings_from_server = false`，服务器将不会发送设置）。

请注意，最初（24.12）有一个服务器设置（`send_settings_to_client`），但后来它被此客户端设置替换，以提高可用性。
## asterisk_include_alias_columns {#asterisk_include_alias_columns} 



<SettingsInfoBlock type="Bool" default_value="0" />

包含用于通配符查询（`SELECT *`）的 [ALIAS](../../sql-reference/statements/create/table.md/#alias) 列。

可能的值：

- 0 - 禁用
- 1 - 启用
## asterisk_include_materialized_columns {#asterisk_include_materialized_columns} 



<SettingsInfoBlock type="Bool" default_value="0" />

包含用于通配符查询（`SELECT *`）的 [MATERIALIZED](/sql-reference/statements/create/view#materialized-view) 列。

可能的值：

- 0 - 禁用
- 1 - 启用
## async_insert {#async_insert} 



<SettingsInfoBlock type="Bool" default_value="0" />

如果为真，INSERT 查询中的数据存储在队列中，稍后在后台刷新到表中。如果 wait_for_async_insert 为假，INSERT 查询将几乎瞬间处理，否则客户端将等待直到数据刷新到表中。
## async_insert_busy_timeout_decrease_rate {#async_insert_busy_timeout_decrease_rate} 



<SettingsInfoBlock type="Double" default_value="0.2" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.2"},{"label": "0.2"},{"label": "The exponential growth rate at which the adaptive asynchronous insert timeout decreases"}]}]}/>

自适应异步插入超时递减的指数增长率。
## async_insert_busy_timeout_increase_rate {#async_insert_busy_timeout_increase_rate} 



<SettingsInfoBlock type="Double" default_value="0.2" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.2"},{"label": "0.2"},{"label": "The exponential growth rate at which the adaptive asynchronous insert timeout increases"}]}]}/>

自适应异步插入超时递增的指数增长率。
## async_insert_busy_timeout_max_ms {#async_insert_busy_timeout_max_ms} 



<SettingsInfoBlock type="Milliseconds" default_value="200" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.2"},{"label": "200"},{"label": "The minimum value of the asynchronous insert timeout in milliseconds; async_insert_busy_timeout_ms is aliased to async_insert_busy_timeout_max_ms"}]}]}/>

在第一条数据出现后，按查询收集的数据转储之前能等待的最长时间。
## async_insert_busy_timeout_min_ms {#async_insert_busy_timeout_min_ms} 



<SettingsInfoBlock type="Milliseconds" default_value="50" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.2"},{"label": "50"},{"label": "The minimum value of the asynchronous insert timeout in milliseconds; it also serves as the initial value, which may be increased later by the adaptive algorithm"}]}]}/>

如果通过 async_insert_use_adaptive_busy_timeout 启用自动调节，则按查询收集的数据转储之前的最小等待时间。从而，它也是自适应算法的初始值。
## async_insert_deduplicate {#async_insert_deduplicate} 



<SettingsInfoBlock type="Bool" default_value="0" />

对于复制表中的异步 INSERT 查询，指定应执行插入块的去重。
## async_insert_max_data_size {#async_insert_max_data_size} 



<SettingsInfoBlock type="UInt64" default_value="10485760" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.2"},{"label": "10485760"},{"label": "The previous value appeared to be too small."}]}]}/>

在插入之前收集的每个查询的未解析数据的最大大小（以字节为单位）。
## async_insert_max_query_number {#async_insert_max_query_number} 



<SettingsInfoBlock type="UInt64" default_value="450" />

在插入之前的最大插入查询数。
## async_insert_poll_timeout_ms {#async_insert_poll_timeout_ms} 



<SettingsInfoBlock type="Milliseconds" default_value="10" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.2"},{"label": "10"},{"label": "Timeout in milliseconds for polling data from asynchronous insert queue"}]}]}/>

从异步插入队列轮询数据的超时。
## async_insert_use_adaptive_busy_timeout {#async_insert_use_adaptive_busy_timeout} 



<SettingsInfoBlock type="Bool" default_value="1" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.2"},{"label": "1"},{"label": "Use adaptive asynchronous insert timeout"}]}]}/>

如果设置为 true，则在异步插入中使用自适应忙碌超时。
## async_query_sending_for_remote {#async_query_sending_for_remote} 



<SettingsInfoBlock type="Bool" default_value="1" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "23.3"},{"label": "1"},{"label": "Create connections and send query async across shards"}]}]}/>

在执行远程查询时启用异步连接创建和查询发送。

默认启用。
## async_socket_for_remote {#async_socket_for_remote} 



<SettingsInfoBlock type="Bool" default_value="1" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "21.5"},{"label": "1"},{"label": "Fix all problems and turn on asynchronous reads from socket for remote queries by default again"}]}, {"id": "row-2","items": [{"label": "21.3"},{"label": "0"},{"label": "Turn off asynchronous reads from socket for remote queries because of some problems"}]}]}/>

在执行远程查询时启用从套接字异步读取。

默认启用。

## azure_allow_parallel_part_upload {#azure_allow_parallel_part_upload} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.4"},{"label": "true"},{"label": "Use multiple threads for azure multipart upload."}]}]}/>

使用多个线程进行 Azure 多部分上传。
## azure_check_objects_after_upload {#azure_check_objects_after_upload} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.11"},{"label": "0"},{"label": "Check each uploaded object in azure blob storage to be sure that upload was successful"}]}, {"id": "row-2","items": [{"label": "24.10"},{"label": "0"},{"label": "Check each uploaded object in azure blob storage to be sure that upload was successful"}]}]}/>

在 Azure Blob 存储中检查每个上传的对象，以确保上传成功。
## azure_create_new_file_on_insert {#azure_create_new_file_on_insert} 

<SettingsInfoBlock type="Bool" default_value="0" />

在 Azure 引擎表中启用或禁用在每次插入时创建新文件。
## azure_ignore_file_doesnt_exist {#azure_ignore_file_doesnt_exist} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.6"},{"label": "0"},{"label": "Allow to return 0 rows when the requested files don't exist instead of throwing an exception in AzureBlobStorage table engine"}]}]}/>

在读取某些键时，如果文件不存在，则忽略文件缺失。

可能的值：
- 1 — `SELECT` 返回空结果。
- 0 — `SELECT` 抛出异常。
## azure_list_object_keys_size {#azure_list_object_keys_size} 

<SettingsInfoBlock type="UInt64" default_value="1000" />

ListObject 请求批次中可以返回的最大文件数。
## azure_max_blocks_in_multipart_upload {#azure_max_blocks_in_multipart_upload} 

<SettingsInfoBlock type="UInt64" default_value="50000" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.5"},{"label": "50000"},{"label": "Maximum number of blocks in multipart upload for Azure."}]}]}/>

Azure 中多部分上传的最大块数。
## azure_max_inflight_parts_for_one_file {#azure_max_inflight_parts_for_one_file} 

<SettingsInfoBlock type="UInt64" default_value="20" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.3"},{"label": "20"},{"label": "The maximum number of a concurrent loaded parts in multipart upload request. 0 means unlimited."}]}]}/>

单个文件的多部分上传请求中同时加载的最大部分数。0 表示无限制。
## azure_max_single_part_copy_size {#azure_max_single_part_copy_size} 

<SettingsInfoBlock type="UInt64" default_value="268435456" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.2"},{"label": "268435456"},{"label": "The maximum size of object to copy using single part copy to Azure blob storage."}]}]}/>

使用单部分复制到 Azure Blob 存储的对象的最大大小。
## azure_max_single_part_upload_size {#azure_max_single_part_upload_size} 

<SettingsInfoBlock type="UInt64" default_value="104857600" />

使用单部分上传到 Azure Blob 存储的对象的最大大小。
## azure_max_single_read_retries {#azure_max_single_read_retries} 

<SettingsInfoBlock type="UInt64" default_value="4" />

单次 Azure Blob 存储读取的最大重试次数。
## azure_max_unexpected_write_error_retries {#azure_max_unexpected_write_error_retries} 

<SettingsInfoBlock type="UInt64" default_value="4" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.1"},{"label": "4"},{"label": "The maximum number of retries in case of unexpected errors during Azure blob storage write"}]}]}/>

在 Azure Blob 存储写入时遇到意外错误的最大重试次数。
## azure_max_upload_part_size {#azure_max_upload_part_size} 

<SettingsInfoBlock type="UInt64" default_value="5368709120" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.3"},{"label": "5368709120"},{"label": "The maximum size of part to upload during multipart upload to Azure blob storage."}]}]}/>

在 Azure Blob 存储进行多部分上传时，上传部件的最大大小。
## azure_min_upload_part_size {#azure_min_upload_part_size} 

<SettingsInfoBlock type="UInt64" default_value="16777216" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.3"},{"label": "16777216"},{"label": "The minimum size of part to upload during multipart upload to Azure blob storage."}]}]}/>

在 Azure Blob 存储进行多部分上传时，上传部件的最小大小。
## azure_sdk_max_retries {#azure_sdk_max_retries} 

<SettingsInfoBlock type="UInt64" default_value="10" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.7"},{"label": "10"},{"label": "Maximum number of retries in azure sdk"}]}]}/>

Azure SDK 中最大重试次数。
## azure_sdk_retry_initial_backoff_ms {#azure_sdk_retry_initial_backoff_ms} 

<SettingsInfoBlock type="UInt64" default_value="10" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.7"},{"label": "10"},{"label": "Minimal backoff between retries in azure sdk"}]}]}/>

Azure SDK 中重试之间的最小退避时间（毫秒）.
## azure_sdk_retry_max_backoff_ms {#azure_sdk_retry_max_backoff_ms} 

<SettingsInfoBlock type="UInt64" default_value="1000" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.7"},{"label": "1000"},{"label": "Maximal backoff between retries in azure sdk"}]}]}/>

Azure SDK 中重试之间的最大退避时间（毫秒）.
## azure_skip_empty_files {#azure_skip_empty_files} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.6"},{"label": "0"},{"label": "Allow to skip empty files in azure table engine"}]}]}/>

在 S3 引擎中启用或禁用跳过空文件。

可能的值：
- 0 — 如果空文件与请求的格式不兼容，`SELECT` 抛出异常。
- 1 — `SELECT` 对于空文件返回空结果。
## azure_strict_upload_part_size {#azure_strict_upload_part_size} 

<SettingsInfoBlock type="UInt64" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.3"},{"label": "0"},{"label": "The exact size of part to upload during multipart upload to Azure blob storage."}]}]}/>

在 Azure Blob 存储进行多部分上传时，上传部分的确切大小。
## azure_throw_on_zero_files_match {#azure_throw_on_zero_files_match} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.6"},{"label": "0"},{"label": "Allow to throw an error when ListObjects request cannot match any files in AzureBlobStorage engine instead of empty query result"}]}]}/>

根据 glob 扩展规则，如果匹配零个文件则抛出错误。

可能的值：
- 1 — `SELECT` 抛出异常。
- 0 — `SELECT` 返回空结果。
## azure_truncate_on_insert {#azure_truncate_on_insert} 

<SettingsInfoBlock type="Bool" default_value="0" />

在 Azure 引擎表中启用或禁用插入之前的截断。
## azure_upload_part_size_multiply_factor {#azure_upload_part_size_multiply_factor} 

<SettingsInfoBlock type="UInt64" default_value="2" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.3"},{"label": "2"},{"label": "Multiply azure_min_upload_part_size by this factor each time azure_multiply_parts_count_threshold parts were uploaded from a single write to Azure blob storage."}]}]}/>

在从单个写入上传到 Azure Blob 存储的 azure_multiply_parts_count_threshold 部件之后，每次将 azure_min_upload_part_size 乘以此因子。
## azure_upload_part_size_multiply_parts_count_threshold {#azure_upload_part_size_multiply_parts_count_threshold} 

<SettingsInfoBlock type="UInt64" default_value="500" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.3"},{"label": "500"},{"label": "Each time this number of parts was uploaded to Azure blob storage, azure_min_upload_part_size is multiplied by azure_upload_part_size_multiply_factor."}]}]}/>

每次上传到 Azure Blob 存储的部件数达到此数字时，将 azure_min_upload_part_size 乘以 azure_upload_part_size_multiply_factor。
## backup_restore_batch_size_for_keeper_multi {#backup_restore_batch_size_for_keeper_multi} 

<SettingsInfoBlock type="UInt64" default_value="1000" />

在备份或恢复期间对 [Zoo]Keeper 进行多请求的最大批大小。
## backup_restore_batch_size_for_keeper_multiread {#backup_restore_batch_size_for_keeper_multiread} 

<SettingsInfoBlock type="UInt64" default_value="10000" />

在备份或恢复期间对 [Zoo]Keeper 进行多读取请求的最大批大小。
## backup_restore_failure_after_host_disconnected_for_seconds {#backup_restore_failure_after_host_disconnected_for_seconds} 

<SettingsInfoBlock type="UInt64" default_value="3600" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.11"},{"label": "3600"},{"label": "New setting."}]}, {"id": "row-2","items": [{"label": "24.10"},{"label": "3600"},{"label": "New setting."}]}]}/>

如果在执行 BACKUP ON CLUSTER 或 RESTORE ON CLUSTER 操作期间，主机在此时间内未能在 ZooKeeper 中重新创建其临时 'alive' 节点，则整个备份或恢复被视为失败。
此值应该大于合理范围内的主机在故障后重新连接到 ZooKeeper 的时间。
零意味着无限制。
## backup_restore_finish_timeout_after_error_sec {#backup_restore_finish_timeout_after_error_sec} 

<SettingsInfoBlock type="UInt64" default_value="180" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.11"},{"label": "180"},{"label": "New setting."}]}, {"id": "row-2","items": [{"label": "24.10"},{"label": "180"},{"label": "New setting."}]}]}/>

发起者应等待多长时间，等待其他主机对 'error' 节点的反应，并停止对当前 BACKUP ON CLUSTER 或 RESTORE ON CLUSTER 操作的工作。
## backup_restore_keeper_fault_injection_probability {#backup_restore_keeper_fault_injection_probability} 

<SettingsInfoBlock type="Float" default_value="0" />

备份或恢复期间，keeper 请求出现故障的近似概率。有效值在区间 [0.0f, 1.0f] 内。
## backup_restore_keeper_fault_injection_seed {#backup_restore_keeper_fault_injection_seed} 

<SettingsInfoBlock type="UInt64" default_value="0" />

0 - 随机种子，否则为设置的值。
## backup_restore_keeper_max_retries {#backup_restore_keeper_max_retries} 

<SettingsInfoBlock type="UInt64" default_value="1000" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.11"},{"label": "1000"},{"label": "Should be big enough so the whole operation BACKUP or RESTORE operation won't fail because of a temporary [Zoo]Keeper failure in the middle of it."}]}, {"id": "row-2","items": [{"label": "24.10"},{"label": "1000"},{"label": "Should be big enough so the whole operation BACKUP or RESTORE operation won't fail because of a temporary [Zoo]Keeper failure in the middle of it."}]}]}/>

在 BACKUP 或 RESTORE 操作中，`[Zoo]Keeper` 操作的最大重试次数。
应该足够大，以至于整个操作不会因为暂时的 `[Zoo]Keeper` 故障而失败。
## backup_restore_keeper_max_retries_while_handling_error {#backup_restore_keeper_max_retries_while_handling_error} 

<SettingsInfoBlock type="UInt64" default_value="20" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.11"},{"label": "20"},{"label": "New setting."}]}, {"id": "row-2","items": [{"label": "24.10"},{"label": "20"},{"label": "New setting."}]}]}/>

在处理 BACKUP ON CLUSTER 或 RESTORE ON CLUSTER 操作的错误时，`[Zoo]Keeper` 操作的最大重试次数。
## backup_restore_keeper_max_retries_while_initializing {#backup_restore_keeper_max_retries_while_initializing} 

<SettingsInfoBlock type="UInt64" default_value="20" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.11"},{"label": "20"},{"label": "New setting."}]}, {"id": "row-2","items": [{"label": "24.10"},{"label": "20"},{"label": "New setting."}]}]}/>

在初始化 BACKUP ON CLUSTER 或 RESTORE ON CLUSTER 操作期间，`[Zoo]Keeper` 操作的最大重试次数。
## backup_restore_keeper_retry_initial_backoff_ms {#backup_restore_keeper_retry_initial_backoff_ms} 

<SettingsInfoBlock type="UInt64" default_value="100" />

备份或恢复期间，`[Zoo]Keeper` 操作的初始退避超时。
## backup_restore_keeper_retry_max_backoff_ms {#backup_restore_keeper_retry_max_backoff_ms} 

<SettingsInfoBlock type="UInt64" default_value="5000" />

备份或恢复期间，`[Zoo]Keeper` 操作的最大退避超时。
## backup_restore_keeper_value_max_size {#backup_restore_keeper_value_max_size} 

<SettingsInfoBlock type="UInt64" default_value="1048576" />

备份期间，`[Zoo]Keeper` 节点的数据最大大小。
## backup_restore_s3_retry_attempts {#backup_restore_s3_retry_attempts} 

<SettingsInfoBlock type="UInt64" default_value="1000" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.7"},{"label": "1000"},{"label": "Setting for Aws::Client::RetryStrategy, Aws::Client does retries itself, 0 means no retries. It takes place only for backup/restore."}]}]}/>

对于 Aws::Client::RetryStrategy 的设置，Aws::Client 会自行进行重试，0 表示没有重试。仅适用于备份/恢复。
## cache_warmer_threads {#cache_warmer_threads} 

<CloudAvailableBadge/>

<SettingsInfoBlock type="UInt64" default_value="4" />

仅在 ClickHouse Cloud 中有效。用于后台线程的数量，通过推测下载新数据部分到文件缓存，当 [cache_populated_by_fetch](merge-tree-settings.md/#cache_populated_by_fetch) 启用时。零表示禁用。
## calculate_text_stack_trace {#calculate_text_stack_trace} 

<SettingsInfoBlock type="Bool" default_value="1" />

在查询执行期间发生异常时计算文本堆栈跟踪。这是默认设置。它需要符号查找，并可能在执行大量错误查询时减慢模糊测试。在正常情况下，您不应该禁用此选项。
## cancel_http_readonly_queries_on_client_close {#cancel_http_readonly_queries_on_client_close} 

<SettingsInfoBlock type="Bool" default_value="0" />

当客户端关闭连接而不等待响应时，取消 HTTP 只读查询（例如 SELECT）。

云默认值：`1`。
## cast_ipv4_ipv6_default_on_conversion_error {#cast_ipv4_ipv6_default_on_conversion_error} 

<SettingsInfoBlock type="Bool" default_value="0" />

CAST 操作符转换为 IPv4，CAST 操作符转换为 IPV6 类型，toIPv4，toIPv6 函数将在转换错误时返回默认值，而不是抛出异常。
## cast_keep_nullable {#cast_keep_nullable} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用在 [CAST](/sql-reference/functions/type-conversion-functions#cast) 操作中保持 `Nullable` 数据类型。

当设置启用且 `CAST` 函数的参数为 `Nullable` 时，结果也将转换为 `Nullable` 类型。当设置禁用时，结果始终具有确切的目标类型。

可能的值：

- 0 — `CAST` 结果具有确切指定的目标类型。
- 1 — 如果参数类型为 `Nullable`，则 `CAST` 结果转换为 `Nullable(DestinationDataType)`。

**示例**

以下查询的结果具有确切的目标数据类型：

```sql
SET cast_keep_nullable = 0;
SELECT CAST(toNullable(toInt32(0)) AS Int32) as x, toTypeName(x);
```

结果：

```text
┌─x─┬─toTypeName(CAST(toNullable(toInt32(0)), 'Int32'))─┐
│ 0 │ Int32                                             │
└───┴───────────────────────────────────────────────────┘
```

以下查询的结果在目标数据类型上具有 `Nullable` 修改：

```sql
SET cast_keep_nullable = 1;
SELECT CAST(toNullable(toInt32(0)) AS Int32) as x, toTypeName(x);
```

结果：

```text
┌─x─┬─toTypeName(CAST(toNullable(toInt32(0)), 'Int32'))─┐
│ 0 │ Nullable(Int32)                                   │
└───┴───────────────────────────────────────────────────┘
```

**另请参见**

- [CAST](/sql-reference/functions/type-conversion-functions#cast) 函数
## cast_string_to_dynamic_use_inference {#cast_string_to_dynamic_use_inference} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.5"},{"label": "0"},{"label": "Add setting to allow converting String to Dynamic through parsing"}]}]}/>

在字符串转换为动态类型时使用类型推断。
## cast_string_to_variant_use_inference {#cast_string_to_variant_use_inference} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.4"},{"label": "1"},{"label": "New setting to enable/disable types inference during CAST from String to Variant"}]}]}/>

在字符串转换为变体时使用类型推断。
## check_query_single_value_result {#check_query_single_value_result} 

<SettingsInfoBlock type="Bool" default_value="1" />

定义 `MergeTree` 家族引擎的 [CHECK TABLE](/sql-reference/statements/check-table) 查询结果的详细级别。

可能的值：

- 0 — 查询显示每个数据部分的检查状态。
- 1 — 查询显示表的总体检查状态。
## check_referential_table_dependencies {#check_referential_table_dependencies} 

<SettingsInfoBlock type="Bool" default_value="0" />

检查 DDL 查询（如 DROP TABLE 或 RENAME）是否会破坏关联依赖关系。
## check_table_dependencies {#check_table_dependencies} 

<SettingsInfoBlock type="Bool" default_value="1" />

检查 DDL 查询（如 DROP TABLE 或 RENAME）是否会破坏依赖关系。
## checksum_on_read {#checksum_on_read} 

<SettingsInfoBlock type="Bool" default_value="1" />

读取时验证校验和。默认情况下启用，并且在生产中应始终启用。请不要期待禁用此设置会带来任何好处。只能用于实验和基准测试。该设置仅适用于 MergeTree 家族的表。对于其他表引擎和通过网络接收数据时，始终验证校验和。
## cloud_mode {#cloud_mode} 

<SettingsInfoBlock type="Bool" default_value="0" />

云模式
## cloud_mode_database_engine {#cloud_mode_database_engine} 

<SettingsInfoBlock type="UInt64" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.10"},{"label": "1"},{"label": "A setting for ClickHouse Cloud"}]}]}/>

云中允许使用的数据库引擎。 1 - 重写 DDL 使用复制数据库， 2 - 重写 DDL 使用共享数据库。
## cloud_mode_engine {#cloud_mode_engine} 

<SettingsInfoBlock type="UInt64" default_value="1" />

云中允许的引擎家族。

- 0 - 允许所有
- 1 - 重写 DDL 使用 *ReplicatedMergeTree
- 2 - 重写 DDL 使用 SharedMergeTree
- 3 - 重写 DDL 使用 SharedMergeTree，除非显式传递了远程磁盘。
     
UInt64 以缩小公共部分。
## cluster_for_parallel_replicas {#cluster_for_parallel_replicas} 

<BetaBadge/>

当前服务器所在分片的集群。
## collect_hash_table_stats_during_aggregation {#collect_hash_table_stats_during_aggregation} 

<SettingsInfoBlock type="Bool" default_value="1" />

启用收集哈希表统计信息以优化内存分配。
## collect_hash_table_stats_during_joins {#collect_hash_table_stats_during_joins} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.7"},{"label": "1"},{"label": "New setting."}]}]}/>

启用收集哈希表统计信息以优化内存分配。
## compatibility {#compatibility} 

`compatibility` 设置使 ClickHouse 使用先前版本的默认设置，版本由该设置提供。

如果设置为非默认值，则将尊重这些设置（只有未修改的设置会受到 `compatibility` 设置的影响）。

该设置采用 ClickHouse 版本号作为字符串，例如 `22.3`，`22.8`。空值表示禁用此设置。

默认禁用。

:::note
在 ClickHouse Cloud 中，兼容性设置必须由 ClickHouse Cloud 支持进行设置。请 [提交请求](https://clickhouse.cloud/support) 以进行设置。
:::
## compatibility_ignore_auto_increment_in_create_table {#compatibility_ignore_auto_increment_in_create_table} 

<SettingsInfoBlock type="Bool" default_value="0" />

如果为 true，忽略列声明中的 AUTO_INCREMENT 关键字，否则返回错误。它简化了从 MySQL 的迁移。
## compatibility_ignore_collation_in_create_table {#compatibility_ignore_collation_in_create_table} 

<SettingsInfoBlock type="Bool" default_value="1" />

在创建表时兼容性忽略排序规则。
## compile_aggregate_expressions {#compile_aggregate_expressions} 

<SettingsInfoBlock type="Bool" default_value="1" />

启用或禁用聚合函数 JIT 编译为本机代码。启用此设置可以提高性能。

可能的值：

- 0 — 聚合在没有 JIT 编译的情况下完成。
- 1 — 使用 JIT 编译进行聚合。

**另请参见**

- [min_count_to_compile_aggregate_expression](#min_count_to_compile_aggregate_expression)
## compile_expressions {#compile_expressions} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.5"},{"label": "1"},{"label": "We believe that the LLVM infrastructure behind the JIT compiler is stable enough to enable this setting by default."}]}]}/>

将某些标量函数和操作符编译为本机代码。
## compile_sort_description {#compile_sort_description} 

<SettingsInfoBlock type="Bool" default_value="1" />

将排序描述编译为本机代码。
## connect_timeout {#connect_timeout} 

无副本时的连接超时。
## connect_timeout_with_failover_ms {#connect_timeout_with_failover_ms} 

<SettingsInfoBlock type="Milliseconds" default_value="1000" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "23.4"},{"label": "1000"},{"label": "Increase default connect timeout because of async connect"}]}]}/>

如果在集群定义中使用了 'shard' 和 'replica' 部分，与远程服务器连接的超时时间（毫秒）。
如果失败，将尝试多次连接到不同的副本。
## connect_timeout_with_failover_secure_ms {#connect_timeout_with_failover_secure_ms} 

<SettingsInfoBlock type="Milliseconds" default_value="1000" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "23.4"},{"label": "1000"},{"label": "Increase default secure connect timeout because of async connect"}]}]}/>

选择第一个健康副本的连接超时（安全连接）。
## connection_pool_max_wait_ms {#connection_pool_max_wait_ms} 

<SettingsInfoBlock type="Milliseconds" default_value="0" />

连接池满时连接的等待时间（毫秒）。

可能的值：

- 正整数。
- 0 — 无限超时。
## connections_with_failover_max_tries {#connections_with_failover_max_tries} 

<SettingsInfoBlock type="UInt64" default_value="3" />

分布式表引擎的每个副本的最大连接尝试次数。
## convert_query_to_cnf {#convert_query_to_cnf} 

<SettingsInfoBlock type="Bool" default_value="0" />

设置为 `true` 时，将 `SELECT` 查询转换为合取标准形式（CNF）。在某些情况下，将查询重写为 CNF 可以更快地执行（查看此 [Github 问题](https://github.com/ClickHouse/ClickHouse/issues/11749) 以获取说明）。

例如，请注意以下 `SELECT` 查询未被修改（默认行为）：

```sql
EXPLAIN SYNTAX
SELECT *
FROM
(
    SELECT number AS x
    FROM numbers(20)
) AS a
WHERE ((x >= 1) AND (x <= 5)) OR ((x >= 10) AND (x <= 15))
SETTINGS convert_query_to_cnf = false;
```

结果是：

```response
┌─explain────────────────────────────────────────────────────────┐
│ SELECT x                                                       │
│ FROM                                                           │
│ (                                                              │
│     SELECT number AS x                                         │
│     FROM numbers(20)                                           │
│     WHERE ((x >= 1) AND (x <= 5)) OR ((x >= 10) AND (x <= 15)) │
│ ) AS a                                                         │
│ WHERE ((x >= 1) AND (x <= 5)) OR ((x >= 10) AND (x <= 15))     │
│ SETTINGS convert_query_to_cnf = 0                              │
└────────────────────────────────────────────────────────────────┘
```

让我们将 `convert_query_to_cnf` 设置为 `true`，看看有什么变化：

```sql
EXPLAIN SYNTAX
SELECT *
FROM
(
    SELECT number AS x
    FROM numbers(20)
) AS a
WHERE ((x >= 1) AND (x <= 5)) OR ((x >= 10) AND (x <= 15))
SETTINGS convert_query_to_cnf = true;
```

注意 `WHERE` 子句被重写为 CNF，但结果集是相同的 - 布尔逻辑未更改：

```response
┌─explain───────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ SELECT x                                                                                                              │
│ FROM                                                                                                                  │
│ (                                                                                                                     │
│     SELECT number AS x                                                                                                │
│     FROM numbers(20)                                                                                                  │
│     WHERE ((x <= 15) OR (x <= 5)) AND ((x <= 15) OR (x >= 1)) AND ((x >= 10) OR (x <= 5)) AND ((x >= 10) OR (x >= 1)) │
│ ) AS a                                                                                                                │
│ WHERE ((x >= 10) OR (x >= 1)) AND ((x >= 10) OR (x <= 5)) AND ((x <= 15) OR (x >= 1)) AND ((x <= 15) OR (x <= 5))     │
│ SETTINGS convert_query_to_cnf = 1                                                                                     │
└───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
```

可能的值： true, false
## count_distinct_implementation {#count_distinct_implementation} 

<SettingsInfoBlock type="String" default_value="uniqExact" />

指定应使用哪个 `uniq*` 函数来执行 [COUNT(DISTINCT ...)](/sql-reference/aggregate-functions/reference/count) 构造。

可能的值：

- [uniq](/sql-reference/aggregate-functions/reference/uniq)
- [uniqCombined](/sql-reference/aggregate-functions/reference/uniqcombined)
- [uniqCombined64](/sql-reference/aggregate-functions/reference/uniqcombined64)
- [uniqHLL12](/sql-reference/aggregate-functions/reference/uniqhll12)
- [uniqExact](/sql-reference/aggregate-functions/reference/uniqexact)
## count_distinct_optimization {#count_distinct_optimization} 

<SettingsInfoBlock type="Bool" default_value="0" />

将计数唯一重写为分组的子查询。
## create_if_not_exists {#create_if_not_exists} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.9"},{"label": "0"},{"label": "New setting."}]}]}/>

默认启用 `IF NOT EXISTS` 的 `CREATE` 语句。如果未指定此设置或 `IF NOT EXISTS`，而提供的名称的表已存在，则不会抛出异常。
## create_index_ignore_unique {#create_index_ignore_unique} 

<SettingsInfoBlock type="Bool" default_value="0" />

在创建 UNIQUE 索引时忽略 UNIQUE 关键字。为 SQL 兼容性测试而设。
## create_replicated_merge_tree_fault_injection_probability {#create_replicated_merge_tree_fault_injection_probability} 

<SettingsInfoBlock type="Float" default_value="0" />

在创建表中，创建 ZooKeeper 中的元数据后，故障注入的概率。
## create_table_empty_primary_key_by_default {#create_table_empty_primary_key_by_default} 

<SettingsInfoBlock type="Bool" default_value="0" />

允许在未指定 ORDER BY 和 PRIMARY KEY 时创建 *MergeTree 表时的空主键。
## cross_join_min_bytes_to_compress {#cross_join_min_bytes_to_compress} 

<SettingsInfoBlock type="UInt64" default_value="1073741824" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.5"},{"label": "1073741824"},{"label": "Minimal size of block to compress in CROSS JOIN. Zero value means - disable this threshold. This block is compressed when any of the two thresholds (by rows or by bytes) are reached."}]}]}/>

CROSS JOIN 中要压缩的块的最小大小。零值意味着 - 禁用此阈值。只有在达到任一阈值（按行数或字节数）时，该块才会被压缩。
## cross_join_min_rows_to_compress {#cross_join_min_rows_to_compress} 

<SettingsInfoBlock type="UInt64" default_value="10000000" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.5"},{"label": "10000000"},{"label": "Minimal count of rows to compress block in CROSS JOIN. Zero value means - disable this threshold. This block is compressed when any of the two thresholds (by rows or by bytes) are reached."}]}]}/>

CROSS JOIN 中要压缩块的最小行数。零值意味着 - 禁用此阈值。只有在达到任一阈值（按行数或字节数）时，该块才会被压缩。
## data_type_default_nullable {#data_type_default_nullable} 

<SettingsInfoBlock type="Bool" default_value="0" />

允许在列定义中未明确包含修饰符 [NULL 或 NOT NULL](/sql-reference/statements/create/table#null-or-not-null-modifiers) 的数据类型将被视为 [Nullable](/sql-reference/data-types/nullable)。

可能的值：

- 1 — 列定义中的数据类型默认为 `Nullable`。
- 0 — 列定义中的数据类型默认为非 `Nullable`。
## database_atomic_wait_for_drop_and_detach_synchronously {#database_atomic_wait_for_drop_and_detach_synchronously} 

<SettingsInfoBlock type="Bool" default_value="0" />

为所有 `DROP` 和 `DETACH` 查询添加 `SYNC` 修饰符。

可能的值：

- 0 — 查询将延迟执行。
- 1 — 查询将同时执行，无延迟。
## database_replicated_allow_explicit_uuid {#database_replicated_allow_explicit_uuid} 

<SettingsInfoBlock type="UInt64" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.9"},{"label": "0"},{"label": "Added a new setting to disallow explicitly specifying table UUID"}]}]}/>

0 - 不允许在复制数据库的表中显式指定 UUID。1 - 允许。2 - 允许，但忽略指定的 UUID，并生成随机 UUID。
## database_replicated_allow_heavy_create {#database_replicated_allow_heavy_create} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.7"},{"label": "0"},{"label": "Long-running DDL queries (CREATE AS SELECT and POPULATE) for Replicated database engine was forbidden"}]}]}/>

允许在复制数据库引擎中执行长时间 DDL 查询（CREATE AS SELECT 和 POPULATE）。请注意，这可能会长时间阻塞 DDL 队列。
## database_replicated_allow_only_replicated_engine {#database_replicated_allow_only_replicated_engine} 

<SettingsInfoBlock type="Bool" default_value="0" />

允许在复制引擎数据库中仅创建复制表。
## database_replicated_allow_replicated_engine_arguments {#database_replicated_allow_replicated_engine_arguments} 

<SettingsInfoBlock type="UInt64" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.9"},{"label": "0"},{"label": "Don't allow explicit arguments by default"}]}]}/>

0 - 不允许显式指定 ZooKeeper 路径和副本名称，适用于复制数据库中的 *MergeTree 表。1 - 允许。2 - 允许，但忽略指定的路径，而使用默认路径。3 - 允许且不记录警告。
## database_replicated_always_detach_permanently {#database_replicated_always_detach_permanently} 

<SettingsInfoBlock type="Bool" default_value="0" />

如果数据库引擎为复制，则执行 DETACH TABLE 作为 DETACH TABLE PERMANENTLY。
## database_replicated_enforce_synchronous_settings {#database_replicated_enforce_synchronous_settings} 

<SettingsInfoBlock type="Bool" default_value="0" />

强制某些查询的同步等待（另请参见 database_atomic_wait_for_drop_and_detach_synchronously、mutations_sync、alter_sync）。不推荐启用这些设置。
## database_replicated_initial_query_timeout_sec {#database_replicated_initial_query_timeout_sec} 

<SettingsInfoBlock type="UInt64" default_value="300" />

设置初始 DDL 查询应等待复制数据库处理之前 DDL 队列条目的时间，单位为秒。

可能的值：

- 正整数。
- 0 — 无限制。
## decimal_check_overflow {#decimal_check_overflow} 

<SettingsInfoBlock type="Bool" default_value="1" />

检查十进制算术/比较操作的溢出。
## deduplicate_blocks_in_dependent_materialized_views {#deduplicate_blocks_in_dependent_materialized_views} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用接收来自 Replicated* 表的数据的物化视图中的去重检查。

可能的值：

      0 — 禁用。
      1 — 启用。

用法

默认情况下，物化视图不执行去重，而是在源表中执行。
如果由于在源表中去重而跳过插入的块，则不会插入到附加的物化视图中。这种行为使得可以将高度聚合的数据插入到物化视图中，适用于在物化视图聚合后插入的块相同但源表中的不同 INSERT 产生的情况。
与此同时，这种行为“破坏”了 `INSERT` 的幂等性。如果对主表的 `INSERT` 成功，而对物化视图的 `INSERT` 失败（例如，由于与 ClickHouse Keeper 的通信故障），那么客户端将收到错误并可以重试操作。但是，物化视图不会收到第二个插入，因为它将被主（源）表中的去重丢弃。设置 `deduplicate_blocks_in_dependent_materialized_views` 允许更改这种行为。在重试时，物化视图将接收重复的插入，并将自行进行去重检查，忽略源表的检查结果，并插入由于第一次失败而丢失的行。
## default_materialized_view_sql_security {#default_materialized_view_sql_security} 

<SettingsInfoBlock type="SQLSecurityType" default_value="DEFINER" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.2"},{"label": "DEFINER"},{"label": "Allows to set a default value for SQL SECURITY option when creating a materialized view"}]}]}/>

允许在创建物化视图时设置 SQL SECURITY 选项的默认值。[有关 SQL 安全性的更多信息](../../sql-reference/statements/create/view.md/#sql_security)。

默认值为 `DEFINER`。
## default_max_bytes_in_join {#default_max_bytes_in_join} 

右侧表的最大大小，如果需要限制但未设置 `max_bytes_in_join`。
## default_normal_view_sql_security {#default_normal_view_sql_security} 

<SettingsInfoBlock type="SQLSecurityType" default_value="INVOKER" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.2"},{"label": "INVOKER"},{"label": "Allows to set default `SQL SECURITY` option while creating a normal view"}]}]}/>

允许在创建普通视图时设置默认的 `SQL SECURITY` 选项。[有关 SQL 安全性的更多信息](../../sql-reference/statements/create/view.md/#sql_security)。

默认值为 `INVOKER`。
## default_table_engine {#default_table_engine} 

<SettingsInfoBlock type="DefaultTableEngine" default_value="MergeTree" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.3"},{"label": "MergeTree"},{"label": "Set default table engine to MergeTree for better usability"}]}]}/>

当在 `CREATE` 语句中未设置 `ENGINE` 时使用的默认表引擎。

可能的值：

- 一个字符串，表示任何有效的表引擎名称。

云默认值：`SharedMergeTree`。

**示例**

查询：

```sql
SET default_table_engine = 'Log';

SELECT name, value, changed FROM system.settings WHERE name = 'default_table_engine';
```

结果：

```response
┌─name─────────────────┬─value─┬─changed─┐
│ default_table_engine │ Log   │       1 │
└──────────────────────┴───────┴─────────┘
```

在这个示例中，任何未指定 `Engine` 的新表都将使用 `Log` 表引擎：

查询：

```sql
CREATE TABLE my_table (
    x UInt32,
    y UInt32
);

SHOW CREATE TABLE my_table;
```

结果：

```response
┌─statement────────────────────────────────────────────────────────────────┐
│ CREATE TABLE default.my_table
(
    `x` UInt32,
    `y` UInt32
)
ENGINE = Log
└──────────────────────────────────────────────────────────────────────────┘
```
## default_temporary_table_engine {#default_temporary_table_engine} 

<SettingsInfoBlock type="DefaultTableEngine" default_value="Memory" />

与 [default_table_engine](#default_table_engine) 相同，但适用于临时表。

在这个示例中，任何未指定 `Engine` 的新临时表都将使用 `Log` 表引擎：

查询：

```sql
SET default_temporary_table_engine = 'Log';

CREATE TEMPORARY TABLE my_table (
    x UInt32,
    y UInt32
);

SHOW CREATE TEMPORARY TABLE my_table;
```

结果：

```response
┌─statement────────────────────────────────────────────────────────────────┐
│ CREATE TEMPORARY TABLE default.my_table
(
    `x` UInt32,
    `y` UInt32
)
ENGINE = Log
└──────────────────────────────────────────────────────────────────────────┘
```
## default_view_definer {#default_view_definer} 

<SettingsInfoBlock type="String" default_value="CURRENT_USER" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.2"},{"label": "CURRENT_USER"},{"label": "Allows to set default `DEFINER` option while creating a view"}]}]}/>

允许在创建视图时设置默认的 `DEFINER` 选项。[有关 SQL 安全性的更多信息](../../sql-reference/statements/create/view.md/#sql_security)。

默认值为 `CURRENT_USER`。
## describe_compact_output {#describe_compact_output} 

<SettingsInfoBlock type="Bool" default_value="0" />

如果为 true，则仅在 DESCRIBE 查询的结果中包含列名和类型。
## describe_extend_object_types {#describe_extend_object_types} 

<SettingsInfoBlock type="Bool" default_value="0" />

在 DESCRIBE 查询中推断对象类型的具体类型。
## describe_include_subcolumns {#describe_include_subcolumns} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用描述 [DESCRIBE](../../sql-reference/statements/describe-table.md) 查询的子列。例如，成员的 [Tuple](../../sql-reference/data-types/tuple.md) 或 [Map](/sql-reference/data-types/map#reading-subcolumns-of-map)、[Nullable](../../sql-reference/data-types/nullable.md/#finding-null) 或 [Array](../../sql-reference/data-types/array.md/#array-size) 数据类型的子列。

可能的值：

- 0 — 子列未包含在 `DESCRIBE` 查询中。
- 1 — 子列包含在 `DESCRIBE` 查询中。

**示例**

查看 [DESCRIBE](../../sql-reference/statements/describe-table.md) 语句的一个示例。
## describe_include_virtual_columns {#describe_include_virtual_columns} 

<SettingsInfoBlock type="Bool" default_value="0" />

如果为 true，则表的虚拟列将包含在 DESCRIBE 查询的结果中。
## dialect {#dialect} 

<SettingsInfoBlock type="Dialect" default_value="clickhouse" />

将用于解析查询的方言。
## dictionary_validate_primary_key_type {#dictionary_validate_primary_key_type} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.7"},{"label": "0"},{"label": "Validate primary key type for dictionaries. By default id type for simple layouts will be implicitly converted to UInt64."}]}]}/>

验证字典的主键类型。默认情况下，简单布局的 ID 类型将隐式转换为 UInt64。
## distinct_overflow_mode {#distinct_overflow_mode} 

<SettingsInfoBlock type="OverflowMode" default_value="throw" />

设置数据量超过一个限制时的处理方式。

可能的值：
- `throw`：抛出异常（默认）。
- `break`：停止执行查询并返回部分结果，就像源数据已用尽一样。
## distributed_aggregation_memory_efficient {#distributed_aggregation_memory_efficient} 

<SettingsInfoBlock type="Bool" default_value="1" />

是否启用分布式聚合的节省内存模式。
## distributed_background_insert_batch {#distributed_background_insert_batch} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用/禁用插入数据以批处理方式发送。

启用批处理发送时，[Distributed](../../engines/table-engines/special/distributed.md) 表引擎尝试在一个操作中发送多批插入的数据文件，而不是分别发送。批处理发送通过更好地利用服务器和网络资源来提高集群性能。

可能的值：

- 1 — 启用。
- 0 — 禁用。
## distributed_background_insert_max_sleep_time_ms {#distributed_background_insert_max_sleep_time_ms} 

<SettingsInfoBlock type="Milliseconds" default_value="30000" />

[Distributed](../../engines/table-engines/special/distributed.md) 表引擎发送数据的最大间隔。限制在 [distributed_background_insert_sleep_time_ms](#distributed_background_insert_sleep_time_ms) 设置中设置的间隔的指数增长。

可能的值：

- 正整数，单位为毫秒。
## distributed_background_insert_sleep_time_ms {#distributed_background_insert_sleep_time_ms} 

<SettingsInfoBlock type="Milliseconds" default_value="100" />

[Distributed](../../engines/table-engines/special/distributed.md) 表引擎发送数据的基本间隔。实际间隔在发生错误时会呈指数增长。

可能的值：

- 正整数，单位为毫秒。
## distributed_background_insert_split_batch_on_failure {#distributed_background_insert_split_batch_on_failure} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用/禁用在失败时分割批处理。

有时，将特定批发送到远程分片可能会失败，因为某些复杂流程导致（例如，带有 `GROUP BY` 的 `MATERIALIZED VIEW`），最终由于 `Memory limit exceeded` 或类似错误。在这种情况下，重试将无济于事（这将阻塞表的分布式发送），但是逐个发送该批中的文件可能会成功执行 INSERT。

因此，将此设置为 `1` 将禁用此类批处理（即暂时禁用失败批处理的 `distributed_background_insert_batch`）。

可能的值：

- 1 — 启用。
- 0 — 禁用。

:::note
此设置还影响到由于异常服务器（机器）终止和没有 `fsync_after_insert`/`fsync_directories` 而出现的损坏批处理 [Distributed](../../engines/table-engines/special/distributed.md) 表引擎。
:::

:::note
您不应依赖自动批处理拆分，因为这可能会影响性能。
:::
## distributed_background_insert_timeout {#distributed_background_insert_timeout} 

<SettingsInfoBlock type="UInt64" default_value="0" />

分布式插入查询的超时。仅在启用 insert_distributed_sync 时使用。零值表示没有超时。
## distributed_cache_bypass_connection_pool {#distributed_cache_bypass_connection_pool} 

<CloudAvailableBadge/>

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.10"},{"label": "0"},{"label": "A setting for ClickHouse Cloud"}]}]}/>

仅在 ClickHouse Cloud 中有效。允许绕过分布式缓存连接池。
## distributed_cache_connect_max_tries {#distributed_cache_connect_max_tries} 

<CloudAvailableBadge/>

<SettingsInfoBlock type="UInt64" default_value="20" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.1"},{"label": "20"},{"label": "Cloud only"}]}, {"id": "row-2","items": [{"label": "24.10"},{"label": "20"},{"label": "A setting for ClickHouse Cloud"}]}]}/>

仅在 ClickHouse Cloud 中有效。如果连接失败，连接到分布式缓存的尝试次数。

## distributed_cache_data_packet_ack_window {#distributed_cache_data_packet_ack_window} 

<CloudAvailableBadge/>

<SettingsInfoBlock type="UInt64" default_value="5" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.10"},{"label": "5"},{"label": "A setting for ClickHouse Cloud"}]}]}/>

仅在 ClickHouse Cloud 中有效。一次分布式缓存读取请求中发送 ACK 的数据包序列的窗口。
## distributed_cache_discard_connection_if_unread_data {#distributed_cache_discard_connection_if_unread_data} 

<CloudAvailableBadge/>

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.11"},{"label": "1"},{"label": "New setting"}]}, {"id": "row-2","items": [{"label": "24.10"},{"label": "1"},{"label": "New setting"}]}]}/>

仅在 ClickHouse Cloud 中有效。在某些数据未被读取的情况下丢弃连接。
## distributed_cache_fetch_metrics_only_from_current_az {#distributed_cache_fetch_metrics_only_from_current_az} 

<CloudAvailableBadge/>

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.10"},{"label": "1"},{"label": "A setting for ClickHouse Cloud"}]}]}/>

仅在 ClickHouse Cloud 中有效。仅从当前可用区域获取系统.distributed_cache_metrics，system.distributed_cache_events 中的指标。
## distributed_cache_log_mode {#distributed_cache_log_mode} 

<CloudAvailableBadge/>

<SettingsInfoBlock type="DistributedCacheLogMode" default_value="on_error" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.10"},{"label": "on_error"},{"label": "A setting for ClickHouse Cloud"}]}]}/>

仅在 ClickHouse Cloud 中有效。写入系统.distributed_cache_log 的模式。
## distributed_cache_max_unacked_inflight_packets {#distributed_cache_max_unacked_inflight_packets} 

<CloudAvailableBadge/>

<SettingsInfoBlock type="UInt64" default_value="10" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.10"},{"label": "10"},{"label": "A setting for ClickHouse Cloud"}]}]}/>

仅在 ClickHouse Cloud 中有效。一次分布式缓存读取请求中未确认的在途数据包的最大数量。
## distributed_cache_min_bytes_for_seek {#distributed_cache_min_bytes_for_seek} 

<CloudAvailableBadge/>

<SettingsInfoBlock type="UInt64" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.1"},{"label": "0"},{"label": "New private setting."}]}]}/>

仅在 ClickHouse Cloud 中有效。在分布式缓存中进行查找的最小字节数。
## distributed_cache_pool_behaviour_on_limit {#distributed_cache_pool_behaviour_on_limit} 

<CloudAvailableBadge/>

<SettingsInfoBlock type="DistributedCachePoolBehaviourOnLimit" default_value="wait" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.1"},{"label": "wait"},{"label": "Cloud only"}]}, {"id": "row-2","items": [{"label": "24.10"},{"label": "allocate_bypassing_pool"},{"label": "A setting for ClickHouse Cloud"}]}]}/>

仅在 ClickHouse Cloud 中有效。标识在池限制达到时分布式缓存连接的行为。
## distributed_cache_read_alignment {#distributed_cache_read_alignment} 

<CloudAvailableBadge/>

<SettingsInfoBlock type="UInt64" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.10"},{"label": "0"},{"label": "A setting for ClickHouse Cloud"}]}]}/>

仅在 ClickHouse Cloud 中有效。用于测试目的的设置，请勿更改。
## distributed_cache_read_only_from_current_az {#distributed_cache_read_only_from_current_az} 

<CloudAvailableBadge/>

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.5"},{"label": "1"},{"label": "New setting"}]}]}/>

仅在 ClickHouse Cloud 中有效。仅允许从当前可用区域读取。如果禁用，将从所有可用区域的所有缓存服务器进行读取。
## distributed_cache_read_request_max_tries {#distributed_cache_read_request_max_tries} 

<CloudAvailableBadge/>

<SettingsInfoBlock type="UInt64" default_value="20" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.4"},{"label": "20"},{"label": "New setting"}]}]}/>

仅在 ClickHouse Cloud 中有效。如果请求失败，尝试进行分布式缓存请求的次数。
## distributed_cache_receive_response_wait_milliseconds {#distributed_cache_receive_response_wait_milliseconds} 

<CloudAvailableBadge/>

<SettingsInfoBlock type="UInt64" default_value="60000" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.10"},{"label": "60000"},{"label": "A setting for ClickHouse Cloud"}]}]}/>

仅在 ClickHouse Cloud 中有效。接收来自分布式缓存请求的数据的等待时间（以毫秒为单位）。
## distributed_cache_receive_timeout_milliseconds {#distributed_cache_receive_timeout_milliseconds} 

<CloudAvailableBadge/>

<SettingsInfoBlock type="UInt64" default_value="10000" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.10"},{"label": "10000"},{"label": "A setting for ClickHouse Cloud"}]}]}/>

仅在 ClickHouse Cloud 中有效。等待从分布式缓存接收任何响应的时间（以毫秒为单位）。
## distributed_cache_throw_on_error {#distributed_cache_throw_on_error} 

<CloudAvailableBadge/>

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.10"},{"label": "0"},{"label": "A setting for ClickHouse Cloud"}]}]}/>

仅在 ClickHouse Cloud 中有效。在与分布式缓存通信时发生异常或从分布式缓存收到异常时，要重新抛出异常。否则在出错时将回退到跳过分布式缓存。
## distributed_cache_wait_connection_from_pool_milliseconds {#distributed_cache_wait_connection_from_pool_milliseconds} 

<CloudAvailableBadge/>

<SettingsInfoBlock type="UInt64" default_value="100" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.10"},{"label": "100"},{"label": "A setting for ClickHouse Cloud"}]}]}/>

仅在 ClickHouse Cloud 中有效。如果 distributed_cache_pool_behaviour_on_limit 设置为 wait，从连接池接收连接的等待时间（以毫秒为单位）。
## distributed_connections_pool_size {#distributed_connections_pool_size} 

<SettingsInfoBlock type="UInt64" default_value="1024" />

与远程服务器的并发连接数的最大值，用于对单个分布式表的所有查询进行分布式处理。我们建议将值设置为不小于集群中服务器的数量。
## distributed_ddl_entry_format_version {#distributed_ddl_entry_format_version} 

<SettingsInfoBlock type="UInt64" default_value="5" />

分布式 DDL（ON CLUSTER）查询的兼容性版本。
## distributed_ddl_output_mode {#distributed_ddl_output_mode} 

<SettingsInfoBlock type="DistributedDDLOutputMode" default_value="throw" />

设置分布式 DDL 查询结果的格式。

可能的值：

- `throw` — 返回查询执行状态的结果集，对于查询已完成的所有主机。如果某些主机上的查询失败，则将重新抛出第一个异常。如果某些主机上的查询尚未完成，并且 [distributed_ddl_task_timeout](#distributed_ddl_task_timeout) 超过，则将抛出 `TIMEOUT_EXCEEDED` 异常。
- `none` — 类似于 throw，但分布式 DDL 查询不返回结果集。
- `null_status_on_timeout` — 在结果集的某些行中返回 `NULL` 作为执行状态，而不是抛出 `TIMEOUT_EXCEEDED`，如果查询在相应主机上尚未完成。
- `never_throw` — 不抛出 `TIMEOUT_EXCEEDED`，如果某些主机上的查询失败，则不重新抛出异常。
- `none_only_active` - 类似于 `none`，但不等待 `Replicated` 数据库的非活动副本。注意：使用此模式无法确定查询未在某些副本上执行且将在后台执行。
- `null_status_on_timeout_only_active` — 类似于 `null_status_on_timeout`，但不等待 `Replicated` 数据库的非活动副本。
- `throw_only_active` — 类似于 `throw`，但不等待 `Replicated` 数据库的非活动副本。

Cloud 默认值：`none`。
## distributed_ddl_task_timeout {#distributed_ddl_task_timeout} 

<SettingsInfoBlock type="Int64" default_value="180" />

设置集群中所有主机响应 DDL 查询的超时时间。如果未在所有主机上执行 DDL 请求，则响应将包含超时错误，请求将在异步模式下执行。负值表示无限制。

可能的值：

- 正整数。
- 0 — 异步模式。
- 负整数 — 无限超时。
## distributed_foreground_insert {#distributed_foreground_insert} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用对 [Distributed](/engines/table-engines/special/distributed) 表的同步数据插入。

默认情况下，在向 `Distributed` 表插入数据时，ClickHouse 服务器会以后台模式将数据发送到集群节点。当 `distributed_foreground_insert=1` 时，数据以同步模式处理，只有在所有数据保存到所有分片之后（每个分片至少有一个副本，如果 `internal_replication` 为 true），`INSERT` 操作才会成功。

可能的值：

- 0 — 以后台模式插入数据。
- 1 — 以同步模式插入数据。

Cloud 默认值：`1`。

**另请参见**

- [分布式表引擎](/engines/table-engines/special/distributed)
- [管理分布式表](/sql-reference/statements/system#managing-distributed-tables)
## distributed_group_by_no_merge {#distributed_group_by_no_merge} 

<SettingsInfoBlock type="UInt64" default_value="0" />

在分布式查询处理时，不从不同服务器合并聚合状态，如果可以确定不同分片上存在不同的键，可以使用此选项。

可能的值：

- `0` — 禁用（最终查询处理在发起节点上完成）。
- `1` - 不从不同服务器合并聚合状态进行分布式查询处理（查询在分片上完全处理，发起者仅代理数据），在可以确定不同分片上存在不同键的情况下可以使用。
- `2` - 与 `1` 类似，但在发起者上应用 `ORDER BY` 和 `LIMIT`（当查询在远程节点上完全处理时，如对 `distributed_group_by_no_merge=1`）无法进行。

**示例**

```sql
SELECT *
FROM remote('127.0.0.{2,3}', system.one)
GROUP BY dummy
LIMIT 1
SETTINGS distributed_group_by_no_merge = 1
FORMAT PrettyCompactMonoBlock

┌─dummy─┐
│     0 │
│     0 │
└───────┘
```

```sql
SELECT *
FROM remote('127.0.0.{2,3}', system.one)
GROUP BY dummy
LIMIT 1
SETTINGS distributed_group_by_no_merge = 2
FORMAT PrettyCompactMonoBlock

┌─dummy─┐
│     0 │
└───────┘
```
## distributed_insert_skip_read_only_replicas {#distributed_insert_skip_read_only_replicas} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.3"},{"label": "0"},{"label": "If true, INSERT into Distributed will skip read-only replicas"}]}]}/>

启用跳过只读副本插入分布式的 INSERT 查询。

可能的值：

- 0 — INSERT 按照 usual 进行，如果它将转到只读副本，则会失败。
- 1 — 发起者在将数据发送到分片之前将跳过只读副本。
## distributed_plan_default_reader_bucket_count {#distributed_plan_default_reader_bucket_count} 

<ExperimentalBadge/>

<SettingsInfoBlock type="UInt64" default_value="8" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.5"},{"label": "8"},{"label": "New experimental setting."}]}]}/>

分布式查询的并行读取的默认任务数量。任务在副本之间分配。
## distributed_plan_default_shuffle_join_bucket_count {#distributed_plan_default_shuffle_join_bucket_count} 

<ExperimentalBadge/>

<SettingsInfoBlock type="UInt64" default_value="8" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.5"},{"label": "8"},{"label": "New experimental setting."}]}]}/>

分布式 shuffle-hash-join 的默认桶数量。
## distributed_plan_execute_locally {#distributed_plan_execute_locally} 

<ExperimentalBadge/>

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.5"},{"label": "0"},{"label": "New experimental setting."}]}]}/>

在本地运行分布式查询计划的所有任务。对于测试和调试很有用。
## distributed_plan_force_exchange_kind {#distributed_plan_force_exchange_kind} 

<ExperimentalBadge/>

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.5"},{"label": ""},{"label": "New experimental setting."}]}]}/>

在分布式查询阶段之间强制指定类型的交换操作符。

可能的值：

 - '' - 不强制任何类型的交换操作符，让优化器选择，
 - 'Persisted' - 使用对象存储中的临时文件，
 - 'Streaming' - 通过网络流交换数据。
## distributed_plan_optimize_exchanges {#distributed_plan_optimize_exchanges} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.5"},{"label": "1"},{"label": "New experimental setting."}]}]}/>

移除分布式查询计划中的不必要交换。在调试时禁用。
## distributed_product_mode {#distributed_product_mode} 

<SettingsInfoBlock type="DistributedProductMode" default_value="deny" />

更改 [分布式子查询](../../sql-reference/operators/in.md) 的行为。

ClickHouse 在查询包含分布式表的乘积时应用此设置，即当分布式表的查询包含针对该分布式表的非 GLOBAL 子查询时。

限制：

- 仅适用于 IN 和 JOIN 子查询。
- 仅在 FROM 部分使用包含多个分片的分布式表时。
- 如果子查询涉及包含多个分片的分布式表。
- 不用于表值 [remote](../../sql-reference/table-functions/remote.md) 函数。

可能的值：

- `deny` — 默认值。禁止使用这些类型的子查询（返回 "禁止使用双分布式 IN/ JOIN 子查询" 异常）。
- `local` — 用目标服务器（分片）的本地数据库和表替换子查询，通过正常的 `IN`/`JOIN`。
- `global` — 将 `IN`/`JOIN` 查询替换为 `GLOBAL IN`/`GLOBAL JOIN`。
- `allow` — 允许使用这些类型的子查询。
## distributed_push_down_limit {#distributed_push_down_limit} 

<SettingsInfoBlock type="UInt64" default_value="1" />

启用或禁用在每个分片上单独应用 [LIMIT](#limit)。

这将可以避免：
- 通过网络发送额外的行；
- 在发起者上处理超出限制的行。

从 21.9 版本开始，如果 `distributed_push_down_limit` 仅在满足以下条件之一时更改查询执行，则不能再得到不准确的结果：
- [distributed_group_by_no_merge](#distributed_group_by_no_merge) > 0。
- 查询 **没有** `GROUP BY`/`DISTINCT`/`LIMIT BY`，但具有 `ORDER BY`/`LIMIT`。
- 查询 **具有** `GROUP BY`/`DISTINCT`/`LIMIT BY` 且有 `ORDER BY`/`LIMIT` 并且：
    - [optimize_skip_unused_shards](#optimize_skip_unused_shards) 已启用。
    - [optimize_distributed_group_by_sharding_key](#optimize_distributed_group_by_sharding_key) 已启用。

可能的值：

- 0 — 禁用。
- 1 — 启用。

另请参见：

- [distributed_group_by_no_merge](#distributed_group_by_no_merge)
- [optimize_skip_unused_shards](#optimize_skip_unused_shards)
- [optimize_distributed_group_by_sharding_key](#optimize_distributed_group_by_sharding_key)
## distributed_replica_error_cap {#distributed_replica_error_cap} 

<SettingsInfoBlock type="UInt64" default_value="1000" />

- 类型：无符号整数
- 默认值：1000

每个副本的错误计数限制在此值，防止单个副本累积过多错误。

另请参见：

- [load_balancing](#load_balancing-round_robin)
- [表引擎 Distributed](../../engines/table-engines/special/distributed.md)
- [distributed_replica_error_half_life](#distributed_replica_error_half_life)
- [distributed_replica_max_ignored_errors](#distributed_replica_max_ignored_errors)
## distributed_replica_error_half_life {#distributed_replica_error_half_life} 

<SettingsInfoBlock type="Seconds" default_value="60" />

- 类型：秒
- 默认值：60秒

控制分布式表的错误归零速度。如果某个副本在一段时间内不可用，累积 5 个错误，并且 distributed_replica_error_half_life 设置为 1 秒，则在最后一个错误后 3 秒后，该副本被认为是正常的。

另请参见：

- [load_balancing](#load_balancing-round_robin)
- [表引擎 Distributed](../../engines/table-engines/special/distributed.md)
- [distributed_replica_error_cap](#distributed_replica_error_cap)
- [distributed_replica_max_ignored_errors](#distributed_replica_max_ignored_errors)
## distributed_replica_max_ignored_errors {#distributed_replica_max_ignored_errors} 

<SettingsInfoBlock type="UInt64" default_value="0" />

- 类型：无符号整数
- 默认值：0

在选择副本时将忽略的错误数量（根据 `load_balancing` 算法）。

另请参见：

- [load_balancing](#load_balancing-round_robin)
- [表引擎 Distributed](../../engines/table-engines/special/distributed.md)
- [distributed_replica_error_cap](#distributed_replica_error_cap)
- [distributed_replica_error_half_life](#distributed_replica_error_half_life)
## do_not_merge_across_partitions_select_final {#do_not_merge_across_partitions_select_final} 

<SettingsInfoBlock type="Bool" default_value="0" />

在选择最终时仅合并一个分区中的部分。
## empty_result_for_aggregation_by_constant_keys_on_empty_set {#empty_result_for_aggregation_by_constant_keys_on_empty_set} 

<SettingsInfoBlock type="Bool" default_value="1" />

在对空集合进行常量键聚合时返回空结果。
## empty_result_for_aggregation_by_empty_set {#empty_result_for_aggregation_by_empty_set} 

<SettingsInfoBlock type="Bool" default_value="0" />

在对空集合进行无键聚合时返回空结果。
## enable_adaptive_memory_spill_scheduler {#enable_adaptive_memory_spill_scheduler} 

<ExperimentalBadge/>

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.2"},{"label": "0"},{"label": "New setting. Enable spill memory data into external storage adaptively."}]}]}/>

触发处理器自适应地将数据溢出到外部存储。当前支持 grace join。
## enable_blob_storage_log {#enable_blob_storage_log} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.6"},{"label": "1"},{"label": "Write information about blob storage operations to system.blob_storage_log table"}]}]}/>

将关于 blob 存储操作的信息写入 system.blob_storage_log 表。
## enable_deflate_qpl_codec {#enable_deflate_qpl_codec} 

<SettingsInfoBlock type="Bool" default_value="0" />

如果启用，则可以使用 DEFLATE_QPL 编解码器来压缩列。
## enable_early_constant_folding {#enable_early_constant_folding} 

<SettingsInfoBlock type="Bool" default_value="1" />

启用查询优化，我们会分析函数和子查询的结果，如果其中存在常量，则重写查询。
## enable_extended_results_for_datetime_functions {#enable_extended_results_for_datetime_functions} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用返回类型的结果：
- `Date32` 具有扩展范围（与类型 `Date` 相比），用于函数 [toStartOfYear](../../sql-reference/functions/date-time-functions.md/#tostartofyear)， [toStartOfISOYear](../../sql-reference/functions/date-time-functions.md/#tostartofisoyear)， [toStartOfQuarter](../../sql-reference/functions/date-time-functions.md/#tostartofquarter)， [toStartOfMonth](../../sql-reference/functions/date-time-functions.md/#tostartofmonth)， [toLastDayOfMonth](../../sql-reference/functions/date-time-functions.md/#tolastdayofmonth)， [toStartOfWeek](../../sql-reference/functions/date-time-functions.md/#tostartofweek)， [toLastDayOfWeek](../../sql-reference/functions/date-time-functions.md/#tolastdayofweek) 和 [toMonday](../../sql-reference/functions/date-time-functions.md/#tomonday)。
- `DateTime64` 具有扩展范围（与类型 `DateTime` 相比），用于函数 [toStartOfDay](../../sql-reference/functions/date-time-functions.md/#tostartofday)， [toStartOfHour](../../sql-reference/functions/date-time-functions.md/#tostartofhour)， [toStartOfMinute](../../sql-reference/functions/date-time-functions.md/#tostartofminute)， [toStartOfFiveMinutes](../../sql-reference/functions/date-time-functions.md/#tostartoftenminutes)， [toStartOfTenMinutes](../../sql-reference/functions/date-time-functions.md/#tostartoftenminutes)， [toStartOfFifteenMinutes](../../sql-reference/functions/date-time-functions.md/#tostartoffifteenminutes) 和 [timeSlot](../../sql-reference/functions/date-time-functions.md/#timeslot)。

可能的值：

- 0 — 对所有类型的参数，函数返回 `Date` 或 `DateTime`。
- 1 — 对于 `Date32` 或 `DateTime64` 参数，函数返回 `Date32` 或 `DateTime64`，否则返回 `Date` 或 `DateTime`。
## enable_filesystem_cache {#enable_filesystem_cache} 

<SettingsInfoBlock type="Bool" default_value="1" />

使用远程文件系统的缓存。此设置不会开启/关闭磁盘的缓存（必须通过磁盘配置完成），但允许对某些查询绕过缓存（如果有意）。
## enable_filesystem_cache_log {#enable_filesystem_cache_log} 

<SettingsInfoBlock type="Bool" default_value="0" />

允许记录每个查询的文件系统缓存日志。
## enable_filesystem_cache_on_write_operations {#enable_filesystem_cache_on_write_operations} 

<SettingsInfoBlock type="Bool" default_value="0" />

在写操作时写入缓存。为了实际工作，此设置需要在磁盘配置中添加。
## enable_filesystem_read_prefetches_log {#enable_filesystem_read_prefetches_log} 

<SettingsInfoBlock type="Bool" default_value="0" />

在查询期间将日志写入 system.filesystem 的 prefetch_log。仅用于测试或调试，不建议默认开启此功能。
## enable_global_with_statement {#enable_global_with_statement} 

<SettingsInfoBlock type="Bool" default_value="1" />

将 WITH 语句传播到 UNION 查询和所有子查询。
## enable_hdfs_pread {#enable_hdfs_pread} 

<SettingsInfoBlock type="Bool" default_value="1" />

启用或禁用对 HDFS 文件的 pread。默认情况下，使用 `hdfsPread`。如果禁用，将使用 `hdfsRead` 和 `hdfsSeek` 来读取 hdfs 文件。
## enable_http_compression {#enable_http_compression} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用对 HTTP 请求的响应中的数据压缩。

有关详细信息，请阅读 [HTTP 接口描述](../../interfaces/http.md)。

可能的值：

- 0 — 禁用。
- 1 — 启用。
## enable_job_stack_trace {#enable_job_stack_trace} 

<SettingsInfoBlock type="Bool" default_value="1" />

在任务结果异常时输出任务创建者的堆栈跟踪。
## enable_lightweight_delete {#enable_lightweight_delete} 

<SettingsInfoBlock type="Bool" default_value="1" />

启用对 mergetree 表的轻量级 DELETE 修改。
## enable_memory_bound_merging_of_aggregation_results {#enable_memory_bound_merging_of_aggregation_results} 

<SettingsInfoBlock type="Bool" default_value="1" />

启用聚合的内存限制合并策略。
## enable_multiple_prewhere_read_steps {#enable_multiple_prewhere_read_steps} 

<SettingsInfoBlock type="Bool" default_value="1" />

将更多条件从 WHERE 移动到 PREWHERE， 如果存在多个与 AND 结合的条件，进行多步从磁盘读取和过滤。
## enable_named_columns_in_function_tuple {#enable_named_columns_in_function_tuple} 

<SettingsInfoBlock type="Bool" default_value="0" />

在函数 tuple() 中生成命名元组，当所有名称都是唯一且可作为未引用标识符处理时。
## enable_optimize_predicate_expression {#enable_optimize_predicate_expression} 

<SettingsInfoBlock type="Bool" default_value="1" />

在 `SELECT` 查询中启用谓词下推。

谓词下推可能显著减少分布式查询的网络流量。

可能的值：

- 0 — 禁用。
- 1 — 启用。

使用示例

考虑以下查询：

1.  `SELECT count() FROM test_table WHERE date = '2018-10-10'`
2.  `SELECT count() FROM (SELECT * FROM test_table) WHERE date = '2018-10-10'`

如果 `enable_optimize_predicate_expression = 1`，则这两个查询的执行时间相同，因为 ClickHouse 在处理子查询时应用了 `WHERE`。

如果 `enable_optimize_predicate_expression = 0`，则第二个查询的执行时间要长得多，因为 `WHERE` 子句在子查询结束后对所有数据进行应用。
## enable_optimize_predicate_expression_to_final_subquery {#enable_optimize_predicate_expression_to_final_subquery} 

<SettingsInfoBlock type="Bool" default_value="1" />

允许将谓词推送到最终子查询。
## enable_order_by_all {#enable_order_by_all} 

<SettingsInfoBlock type="Bool" default_value="1" />

启用或禁用使用 `ORDER BY ALL` 语法进行排序，请参见 [ORDER BY](../../sql-reference/statements/select/order-by.md)。

可能的值：

- 0 — 禁用 ORDER BY ALL。
- 1 — 启用 ORDER BY ALL。

**示例**

查询：

```sql
CREATE TABLE TAB(C1 Int, C2 Int, ALL Int) ENGINE=Memory();

INSERT INTO TAB VALUES (10, 20, 30), (20, 20, 10), (30, 10, 20);

SELECT * FROM TAB ORDER BY ALL; -- returns an error that ALL is ambiguous

SELECT * FROM TAB ORDER BY ALL SETTINGS enable_order_by_all = 0;
```

结果：

```text
┌─C1─┬─C2─┬─ALL─┐
│ 20 │ 20 │  10 │
│ 30 │ 10 │  20 │
│ 10 │ 20 │  30 │
└────┴────┴─────┘
```
## enable_parsing_to_custom_serialization {#enable_parsing_to_custom_serialization} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.10"},{"label": "1"},{"label": "New setting"}]}]}/>

如果为 true，则可以直接将数据解析到带有自定义序列化的列（例如 稀疏），根据来自表的序列化提示。
## enable_positional_arguments {#enable_positional_arguments} 

<SettingsInfoBlock type="Bool" default_value="1" />

启用或禁用支持 [GROUP BY](/sql-reference/statements/select/group-by)， [LIMIT BY](../../sql-reference/statements/select/limit-by.md)， [ORDER BY](../../sql-reference/statements/select/order-by.md) 语句的定位参数。

可能的值：

- 0 — 不支持定位参数。
- 1 — 支持定位参数：可以用列号代替列名称。

**示例**

查询：

```sql
CREATE TABLE positional_arguments(one Int, two Int, three Int) ENGINE=Memory();

INSERT INTO positional_arguments VALUES (10, 20, 30), (20, 20, 10), (30, 10, 20);

SELECT * FROM positional_arguments ORDER BY 2,3;
```

结果：

```text
┌─one─┬─two─┬─three─┐
│  30 │  10 │   20  │
│  20 │  20 │   10  │
│  10 │  20 │   30  │
└─────┴─────┴───────┘
```
## enable_reads_from_query_cache {#enable_reads_from_query_cache} 

<SettingsInfoBlock type="Bool" default_value="1" />

如果启用，将从 [查询缓存](../query-cache.md) 中检索 `SELECT` 查询的结果。

可能的值：

- 0 - 禁用
- 1 - 启用
## enable_s3_requests_logging {#enable_s3_requests_logging} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用 S3 请求的非常明确的日志记录。仅对调试有意义。
## enable_scalar_subquery_optimization {#enable_scalar_subquery_optimization} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "19.18"},{"label": "1"},{"label": "Prevent scalar subqueries from (de)serializing large scalar values and possibly avoid running the same subquery more than once"}]}]}/>

如果设置为 true，防止标量子查询（反序列化大型标量值）并可能避免运行同一个子查询超过一次。
## enable_sharing_sets_for_mutations {#enable_sharing_sets_for_mutations} 

<SettingsInfoBlock type="Bool" default_value="1" />

允许在同一事务的不同任务之间共享为 IN 子查询构建的集合对象。这减少了内存使用和 CPU 消耗。
## enable_software_prefetch_in_aggregation {#enable_software_prefetch_in_aggregation} 

<SettingsInfoBlock type="Bool" default_value="1" />

启用在聚合中使用软件预取。
## enable_unaligned_array_join {#enable_unaligned_array_join} 

<SettingsInfoBlock type="Bool" default_value="0" />

允许 ARRAY JOIN 使用具有不同大小的多个数组。当启用此设置时，数组将调整为最长的数组大小。
## enable_url_encoding {#enable_url_encoding} 

<SettingsInfoBlock type="Bool" default_value="0" />

允许在 [URL](../../engines/table-engines/special/url.md) 引擎表中启用/禁用解码/编码路径。

默认为禁用。
## enable_vertical_final {#enable_vertical_final} 

<SettingsInfoBlock type="Bool" default_value="1" />

如果启用，在最终阶段通过标记行以删除并稍后过滤，而不是合并行来删除重复行。
## enable_writes_to_query_cache {#enable_writes_to_query_cache} 

<SettingsInfoBlock type="Bool" default_value="1" />

如果启用，`SELECT` 查询的结果将存储在 [查询缓存](../query-cache.md) 中。

可能的值：

- 0 - 禁用
- 1 - 启用
## enable_zstd_qat_codec {#enable_zstd_qat_codec} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.1"},{"label": "0"},{"label": "Add new ZSTD_QAT codec"}]}]}/>

如果启用，可以使用 ZSTD_QAT 编解码器来压缩列。
## enforce_strict_identifier_format {#enforce_strict_identifier_format} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.10"},{"label": "0"},{"label": "New setting."}]}]}/>

如果启用，仅允许包含字母数字字符和下划线的标识符。
## engine_file_allow_create_multiple_files {#engine_file_allow_create_multiple_files} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用在文件引擎表中每次插入时创建新文件，如果格式具有后缀（`JSON`，`ORC`，`Parquet` 等）。如果启用，在每次插入时将创建一个新文件，其名称遵循此模式：

`data.Parquet` -> `data.1.Parquet` -> `data.2.Parquet` 等。

可能的值：
- 0 — `INSERT` 查询将新数据附加到文件末尾。
- 1 — `INSERT` 查询将创建一个新文件。
## engine_file_empty_if_not_exists {#engine_file_empty_if_not_exists} 

<SettingsInfoBlock type="Bool" default_value="0" />

允许从没有文件的文件引擎表中选择数据。

可能的值：
- 0 — `SELECT` 抛出异常。
- 1 — `SELECT` 返回空结果。
## engine_file_skip_empty_files {#engine_file_skip_empty_files} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用在 [File](../../engines/table-engines/special/file.md) 引擎表中跳过空文件。

可能的值：
- 0 — 如果空文件与请求的格式不兼容，则 `SELECT` 抛出异常。
- 1 — 对于空文件，`SELECT` 返回空结果。
## engine_file_truncate_on_insert {#engine_file_truncate_on_insert} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用在 [File](../../engines/table-engines/special/file.md) 引擎表中插入前截断。

可能的值：
- 0 — `INSERT` 查询将新数据附加到文件末尾。
- 1 — `INSERT` 查询将用新数据替换文件的现有内容。
## engine_url_skip_empty_files {#engine_url_skip_empty_files} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用在 [URL](../../engines/table-engines/special/url.md) 引擎表中跳过空文件。

可能的值：
- 0 — 如果空文件与请求的格式不兼容，则 `SELECT` 抛出异常。
- 1 — 对于空文件，`SELECT` 返回空结果。
## except_default_mode {#except_default_mode} 

<SettingsInfoBlock type="SetOperationMode" default_value="ALL" />

设置 EXCEPT 查询中的默认模式。可能的值：空字符串，'ALL'，'DISTINCT'。如果为空，未指定模式的查询将抛出异常。
## external_storage_connect_timeout_sec {#external_storage_connect_timeout_sec} 

<SettingsInfoBlock type="UInt64" default_value="10" />

连接超时（以秒为单位）。目前仅支持用于 MySQL。
## external_storage_max_read_bytes {#external_storage_max_read_bytes} 

<SettingsInfoBlock type="UInt64" default_value="0" />

在使用外部引擎的表时，在刷写历史数据时最大读取字节数限制。当前仅支持 MySQL 表引擎、数据库引擎和字典。如果等于 0，则此设置被禁用。
## external_storage_max_read_rows {#external_storage_max_read_rows} 

<SettingsInfoBlock type="UInt64" default_value="0" />

在使用外部引擎的表时，当刷写历史数据时，最大读取行数限制。当前仅支持 MySQL 表引擎、数据库引擎和字典。如果等于 0，则此设置被禁用。
## external_storage_rw_timeout_sec {#external_storage_rw_timeout_sec} 

<SettingsInfoBlock type="UInt64" default_value="300" />

读/写超时（以秒为单位）。目前仅支持用于 MySQL。
## external_table_functions_use_nulls {#external_table_functions_use_nulls} 

<SettingsInfoBlock type="Bool" default_value="1" />

定义 [mysql](../../sql-reference/table-functions/mysql.md)， [postgresql](../../sql-reference/table-functions/postgresql.md) 和 [odbc](../../sql-reference/table-functions/odbc.md) 表函数如何使用 Nullable 列。

可能的值：

- 0 — 表函数显式使用 Nullable 列。
- 1 — 表函数隐式使用 Nullable 列。

**用法**

如果设置为 `0`，则表函数不会创建 Nullable 列，并插入默认值而不是 NULL。这也适用于数组内的 NULL 值。
## external_table_strict_query {#external_table_strict_query} 

<SettingsInfoBlock type="Bool" default_value="0" />

如果设置为 true，则禁止将表达式转换为外部表查询的本地过滤器。
## extract_key_value_pairs_max_pairs_per_row {#extract_key_value_pairs_max_pairs_per_row} 

<SettingsInfoBlock type="UInt64" default_value="1000" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.2"},{"label": "0"},{"label": "Max number of pairs that can be produced by the `extractKeyValuePairs` function. Used as a safeguard against consuming too much memory."}]}]}/>

`extractKeyValuePairs` 函数可以产生的最大对数。用于防止消耗过多的内存。
## extremes {#extremes} 

<SettingsInfoBlock type="Bool" default_value="0" />

是否计算极值（查询结果列中的最小值和最大值）。接受 0 或 1。默认为 0（禁用）。
有关更多信息，请参阅“极值”部分。
## fallback_to_stale_replicas_for_distributed_queries {#fallback_to_stale_replicas_for_distributed_queries} 

<SettingsInfoBlock type="Bool" default_value="1" />

如果没有可用的更新数据，则强制查询到过时的副本。请参见 [复制](../../engines/table-engines/mergetree-family/replication.md)。

ClickHouse 从表的过时副本中选择最相关的副本。

在执行从指向复制表的分布式表的 `SELECT` 时使用。

默认为 1（启用）。
## filesystem_cache_boundary_alignment {#filesystem_cache_boundary_alignment} 

<SettingsInfoBlock type="UInt64" default_value="0" />

文件系统缓存边界对齐。此设置仅应用于非磁盘读取（例如，适用于远程表引擎/表函数的缓存，但不适用于 MergeTree 表的存储配置）。值 0 表示没有对齐。
## filesystem_cache_enable_background_download_during_fetch {#filesystem_cache_enable_background_download_during_fetch} 

<CloudAvailableBadge/>

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.11"},{"label": "1"},{"label": "New setting"}]}]}/>

仅在 ClickHouse Cloud 中有效。在文件系统缓存中等待锁定以进行空间预留的时间。
## filesystem_cache_enable_background_download_for_metadata_files_in_packed_storage {#filesystem_cache_enable_background_download_for_metadata_files_in_packed_storage} 

<CloudAvailableBadge/>

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.11"},{"label": "1"},{"label": "New setting"}]}]}/>

仅在 ClickHouse Cloud 中有效。在文件系统缓存中等待锁定以进行空间预留的时间。
## filesystem_cache_max_download_size {#filesystem_cache_max_download_size} 

<SettingsInfoBlock type="UInt64" default_value="137438953472" />

单个查询可以下载的最大远程文件系统缓存大小。
## filesystem_cache_name {#filesystem_cache_name} 

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.10"},{"label": ""},{"label": "Filesystem cache name to use for stateless table engines or data lakes"}]}]}/>

用于无状态表引擎或数据湖的文件系统缓存名称
## filesystem_cache_prefer_bigger_buffer_size {#filesystem_cache_prefer_bigger_buffer_size} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.11"},{"label": "1"},{"label": "New setting"}]}]}/>

如果启用了文件系统缓存，则优先使用更大的缓冲区大小，以避免写入过小的文件片段，这会降低缓存性能。另一方面，启用此设置可能会增加内存使用量。
## filesystem_cache_reserve_space_wait_lock_timeout_milliseconds {#filesystem_cache_reserve_space_wait_lock_timeout_milliseconds} 

<SettingsInfoBlock type="UInt64" default_value="1000" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.3"},{"label": "1000"},{"label": "Wait time to lock cache for sapce reservation in filesystem cache"}]}]}/>

锁定缓存以进行空间保留的等待时间
## filesystem_cache_segments_batch_size {#filesystem_cache_segments_batch_size} 

<SettingsInfoBlock type="UInt64" default_value="20" />

读取缓冲区可以从缓存请求的单个文件片段的批处理大小限制。值过低会导致对缓存的请求过多，值过大可能会减慢从缓存中驱逐的速度。
## filesystem_cache_skip_download_if_exceeds_per_query_cache_write_limit {#filesystem_cache_skip_download_if_exceeds_per_query_cache_write_limit} 

<SettingsInfoBlock type="Bool" default_value="1" />

如果超过查询缓存大小，则跳过从远程文件系统下载
## filesystem_prefetch_max_memory_usage {#filesystem_prefetch_max_memory_usage} 

<SettingsInfoBlock type="NonZeroUInt64" default_value="1073741824" />

预取的最大内存使用量。
## filesystem_prefetch_step_bytes {#filesystem_prefetch_step_bytes} 

<SettingsInfoBlock type="UInt64" default_value="0" />

以字节为单位的预取步长。零表示 `auto` - 自动推断出大约最佳的预取步长，但可能不是 100% 的最佳。由于设置 filesystem_prefetch_min_bytes_for_single_read_task，实际值可能不同。
## filesystem_prefetch_step_marks {#filesystem_prefetch_step_marks} 

<SettingsInfoBlock type="UInt64" default_value="0" />

以标记为单位的预取步长。零表示 `auto` - 自动推断出大约最佳的预取步长，但可能不是 100% 的最佳。由于设置 filesystem_prefetch_min_bytes_for_single_read_task，实际值可能不同。
## filesystem_prefetches_limit {#filesystem_prefetches_limit} 

<SettingsInfoBlock type="UInt64" default_value="200" />

预取的最大数量。零表示无限。如果要限制预取的数量，建议设置 `filesystem_prefetches_max_memory_usage`。
## final {#final} 

<SettingsInfoBlock type="Bool" default_value="0" />

自动将 [FINAL](../../sql-reference/statements/select/from.md/#final-modifier) 修饰符应用于查询中的所有表，对于适用 [FINAL](../../sql-reference/statements/select/from.md/#final-modifier) 的表，包括连接表和子查询中的表，以及分布式表。

可能的值：

- 0 - 禁用
- 1 - 启用

示例：

```sql
CREATE TABLE test
(
    key Int64,
    some String
)
ENGINE = ReplacingMergeTree
ORDER BY key;

INSERT INTO test FORMAT Values (1, 'first');
INSERT INTO test FORMAT Values (1, 'second');

SELECT * FROM test;
┌─key─┬─some───┐
│   1 │ second │
└─────┴────────┘
┌─key─┬─some──┐
│   1 │ first │
└─────┴───────┘

SELECT * FROM test SETTINGS final = 1;
┌─key─┬─some───┐
│   1 │ second │
└─────┴────────┘

SET final = 1;
SELECT * FROM test;
┌─key─┬─some───┐
│   1 │ second │
└─────┴────────┘
```
## flatten_nested {#flatten_nested} 

<SettingsInfoBlock type="Bool" default_value="1" />

设置 [nested](../../sql-reference/data-types/nested-data-structures/index.md) 列的数据格式。

可能的值：

- 1 — 嵌套列被展平为单独的数组。
- 0 — 嵌套列保持为单个元组的数组。

**用法**

如果设置为 `0`，则可以使用任意级别的嵌套。

**示例**

查询：

```sql
SET flatten_nested = 1;
CREATE TABLE t_nest (`n` Nested(a UInt32, b UInt32)) ENGINE = MergeTree ORDER BY tuple();

SHOW CREATE TABLE t_nest;
```

结果：

```text
┌─statement───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ CREATE TABLE default.t_nest
(
    `n.a` Array(UInt32),
    `n.b` Array(UInt32)
)
ENGINE = MergeTree
ORDER BY tuple()
SETTINGS index_granularity = 8192 │
└─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
```

查询：

```sql
SET flatten_nested = 0;

CREATE TABLE t_nest (`n` Nested(a UInt32, b UInt32)) ENGINE = MergeTree ORDER BY tuple();

SHOW CREATE TABLE t_nest;
```

结果：

```text
┌─statement──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ CREATE TABLE default.t_nest
(
    `n` Nested(a UInt32, b UInt32)
)
ENGINE = MergeTree
ORDER BY tuple()
SETTINGS index_granularity = 8192 │
└────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
```
## force_aggregate_partitions_independently {#force_aggregate_partitions_independently} 

<SettingsInfoBlock type="Bool" default_value="0" />

在适用时强制使用优化，但启发式算法决定不使用它。
## force_aggregation_in_order {#force_aggregation_in_order} 

<SettingsInfoBlock type="Bool" default_value="0" />

此设置由服务器本身使用，以支持分布式查询。请勿手动更改，因为这会破坏正常操作。（在分布式聚合期间强制在远程节点上按顺序使用聚合）。
## force_data_skipping_indices {#force_data_skipping_indices} 

如果未使用传递的数据跳过索引，则禁用查询执行。

考虑以下示例：

```sql
CREATE TABLE data
(
    key Int,
    d1 Int,
    d1_null Nullable(Int),
    INDEX d1_idx d1 TYPE minmax GRANULARITY 1,
    INDEX d1_null_idx assumeNotNull(d1_null) TYPE minmax GRANULARITY 1
)
Engine=MergeTree()
ORDER BY key;

SELECT * FROM data_01515;
SELECT * FROM data_01515 SETTINGS force_data_skipping_indices=''; -- query will produce CANNOT_PARSE_TEXT error.
SELECT * FROM data_01515 SETTINGS force_data_skipping_indices='d1_idx'; -- query will produce INDEX_NOT_USED error.
SELECT * FROM data_01515 WHERE d1 = 0 SETTINGS force_data_skipping_indices='d1_idx'; -- Ok.
SELECT * FROM data_01515 WHERE d1 = 0 SETTINGS force_data_skipping_indices='`d1_idx`'; -- Ok (example of full featured parser).
SELECT * FROM data_01515 WHERE d1 = 0 SETTINGS force_data_skipping_indices='`d1_idx`, d1_null_idx'; -- query will produce INDEX_NOT_USED error, since d1_null_idx is not used.
SELECT * FROM data_01515 WHERE d1 = 0 AND assumeNotNull(d1_null) = 0 SETTINGS force_data_skipping_indices='`d1_idx`, d1_null_idx'; -- Ok.
```
## force_grouping_standard_compatibility {#force_grouping_standard_compatibility} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "22.9"},{"label": "1"},{"label": "Make GROUPING function output the same as in SQL standard and other DBMS"}]}]}/>

当参数未用作聚合键时，确保 GROUPING 函数返回 1。
## force_index_by_date {#force_index_by_date} 

<SettingsInfoBlock type="Bool" default_value="0" />

如果索引无法按日期使用，则禁用查询执行。

仅适用于 MergeTree 家族中的表。

如果 `force_index_by_date=1`，则 ClickHouse 检查查询是否具有可以用于限制数据范围的日期键条件。如果没有合适的条件，则抛出异常。但是，它不会检查条件是否减少了要读取的数据量。例如，条件 `Date != '2000-01-01'` 是可以接受的，即使它与表中的所有数据匹配（即，运行查询需要完整扫描）。有关 MergeTree 表中数据范围的更多信息，请参阅 [MergeTree](../../engines/table-engines/mergetree-family/mergetree.md)。
## force_optimize_projection {#force_optimize_projection} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用在 `SELECT` 查询中强制使用 [projections](../../engines/table-engines/mergetree-family/mergetree.md/#projections)，当投影优化已启用时（见 [optimize_use_projections](#optimize_use_projections) 设置）。

可能的值：

- 0 — 投影优化不是强制性的。
- 1 — 投影优化是强制性的。
## force_optimize_projection_name {#force_optimize_projection_name} 

如果设置为非空字符串，检查此投影是否在查询中至少被使用一次。

可能的值：

- 字符串：在查询中使用的投影名称。
## force_optimize_skip_unused_shards {#force_optimize_skip_unused_shards} 

<SettingsInfoBlock type="UInt64" default_value="0" />

启用或禁用查询执行，如果 [optimize_skip_unused_shards](#optimize_skip_unused_shards) 启用且无法跳过未使用的分片。如果跳过不可能并且启用此设置，将抛出异常。

可能的值：

- 0 — 禁用。ClickHouse 不会抛出异常。
- 1 — 启用。仅当表具有分片键时才禁用查询执行。
- 2 — 启用。无论表是否定义了分片键，查询执行均被禁用。
## force_optimize_skip_unused_shards_nesting {#force_optimize_skip_unused_shards_nesting} 

<SettingsInfoBlock type="UInt64" default_value="0" />

控制 [`force_optimize_skip_unused_shards`](#force_optimize_skip_unused_shards)（因此仍然需要 [`force_optimize_skip_unused_shards`](#force_optimize_skip_unused_shards)），取决于分布式查询的嵌套级别（当您有一个 `Distributed` 表查看另一个 `Distributed` 表时）。

可能的值：

- 0 - 禁用，`force_optimize_skip_unused_shards` 始终有效。
- 1 — 仅在第一级启用 `force_optimize_skip_unused_shards`。
- 2 — 在第二级启用 `force_optimize_skip_unused_shards`。
## force_primary_key {#force_primary_key} 

<SettingsInfoBlock type="Bool" default_value="0" />

如果无法按主键索引，则禁用查询执行。

适用于 MergeTree 家族中的表。

如果 `force_primary_key=1`，则 ClickHouse 将检查查询是否具有可用于限制数据范围的主键条件。如果没有合适的条件，则抛出异常。但是，实际上并不检查条件是否减少要读取的数据量。有关 MergeTree 表中数据范围的更多信息，请参阅 [MergeTree](../../engines/table-engines/mergetree-family/mergetree.md)。
## force_remove_data_recursively_on_drop {#force_remove_data_recursively_on_drop} 

<SettingsInfoBlock type="Bool" default_value="0" />

在 DROP 查询中递归删除数据。避免 '目录非空' 错误，但可能会静默删除已分离的数据。
## formatdatetime_e_with_space_padding {#formatdatetime_e_with_space_padding} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.5"},{"label": "0"},{"label": "Improved compatibility with MySQL DATE_FORMAT/STR_TO_DATE"}]}]}/>

格式化器 '%e' 在函数 'formatDateTime' 中打印单数天并加前导空格，例如 ' 2' 而不是 '2'。
## formatdatetime_f_prints_scale_number_of_digits {#formatdatetime_f_prints_scale_number_of_digits} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.1"},{"label": "0"},{"label": "New setting."}]}]}/>

格式化器 '%f' 在函数 'formatDateTime' 中只打印 DateTime64 的数字位数，而不是固定的 6 位数。
## formatdatetime_f_prints_single_zero {#formatdatetime_f_prints_single_zero} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "23.4"},{"label": "0"},{"label": "Improved compatibility with MySQL DATE_FORMAT()/STR_TO_DATE()"}]}]}/>

格式化器 '%f' 在函数 'formatDateTime' 中，如果格式化值没有小数秒，则打印单个零，而不是六个零。
## formatdatetime_format_without_leading_zeros {#formatdatetime_format_without_leading_zeros} 

<SettingsInfoBlock type="Bool" default_value="0" />

格式化器 '%c'，'%l' 和 '%k' 在函数 'formatDateTime' 中打印月份和小时时没有前导零。
## formatdatetime_parsedatetime_m_is_month_name {#formatdatetime_parsedatetime_m_is_month_name} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "23.4"},{"label": "1"},{"label": "Improved compatibility with MySQL DATE_FORMAT/STR_TO_DATE"}]}]}/>

格式化器 '%M' 在函数 'formatDateTime' 和 'parseDateTime' 中打印/解析月份名称而不是分钟。
## fsync_metadata {#fsync_metadata} 

<SettingsInfoBlock type="Bool" default_value="1" />

在写入 `.sql` 文件时启用或禁用 [fsync](http://pubs.opengroup.org/onlinepubs/9699919799/functions/fsync.html)。默认启用。

如果服务器有数百万个不断创建和销毁的小表，禁用它是有意义的。
## function_implementation {#function_implementation} 

选择特定目标或变体（实验性）的函数实现。如果为空，则启用所有实现。
## function_json_value_return_type_allow_complex {#function_json_value_return_type_allow_complex} 

<SettingsInfoBlock type="Bool" default_value="0" />

控制是否允许为 json_value 函数返回复杂类型（例如：结构体、数组、映射）。

```sql
SELECT JSON_VALUE('{"hello":{"world":"!"}}', '$.hello') settings function_json_value_return_type_allow_complex=true

┌─JSON_VALUE('{"hello":{"world":"!"}}', '$.hello')─┐
│ {"world":"!"}                                    │
└──────────────────────────────────────────────────┘

1 row in set. Elapsed: 0.001 sec.
```

可能的值：

- true — 允许。
- false — 不允许。
## function_json_value_return_type_allow_nullable {#function_json_value_return_type_allow_nullable} 

<SettingsInfoBlock type="Bool" default_value="0" />

控制是否允许当 JSON_VALUE 函数的值不存在时返回 `NULL`。

```sql
SELECT JSON_VALUE('{"hello":"world"}', '$.b') settings function_json_value_return_type_allow_nullable=true;

┌─JSON_VALUE('{"hello":"world"}', '$.b')─┐
│ ᴺᵁᴸᴸ                                   │
└────────────────────────────────────────┘

1 row in set. Elapsed: 0.001 sec.
```

可能的值：

- true — 允许。
- false — 不允许。
## function_locate_has_mysql_compatible_argument_order {#function_locate_has_mysql_compatible_argument_order} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.3"},{"label": "1"},{"label": "Increase compatibility with MySQL's locate function."}]}]}/>

控制函数 [locate](../../sql-reference/functions/string-search-functions.md/#locate) 中参数的顺序。

可能的值：

- 0 — 函数 `locate` 接受参数 `(haystack, needle[, start_pos])`。
- 1 — 函数 `locate` 接受参数 `(needle, haystack, [, start_pos])` （与 MySQL 兼容的行为）。
## function_range_max_elements_in_block {#function_range_max_elements_in_block} 

<SettingsInfoBlock type="UInt64" default_value="500000000" />

设置函数 [range](/sql-reference/functions/array-functions#rangeend-rangestart--end--step) 生成的数据量安全阈值。定义函数每个数据块生成的最大值数量（每个块中每行数组大小的总和）。

可能的值：

- 正整数。

**另请参见**

- [max_block_size](#max_block_size)
- [min_insert_block_size_rows](#min_insert_block_size_rows)
## function_sleep_max_microseconds_per_block {#function_sleep_max_microseconds_per_block} 

<SettingsInfoBlock type="UInt64" default_value="3000000" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "23.7"},{"label": "3000000"},{"label": "In previous versions, the maximum sleep time of 3 seconds was applied only for `sleep`, but not for `sleepEachRow` function. In the new version, we introduce this setting. If you set compatibility with the previous versions, we will disable the limit altogether."}]}]}/>

函数 `sleep` 每个块允许睡眠的最大微秒数。如果用户使用更大的值调用它，则会抛出异常。这是一个安全阈值。
## function_visible_width_behavior {#function_visible_width_behavior} 

<SettingsInfoBlock type="UInt64" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.1"},{"label": "1"},{"label": "We changed the default behavior of `visibleWidth` to be more precise"}]}]}/>

`visibleWidth` 行为的版本。0 - 仅计算代码点的数量；1 - 正确计算零宽和组合字符，将全宽字符算作两个，估算制表符宽度，计算删除字符。
## geo_distance_returns_float64_on_float64_arguments {#geo_distance_returns_float64_on_float64_arguments} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.3"},{"label": "1"},{"label": "Increase the default precision."}]}]}/>

如果 `geoDistance`、`greatCircleDistance`、`greatCircleAngle` 函数的所有四个参数都是 Float64，则返回 Float64，并在内部计算中使用双精度。在以前的 ClickHouse 版本中，这些函数始终返回 Float32。
## geotoh3_argument_order {#geotoh3_argument_order} 

<BetaBadge/>

<SettingsInfoBlock type="GeoToH3ArgumentOrder" default_value="lat_lon" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.5"},{"label": "lat_lon"},{"label": "A new setting for legacy behaviour to set lon and lat argument order"}]}]}/>

函数 'geoToH3' 接受(lon, lat)如果设置为'lons_lat'，并接受(lat, lon)如果设置为'lat_lon'。
## glob_expansion_max_elements {#glob_expansion_max_elements} 

<SettingsInfoBlock type="UInt64" default_value="1000" />

允许的最大地址数量（对于外部存储、表函数等）。
## grace_hash_join_initial_buckets {#grace_hash_join_initial_buckets} 

<ExperimentalBadge/>

<SettingsInfoBlock type="NonZeroUInt64" default_value="1" />

初始的 Grace Hash Join 桶的数量。
## grace_hash_join_max_buckets {#grace_hash_join_max_buckets} 

<ExperimentalBadge/>

<SettingsInfoBlock type="NonZeroUInt64" default_value="1024" />

Grace Hash Join 桶的数量限制。
## group_by_overflow_mode {#group_by_overflow_mode} 

<SettingsInfoBlock type="OverflowModeGroupBy" default_value="throw" />

设置当聚合的唯一键数量超过限制时发生的情况：
- `throw`：抛出异常
- `break`：停止执行查询并返回部分结果
- `any`：继续对进入集合的键进行聚合，但不为新键添加到集合中。

使用 'any' 值可以运行 GROUP BY 的近似值。此近似值的质量取决于数据的统计特性。
## group_by_two_level_threshold {#group_by_two_level_threshold} 

从多少键的数量开始，才开始两级聚合。0 - 未设置阈值。
## group_by_two_level_threshold_bytes {#group_by_two_level_threshold_bytes} 

从聚合状态的大小（以字节为单位）开始，才会使用两级聚合。0 - 未设置阈值。当至少一个阈值被触发时使用两级聚合。
## group_by_use_nulls {#group_by_use_nulls} 

<SettingsInfoBlock type="Bool" default_value="0" />

更改 [GROUP BY 子句](/sql-reference/statements/select/group-by) 如何处理聚合键的类型。
当使用 `ROLLUP`、`CUBE` 或 `GROUPING SETS` 指示符时，某些聚合键可能不会用于生成某些结果行。根据此设置，相应行中的这些键的列用默认值或 `NULL` 填充。

可能的值：

- 0 — 使用聚合键类型的默认值来生成缺失值。
- 1 — ClickHouse 以 SQL 标准规定的方式执行 `GROUP BY`。聚合键的类型将转换为 [Nullable](/sql-reference/data-types/nullable)。对于未使用它的行，相应聚合键的列用 [NULL](/sql-reference/syntax#null) 填充。

另请参见：

- [GROUP BY 子句](/sql-reference/statements/select/group-by)
## h3togeo_lon_lat_result_order {#h3togeo_lon_lat_result_order} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.1"},{"label": "0"},{"label": "A new setting"}]}]}/>

函数 'h3ToGeo' 返回 (lon, lat) 如果为真，否则返回 (lat, lon)。
## handshake_timeout_ms {#handshake_timeout_ms} 

<SettingsInfoBlock type="Milliseconds" default_value="10000" />

在握手期间，接收来自副本的 Hello 数据包的超时时间（以毫秒为单位）。
## hdfs_create_new_file_on_insert {#hdfs_create_new_file_on_insert} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用在 HDFS 引擎表中每次插入时创建新文件。如果启用，在每次插入时将创建一个新的 HDFS 文件，其名称类似于以下模式：

初始： `data.Parquet.gz` -> `data.1.Parquet.gz` -> `data.2.Parquet.gz`，等。

可能的值：
- 0 — `INSERT` 查询将新数据附加到文件末尾。
- 1 — `INSERT` 查询将创建新文件。
## hdfs_ignore_file_doesnt_exist {#hdfs_ignore_file_doesnt_exist} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.6"},{"label": "0"},{"label": "Allow to return 0 rows when the requested files don't exist instead of throwing an exception in HDFS table engine"}]}]}/>

在读取某些键时，如果文件不存在则忽略缺失文件。

可能的值：
- 1 — `SELECT` 返回空结果。
- 0 — `SELECT` 抛出异常。
## hdfs_replication {#hdfs_replication} 

<SettingsInfoBlock type="UInt64" default_value="0" />

可以在创建 hdfs 文件时指定实际副本数量。
## hdfs_skip_empty_files {#hdfs_skip_empty_files} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用跳过 [HDFS](../../engines/table-engines/integrations/hdfs.md) 引擎表中的空文件。

可能的值：
- 0 — 如果空文件与请求的格式不兼容，则 `SELECT` 抛出异常。
- 1 — `SELECT` 对于空文件返回空结果。
## hdfs_throw_on_zero_files_match {#hdfs_throw_on_zero_files_match} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.6"},{"label": "0"},{"label": "Allow to throw an error when ListObjects request cannot match any files in HDFS engine instead of empty query result"}]}]}/>

如果根据模式扩展规则匹配了零个文件，则抛出错误。

可能的值：
- 1 — `SELECT` 抛出异常。
- 0 — `SELECT` 返回空结果。
## hdfs_truncate_on_insert {#hdfs_truncate_on_insert} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用在 HDFS 引擎表中插入前截断。如果禁用，则在尝试插入时如果 HDFS 中已存在文件，将抛出异常。

可能的值：
- 0 — `INSERT` 查询将新数据附加到文件末尾。
- 1 — `INSERT` 查询将用新数据替换文件的现有内容。
## hedged_connection_timeout_ms {#hedged_connection_timeout_ms} 

<SettingsInfoBlock type="Milliseconds" default_value="50" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "23.4"},{"label": "50"},{"label": "Start new connection in hedged requests after 50 ms instead of 100 to correspond with previous connect timeout"}]}]}/>

用于建立与副本的连接的超时时间（以毫秒为单位）以进行 Hedged 请求。
## hnsw_candidate_list_size_for_search {#hnsw_candidate_list_size_for_search} 

<BetaBadge/>

<SettingsInfoBlock type="UInt64" default_value="256" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.10"},{"label": "256"},{"label": "New setting. Previously, the value was optionally specified in CREATE INDEX and 64 by default."}]}]}/>

在搜索向量相似性索引时，动态候选列表的大小，也称为 'ef_search'。
## hsts_max_age {#hsts_max_age} 

过期 HSTS 的时间。0 意味着禁用 HSTS。
## http_connection_timeout {#http_connection_timeout} 

HTTP 连接超时（以秒为单位）。

可能的值：

- 任何正整数。
- 0 - 禁用（无限超时）。
## http_headers_progress_interval_ms {#http_headers_progress_interval_ms} 

不应比每个指定间隔更频繁地发送 HTTP 头 X-ClickHouse-Progress。
## http_make_head_request {#http_make_head_request} 

`http_make_head_request` 设置允许在从 HTTP 读取数据时执行 `HEAD` 请求以检索要读取的文件的信息，例如其大小。由于默认启用，因此在服务器不支持 `HEAD` 请求的情况下，可能希望禁用此设置。
## http_max_field_name_size {#http_max_field_name_size} 

HTTP 头中字段名称的最大长度。
## http_max_field_value_size {#http_max_field_value_size} 

HTTP 头中字段值的最大长度。
## http_max_fields {#http_max_fields} 

HTTP 头中字段的最大数量。
## http_max_multipart_form_data_size {#http_max_multipart_form_data_size} 

multipart/form-data 内容的大小限制。此设置不能从 URL 参数进行解析，应在用户配置文件中设置。请注意，在开始查询执行之前，内容被解析并在内存中创建外部表。并且这是在该阶段唯一有效的限制（在读取 HTTP 表单数据时，最大内存使用量和最大执行时间的限制没有影响）。
## http_max_request_param_data_size {#http_max_request_param_data_size} 

预定义 HTTP 请求中用作查询参数的请求数据的大小限制。
## http_max_tries {#http_max_tries} 

通过 http 读取的最大尝试次数。
## http_max_uri_size {#http_max_uri_size} 

设置 HTTP 请求的最大 URI 长度。

可能的值：

- 正整数。
## http_native_compression_disable_checksumming_on_decompress {#http_native_compression_disable_checksumming_on_decompress} 

启用或禁用在解压缩来自客户端的 HTTP POST 数据时进行校验和验证。仅用于 ClickHouse 原生压缩格式（不用于 `gzip` 或 `deflate`）。

有关更多信息，请参阅 [HTTP 接口描述](../../interfaces/http.md)。

可能的值：

- 0 — 禁用。
- 1 — 启用。
## http_receive_timeout {#http_receive_timeout} 

<SettingsInfoBlock type="Seconds" default_value="30" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "23.6"},{"label": "30"},{"label": "See http_send_timeout."}]}]}/>

HTTP 接收超时（以秒为单位）。

可能的值：

- 任何正整数。
- 0 - 禁用（无限超时）。
## http_response_buffer_size {#http_response_buffer_size} 

在向客户端发送 HTTP 响应或冲刷到磁盘（当 http_wait_end_of_query 启用时）之前，在服务器内存中缓冲的字节数。
## http_response_headers {#http_response_headers} 

<SettingsInfoBlock type="Map" default_value="{}" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.12"},{"label": ""},{"label": "New setting."}]}]}/>

允许添加或覆盖服务器将在成功查询结果的响应中返回的 HTTP 头。这仅影响 HTTP 接口。

如果头已默认设置，则提供的值将覆盖它。
如果头未默认设置，则将添加到头列表中。
默认由服务器设置且未被此设置覆盖的头将保持不变。

该设置允许您将头设置为常量值。目前没有办法将头设置为动态计算的值。

名称或值都不能包含 ASCII 控制字符。

如果您实现一个 UI 应用程序，允许用户修改设置，但同时根据返回的头做出决策，则建议将此设置限制为只读。

示例： `SET http_response_headers = '{"Content-Type": "image/png"}'`
## http_retry_initial_backoff_ms {#http_retry_initial_backoff_ms} 

重试通过 http 读取时的初始回退的最小毫秒数。
## http_retry_max_backoff_ms {#http_retry_max_backoff_ms} 

重试通过 http 读取时的最大回退的毫秒数。
## http_send_timeout {#http_send_timeout} 

<SettingsInfoBlock type="Seconds" default_value="30" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "23.6"},{"label": "30"},{"label": "3 minutes seems crazy long. Note that this is timeout for a single network write call, not for the whole upload operation."}]}]}/>

HTTP 发送超时（以秒为单位）。

可能的值：

- 任何正整数。
- 0 - 禁用（无限超时）。

:::note
仅适用于默认配置文件。需要重启服务器以使更改生效。
:::
## http_skip_not_found_url_for_globs {#http_skip_not_found_url_for_globs} 

跳过具有 HTTP_NOT_FOUND 错误的 glob 的 URL。
## http_wait_end_of_query {#http_wait_end_of_query} 

启用服务器端的 HTTP 响应缓冲。
## http_write_exception_in_output_format {#http_write_exception_in_output_format} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "23.9"},{"label": "1"},{"label": "Output valid JSON/XML on exception in HTTP streaming."}]}]}/>

以输出格式写入异常以生成有效的输出。适用于 JSON 和 XML 格式。
## http_zlib_compression_level {#http_zlib_compression_level} 

设置对 HTTP 请求的响应中数据压缩的级别，如果 [enable_http_compression = 1](#enable_http_compression)。

可能的值：数字从 1 到 9。
## iceberg_snapshot_id {#iceberg_snapshot_id} 

<SettingsInfoBlock type="Int64" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.4"},{"label": "0"},{"label": "New setting."}]}]}/>

使用特定快照 ID 查询 Iceberg 表。
## iceberg_timestamp_ms {#iceberg_timestamp_ms} 

<SettingsInfoBlock type="Int64" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.4"},{"label": "0"},{"label": "New setting."}]}]}/>

使用特定时间戳时查询 Iceberg 表。
## idle_connection_timeout {#idle_connection_timeout} 

超时时间以关闭空闲 TCP 连接，单位为指定的秒数。

可能的值：

- 正整数（0 - 立即关闭，0 秒后）。
## ignore_cold_parts_seconds {#ignore_cold_parts_seconds} 

<CloudAvailableBadge/>

<SettingsInfoBlock type="Int64" default_value="0" />

仅在 ClickHouse Cloud 中有效。在 SELECT 查询中排除新数据部分，直到它们被预热（请参阅 [cache_populated_by_fetch](merge-tree-settings.md/#cache_populated_by_fetch)）或达到该多少秒的老旧。仅适用于 Replicated-/SharedMergeTree。
## ignore_data_skipping_indices {#ignore_data_skipping_indices} 

如果查询中使用了跳过索引，则忽略指定的跳过索引。

考虑以下示例：

```sql
CREATE TABLE data
(
    key Int,
    x Int,
    y Int,
    INDEX x_idx x TYPE minmax GRANULARITY 1,
    INDEX y_idx y TYPE minmax GRANULARITY 1,
    INDEX xy_idx (x,y) TYPE minmax GRANULARITY 1
)
Engine=MergeTree()
ORDER BY key;

INSERT INTO data VALUES (1, 2, 3);

SELECT * FROM data;
SELECT * FROM data SETTINGS ignore_data_skipping_indices=''; -- query will produce CANNOT_PARSE_TEXT error.
SELECT * FROM data SETTINGS ignore_data_skipping_indices='x_idx'; -- Ok.
SELECT * FROM data SETTINGS ignore_data_skipping_indices='na_idx'; -- Ok.

SELECT * FROM data WHERE x = 1 AND y = 1 SETTINGS ignore_data_skipping_indices='xy_idx',force_data_skipping_indices='xy_idx' ; -- query will produce INDEX_NOT_USED error, since xy_idx is explicitly ignored.
SELECT * FROM data WHERE x = 1 AND y = 2 SETTINGS ignore_data_skipping_indices='xy_idx';
```

未忽略任何索引的查询：
```sql
EXPLAIN indexes = 1 SELECT * FROM data WHERE x = 1 AND y = 2;

Expression ((Projection + Before ORDER BY))
  Filter (WHERE)
    ReadFromMergeTree (default.data)
    Indexes:
      PrimaryKey
        Condition: true
        Parts: 1/1
        Granules: 1/1
      Skip
        Name: x_idx
        Description: minmax GRANULARITY 1
        Parts: 0/1
        Granules: 0/1
      Skip
        Name: y_idx
        Description: minmax GRANULARITY 1
        Parts: 0/0
        Granules: 0/0
      Skip
        Name: xy_idx
        Description: minmax GRANULARITY 1
        Parts: 0/0
        Granules: 0/0
```

忽略 `xy_idx` 索引：
```sql
EXPLAIN indexes = 1 SELECT * FROM data WHERE x = 1 AND y = 2 SETTINGS ignore_data_skipping_indices='xy_idx';

Expression ((Projection + Before ORDER BY))
  Filter (WHERE)
    ReadFromMergeTree (default.data)
    Indexes:
      PrimaryKey
        Condition: true
        Parts: 1/1
        Granules: 1/1
      Skip
        Name: x_idx
        Description: minmax GRANULARITY 1
        Parts: 0/1
        Granules: 0/1
      Skip
        Name: y_idx
        Description: minmax GRANULARITY 1
        Parts: 0/0
        Granules: 0/0
```

适用于 MergeTree 家族中的表。
## ignore_drop_queries_probability {#ignore_drop_queries_probability} 

<SettingsInfoBlock type="Float" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.4"},{"label": "0"},{"label": "Allow to ignore drop queries in server with specified probability for testing purposes"}]}]}/>

如果启用，服务器将以指定概率忽略所有 DROP 表查询（对于内存和 JOIN 引擎，将将 DROP 替换为 TRUNCATE）。用于测试目的。
## ignore_materialized_views_with_dropped_target_table {#ignore_materialized_views_with_dropped_target_table} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.1"},{"label": "0"},{"label": "Add new setting to allow to ignore materialized views with dropped target table"}]}]}/>

忽略在推送视图时已删除目标表的物化视图。
## ignore_on_cluster_for_replicated_access_entities_queries {#ignore_on_cluster_for_replicated_access_entities_queries} 

忽略用于管理复制访问实体查询的 ON CLUSTER 子句。
## ignore_on_cluster_for_replicated_named_collections_queries {#ignore_on_cluster_for_replicated_named_collections_queries} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.7"},{"label": "0"},{"label": "Ignore ON CLUSTER clause for replicated named collections management queries."}]}]}/>

忽略用于管理复制命名集合查询的 ON CLUSTER 子句。
## ignore_on_cluster_for_replicated_udf_queries {#ignore_on_cluster_for_replicated_udf_queries} 

<SettingsInfoBlock type="Bool" default_value="0" />

忽略用于管理复制 UDF 查询的 ON CLUSTER 子句。
## implicit_select {#implicit_select} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.10"},{"label": "0"},{"label": "A new setting."}]}]}/>

允许在没有前导 SELECT 关键字的情况下编写简单的 SELECT 查询，使其适合计算器风格的用法，例如 `1 + 2` 成为有效查询。

在 `clickhouse-local` 中默认启用，可以明确禁用。
## implicit_table_at_top_level {#implicit_table_at_top_level} 

<SettingsInfoBlock type="Bool" default_value="1" />

如果不为空，顶层没有 FROM 的查询将从此表中读取，而不是从 system.one 中读取。

这用于 clickhouse-local 进行输入数据处理。
该设置可以由用户显式设置，但不打算用于此类型的用法。

子查询不受此设置影响（无论是标量、FROM 还是 IN 子查询）。
UNION、INTERSECT、EXCEPT 链的顶层 SELECT 被统一处理，受此设置影响，无论其在括号中的分组情况。
该设置对视图和分布式查询的影响未明确说明。

该设置接受表名（然后从当前数据库解析表）或以 'database.table' 形式的合格名称。
数据库和表名称必须是未加引号的 - 仅允许简单标识符。
## implicit_transaction {#implicit_transaction} 

<ExperimentalBadge/>

<SettingsInfoBlock type="Bool" default_value="0" />

如果启用且尚未处于事务中，则将查询包装在完整事务中（开始 + 提交或回滚）。
## input_format_parallel_parsing {#input_format_parallel_parsing} 

<SettingsInfoBlock type="Bool" default_value="1" />

启用或禁用数据格式的保持顺序的并行解析。仅支持 [TSV](../../interfaces/formats.md/#tabseparated)、[TSKV](../../interfaces/formats.md/#tskv)、[CSV](../../interfaces/formats.md/#csv) 和 [JSONEachRow](../../interfaces/formats.md/#jsoneachrow) 格式。

可能的值：

- 1 — 启用。
- 0 — 禁用。
## insert_allow_materialized_columns {#insert_allow_materialized_columns} 

<SettingsInfoBlock type="Bool" default_value="0" />

如果设置启用，允许在 INSERT 中使用物化列。
## insert_deduplicate {#insert_deduplicate} 

<SettingsInfoBlock type="Bool" default_value="1" />

启用或禁用 `INSERT` 的块去重（适用于 Replicated* 表）。

可能的值：

- 0 — 禁用。
- 1 — 启用。

默认情况下，通过 `INSERT` 语句插入到复制表的块会被去重（请参阅 [数据复制](../../engines/table-engines/mergetree-family/replication.md)）。
对于复制表，默认情况下仅去重每个分区中最近的 100 个块（请参阅 [replicated_deduplication_window](merge-tree-settings.md/#replicated_deduplication_window)、[replicated_deduplication_window_seconds](merge-tree-settings.md/#replicated_deduplication_window_seconds)）。
对于非复制表，请参阅 [non_replicated_deduplication_window](merge-tree-settings.md/#non_replicated_deduplication_window)。
## insert_deduplication_token {#insert_deduplication_token} 

该设置允许用户在 MergeTree/ReplicatedMergeTree 中提供自己的去重语义。
例如，通过在每个 INSERT 语句中提供一个唯一值，该用户可以避免相同插入的数据被去重。

可能的值：

- 任何字符串

`insert_deduplication_token` 仅在不为空时用于去重。

对于复制表，默认情况下仅去重每个分区中最近的 100 个插入（请参阅 [replicated_deduplication_window](merge-tree-settings.md/#replicated_deduplication_window)、[replicated_deduplication_window_seconds](merge-tree-settings.md/#replicated_deduplication_window_seconds)）。
对于非复制表，请参阅 [non_replicated_deduplication_window](merge-tree-settings.md/#non_replicated_deduplication_window)。

:::note
`insert_deduplication_token` 在分区级别工作（与 `insert_deduplication` 校验和相同）。多个分区可以具有相同的 `insert_deduplication_token`。
:::

示例：

```sql
CREATE TABLE test_table
( A Int64 )
ENGINE = MergeTree
ORDER BY A
SETTINGS non_replicated_deduplication_window = 100;

INSERT INTO test_table SETTINGS insert_deduplication_token = 'test' VALUES (1);

-- the next insert won't be deduplicated because insert_deduplication_token is different
INSERT INTO test_table SETTINGS insert_deduplication_token = 'test1' VALUES (1);

-- the next insert will be deduplicated because insert_deduplication_token
-- is the same as one of the previous
INSERT INTO test_table SETTINGS insert_deduplication_token = 'test' VALUES (2);

SELECT * FROM test_table

┌─A─┐
│ 1 │
└───┘
┌─A─┐
│ 1 │
└───┘
```
## insert_keeper_fault_injection_probability {#insert_keeper_fault_injection_probability} 

<SettingsInfoBlock type="Float" default_value="0" />

插入过程中 keeper 请求的故障概率。有效值在区间 [0.0f, 1.0f] 内。
## insert_keeper_fault_injection_seed {#insert_keeper_fault_injection_seed} 

<SettingsInfoBlock type="UInt64" default_value="0" />

0 - 随机种子，否则为设置值。
## insert_keeper_max_retries {#insert_keeper_max_retries} 

<SettingsInfoBlock type="UInt64" default_value="20" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "23.2"},{"label": "20"},{"label": "Enable reconnections to Keeper on INSERT, improve reliability"}]}]}/>

该设置指定在插入到复制的 MergeTree 时，对于 ClickHouse Keeper（或 ZooKeeper）请求的最大重试次数。仅考虑由于网络错误、Keeper 会话超时或请求超时而失败的 Keeper 请求进行重试。

可能的值：

- 正整数。
- 0 — 禁用重试。

云默认值：`20`。

Keeper 请求的重试是在某个超时后进行的。超时由以下设置控制：`insert_keeper_retry_initial_backoff_ms`，`insert_keeper_retry_max_backoff_ms`。第一次重试在 `insert_keeper_retry_initial_backoff_ms` 超时后进行。随后的超时将按以下方式计算：
```
timeout = min(insert_keeper_retry_max_backoff_ms, latest_timeout * 2)
```

例如，如果 `insert_keeper_retry_initial_backoff_ms=100`，`insert_keeper_retry_max_backoff_ms=10000` 和 `insert_keeper_max_retries=8`，则超时将为 `100, 200, 400, 800, 1600, 3200, 6400, 10000`。

除了容错能力，重试还旨在提供更好的用户体验——它们可以避免在执行 INSERT 时因 Keeper 重启而返回错误，例如由于升级。
## insert_keeper_retry_initial_backoff_ms {#insert_keeper_retry_initial_backoff_ms} 

<SettingsInfoBlock type="UInt64" default_value="100" />

在 INSERT 查询执行期间重试失败的 Keeper 请求的初始超时（以毫秒为单位）。

可能的值：

- 正整数。
- 0 — 没有超时。
## insert_keeper_retry_max_backoff_ms {#insert_keeper_retry_max_backoff_ms} 

<SettingsInfoBlock type="UInt64" default_value="10000" />

在 INSERT 查询执行期间重试失败的 Keeper 请求的最大超时（以毫秒为单位）。

可能的值：

- 正整数。
- 0 — 最大超时不受限制。
## insert_null_as_default {#insert_null_as_default} 

<SettingsInfoBlock type="Bool" default_value="1" />

启用或禁用在不为 [nullable](/sql-reference/data-types/nullable) 数据类型的列中插入默认值而不是 [NULL](/sql-reference/syntax#null)。如果列类型不可为 null 并且此设置被禁用，则插入 `NULL` 会导致异常。如果列类型是可为 null 的，则无论此设置如何，`NULL` 值将按原样插入。

该设置适用于 [INSERT ... SELECT](../../sql-reference/statements/insert-into.md/#inserting-the-results-of-select) 查询。请注意，`SELECT` 子查询可以与 `UNION ALL` 子句连接。

可能的值：

- 0 — 向不可为 null 的列插入 `NULL` 会导致异常。
- 1 — 默认列值被插入而不是 `NULL`。
## insert_quorum {#insert_quorum} 

<SettingsInfoBlock type="UInt64Auto" default_value="0" />

:::note
该设置不适用于 SharedMergeTree，更多信息请参见 [SharedMergeTree 一致性](/cloud/reference/shared-merge-tree#consistency)。
:::

启用法定写入。

- 如果 `insert_quorum < 2`，法定写入被禁用。
- 如果 `insert_quorum >= 2`，法定写入被启用。
- 如果 `insert_quorum = 'auto'`，使用多数（`number_of_replicas / 2 + 1`）作为法定数字。

法定写入

`INSERT` 仅在 ClickHouse 成功将数据正确写入 `insert_quorum` 的副本并且在 `insert_quorum_timeout` 内部时才成功。如果由于任何原因，成功写入的副本数量未达到 `insert_quorum`，则写入被视为失败，并且 ClickHouse 将从所有已写入数据的副本中删除插入的块。

当 `insert_quorum_parallel` 被禁用时，法定副本是一致的，即它们包含所有先前 `INSERT` 查询的数据（`INSERT` 序列是线性的）。在使用 `insert_quorum` 和 `insert_quorum_parallel` 被禁用时，您可以通过使用 [select_sequential_consistency](#select_sequential_consistency) 启用 `SELECT` 查询的顺序一致性。

ClickHouse 会生成异常：

- 如果在查询时可用副本的数量少于 `insert_quorum`。
- 当 `insert_quorum_parallel` 被禁用并尝试写入数据，而前一个块尚未在 `insert_quorum` 的副本中插入时。如果用户在前一个 `INSERT` 查询完成之前尝试对同一表执行另一个 `INSERT` 查询，则可能会发生这种情况。

另请参见：

- [insert_quorum_timeout](#insert_quorum_timeout)
- [insert_quorum_parallel](#insert_quorum_parallel)
- [select_sequential_consistency](#select_sequential_consistency)
## insert_quorum_parallel {#insert_quorum_parallel} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "21.1"},{"label": "1"},{"label": "Use parallel quorum inserts by default. It is significantly more convenient to use than sequential quorum inserts"}]}]}/>

:::note
该设置不适用于 SharedMergeTree，更多信息请参见 [SharedMergeTree 一致性](/cloud/reference/shared-merge-tree#consistency)。
:::

启用或禁用法定 `INSERT` 查询的并行性。如果启用，可以在先前的查询尚未完成时发送额外的 `INSERT` 查询。如果禁用，对同一表的额外写入将被拒绝。

可能的值：

- 0 — 禁用。
- 1 — 启用。

另请参见：

- [insert_quorum](#insert_quorum)
- [insert_quorum_timeout](#insert_quorum_timeout)
- [select_sequential_consistency](#select_sequential_consistency)
## insert_quorum_timeout {#insert_quorum_timeout} 

<SettingsInfoBlock type="Milliseconds" default_value="600000" />

在法定超时中写入的超时（以毫秒为单位）。如果超时已过并且尚未发生写入，ClickHouse 将生成异常，客户端必须重复查询以将相同块写入相同或任何其他副本。

另请参见：

- [insert_quorum](#insert_quorum)
- [insert_quorum_parallel](#insert_quorum_parallel)
- [select_sequential_consistency](#select_sequential_consistency)
## insert_shard_id {#insert_shard_id} 

<SettingsInfoBlock type="UInt64" default_value="0" />

如果不为 `0`，指定要同步插入数据的 [Distributed](/engines/table-engines/special/distributed) 表的分片。

如果 `insert_shard_id` 值不正确，服务器将抛出异常。

要获取 `requested_cluster` 中的分片数量，可以检查服务器配置或使用以下查询：

```sql
SELECT uniq(shard_num) FROM system.clusters WHERE cluster = 'requested_cluster';
```

可能的值：

- 0 — 禁用。
- 从 `1` 到对应 [Distributed](/engines/table-engines/special/distributed) 表的 `shards_num` 的任何数字。

**示例**

查询：

```sql
CREATE TABLE x AS system.numbers ENGINE = MergeTree ORDER BY number;
CREATE TABLE x_dist AS x ENGINE = Distributed('test_cluster_two_shards_localhost', currentDatabase(), x);
INSERT INTO x_dist SELECT * FROM numbers(5) SETTINGS insert_shard_id = 1;
SELECT * FROM x_dist ORDER BY number ASC;
```

结果：

```text
┌─number─┐
│      0 │
│      0 │
│      1 │
│      1 │
│      2 │
│      2 │
│      3 │
│      3 │
│      4 │
│      4 │
└────────┘
```
## interactive_delay {#interactive_delay} 

<SettingsInfoBlock type="UInt64" default_value="100000" />

检查请求执行是否已取消并发送进度的间隔（以微秒为单位）。
## intersect_default_mode {#intersect_default_mode} 

<SettingsInfoBlock type="SetOperationMode" default_value="ALL" />

在 INTERSECT 查询中设置默认模式。可能的值：空字符串、'ALL'、'DISTINCT'。如果为空，未指定模式的查询将抛出异常。
## join_algorithm {#join_algorithm} 

<SettingsInfoBlock type="JoinAlgorithm" default_value="direct,parallel_hash,hash" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.12"},{"label": "direct,parallel_hash,hash"},{"label": "'default' was deprecated in favor of explicitly specified join algorithms, also parallel_hash is now preferred over hash"}]}]}/>

指定使用的 [JOIN](../../sql-reference/statements/select/join.md) 算法。

可以指定多个算法，具体查询将根据类型/严格性和表引擎选择可用的算法。

可能的值：

- grace_hash

 [Grace hash join](https://en.wikipedia.org/wiki/Hash_join#Grace_hash_join) 被使用。Grace hash 提供了一种算法选项，可提供高性能的复杂联接，同时限制内存使用。

 Grace 连接的第一阶段读取右表并根据键列的哈希值将其拆分为 N 个存储桶（最初，N 为 `grace_hash_join_initial_buckets`）。这样做是为了确保每个存储桶可以独立处理。第一个存储桶的行被添加到内存中的哈希表中，而其他的则保存在磁盘上。如果哈希表超过内存限制（例如，由 [`max_bytes_in_join`](/operations/settings/settings#max_bytes_in_join) 设置的限制），则增加存储桶的数量并重新分配每行的存储桶。任何不属于当前存储桶的行都会被刷新并重新分配。

 支持 `INNER/LEFT/RIGHT/FULL ALL/ANY JOIN`。

- hash

 使用 [Hash join algorithm](https://en.wikipedia.org/wiki/Hash_join)。最通用的实现，支持所有类型和严格性及多个联接键的组合，在 `JOIN ON` 部分用 `OR` 连接。

 使用 `hash` 算法时，`JOIN` 的右部分被加载到 RAM 中。

- parallel_hash

 一种 `hash` 连接的变化，它将数据拆分为存储桶，并同时构建多个哈希表，以加快此过程。

 使用 `parallel_hash` 算法时，`JOIN` 的右部分被加载到 RAM 中。

- partial_merge

 一种 [sort-merge algorithm](https://en.wikipedia.org/wiki/Sort-merge_join) 的变化，其中仅完全排序右表。

 `RIGHT JOIN` 和 `FULL JOIN` 仅支持 `ALL` 严格性（`SEMI`、`ANTI`、`ANY` 和 `ASOF` 不受支持）。

 使用 `partial_merge` 算法时，ClickHouse 对数据进行排序并将其转储到磁盘。ClickHouse 中的 `partial_merge` 算法与经典实现略有不同。首先，ClickHouse 按连接键对右表进行排序并将其分块，然后为已排序块创建最小-最大索引。然后，它按 `join key` 对左表的部分进行排序并与右表进行连接。最小-最大索引也用于跳过不需要的右表块。

- direct

 该算法可以应用于右表支持键值请求的储存。

 `direct` 算法通过使用左表的行作为键来查找右表。仅由特殊存储支持，例如 [Dictionary](/engines/table-engines/special/dictionary) 或 [EmbeddedRocksDB](../../engines/table-engines/integrations/embedded-rocksdb.md)，并且仅支持 `LEFT` 和 `INNER` JOIN。

- auto

 设置为 `auto` 时，首先尝试 `hash` 连接，如果违反内存限制，则会动态切换到其他算法。

- full_sorting_merge

 [Sort-merge algorithm](https://en.wikipedia.org/wiki/Sort-merge_join) 在连接之前对连接的表进行完全排序。

- prefer_partial_merge

 如果可能，ClickHouse 始终尝试使用 `partial_merge` 连接，否则使用 `hash`。*已弃用*，与 `partial_merge,hash` 相同。

- default (deprecated)

 旧值，请不要再使用。
 与 `direct,hash` 相同，即尝试按此顺序使用直连和哈希连接。
## join_any_take_last_row {#join_any_take_last_row} 

<SettingsInfoBlock type="Bool" default_value="0" />

更改 `ANY` 严格性的连接操作的行为。

:::note
此设置仅适用于具有 [Join](../../engines/table-engines/special/join.md) 引擎表的 `JOIN` 操作。
:::

可能的值：

- 0 — 如果右表有多于一条匹配的行，则仅连接找到的第一行。
- 1 — 如果右表有多于一条匹配的行，则仅连接找到的最后一行。

另请参见：

- [JOIN clause](/sql-reference/statements/select/join)
- [Join table engine](../../engines/table-engines/special/join.md)
- [join_default_strictness](#join_default_strictness)
## join_default_strictness {#join_default_strictness} 

<SettingsInfoBlock type="JoinStrictness" default_value="ALL" />

为 [JOIN clauses](/sql-reference/statements/select/join) 设置默认严格性。

可能的值：

- `ALL` — 如果右表有多个匹配行，ClickHouse 会从匹配行中创建 [Cartesian product](https://en.wikipedia.org/wiki/Cartesian_product)。这是标准 SQL 中正常的 `JOIN` 行为。
- `ANY` — 如果右表有多个匹配行，仅连接找到的第一行。如果右表只有一行匹配行，则 `ANY` 和 `ALL` 的结果相同。
- `ASOF` — 用于连接匹配不确定的序列。
- `空字符串` — 如果查询中未指定 `ALL` 或 `ANY`，ClickHouse 将抛出异常。
## join_on_disk_max_files_to_merge {#join_on_disk_max_files_to_merge} 

<SettingsInfoBlock type="UInt64" default_value="64" />

限制在磁盘上执行 MergeJoin 操作时允许的并行排序文件的数量。

该设置值越大，使用的 RAM 就越多，磁盘 I/O 所需的数量就越少。

可能的值：

- 从 2 开始的任何正整数。
## join_output_by_rowlist_perkey_rows_threshold {#join_output_by_rowlist_perkey_rows_threshold} 

<SettingsInfoBlock type="UInt64" default_value="5" />

右表中每个键的平均行数的下限，以决定是否在哈希连接中通过行列表输出。
## join_overflow_mode {#join_overflow_mode} 

定义 ClickHouse 在达到以下任一连接限制时执行的操作：

- [max_bytes_in_join](/operations/settings/settings#max_bytes_in_join)
- [max_rows_in_join](/operations/settings/settings#max_rows_in_join)

可能的值：

- `THROW` — ClickHouse 抛出异常并中断操作。
- `BREAK` — ClickHouse 中断操作并且不抛出异常。

默认值：`THROW`。

**另请参见**

- [JOIN clause](/sql-reference/statements/select/join)
- [Join table engine](/engines/table-engines/special/join)
## join_to_sort_maximum_table_rows {#join_to_sort_maximum_table_rows} 

<ExperimentalBadge/>

<SettingsInfoBlock type="UInt64" default_value="10000" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.9"},{"label": "10000"},{"label": "The maximum number of rows in the right table to determine whether to rerange the right table by key in left or inner join"}]}]}/>

右表中行的最大数量，以确定在左连接或内连接时是否重新排列右表。
## join_to_sort_minimum_perkey_rows {#join_to_sort_minimum_perkey_rows} 

<ExperimentalBadge/>

<SettingsInfoBlock type="UInt64" default_value="40" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.9"},{"label": "40"},{"label": "The lower limit of per-key average rows in the right table to determine whether to rerange the right table by key in left or inner join. This setting ensures that the optimization is not applied for sparse table keys"}]}]}/>

右表中每个键的平均行数的下限，以确定在左连接或内连接时是否重新排列右表。该设置确保此优化不应用于稀疏表键。
## join_use_nulls {#join_use_nulls} 

<SettingsInfoBlock type="Bool" default_value="0" />

设置 [JOIN](../../sql-reference/statements/select/join.md) 行为的类型。在合并表时，可能会出现空单元格。ClickHouse 根据此设置以不同的方式填充这些单元格。

可能的值：

- 0 — 空单元格使用相应字段类型的默认值填充。
- 1 — `JOIN` 的行为与标准 SQL 相同。相应字段的类型被转换为 [Nullable](/sql-reference/data-types/nullable)，空单元格填充为 [NULL](/sql-reference/syntax)。
## joined_subquery_requires_alias {#joined_subquery_requires_alias} 

<SettingsInfoBlock type="Bool" default_value="1" />

强制连接子查询和表函数具有别名以确保正确的名称资格。
## kafka_disable_num_consumers_limit {#kafka_disable_num_consumers_limit} 

禁用基于可用 CPU 核心数的 kafka_num_consumers 数量限制。
## kafka_max_wait_ms {#kafka_max_wait_ms} 

从 [Kafka](/engines/table-engines/integrations/kafka) 读取消息之前的等待时间（以毫秒为单位）。

可能的值：

- 正整数。
- 0 — 无限超时。

另请参见：

- [Apache Kafka](https://kafka.apache.org/)
## keeper_map_strict_mode {#keeper_map_strict_mode} 

在 KeeperMap 操作期间强制执行额外检查。例如，对于已存在的键插入时抛出异常。
## keeper_max_retries {#keeper_max_retries} 

<SettingsInfoBlock type="UInt64" default_value="10" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.3"},{"label": "10"},{"label": "Max retries for general keeper operations"}]}]}/>

一般 keeper 操作的最大重试次数。
## keeper_retry_initial_backoff_ms {#keeper_retry_initial_backoff_ms} 

<SettingsInfoBlock type="UInt64" default_value="100" />

一般 keeper 操作的初始回退超时。
## keeper_retry_max_backoff_ms {#keeper_retry_max_backoff_ms} 

<SettingsInfoBlock type="UInt64" default_value="5000" />

一般 keeper 操作的最大回退超时。
## least_greatest_legacy_null_behavior {#least_greatest_legacy_null_behavior} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.12"},{"label": "0"},{"label": "New setting"}]}]}/>

如果启用，函数 'least' 和 'greatest' 如果其任一参数为 NULL，则返回 NULL。
## legacy_column_name_of_tuple_literal {#legacy_column_name_of_tuple_literal} 

<SettingsInfoBlock type="Bool" default_value="0" />

将大型元组文字中所有元素的名称列出，而不是使用哈希作为其列名。此设置仅出于兼容性原因存在。仅在将集群从低于 21.7 的版本滚动升级到较高版本时，设置为 'true' 是有意义的。
## lightweight_delete_mode {#lightweight_delete_mode} 

<SettingsInfoBlock type="LightweightDeleteMode" default_value="alter_update" />

作为轻量级删除的一部分执行的内部更新查询的模式。

可能的值：

- `alter_update`— 运行创建重量级变更的 `ALTER UPDATE` 查询。
- `lightweight_update` — 尝试轻量级更新，如不可行则运行 `ALTER UPDATE`。
- `lightweight_update_force` — 尝试轻量级更新，如不可行则抛出异常。
## lightweight_deletes_sync {#lightweight_deletes_sync} 

与 [`mutations_sync`](#mutations_sync) 相同，但仅控制轻量级删除的执行。

可能的值：

- 0 — 变更异步执行。
- 1 — 查询等待当前服务器上的轻量级删除完成。
- 2 — 查询等待所有副本上的轻量级删除完成（如果存在）。

**另请参见**

- [ALTER 查询的同步性](../../sql-reference/statements/alter/index.md/#synchronicity-of-alter-queries)
- [变更](../../sql-reference/statements/alter/index.md/#mutations)
## limit {#limit} 

<SettingsInfoBlock type="UInt64" default_value="0" />

设置从查询结果获取的最大行数。它调整由 [LIMIT](/sql-reference/statements/select/limit) 子句设置的值，以使查询中指定的限制不能超过此设置所设定的限制。

可能的值：

- 0 — 行数无限制。
- 正整数。
## live_view_heartbeat_interval {#live_view_heartbeat_interval} 

<ExperimentalBadge/>

<SettingsInfoBlock type="Seconds" default_value="15" />

心跳间隔，以秒为单位，表示实时查询仍在运行。
## load_balancing {#load_balancing} 

指定用于分布式查询处理的副本选择算法。

ClickHouse 支持以下选择副本的算法：

- [随机](#load_balancing-random)（默认）
- [最近主机名](#load_balancing-nearest_hostname)
- [主机名 levenshtein 距离](#load_balancing-hostname_levenshtein_distance)
- [顺序](#load_balancing-in_order)
- [第一个或随机](#load_balancing-first_or_random)
- [轮询](#load_balancing-round_robin)

另请参见：

- [distributed_replica_max_ignored_errors](#distributed_replica_max_ignored_errors)
### 随机（默认） {#load_balancing-random}

```sql
load_balancing = random
```

每个副本的错误数量被计算。查询被发送到错误最少的副本上，如果有多个这样的副本，则发送给其中一个。
缺点：未考虑服务器靠近性；如果副本有不同数据，您还会获得不同数据。
### 最近主机名 {#load_balancing-nearest_hostname}

```sql
load_balancing = nearest_hostname
```

每个副本的错误数量被计算。每 5 分钟，错误数量会整除 2。这样，错误数量根据最近的时间进行指数平滑计算。如果有一个副本的错误数量最少（即最近其他副本中发生了错误），则查询发送给它。如果有多个副本具有相同的最小错误数量，则将查询发送到主机名与配置文件中服务器的主机名最相似的副本（在相同位置的不同字符数量上，以两者主机名的最小长度为界限进行比较）。

例如，example01-01-1 和 example01-01-2 在一个位置上不同，而 example01-01-1 和 example01-02-2 在两个地方不同。
这种方法可能看起来很原始，但不需要关于网络拓扑的外部数据，而且不比较 IP 地址，这对于我们的 IPv6 地址来说便是复杂的。

因此，在存在多个同级副本时，优先考虑名称上最接近的副本。我们还可以假设，在没有失败的情况下，将查询发送到同一服务器时，分布式查询也将发送到同一服务器。因此，即使副本上存储了不同的数据，查询大多数情况下也会返回相同的结果。
### 主机名 levenshtein 距离 {#load_balancing-hostname_levenshtein_distance}

```sql
load_balancing = hostname_levenshtein_distance
```

和 `nearest_hostname` 一样，但以 [levenshtein 距离](https://en.wikipedia.org/wiki/Levenshtein_distance) 的方式比较主机名。例如：

```text
example-clickhouse-0-0 ample-clickhouse-0-0
1

example-clickhouse-0-0 example-clickhouse-1-10
2

example-clickhouse-0-0 example-clickhouse-12-0
3
```
### 顺序 {#load_balancing-in_order}

```sql
load_balancing = in_order
```

具有相同错误数量的副本按照其在配置中的指定顺序访问。
这种方法适仅当您确切知道哪个副本是最优时。
### 第一个或随机 {#load_balancing-first_or_random}

```sql
load_balancing = first_or_random
```

此算法选择集合中的第一个副本，或如果第一个不可用，则选择随机副本。它在跨副本复制拓扑设置中有效，而在其他配置中无用。

`first_or_random` 算法解决了 `in_order` 算法的问题。在 `in_order` 中，如果一个副本出现故障，接下来一个副本的负载会翻倍，而其他副本则处理正常流量。当使用 `first_or_random` 算法时，负载在仍可用的副本之间均匀分配。

可以通过使用设置 `load_balancing_first_offset` 显式定义第一个副本。这可以更好地控制跨副本之间的查询负载平衡。
### 轮询 {#load_balancing-round_robin}

```sql
load_balancing = round_robin
```

该算法在具有相同错误数量的副本之间使用轮询策略（仅统计采用 `round_robin` 策略的查询）。
## load_balancing_first_offset {#load_balancing_first_offset} 

<SettingsInfoBlock type="UInt64" default_value="0" />

在使用 FIRST_OR_RANDOM 负载均衡策略时，优先发送查询的副本。
## load_marks_asynchronously {#load_marks_asynchronously} 

<SettingsInfoBlock type="Bool" default_value="0" />

异步加载 MergeTree 标记。
## local_filesystem_read_method {#local_filesystem_read_method} 

从本地文件系统读取数据的方法，取值为：read、pread、mmap、io_uring、pread_threadpool。

`io_uring` 方法是实验性的，不适用于 Log、TinyLog、StripeLog、File、Set 和 Join，以及在存在并发读取和写入情况下的其他表。
如果您在网上阅读有关 `io_uring` 的各种文章，切勿被其迷惑。它并不是一种更好的文件读取方法，除非是在大量小 I/O 请求的情况下，但在 ClickHouse 中并不如此。没有理由启用 `io_uring`。
## local_filesystem_read_prefetch {#local_filesystem_read_prefetch} 

读取本地文件系统时是否使用预取。
## lock_acquire_timeout {#lock_acquire_timeout} 

定义锁请求在失败之前要等待的秒数。

锁定超时用于在执行表的读/写操作时防止死锁。当超时到期且锁请求失败时，ClickHouse 服务器会抛出异常 "锁定尝试超时！可能避免了死锁。客户端应重试。"，并伴随错误代码 `DEADLOCK_AVOIDED`。

可能的值：

- 正整数（以秒为单位）。
- 0 — 没有锁定超时。
## log_comment {#log_comment} 

指定 [system.query_log](../system-tables/query_log.md) 表的 `log_comment` 字段的值和服务器日志的注释文本。

它可用于提高服务器日志的可读性。此外，帮助在运行 [clickhouse-test](../../development/tests.md) 后从 `system.query_log` 中选择与测试相关的查询。

可能的值：

- 任何字符串，不超过 [max_query_size](#max_query_size)。如果超过 max_query_size，服务器将抛出异常。

**示例**

查询：

```sql
SET log_comment = 'log_comment test', log_queries = 1;
SELECT 1;
SYSTEM FLUSH LOGS;
SELECT type, query FROM system.query_log WHERE log_comment = 'log_comment test' AND event_date >= yesterday() ORDER BY event_time DESC LIMIT 2;
```

结果：

```text
┌─type────────┬─query─────┐
│ QueryStart  │ SELECT 1; │
│ QueryFinish │ SELECT 1; │
└─────────────┴───────────┘
```
## log_formatted_queries {#log_formatted_queries} 

<SettingsInfoBlock type="Bool" default_value="0" />

允许将格式化的查询记录到 [system.query_log](../../operations/system-tables/query_log.md) 系统表中（填充 [system.query_log](../../operations/system-tables/query_log.md) 中的 `formatted_query` 列）。

可能的值：

- 0 — 格式化查询不记录在系统表中。
- 1 — 格式化查询记录在系统表中。
## log_processors_profiles {#log_processors_profiles} 

<SettingsInfoBlock type="Bool" default_value="1" />

将处理器在执行/等待数据期间花费的时间写入 `system.processors_profile_log` 表中。

另请参见：

- [`system.processors_profile_log`](../../operations/system-tables/processors_profile_log.md)
- [`EXPLAIN PIPELINE`](../../sql-reference/statements/explain.md/#explain-pipeline)
## log_profile_events {#log_profile_events} 

将查询性能统计信息记录到 query_log、query_thread_log 和 query_views_log 中。
## log_queries {#log_queries} 

设置查询日志。

使用此设置发送到 ClickHouse 的查询将按照 [query_log](../../operations/server-configuration-parameters/settings.md/#query_log) 服务器配置参数的规则进行记录。

示例：

```text
log_queries=1
```
## log_queries_cut_to_length {#log_queries_cut_to_length} 

<SettingsInfoBlock type="UInt64" default_value="100000" />

如果查询长度超出指定阈值（以字节为单位），则在写入查询日志时剪切查询。还限制常规文本日志中打印查询的长度。
## log_queries_min_query_duration_ms {#log_queries_min_query_duration_ms} 

如果启用（非零），则响应快于此设置值的查询将不会记录（可以将其视为 [MySQL 慢查询日志](https://dev.mysql.com/doc/refman/5.7/slow-query-log.html) 的 `long_query_time`），这基本上意味着您将在以下表中找不到它们：

- `system.query_log`
- `system.query_thread_log`

只有以下类型的查询才会进入日志：

- `QUERY_FINISH`
- `EXCEPTION_WHILE_PROCESSING`

- 类型：毫秒
- 默认值：0（任何查询）
## log_queries_min_type {#log_queries_min_type} 

`query_log` 的最小类型进行日志记录。

可能的值：

- `QUERY_START`（`=1`）
- `QUERY_FINISH`（`=2`）
- `EXCEPTION_BEFORE_START`（`=3`）
- `EXCEPTION_WHILE_PROCESSING`（`=4`）

可用于限制哪些实体将进入 `query_log`，例如您只对错误感兴趣，则可以使用 `EXCEPTION_WHILE_PROCESSING`：

```text
log_queries_min_type='EXCEPTION_WHILE_PROCESSING'
```
## log_queries_probability {#log_queries_probability} 

允许用户仅以指定概率随机写入 [query_log](../../operations/system-tables/query_log.md)、[query_thread_log](../../operations/system-tables/query_thread_log.md) 和 [query_views_log](../../operations/system-tables/query_views_log.md) 系统表中的查询。它帮助减少每秒大量查询造成的负载。

可能的值：

- 0 — 查询不记录在系统表中。
- 在 [0..1] 范围内的正浮点数。例如，如果设置值为 `0.5`，则大约一半的查询记录在系统表中。
- 1 — 所有查询记录在系统表中。
## log_query_settings {#log_query_settings} 

将查询设置记录到 query_log 和 OpenTelemetry span 日志中。
## log_query_threads {#log_query_threads} 

设置查询线程日志。

查询线程记录到 [system.query_thread_log](../../operations/system-tables/query_thread_log.md) 表中。此设置仅在 [log_queries](#log_queries) 为 true 时有效。使用此设置在 ClickHouse 上运行的查询线程将按照 [query_thread_log](/operations/server-configuration-parameters/settings#query_thread_log) 服务器配置参数中的规则进行记录。

可能的值：

- 0 — 禁用。
- 1 — 启用。

**示例**

```text
log_query_threads=1
```
## log_query_views {#log_query_views} 

<SettingsInfoBlock type="Bool" default_value="1" />

设置查询视图日志。

当启用此设置的 ClickHouse 运行查询时，具有相关视图（物化或实时视图）时，它们会记录在 [query_views_log](/operations/server-configuration-parameters/settings#query_views_log) 服务器配置参数中。 

示例：

```text
log_query_views=1
```
## low_cardinality_allow_in_native_format {#low_cardinality_allow_in_native_format} 

<SettingsInfoBlock type="Bool" default_value="1" />

允许或限制在 [Native](../../interfaces/formats.md/#native) 格式中使用 [LowCardinality](../../sql-reference/data-types/lowcardinality.md) 数据类型。

如果限制使用 `LowCardinality`，ClickHouse 服务器将在 `SELECT` 查询中将 `LowCardinality` 列转换为普通列，并在 `INSERT` 查询中将普通列转换为 `LowCardinality` 列。

此设置主要用于不支持 `LowCardinality` 数据类型的第三方客户端。

可能的值：

- 1 — 不限制使用 `LowCardinality`。
- 0 — 限制使用 `LowCardinality`。
## low_cardinality_max_dictionary_size {#low_cardinality_max_dictionary_size} 

<SettingsInfoBlock type="UInt64" default_value="8192" />

为可以写入存储文件系统的 [LowCardinality](../../sql-reference/data-types/lowcardinality.md) 数据类型的共享全局字典设置最大行数。此设置防止字典无限增长导致的 RAM 问题。ClickHouse 将以普通方式写入无法编码的数据，因超过字典最大大小限制。

可能的值：

- 任何正整数。
## low_cardinality_use_single_dictionary_for_part {#low_cardinality_use_single_dictionary_for_part} 

启用或禁止为数据部分使用单个字典。

默认情况下，ClickHouse 服务器监控字典的大小。如果字典溢出，服务器将开始写入下一个字典。要禁止为数据部分创建多个字典，请设置 `low_cardinality_use_single_dictionary_for_part = 1`。

可能的值：

- 1 — 禁止为数据部分创建多个字典。
- 0 — 不禁止为数据部分创建多个字典。
## low_priority_query_wait_time_ms {#low_priority_query_wait_time_ms} 

<BetaBadge/>



<SettingsInfoBlock type="Milliseconds" default_value="1000" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.4"},{"label": "1000"},{"label": "New setting."}]}]}/>

当查询优先级机制被使用时（参见设置 `priority`），低优先级查询会等待高优先级查询结束。此设置指定等待的持续时间。
## make_distributed_plan {#make_distributed_plan} 

<ExperimentalBadge/>



<SettingsInfoBlock type="Bool" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.5"},{"label": "0"},{"label": "New experimental setting."}]}]}/>

生成分布式查询计划。
## materialize_skip_indexes_on_insert {#materialize_skip_indexes_on_insert} 



<SettingsInfoBlock type="Bool" default_value="1" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.6"},{"label": "1"},{"label": "Added new setting to allow to disable materialization of skip indexes on insert"}]}]}/>

在插入时构建和存储跳过索引。 如果禁用，跳过索引将在合并期间或通过显式 MATERIALIZE INDEX 构建和存储。
## materialize_statistics_on_insert {#materialize_statistics_on_insert} 



<SettingsInfoBlock type="Bool" default_value="1" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.6"},{"label": "1"},{"label": "Added new setting to allow to disable materialization of statistics on insert"}]}]}/>

在插入时构建和插入统计信息。 如果禁用，统计信息将在合并期间或通过显式 MATERIALIZE STATISTICS 构建和存储。
## materialize_ttl_after_modify {#materialize_ttl_after_modify} 



<SettingsInfoBlock type="Bool" default_value="1" />

在 ALTER MODIFY TTL 查询后，对旧数据应用 TTL。
## materialized_views_ignore_errors {#materialized_views_ignore_errors} 



<SettingsInfoBlock type="Bool" default_value="0" />

允许忽略 MATERIALIZED VIEW 的错误，并将原始块传递给表，而不管物化视图的情况。
## max_analyze_depth {#max_analyze_depth} 



<SettingsInfoBlock type="UInt64" default_value="5000" />

解释器执行的分析最大次数。
## max_ast_depth {#max_ast_depth} 



<SettingsInfoBlock type="UInt64" default_value="1000" />

查询语法树的最大嵌套深度。 如果超过，则抛出异常。

:::note
目前在解析时没有检查，而是在解析查询后检查。
这意味着在解析期间可以创建一个过深的语法树，
但查询将失败。
:::
## max_ast_elements {#max_ast_elements} 



<SettingsInfoBlock type="UInt64" default_value="50000" />

查询语法树中元素的最大数量。 如果超过，则抛出异常。

:::note
目前在解析时没有检查，而是在解析查询后检查。
这意味着在解析期间可以创建一个过深的语法树，
但查询将失败。
:::
## max_autoincrement_series {#max_autoincrement_series} 



<SettingsInfoBlock type="UInt64" default_value="1000" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.1"},{"label": "1000"},{"label": "A new setting"}]}]}/>

由 `generateSeriesID` 函数创建的序列的最大数量。

由于每个序列代表 Keeper 中的一个节点，因此建议不要超过几百万个。
## max_backup_bandwidth {#max_backup_bandwidth} 



<SettingsInfoBlock type="UInt64" default_value="0" />

特定备份在服务器上的最大读取速度（以字节每秒为单位）。零表示无限制。
## max_block_size {#max_block_size} 



<SettingsInfoBlock type="NonZeroUInt64" default_value="65409" />

在 ClickHouse 中，数据是通过块处理的，块是列部分的集合。单个块的内部处理周期是高效的，但处理每个块时会有明显的成本。

`max_block_size` 设置指示在从表加载数据时单个块中包含的推荐最大行数。不是所有的 `max_block_size` 大小的块都从表中加载：如果 ClickHouse 确定需要检索的较少数据，则会处理较小的块。

块大小不应过小，以避免处理每个块时产生明显的成本。它也不应过大，以确保执行带有 LIMIT 子句的查询时，在处理第一个块后可以快速执行。当设置 `max_block_size` 时，目标应是避免在多个线程中提取大量列时消耗太多内存，并至少保留一些缓存局部性。
## max_bytes_before_external_group_by {#max_bytes_before_external_group_by} 



<SettingsInfoBlock type="UInt64" default_value="0" />

云默认值：每个副本内存量的一半。

启用或禁用在外部内存中执行 `GROUP BY` 子句。
（参见 [GROUP BY in external memory](/sql-reference/statements/select/group-by#group-by-in-external-memory)）

可能的值：

- 一个单个 [GROUP BY](/sql-reference/statements/select/group-by) 操作可以使用的最大 RAM（以字节为单位）。
- `0` — 禁用外部内存中的 `GROUP BY`。

:::note
如果在 GROUP BY 操作期间的内存使用量超过该阈值（以字节为单位），
则激活“外部聚合”模式（将数据溢出到磁盘）。

推荐值是可用系统内存的一半。
:::
## max_bytes_before_external_sort {#max_bytes_before_external_sort} 



<SettingsInfoBlock type="UInt64" default_value="0" />

云默认值：每个副本内存量的一半。

启用或禁用在外部内存中执行 `ORDER BY` 子句。 参见 [ORDER BY Implementation Details](../../sql-reference/statements/select/order-by.md#implementation-details)
如果 ORDER BY 操作期间的内存使用量超过该阈值（以字节为单位），则激活“外部排序”模式（将数据溢出到磁盘）。

可能的值：

- 单个 [ORDER BY](../../sql-reference/statements/select/order-by) 操作可以使用的最大 RAM（以字节为单位）。
  推荐值是可用系统内存的一半。
- `0` — 禁用外部内存中的 `ORDER BY`。
## max_bytes_before_remerge_sort {#max_bytes_before_remerge_sort} 



<SettingsInfoBlock type="UInt64" default_value="1000000000" />

在 ORDER BY 和 LIMIT 结合使用的情况下，当内存使用量超过指定阈值时，执行合并块的额外步骤以保留前 LIMIT 行。
## max_bytes_in_distinct {#max_bytes_in_distinct} 



<SettingsInfoBlock type="UInt64" default_value="0" />

用于 DISTINCT 时哈希表在内存中使用的状态的最大字节数（以未压缩字节为单位）。
## max_bytes_in_join {#max_bytes_in_join} 



<SettingsInfoBlock type="UInt64" default_value="0" />

连接表时使用的哈希表的最大字节数。

此设置适用于 [SELECT ... JOIN](/sql-reference/statements/select/join)
操作和 [Join table engine](/engines/table-engines/special/join)。

如果查询包含连接，ClickHouse 将在每个中间结果检查该设置。

当达到限制时，ClickHouse 可以继续执行不同的操作。请使用
[join_overflow_mode](/operations/settings/settings#join_overflow_mode) 设置来选择操作。

可能的值：

- 正整数。
- 0 — 禁用内存控制。
## max_bytes_in_set {#max_bytes_in_set} 



<SettingsInfoBlock type="UInt64" default_value="0" />

由子查询创建的 IN 子句中使用的集合的最大字节数（未压缩数据）。
## max_bytes_ratio_before_external_group_by {#max_bytes_ratio_before_external_group_by} 



<SettingsInfoBlock type="Double" default_value="0.5" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.1"},{"label": "0.5"},{"label": "Enable automatic spilling to disk by default."}]}, {"id": "row-2","items": [{"label": "24.12"},{"label": "0"},{"label": "New setting."}]}]}/>

允许用于 `GROUP BY` 的可用内存的比例。一旦达到，外部内存将用于聚合。

例如，如果设置为 `0.6`，则 `GROUP BY` 将在执行开始时允许使用 60% 的可用内存（供服务器/用户/合并使用），之后将开始使用外部聚合。
## max_bytes_ratio_before_external_sort {#max_bytes_ratio_before_external_sort} 



<SettingsInfoBlock type="Double" default_value="0.5" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.1"},{"label": "0.5"},{"label": "Enable automatic spilling to disk by default."}]}, {"id": "row-2","items": [{"label": "24.12"},{"label": "0"},{"label": "New setting."}]}]}/>

允许用于 `ORDER BY` 的可用内存的比例。一旦达到，将使用外部排序。

例如，如果设置为 `0.6`，则 `ORDER BY` 将在执行开始时允许使用 `60%` 的可用内存（供服务器/用户/合并使用），之后将开始使用外部排序。
## max_bytes_to_read {#max_bytes_to_read} 



<SettingsInfoBlock type="UInt64" default_value="0" />

执行查询时可以从表中读取的最大字节数（未压缩数据）。
该限制在处理的每个数据块上进行检查，仅适用于最深表表达式，并且在读取远程服务器时，仅在远程服务器上检查。
## max_bytes_to_read_leaf {#max_bytes_to_read_leaf} 



<SettingsInfoBlock type="UInt64" default_value="0" />

在执行分布式查询时，可以从叶节点的本地表中读取的最大字节数（未压缩数据）。虽然分布式查询可以对每个分片（叶子）发出多个子查询——该限制将在叶节点的读取阶段进行检查，而在根节点的合并结果阶段将被忽略。

例如，一个集群由 2 个分片组成，每个分片包含 100 字节数据的表。一个应该从两个表中读取所有数据的分布式查询，将 `max_bytes_to_read=150` 设置将失败，因为总共将是 200 字节。一个 `max_bytes_to_read_leaf=150` 的查询将成功，因为叶节点将最多读取 100 字节。

该限制在处理的每个数据块上进行检查。

:::note
此设置与 `prefer_localhost_replica=1` 不稳定。
:::
## max_bytes_to_sort {#max_bytes_to_sort} 



<SettingsInfoBlock type="UInt64" default_value="0" />

排序之前的最大字节数。如果要处理的未压缩字节数量超过指定的数量，则 ORDER BY 操作的行为将由 `sort_overflow_mode` 决定，默认设置为 `throw`。
## max_bytes_to_transfer {#max_bytes_to_transfer} 



<SettingsInfoBlock type="UInt64" default_value="0" />

在执行 GLOBAL IN/JOIN 部分时，可以传递给远程服务器或保存到临时表中的最大字节数（未压缩数据）。
## max_columns_to_read {#max_columns_to_read} 



<SettingsInfoBlock type="UInt64" default_value="0" />

在单个查询中，可以从表中读取的最大列数。
如果查询需要读取超过指定列数，则抛出异常。

:::tip
此设置有助于防止过于复杂的查询。
:::

`0` 值表示无限制。
## max_compress_block_size {#max_compress_block_size} 



<SettingsInfoBlock type="UInt64" default_value="1048576" />

在写入表之前，未压缩数据块的最大大小。默认值为 1,048,576（1 MiB）。指定较小的块大小通常会导致压缩比略微降低，压缩和解压缩速度略微增加，内存消耗减少。

:::note
这是一个专家级设置，如果你刚开始使用 ClickHouse，建议不要更改它。
:::

不要混淆用于压缩的块（由字节组成的一块内存）与用于查询处理的块（来自表的一组行）。
## max_concurrent_queries_for_all_users {#max_concurrent_queries_for_all_users} 



<SettingsInfoBlock type="UInt64" default_value="0" />

如果此设置的值小于或等于当前正在处理的查询数量，则抛出异常。

示例：`max_concurrent_queries_for_all_users` 可以设置为 99，而数据库管理员可以将其设置为 100，以便在服务器过载的情况下运行查询进行调查。

为一个查询或用户修改设置不会影响其他查询。

可能的值：

- 正整数。
- 0 — 没有限制。

**示例**

```xml
<max_concurrent_queries_for_all_users>99</max_concurrent_queries_for_all_users>
```

**参见**

- [max_concurrent_queries](/operations/server-configuration-parameters/settings#max_concurrent_queries)
## max_concurrent_queries_for_user {#max_concurrent_queries_for_user} 



<SettingsInfoBlock type="UInt64" default_value="0" />

每个用户同时处理的最大查询数量。

可能的值：

- 正整数。
- 0 — 没有限制。

**示例**

```xml
<max_concurrent_queries_for_user>5</max_concurrent_queries_for_user>
```
## max_distributed_connections {#max_distributed_connections} 



<SettingsInfoBlock type="UInt64" default_value="1024" />

与远程服务器同时连接的最大数量，用于对单个分布式表进行分布式处理。建议将值设置为不小于集群中的服务器数量。

以下参数仅在创建分布式表时使用（以及在启动服务器时），因此运行时没有理由更改它们。
## max_distributed_depth {#max_distributed_depth} 



<SettingsInfoBlock type="UInt64" default_value="5" />

限制对 [Distributed](../../engines/table-engines/special/distributed.md) 表的递归查询的最大深度。

如果超过此值，服务器将抛出异常。

可能的值：

- 正整数。
- 0 — 无限深度。
## max_download_buffer_size {#max_download_buffer_size} 



<SettingsInfoBlock type="UInt64" default_value="10485760" />

每个线程并行下载（例如对于 URL 引擎）的最大缓冲区大小。
## max_download_threads {#max_download_threads} 



<SettingsInfoBlock type="MaxThreads" default_value="4" />

下载数据的最大线程数（例如对于 URL 引擎）。
## max_estimated_execution_time {#max_estimated_execution_time} 



<SettingsInfoBlock type="Seconds" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.1"},{"label": "0"},{"label": "Separate max_execution_time and max_estimated_execution_time"}]}]}/>

最大查询估算执行时间（以秒为单位）。在 [`timeout_before_checking_execution_speed`](/operations/settings/settings#timeout_before_checking_execution_speed) 超时到期时检查每个数据块。
## max_execution_speed {#max_execution_speed} 



<SettingsInfoBlock type="UInt64" default_value="0" />

每秒执行的最大行数。`timeout_before_checking_execution_speed` 到期时检查每个数据块。 如果执行速度过高，将降低执行速度。
## max_execution_speed_bytes {#max_execution_speed_bytes} 



<SettingsInfoBlock type="UInt64" default_value="0" />

每秒执行的最大字节数。`timeout_before_checking_execution_speed` 到期时检查每个数据块。 如果执行速度过高，将降低执行速度。
## max_execution_time {#max_execution_time} 



<SettingsInfoBlock type="Seconds" default_value="0" />

查询的最大执行时间（以秒为单位）。

`max_execution_time` 参数可能有点难以理解。
它基于与当前查询执行速度的插值操作（此行为由 [`timeout_before_checking_execution_speed`](/operations/settings/settings#timeout_before_checking_execution_speed) 控制）。

如果预测的执行时间超过指定的 `max_execution_time`，ClickHouse 将中断查询。默认情况下，`timeout_before_checking_execution_speed` 设置为 10 秒。这意味着在查询执行 10 秒后，ClickHouse 将开始估算总执行时间。如果，例如，`max_execution_time` 设置为 3600 秒（1 小时），如果估计的时间超过这个 3600 秒的限制，ClickHouse 将终止查询。如果将 `timeout_before_checking_execution_speed` 设置为 0，ClickHouse 将以时钟时间为基础来计算 `max_execution_time`。

如果查询运行时间超过指定的秒数，其行为将由 'timeout_overflow_mode' 控制，默认设置为 `throw`。

:::note
超时在数据处理的指定位置进行检查，仅在这些位置查询可以停止。
目前在合并聚合状态或查询分析过程中不能停止，实际运行时间将高于此设置的值。
:::
## max_execution_time_leaf {#max_execution_time_leaf} 



<SettingsInfoBlock type="Seconds" default_value="0" />

语义上类似于 [`max_execution_time`](#max_execution_time)，但仅适用于分布式或远程查询的叶节点。

例如，如果我们希望将叶节点的执行时间限制为 `10s` 而对初始节点没有限制，而不是在嵌套子查询设置中使用 `max_execution_time`：

```sql
SELECT count()
FROM cluster(cluster, view(SELECT * FROM t SETTINGS max_execution_time = 10));
```

我们可以使用 `max_execution_time_leaf` 作为查询设置：

```sql
SELECT count()
FROM cluster(cluster, view(SELECT * FROM t)) SETTINGS max_execution_time_leaf = 10;
```
## max_expanded_ast_elements {#max_expanded_ast_elements} 



<SettingsInfoBlock type="UInt64" default_value="500000" />

扩展别名和星号后的查询语法树的最大节点数。
## max_fetch_partition_retries_count {#max_fetch_partition_retries_count} 



<SettingsInfoBlock type="UInt64" default_value="5" />

从另一个主机提取分区时的重试次数。
## max_final_threads {#max_final_threads} 



<SettingsInfoBlock type="MaxThreads" default_value="'auto(N)'" />

设置在带有 [FINAL](/sql-reference/statements/select/from#final-modifier) 修饰符的 `SELECT` 查询数据读取阶段的最大并行线程数。

可能的值：

- 正整数。
- 0 或 1 — 禁用。 `SELECT` 查询在单线程中执行。
## max_http_get_redirects {#max_http_get_redirects} 



<SettingsInfoBlock type="UInt64" default_value="0" />

允许的最大 HTTP GET 重定向跳数。确保采取额外的安全措施，以防恶意服务器将请求重定向到意外的服务。\n\n这种情况发生在外部服务器重定向到另一个地址，但是该地址似乎属于公司的内部基础设施，通过向内部服务器发送 HTTP 请求，可以从内部网络请求内部 API，从而绕过认证，甚至查询其他服务，如 Redis 或 Memcached。当你没有内部基础设施（包括在本地运行的任何东西），或你信任服务器时，允许重定向是安全的。不过请记住，如果 URL 使用 HTTP 而不是 HTTPS，你需要不仅信任远程服务器，还要信任你的 ISP 以及中间的每个网络。
## max_hyperscan_regexp_length {#max_hyperscan_regexp_length} 



<SettingsInfoBlock type="UInt64" default_value="0" />

定义 [hyperscan multi-match functions](/sql-reference/functions/string-search-functions#multimatchany) 中每个正则表达式的最大长度。

可能的值：

- 正整数。
- 0 - 长度不受限制。

**示例**

查询：

```sql
SELECT multiMatchAny('abcd', ['ab','bcd','c','d']) SETTINGS max_hyperscan_regexp_length = 3;
```

结果：

```text
┌─multiMatchAny('abcd', ['ab', 'bcd', 'c', 'd'])─┐
│                                              1 │
└────────────────────────────────────────────────┘
```

查询：

```sql
SELECT multiMatchAny('abcd', ['ab','bcd','c','d']) SETTINGS max_hyperscan_regexp_length = 2;
```

结果：

```text
Exception: Regexp length too large.
```

**参见**

- [max_hyperscan_regexp_total_length](#max_hyperscan_regexp_total_length)
## max_hyperscan_regexp_total_length {#max_hyperscan_regexp_total_length} 



<SettingsInfoBlock type="UInt64" default_value="0" />

设置每个 [hyperscan multi-match function](/sql-reference/functions/string-search-functions#multimatchany) 中所有正则表达式的最大总长度。

可能的值：

- 正整数。
- 0 - 长度不受限制。

**示例**

查询：

```sql
SELECT multiMatchAny('abcd', ['a','b','c','d']) SETTINGS max_hyperscan_regexp_total_length = 5;
```

结果：

```text
┌─multiMatchAny('abcd', ['a', 'b', 'c', 'd'])─┐
│                                           1 │
└─────────────────────────────────────────────┘
```

查询：

```sql
SELECT multiMatchAny('abcd', ['ab','bc','c','d']) SETTINGS max_hyperscan_regexp_total_length = 5;
```

结果：

```text
Exception: Total regexp lengths too large.
```

**参见**

- [max_hyperscan_regexp_length](#max_hyperscan_regexp_length)
## max_insert_block_size {#max_insert_block_size} 



<SettingsInfoBlock type="UInt64" default_value="1048449" />

插入到表中的块大小（以行数计）。
此设置仅在服务器形成块的情况下适用。
例如，对于通过 HTTP 接口的 INSERT，服务器解析数据格式并形成指定大小的块。
但在使用 clickhouse-client 时，客户端自己解析数据，服务器上的 'max_insert_block_size' 设置不影响插入块的大小。
当使用 INSERT SELECT 时，此设置也没有作用，因为数据是使用 SELECT 后形成的相同块插入的。

默认值略大于 `max_block_size`。这样做的原因是某些表引擎（`*MergeTree`）为每个插入块在磁盘上形成数据部分，这是一个相当大的实体。类似地，`*MergeTree` 表在插入时对数据进行排序，足够大的块大小允许在内存中排序更多数据。
## max_insert_delayed_streams_for_parallel_write {#max_insert_delayed_streams_for_parallel_write} 



<SettingsInfoBlock type="UInt64" default_value="0" />

最大延迟最终部分刷新的流（列）数。默认 - 自动（在基础存储支持并行写入的情况下为 100，否则禁用）
## max_insert_threads {#max_insert_threads} 



<SettingsInfoBlock type="UInt64" default_value="0" />

执行 `INSERT SELECT` 查询的最大线程数。

可能的值：

- 0（或 1） — `INSERT SELECT` 没有并行执行。
- 正整数，大于 1。

云默认值：根据服务大小从 `2` 到 `4`。

并行 `INSERT SELECT` 只有在 `SELECT` 部分并行执行时才有效，见 [max_threads](#max_threads) 设置。
更高的值将导致更高的内存使用。
## max_joined_block_size_rows {#max_joined_block_size_rows} 



<SettingsInfoBlock type="UInt64" default_value="65409" />

JOIN 结果的最大块大小（如果连接算法支持的话）。0 表示不限制。
## max_limit_for_vector_search_queries {#max_limit_for_vector_search_queries} 

<BetaBadge/>



<SettingsInfoBlock type="UInt64" default_value="1000" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.5"},{"label": "1000"},{"label": "New setting"}]}]}/>

LIMIT 大于此设置的 SELECT 查询无法使用向量相似性索引。帮助防止向量相似性索引中的内存溢出。
## max_live_view_insert_blocks_before_refresh {#max_live_view_insert_blocks_before_refresh} 

<ExperimentalBadge/>



<SettingsInfoBlock type="UInt64" default_value="64" />

限制插入的块的最大数量，超过后可合并的块将被丢弃并重新执行查询。
## max_local_read_bandwidth {#max_local_read_bandwidth} 



<SettingsInfoBlock type="UInt64" default_value="0" />

本地读取的最大速度（以字节每秒为单位）。
## max_local_write_bandwidth {#max_local_write_bandwidth} 



<SettingsInfoBlock type="UInt64" default_value="0" />

本地写入的最大速度（以字节每秒为单位）。
## max_memory_usage {#max_memory_usage} 



<SettingsInfoBlock type="UInt64" default_value="0" />

云默认值：依赖于副本的 RAM 数量。

在单台服务器上运行查询时可使用的最大 RAM。
值为 `0` 表示无限制。

此设置不考虑可用内存的体积或机器的总内存量。该限制适用于单个查询在单台服务器内。

您可以使用 `SHOW PROCESSLIST` 查看每个查询的当前内存消耗。
每个查询的峰值内存消耗都会被追踪并写入日志。

以下聚合函数的状态的内存使用量不完全跟踪：
- `min`
- `max`
- `any`
- `anyLast`
- `argMin`
- `argMax`

内存消耗还受以下参数的限制 [`max_memory_usage_for_user`](/operations/settings/settings#max_memory_usage_for_user)
和 [`max_server_memory_usage`](/operations/server-configuration-parameters/settings#max_server_memory_usage)。
## max_memory_usage_for_user {#max_memory_usage_for_user} 



<SettingsInfoBlock type="UInt64" default_value="0" />

在单台服务器上为运行用户查询所使用的最大 RAM。零表示无限制。

默认情况下，金额没有限制（`max_memory_usage_for_user = 0`）。

另请参阅 [`max_memory_usage`](/operations/settings/settings#max_memory_usage) 的描述。

例如，如果您想将 `max_memory_usage_for_user` 设置为 1000 字节，对名为 `clickhouse_read` 的用户，您可以使用以下语句

```sql
ALTER USER clickhouse_read SETTINGS max_memory_usage_for_user = 1000;
```

您可以通过注销客户端，然后重新登录，再使用 `getSetting` 函数来验证它是否有效：

```sql
SELECT getSetting('max_memory_usage_for_user');
```
## max_network_bandwidth {#max_network_bandwidth} 



<SettingsInfoBlock type="UInt64" default_value="0" />

限制通过网络交换数据的速度（以字节每秒为单位）。此设置适用于每个查询。

可能的值：

- 正整数。
- 0 — 禁用带宽控制。
## max_network_bandwidth_for_all_users {#max_network_bandwidth_for_all_users} 



<SettingsInfoBlock type="UInt64" default_value="0" />

限制通过网络交换数据的速度（以字节每秒为单位）。此设置适用于服务器上所有同时运行的查询。

可能的值：

- 正整数。
- 0 — 禁用数据速度控制。
## max_network_bandwidth_for_user {#max_network_bandwidth_for_user} 



<SettingsInfoBlock type="UInt64" default_value="0" />

限制通过网络交换数据的速度（以字节每秒为单位）。此设置适用于单个用户执行的所有同时运行的查询。

可能的值：

- 正整数。
- 0 — 禁用数据速度控制。
## max_network_bytes {#max_network_bytes} 



<SettingsInfoBlock type="UInt64" default_value="0" />

限制在执行查询时通过网络接收或传输的数据量（以字节为单位）。此设置适用于每个单独查询。

可能的值：

- 正整数。
- 0 — 禁用数据量控制。
## max_number_of_partitions_for_independent_aggregation {#max_number_of_partitions_for_independent_aggregation} 



<SettingsInfoBlock type="UInt64" default_value="128" />

表中可应用优化的最大分区数量。
## max_os_cpu_wait_time_ratio_to_throw {#max_os_cpu_wait_time_ratio_to_throw} 



<SettingsInfoBlock type="Float" default_value="0" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.5"},{"label": "0"},{"label": "Setting values were changed and backported to 25.4"}]}, {"id": "row-2","items": [{"label": "25.4"},{"label": "0"},{"label": "New setting"}]}]}/>

考虑拒绝查询的操作系统 CPU 等待（OSCPUWaitMicroseconds 指标）和繁忙（OSCPUVirtualTimeMicroseconds 指标）时间之间的最大比例。使用线性插值在最小比率和最大比率之间计算概率， 在这一点上，概率为 1。
## max_parallel_replicas {#max_parallel_replicas} 



<SettingsInfoBlock type="NonZeroUInt64" default_value="1000" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.1"},{"label": "1000"},{"label": "Use up to 1000 parallel replicas by default."}]}]}/>

每个分片执行查询时的最大副本数。

可能的值：

- 正整数。

**附加信息**

此选项将根据使用的设置产生不同的结果。

:::note
当连接或子查询涉及时，此设置将产生不正确的结果，并且所有表不满足某些要求。有关详细信息，请参见 [Distributed Subqueries and max_parallel_replicas](/operations/settings/settings#max_parallel_replicas)。
:::
### 使用 `SAMPLE` 键的并行处理

如果在多个服务器上并行执行查询，则查询可能处理得更快。但在以下情况下，查询性能可能会下降：

- 采样键在分区键中的位置不允许有效的范围扫描。
- 向表中添加采样键使其他列的过滤变得效率低下。
- 采样键是一个计算成本高昂的表达式。
- 集群延迟分布有很长的尾巴，因此查询更多服务器会增加查询的整体延迟。
### 使用 [parallel_replicas_custom_key](#parallel_replicas_custom_key) 的并行处理

此设置对任何复制表都很有用。
## max_parser_backtracks {#max_parser_backtracks} 



<SettingsInfoBlock type="UInt64" default_value="1000000" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.3"},{"label": "1000000"},{"label": "Limiting the complexity of parsing"}]}]}/>

最大解析器回溯（在递归下降解析过程中尝试不同替代方案的次数）。
## max_parser_depth {#max_parser_depth} 



<SettingsInfoBlock type="UInt64" default_value="1000" />

限制递归下降解析器的最大递归深度。允许控制堆栈大小。

可能的值：

- 正整数。
- 0 — 递归深度不限制。
## max_parsing_threads {#max_parsing_threads} 



<SettingsInfoBlock type="MaxThreads" default_value="'auto(N)'" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.4"},{"label": "0"},{"label": "Add a separate setting to control number of threads in parallel parsing from files"}]}]}/>

解析支持并行解析的输入格式数据的最大线程数。默认情况下，自动确定。
## max_partition_size_to_drop {#max_partition_size_to_drop} 



<SettingsInfoBlock type="UInt64" default_value="50000000000" />

在查询时间丢弃分区的限制。值为 0 表示可以不受限制地丢弃分区。

云默认值：1 TB。

:::note
此查询设置会覆盖其服务器设置等效值，请参见 [max_partition_size_to_drop](/operations/server-configuration-parameters/settings#max_partition_size_to_drop)。
:::
## max_partitions_per_insert_block {#max_partitions_per_insert_block} 



<SettingsInfoBlock type="UInt64" default_value="100" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "19.5"},{"label": "100"},{"label": "Add a limit for the number of partitions in one block"}]}]}/>

限制单个插入块中最大分区数，如果块包含太多分区，将抛出异常。

- 正整数。
- `0` — 分区数无限制。

**详细信息**

在插入数据时，ClickHouse 计算插入块中的分区数量。如果分区数量超过 `max_partitions_per_insert_block`，ClickHouse 会根据 `throw_on_max_partitions_per_insert_block` 记录警告或抛出异常。异常的文本如下：

> "单个 INSERT 块的分区过多（`partitions_count` 个分区，限制是 " + toString(max_partitions) + "）。
  该限制由 'max_partitions_per_insert_block' 设置控制。
  分区数量过多是一个常见的误解。它会导致严重的负面性能影响，包括服务器启动缓慢、INSERT 查询缓慢和 SELECT 查询缓慢。建议表的分区总数低于 1000..10000。请注意，分区的目的不是加速 SELECT 查询（ORDER BY 键足以使范围查询快速）。分区用于数据操作（DROP PARTITION 等）。"

:::note
此设置是一个安全阈值，因为使用大量分区是一个常见的误解。
:::
## max_partitions_to_read {#max_partitions_to_read} 



<SettingsInfoBlock type="Int64" default_value="-1" />

限制在单个查询中可以访问的最大分区数。

在创建表时指定的设置值可以通过查询级别设置进行覆盖。

可能的值：

- 正整数。
- `-1` - 无限制（默认）

:::note
您还可以在表设置中指定 MergeTree 设置 [`max_partitions_to_read`](/operations/settings/settings#max_partitions_to_read)。
:::
## max_parts_to_move {#max_parts_to_move} 



<SettingsInfoBlock type="UInt64" default_value="1000" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.10"},{"label": "1000"},{"label": "New setting"}]}]}/>

限制在单个查询中可以移动的部分数量。零表示不限制。
## max_query_size {#max_query_size} 



<SettingsInfoBlock type="UInt64" default_value="262144" />

SQL 解析器解析的查询字符串的最大字节数。
INSERT 查询的 VALUES 子句中的数据由独立的流解析器处理（消耗 O(1) RAM），不会受到此限制的影响。

:::note
`max_query_size` 不能在 SQL 查询中设置（例如，`SELECT now() SETTINGS max_query_size=10000`），因为 ClickHouse 需要分配一个缓冲区来解析查询，而该缓冲区的大小由 `max_query_size` 设置决定，必须在执行查询之前进行配置。
:::
## max_read_buffer_size {#max_read_buffer_size} 



<SettingsInfoBlock type="NonZeroUInt64" default_value="1048576" />

从文件系统读取的最大缓冲区大小。
## max_read_buffer_size_local_fs {#max_read_buffer_size_local_fs} 



<SettingsInfoBlock type="UInt64" default_value="131072" />

从本地文件系统读取的最大缓冲区大小。如果设置为 0，则将使用 max_read_buffer_size。
## max_read_buffer_size_remote_fs {#max_read_buffer_size_remote_fs} 



<SettingsInfoBlock type="UInt64" default_value="0" />

从远程文件系统读取的最大缓冲区大小。如果设置为 0，则将使用 max_read_buffer_size。
## max_recursive_cte_evaluation_depth {#max_recursive_cte_evaluation_depth} 



<SettingsInfoBlock type="UInt64" default_value="1000" />



<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.4"},{"label": "1000"},{"label": "Maximum limit on recursive CTE evaluation depth"}]}]}/>

对递归 CTE 评估深度的最大限制。
## max_remote_read_network_bandwidth {#max_remote_read_network_bandwidth} 



<SettingsInfoBlock type="UInt64" default_value="0" />

远程读取时网络数据交换的最大速度（以字节每秒为单位）。
## max_remote_write_network_bandwidth {#max_remote_write_network_bandwidth} 



<SettingsInfoBlock type="UInt64" default_value="0" />

远程写入时网络数据交换的最大速度（以字节每秒为单位）。
## max_replica_delay_for_distributed_queries {#max_replica_delay_for_distributed_queries} 

<SettingsInfoBlock type="UInt64" default_value="300" />

禁用滞后副本以进行分布式查询。请参阅 [Replication](../../engines/table-engines/mergetree-family/replication.md)。

设置以秒为单位的时间。如果一个副本的滞后大于或等于设置的值，则不使用该副本。

可能的值：

- 正整数。
- 0 — 不检查副本滞后。

要防止使用任何具有非零滞后的副本，请将此参数设置为 1。

在从指向已复制表的分布式表执行 `SELECT` 时使用该参数。
## max_result_bytes {#max_result_bytes} 

<SettingsInfoBlock type="UInt64" default_value="0" />

限制结果大小（未压缩），以字节为单位。如果处理数据块时达到阈值，查询将停止，
但不会裁剪结果的最后一个块，因此结果大小可能会大于阈值。

**注意事项**

此阈值会考虑内存中的结果大小。
即使结果大小很小，它也可能引用内存中更大的数据结构，
表示 LowCardinality 列的字典和 AggregateFunction 列的 Arenas，
因此尽管结果大小很小，阈值仍可能超出。

:::warning
该设置相当低级，应谨慎使用。
:::
## max_result_rows {#max_result_rows} 

<SettingsInfoBlock type="UInt64" default_value="0" />

云默认值：`0`。

限制结果中的行数。子查询和在远程服务器上运行分布式查询的一部分时也会检查。
当值为 `0` 时不施加限制。

如果处理数据块时达到阈值，查询将停止，但
不会裁剪结果的最后一个块，因此结果大小可能会
大于阈值。
## max_rows_in_distinct {#max_rows_in_distinct} 

<SettingsInfoBlock type="UInt64" default_value="0" />

使用 DISTINCT 时最大不同行数。
## max_rows_in_join {#max_rows_in_join} 

<SettingsInfoBlock type="UInt64" default_value="0" />

限制连接表时使用的哈希表中的行数。

此设置适用于 [SELECT ... JOIN](/sql-reference/statements/select/join)
操作和 [Join](/engines/table-engines/special/join) 表引擎。

如果查询包含多个连接，ClickHouse 会对每个中间结果检查此设置。

当达到限制时，ClickHouse 可以采取不同的操作。使用
[`join_overflow_mode`](/operations/settings/settings#join_overflow_mode) 设置来选择操作。

可能的值：

- 正整数。
- `0` — 行数不受限制。
## max_rows_in_set {#max_rows_in_set} 

<SettingsInfoBlock type="UInt64" default_value="0" />

从子查询创建的 IN 子句的数据集的最大行数。
## max_rows_in_set_to_optimize_join {#max_rows_in_set_to_optimize_join} 

<SettingsInfoBlock type="UInt64" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.1"},{"label": "0"},{"label": "Disable join optimization as it prevents from read in order optimization"}]}]}/>

在连接之前，按彼此的行集过滤连接表的最大集合大小。

可能值：

- 0 — 禁用。
- 任何正整数。
## max_rows_to_group_by {#max_rows_to_group_by} 

<SettingsInfoBlock type="UInt64" default_value="0" />

从聚合接收到的唯一键的最大数量。此设置允许您限制聚合时的内存消耗。

如果在 GROUP BY 期间的聚合生成的行数（唯一 GROUP BY 键）超过指定数量，
则行为将由默认为 `throw` 的 'group_by_overflow_mode' 决定，但也可以更改为近似 GROUP BY 模式。
## max_rows_to_read {#max_rows_to_read} 

<SettingsInfoBlock type="UInt64" default_value="0" />

运行查询时可以从表中读取的最大行数。
对每个处理的数据块检查限制，仅适用于最深的表表达式，
并且在从远程服务器读取时仅在远程服务器上检查。
## max_rows_to_read_leaf {#max_rows_to_read_leaf} 

<SettingsInfoBlock type="UInt64" default_value="0" />

在运行分布式查询时，可以从叶子节点的本地表中读取的最大行数。虽然分布式查询可以向每个分片（叶子）发出多个子查询，但这个限制将在读取阶段仅在叶子节点上检查，并在根节点的结果合并阶段被忽略。

例如，一个集群由 2 个分片组成，每个分片包含一个 100 行的表。假设分布式查询的设置为 `max_rows_to_read=150`，该查询将失败，因为总共有 200 行。一个查询设置为 `max_rows_to_read_leaf=150` 将成功，因为叶子节点最多将读取 100 行。

对每个处理的数据块检查限制。

:::note
在 `prefer_localhost_replica=1` 的情况下，此设置不稳定。
:::
## max_rows_to_sort {#max_rows_to_sort} 

<SettingsInfoBlock type="UInt64" default_value="0" />

排序之前的最大行数。这允许您在排序时限制内存消耗。
如果必须处理的记录数量超过指定数量，
则行为将由默认设置为 `throw` 的 `sort_overflow_mode` 决定。
## max_rows_to_transfer {#max_rows_to_transfer} 

<SettingsInfoBlock type="UInt64" default_value="0" />

执行 GLOBAL IN/JOIN 部分时可以传递给远程服务器或保存到临时表的最大大小（以行数计）。
## max_sessions_for_user {#max_sessions_for_user} 

<SettingsInfoBlock type="UInt64" default_value="0" />

每个经过身份验证的用户对 ClickHouse 服务器的最大同时会话数。

示例：

```xml
<profiles>
    <single_session_profile>
        <max_sessions_for_user>1</max_sessions_for_user>
    </single_session_profile>
    <two_sessions_profile>
        <max_sessions_for_user>2</max_sessions_for_user>
    </two_sessions_profile>
    <unlimited_sessions_profile>
        <max_sessions_for_user>0</max_sessions_for_user>
    </unlimited_sessions_profile>
</profiles>
<users>
    <!-- User Alice can connect to a ClickHouse server no more than once at a time. -->
    <Alice>
        <profile>single_session_user</profile>
    </Alice>
    <!-- User Bob can use 2 simultaneous sessions. -->
    <Bob>
        <profile>two_sessions_profile</profile>
    </Bob>
    <!-- User Charles can use arbitrarily many of simultaneous sessions. -->
    <Charles>
        <profile>unlimited_sessions_profile</profile>
    </Charles>
</users>
```

可能的值：
- 正整数
- `0` - 无限制的同时会话数（默认）
## max_size_to_preallocate_for_aggregation {#max_size_to_preallocate_for_aggregation} 

<SettingsInfoBlock type="UInt64" default_value="1000000000000" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.12"},{"label": "1000000000000"},{"label": "Enable optimisation for bigger tables."}]}, {"id": "row-2","items": [{"label": "22.12"},{"label": "100000000"},{"label": "This optimizes performance"}]}]}/>

允许在聚合之前为所有哈希表预分配的元素总数
## max_size_to_preallocate_for_joins {#max_size_to_preallocate_for_joins} 

<SettingsInfoBlock type="UInt64" default_value="1000000000000" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.7"},{"label": "100000000"},{"label": "New setting."}]}, {"id": "row-2","items": [{"label": "24.12"},{"label": "1000000000000"},{"label": "Enable optimisation for bigger tables."}]}]}/>

允许在连接之前为所有哈希表预分配的元素总数
## max_streams_for_merge_tree_reading {#max_streams_for_merge_tree_reading} 

<SettingsInfoBlock type="UInt64" default_value="0" />

如果不为零，则限制 MergeTree 表的读取流的数量。
## max_streams_multiplier_for_merge_tables {#max_streams_multiplier_for_merge_tables} 

<SettingsInfoBlock type="Float" default_value="5" />

从合并表读取时请求更多流。流将分布在合并表将使用的表中。这允许更均匀地分配工作到线程中，并且在合并表大小不一时尤其有用。
## max_streams_to_max_threads_ratio {#max_streams_to_max_threads_ratio} 

<SettingsInfoBlock type="Float" default_value="1" />

允许您使用的源数超过线程数 - 以更均匀地分配工作到线程。假设这是一个临时解决方案，因为未来可以使源数等于线程数，但每个源可为自己动态选择可用工作。
## max_subquery_depth {#max_subquery_depth} 

<SettingsInfoBlock type="UInt64" default_value="100" />

如果查询包含超过指定数量的嵌套子查询，则抛出异常。

:::tip
这允许您进行理智检查，以保护集群用户不编写过于复杂的查询。
:::
## max_table_size_to_drop {#max_table_size_to_drop} 

<SettingsInfoBlock type="UInt64" default_value="50000000000" />

查询时间删除表的限制。值为 0 表示您可以删除所有表，而无需任何限制。

云默认值：1 TB。

:::note
此查询设置覆盖其服务器设置等价项，参见 [max_table_size_to_drop](/operations/server-configuration-parameters/settings#max_table_size_to_drop)
:::
## max_temporary_columns {#max_temporary_columns} 

<SettingsInfoBlock type="UInt64" default_value="0" />

在运行查询时，必须在 RAM 中同时保留的临时列的最大数量，包括常量列。如果查询生成的临时列数量超过指定数量，
则会抛出异常。

:::tip
该设置可用于防止过于复杂的查询。
:::

`0` 的值表示无限制。
## max_temporary_data_on_disk_size_for_query {#max_temporary_data_on_disk_size_for_query} 

<SettingsInfoBlock type="UInt64" default_value="0" />

所有同时运行的查询在磁盘上由临时文件消耗的最大数据量，以字节为单位。

可能值：

- 正整数。
- `0` — 无限（默认）
## max_temporary_data_on_disk_size_for_user {#max_temporary_data_on_disk_size_for_user} 

<SettingsInfoBlock type="UInt64" default_value="0" />

所有同时运行的用户查询在磁盘上由临时文件消耗的最大数据量，以字节为单位。

可能值：

- 正整数。
- `0` — 无限（默认）
## max_temporary_non_const_columns {#max_temporary_non_const_columns} 

<SettingsInfoBlock type="UInt64" default_value="0" />

与 `max_temporary_columns` 类似，必须在运行查询时同时保留的最大临时列数，但不计算常量列。

:::note
常量列在运行查询时相当频繁地生成，但它们几乎不需要计算资源。
:::
## max_threads {#max_threads} 

<SettingsInfoBlock type="MaxThreads" default_value="'auto(N)'" />

查询处理线程的最大数量，不包括从远程服务器检索数据的线程（参见 'max_distributed_connections' 参数）。

此参数适用于在并行中执行查询处理管道的相同步骤的线程。
例如，在从表读取时，如果可以使用至少 'max_threads' 数量的线程并行评估带函数的表达式、用 WHERE 过滤并为 GROUP BY 预聚合，则将使用 'max_threads'。

对于由于 LIMIT 而快速完成的查询，可以设置较低的 'max_threads'。例如，如果必要的条目位于每个块中，并且 max_threads = 8，那么将检索 8 个块，尽管只读取一个就足够了。

`max_threads` 值越小，占用的内存越少。
## max_threads_for_indexes {#max_threads_for_indexes} 

<SettingsInfoBlock type="UInt64" default_value="0" />

处理索引的最大线程数。
## max_untracked_memory {#max_untracked_memory} 

<SettingsInfoBlock type="UInt64" default_value="4194304" />

小的分配和回收被分组为线程局部变量，并且仅在数量（绝对值）超过指定值时跟踪或分析。如果值超过 'memory_profiler_step'，它将有效地降低到 'memory_profiler_step'。
## memory_overcommit_ratio_denominator {#memory_overcommit_ratio_denominator} 

<SettingsInfoBlock type="UInt64" default_value="1073741824" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "22.5"},{"label": "1073741824"},{"label": "Enable memory overcommit feature by default"}]}]}/>

表示在全局级别达到硬限制时的软内存限制。
此值用于计算查询的超量比例。
零表示跳过查询。
有关 [memory overcommit](memory-overcommit.md) 的详细信息。
## memory_overcommit_ratio_denominator_for_user {#memory_overcommit_ratio_denominator_for_user} 

<SettingsInfoBlock type="UInt64" default_value="1073741824" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "22.5"},{"label": "1073741824"},{"label": "Enable memory overcommit feature by default"}]}]}/>

表示在用户级别达到硬限制时的软内存限制。
此值用于计算查询的超量比例。
零表示跳过查询。
有关 [memory overcommit](memory-overcommit.md) 的详细信息。
## memory_profiler_sample_max_allocation_size {#memory_profiler_sample_max_allocation_size} 

<SettingsInfoBlock type="UInt64" default_value="0" />

以 `memory_profiler_sample_probability` 的概率收集小于或等于指定值的随机分配。 0 表示禁用。您可能希望将 'max_untracked_memory' 设置为 0，以使此阈值按预期工作。
## memory_profiler_sample_min_allocation_size {#memory_profiler_sample_min_allocation_size} 

<SettingsInfoBlock type="UInt64" default_value="0" />

以 `memory_profiler_sample_probability` 的概率收集大于或等于指定值的随机分配。 0 表示禁用。您可能希望将 'max_untracked_memory' 设置为 0，以使此阈值按预期工作。
## memory_profiler_sample_probability {#memory_profiler_sample_probability} 

<SettingsInfoBlock type="Float" default_value="0" />

收集随机分配和回收，并将其写入 system.trace_log，类型为 'MemorySample'。这个概率适用于每次分配/释放，无论分配的大小（可以通过 `memory_profiler_sample_min_allocation_size` 和 `memory_profiler_sample_max_allocation_size` 进行更改）。请注意，仅当未追踪的内存量超过 'max_untracked_memory' 时，才会进行采样。您可能希望将 'max_untracked_memory' 设置为 0 以进行更细粒度的采样。
## memory_profiler_step {#memory_profiler_step} 

<SettingsInfoBlock type="UInt64" default_value="4194304" />

设置内存分析器的步长。每当查询内存使用量超过每个次步长的字节数时，内存分析器将收集分配堆栈跟踪并将其写入 [trace_log](/operations/system-tables/trace_log)。

可能的值：

- 正整数（字节数）。

- 0 表示关闭内存分析器。
## memory_tracker_fault_probability {#memory_tracker_fault_probability} 

<SettingsInfoBlock type="Float" default_value="0" />

用于测试 `exception safety` - 以指定概率每次分配内存时抛出异常。
## memory_usage_overcommit_max_wait_microseconds {#memory_usage_overcommit_max_wait_microseconds} 

<SettingsInfoBlock type="UInt64" default_value="5000000" />

当用户级别的内存超量使用时，线程将等待释放内存的最大时间。
如果超时到达且内存未释放，则抛出异常。
有关 [memory overcommit](memory-overcommit.md) 的详细信息。
## merge_table_max_tables_to_look_for_schema_inference {#merge_table_max_tables_to_look_for_schema_inference} 

<SettingsInfoBlock type="UInt64" default_value="1000" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.1"},{"label": "1000"},{"label": "A new setting"}]}]}/>

创建没有显式模式的 `Merge` 表或使用 `merge` 表函数时，推断不超过指定数量的匹配表的模式。
如果表的数量较多，则模式将从指定数量的表中推断。
## merge_tree_coarse_index_granularity {#merge_tree_coarse_index_granularity} 

<SettingsInfoBlock type="UInt64" default_value="8" />

在查找数据时，ClickHouse 将检查索引文件中的数据标记。如果 ClickHouse 发现所需键在某个范围内，它将把该范围划分为 `merge_tree_coarse_index_granularity` 子范围，并在其中递归查找所需键。

可能的值：

- 任何正偶数。
## merge_tree_compact_parts_min_granules_to_multibuffer_read {#merge_tree_compact_parts_min_granules_to_multibuffer_read} 

<CloudAvailableBadge/>

<SettingsInfoBlock type="UInt64" default_value="16" />

仅在 ClickHouse Cloud 中有效。MergeTree 表的紧凑部分条带中用于多缓冲读取的粒子数量，该读取支持并行读取和预取。在从远程文件系统读取时，使用多缓冲读取会增加读取请求的数量。
## merge_tree_determine_task_size_by_prewhere_columns {#merge_tree_determine_task_size_by_prewhere_columns} 

<SettingsInfoBlock type="Bool" default_value="1" />

是否仅使用 prewhere 列的大小来确定读取任务的大小。
## merge_tree_max_bytes_to_use_cache {#merge_tree_max_bytes_to_use_cache} 

<SettingsInfoBlock type="UInt64" default_value="2013265920" />

如果 ClickHouse 在一次查询中应读取超过 `merge_tree_max_bytes_to_use_cache` 字节的数据，则不会使用未压缩块的缓存。

未压缩块的缓存存储为查询提取的数据。ClickHouse 使用此缓存加速对重复小查询的响应。该设置保护缓存不被读取大量数据的查询破坏。 [uncompressed_cache_size](/operations/server-configuration-parameters/settings#uncompressed_cache_size) 服务器设置定义未压缩块缓存的大小。

可能的值：

- 任何正整数。
## merge_tree_max_rows_to_use_cache {#merge_tree_max_rows_to_use_cache} 

<SettingsInfoBlock type="UInt64" default_value="1048576" />

如果 ClickHouse 在一次查询中应读取超过 `merge_tree_max_rows_to_use_cache` 行，则不会使用未压缩块的缓存。

未压缩块的缓存存储为查询提取的数据。ClickHouse 使用此缓存加速对重复小查询的响应。该设置保护缓存不被读取大量数据的查询破坏。 [uncompressed_cache_size](/operations/server-configuration-parameters/settings#uncompressed_cache_size) 服务器设置定义未压缩块缓存的大小。

可能的值：

- 任何正整数。
## merge_tree_min_bytes_for_concurrent_read {#merge_tree_min_bytes_for_concurrent_read} 

<SettingsInfoBlock type="UInt64" default_value="251658240" />

如果要从 [MergeTree](../../engines/table-engines/mergetree-family/mergetree.md) 引擎表的一个文件中读取的字节数超过 `merge_tree_min_bytes_for_concurrent_read`，则 ClickHouse 尝试在多个线程中并发读取该文件。

可能的值：

- 正整数。
## merge_tree_min_bytes_for_concurrent_read_for_remote_filesystem {#merge_tree_min_bytes_for_concurrent_read_for_remote_filesystem} 

<SettingsInfoBlock type="UInt64" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.10"},{"label": "0"},{"label": "Setting is deprecated"}]}]}/>

从一个文件读取的最小字节数，才能在读取来自远程文件系统的 [MergeTree](../../engines/table-engines/mergetree-family/mergetree.md) 引擎时实现并行读取。我们不建议使用此设置。

可能的值：

- 正整数。
## merge_tree_min_bytes_for_seek {#merge_tree_min_bytes_for_seek} 

<SettingsInfoBlock type="UInt64" default_value="0" />

如果要读取的两个数据块在一个文件中的距离小于 `merge_tree_min_bytes_for_seek` 字节，则 ClickHouse 将顺序读取包含这两个块的文件范围，从而避免额外的寻道。

可能的值：

- 任何正整数。
## merge_tree_min_bytes_per_task_for_remote_reading {#merge_tree_min_bytes_per_task_for_remote_reading} 

<SettingsInfoBlock type="UInt64" default_value="2097152" />

每个任务的最小读取字节数。
## merge_tree_min_read_task_size {#merge_tree_min_read_task_size} 

<SettingsInfoBlock type="UInt64" default_value="8" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.10"},{"label": "8"},{"label": "New setting"}]}]}/>

任务大小的硬下限（即使粒子数量少且可用线程数量多，我们也不会分配较小的任务）。
## merge_tree_min_rows_for_concurrent_read {#merge_tree_min_rows_for_concurrent_read} 

<SettingsInfoBlock type="UInt64" default_value="163840" />

如果要从 [MergeTree](../../engines/table-engines/mergetree-family/mergetree.md) 表的一个文件中读取的行数超过 `merge_tree_min_rows_for_concurrent_read`，则 ClickHouse 尝试在多个线程中并发读取该文件。

可能的值：

- 正整数。
## merge_tree_min_rows_for_concurrent_read_for_remote_filesystem {#merge_tree_min_rows_for_concurrent_read_for_remote_filesystem} 

<SettingsInfoBlock type="UInt64" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.10"},{"label": "0"},{"label": "Setting is deprecated"}]}]}/>

从一个文件读取的最小行数，才能在读取来自远程文件系统的 [MergeTree](../../engines/table-engines/mergetree-family/mergetree.md) 引擎时实现并行读取。我们不建议使用此设置。

可能的值：

- 正整数。
## merge_tree_min_rows_for_seek {#merge_tree_min_rows_for_seek} 

<SettingsInfoBlock type="UInt64" default_value="0" />

如果要读取的两个数据块在一个文件中的距离小于 `merge_tree_min_rows_for_seek` 行，则 ClickHouse 不会通过文件寻道，而是顺序读取数据。

可能的值：

- 任何正整数。
## merge_tree_read_split_ranges_into_intersecting_and_non_intersecting_injection_probability {#merge_tree_read_split_ranges_into_intersecting_and_non_intersecting_injection_probability} 

<SettingsInfoBlock type="Float" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.3"},{"label": "0"},{"label": "For testing of `PartsSplitter` - split read ranges into intersecting and non intersecting every time you read from MergeTree with the specified probability."}]}]}/>

用于测试 `PartsSplitter` - 每次从 MergeTree 读取时以指定概率将读取范围拆分为交叉和不交叉。
## merge_tree_use_const_size_tasks_for_remote_reading {#merge_tree_use_const_size_tasks_for_remote_reading} 

<SettingsInfoBlock type="Bool" default_value="1" />

是否在从远程表读取时使用固定大小的任务。
## merge_tree_use_deserialization_prefixes_cache {#merge_tree_use_deserialization_prefixes_cache} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.2"},{"label": "1"},{"label": "A new setting to control the usage of deserialization prefixes cache in MergeTree"}]}]}/>

在 MergeTree 中从宽部分读取时启用文件前缀的列元数据缓存。
## merge_tree_use_prefixes_deserialization_thread_pool {#merge_tree_use_prefixes_deserialization_thread_pool} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.2"},{"label": "1"},{"label": "A new setting controlling the usage of the thread pool for parallel prefixes deserialization in MergeTree"}]}]}/>

在 MergeTree 中启用用于并行前缀读取的线程池。该线程池的大小由服务器设置 `max_prefixes_deserialization_thread_pool_size` 控制。
## merge_tree_use_v1_object_and_dynamic_serialization {#merge_tree_use_v1_object_and_dynamic_serialization} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.11"},{"label": "0"},{"label": "Add new serialization V2 version for JSON and Dynamic types"}]}]}/>

启用后，将在 MergeTree 中使用 JSON 和动态类型的 V1 序列化版本，而不是 V2。更改此设置在服务器重启后才会生效。
## metrics_perf_events_enabled {#metrics_perf_events_enabled} 

<SettingsInfoBlock type="Bool" default_value="0" />

如果启用，则在查询执行过程中将测量某些性能事件。
## metrics_perf_events_list {#metrics_perf_events_list} 

通过逗号分隔的性能指标列表，将在查询执行过程中进行测量。为空表示所有事件。有关可用事件，请参见源中的 PerfEventInfo。
## min_bytes_to_use_direct_io {#min_bytes_to_use_direct_io} 

<SettingsInfoBlock type="UInt64" default_value="0" />

使用直接 I/O 访问存储磁盘所需的最小数据量。

ClickHouse 在从表读取数据时使用此设置。如果要读取的所有数据的总存储量超过 `min_bytes_to_use_direct_io` 字节，则 ClickHouse 会通过 `O_DIRECT` 选项从存储磁盘中读取数据。

可能的值：

- 0 — 禁用直接 I/O。
- 正整数。
## min_bytes_to_use_mmap_io {#min_bytes_to_use_mmap_io} 

<SettingsInfoBlock type="UInt64" default_value="0" />

这是一个实验性设置。设置读取大文件所需的最小内存量，以便不从内核复制数据到用户空间。推荐的阈值约为 64 MB，因为 [mmap/munmap](https://en.wikipedia.org/wiki/Mmap) 的性能较低。它仅适用于大文件，并且仅在数据驻留在页面缓存中时才有帮助。

可能的值：

- 正整数。
- 0 — 仅通过从内核到用户空间复制数据读取大文件。
## min_chunk_bytes_for_parallel_parsing {#min_chunk_bytes_for_parallel_parsing} 

<SettingsInfoBlock type="NonZeroUInt64" default_value="10485760" />

- 类型：无符号整数
- 默认值：1 MiB

每个线程并行解析的最小块大小（以字节为单位）。
## min_compress_block_size {#min_compress_block_size} 

<SettingsInfoBlock type="UInt64" default_value="65536" />

对于 [MergeTree](../../engines/table-engines/mergetree-family/mergetree.md) 表。为了减少处理查询的延迟，如果下一个标记的大小至少为 `min_compress_block_size`，则在写入下一个标记时会压缩一个块。默认值为 65,536。

如果未压缩数据的实际块大小小于 `max_compress_block_size`，则不会小于此值，并且不会小于一个标记的数据量。

我们来看一个例子。假设在创建表时设置 `index_granularity` 为 8192。

我们正在写入一个 UInt32 类型的列（每个值 4 字节）。写入 8192 行时，总共将有 32 KB 的数据。由于 min_compress_block_size = 65,536，将为每两个标记形成一个压缩块。

我们正在写入一个 URL 列，其类型为 String（每个值平均大小为 60 字节）。写入 8192 行时，平均将稍少于 500 KB 的数据。由于这超过了 65,536，因此每个标记都会形成一个压缩块。在这种情况下，从磁盘读取数据时，将不会解压缩额外的数据。

:::note
这是一个专家级设置，如果您刚开始使用 ClickHouse，则不应更改它。
:::
## min_count_to_compile_aggregate_expression {#min_count_to_compile_aggregate_expression} 

<SettingsInfoBlock type="UInt64" default_value="3" />

在启动 JIT 编译之前，必须有的最小相同聚合表达式的数量。仅在启用 [compile_aggregate_expressions](#compile_aggregate_expressions) 设置时有效。

可能的值：

- 正整数。
- 0 — 始终 JIT 编译相同的聚合表达式。
## min_count_to_compile_expression {#min_count_to_compile_expression} 

<SettingsInfoBlock type="UInt64" default_value="3" />

执行相同表达式之前的最小计数，以便进行编译。
## min_count_to_compile_sort_description {#min_count_to_compile_sort_description} 

<SettingsInfoBlock type="UInt64" default_value="3" />

在 JIT 编译之前，最小的相同排序描述数量。
## min_execution_speed {#min_execution_speed} 

<SettingsInfoBlock type="UInt64" default_value="0" />

每秒的最小执行速度（行数）。每个数据块在 [`timeout_before_checking_execution_speed`](/operations/settings/settings#timeout_before_checking_execution_speed) 到期时检查。如果执行速度较低，则抛出异常。
## min_execution_speed_bytes {#min_execution_speed_bytes} 

<SettingsInfoBlock type="UInt64" default_value="0" />

每秒的最小执行字节数。每个数据块在 [`timeout_before_checking_execution_speed`](/operations/settings/settings#timeout_before_checking_execution_speed) 到期时检查。如果执行速度较低，则抛出异常。
## min_external_sort_block_bytes {#min_external_sort_block_bytes} 

<SettingsInfoBlock type="UInt64" default_value="104857600" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.1"},{"label": "104857600"},{"label": "New setting."}]}]}/>

外部排序时将转储到磁盘的最小块大小（以字节为单位），以避免创建过多文件。
## min_external_table_block_size_bytes {#min_external_table_block_size_bytes} 

<SettingsInfoBlock type="UInt64" default_value="268402944" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.2"},{"label": "268402944"},{"label": "Squash blocks passed to external table to specified size in bytes, if blocks are not big enough."}]}]}/>

将传递给外部表的块压缩至指定字节大小（如果块不够大）。
## min_external_table_block_size_rows {#min_external_table_block_size_rows} 

<SettingsInfoBlock type="UInt64" default_value="1048449" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.2"},{"label": "1048449"},{"label": "Squash blocks passed to external table to specified size in rows, if blocks are not big enough"}]}]}/>

将传递给外部表的块压缩至指定行数大小（如果块不够大）。
## min_free_disk_bytes_to_perform_insert {#min_free_disk_bytes_to_perform_insert} 

<SettingsInfoBlock type="UInt64" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.9"},{"label": "0"},{"label": "Maintain some free disk space bytes from inserts while still allowing for temporary writing."}]}, {"id": "row-2","items": [{"label": "24.10"},{"label": "0"},{"label": "New setting."}]}]}/>

执行插入操作所需的最小可用磁盘空间（以字节为单位）。
## min_free_disk_ratio_to_perform_insert {#min_free_disk_ratio_to_perform_insert} 

<SettingsInfoBlock type="Float" default_value="0" />

执行插入操作所需的最小可用磁盘空间比例。
## min_free_disk_space_for_temporary_data {#min_free_disk_space_for_temporary_data} 

<SettingsInfoBlock type="UInt64" default_value="0" />

在写入用于外部排序和聚合的临时数据时，必须保持的最小磁盘空间。
## min_hit_rate_to_use_consecutive_keys_optimization {#min_hit_rate_to_use_consecutive_keys_optimization} 

<SettingsInfoBlock type="Float" default_value="0.5" />

用于保持启用聚合中的连续键优化的缓存的最小命中率。
## min_insert_block_size_bytes {#min_insert_block_size_bytes} 

<SettingsInfoBlock type="UInt64" default_value="268402944" />

设置可以通过 `INSERT` 查询插入到表中的块中的最小字节数。小块将被压缩到较大的块中。

可能的值：

- 正整数。
- 0 — 禁用压缩。
## min_insert_block_size_bytes_for_materialized_views {#min_insert_block_size_bytes_for_materialized_views} 

<SettingsInfoBlock type="UInt64" default_value="0" />

设置可以通过 `INSERT` 查询插入到表中的块中的最小字节数。小块将被压缩到较大的块中。此设置仅适用于插入到 [物化视图](../../sql-reference/statements/create/view.md) 的块。通过调整此设置，您可以控制在推送到物化视图时的块压缩，并避免过量使用内存。

可能的值：

- 任何正整数。
- 0 — 禁用压缩。

**另见**

- [min_insert_block_size_bytes](#min_insert_block_size_bytes)
## min_insert_block_size_rows {#min_insert_block_size_rows} 

<SettingsInfoBlock type="UInt64" default_value="1048449" />

设置可以通过 `INSERT` 查询插入到表中的块中的最小行数。小块将被压缩到较大的块中。

可能的值：

- 正整数。
- 0 — 禁用压缩。
## min_insert_block_size_rows_for_materialized_views {#min_insert_block_size_rows_for_materialized_views} 

<SettingsInfoBlock type="UInt64" default_value="0" />

设置可以通过 `INSERT` 查询插入到表中的块中的最小行数。小块将被压缩到较大的块中。此设置仅适用于插入到 [物化视图](../../sql-reference/statements/create/view.md) 的块。通过调整此设置，您可以控制在推送到物化视图时的块压缩，并避免过量使用内存。

可能的值：

- 任何正整数。
- 0 — 禁用压缩。

**另见**

- [min_insert_block_size_rows](#min_insert_block_size_rows)
## min_joined_block_size_bytes {#min_joined_block_size_bytes} 

<SettingsInfoBlock type="UInt64" default_value="524288" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.11"},{"label": "524288"},{"label": "New setting."}]}]}/>

JOIN 结果的最小块大小（如果连接算法支持）。 0 表示无限制。
## min_os_cpu_wait_time_ratio_to_throw {#min_os_cpu_wait_time_ratio_to_throw} 

<SettingsInfoBlock type="Float" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.5"},{"label": "0"},{"label": "Setting values were changed and backported to 25.4"}]}, {"id": "row-2","items": [{"label": "25.4"},{"label": "0"},{"label": "New setting"}]}]}/>

在考虑拒绝查询之前，操作系统 CPU 等待（OSCPUWaitMicroseconds 指标）与忙碌（OSCPUVirtualTimeMicroseconds 指标）时间之间的最小比率。通过线性插值在最小和最大比率之间计算概率，此时概率为 0。
## mongodb_throw_on_unsupported_query {#mongodb_throw_on_unsupported_query} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.9"},{"label": "1"},{"label": "New setting."}]}, {"id": "row-2","items": [{"label": "24.10"},{"label": "1"},{"label": "New setting."}]}]}/>

如果启用，当无法构建 MongoDB 查询时，MongoDB 表将返回错误。否则，ClickHouse 会读取整个表并在本地处理。此选项不适用于 'allow_experimental_analyzer=0'。
## move_all_conditions_to_prewhere {#move_all_conditions_to_prewhere} 

<SettingsInfoBlock type="Bool" default_value="1" />

将所有可行的条件从 WHERE 移动到 PREWHERE。
## move_primary_key_columns_to_end_of_prewhere {#move_primary_key_columns_to_end_of_prewhere} 

<SettingsInfoBlock type="Bool" default_value="1" />

将包含主键列的 PREWHERE 条件移动到 AND 链的末尾。这些条件在主键分析过程中可能会被考虑，因此不会对 PREWHERE 过滤产生很大贡献。
## multiple_joins_try_to_keep_original_names {#multiple_joins_try_to_keep_original_names} 

<SettingsInfoBlock type="Bool" default_value="0" />

在多个连接重写时，不要向顶级表达式列表添加别名。
## mutations_execute_nondeterministic_on_initiator {#mutations_execute_nondeterministic_on_initiator} 

<SettingsInfoBlock type="Bool" default_value="0" />

如果为真，常量非确定性函数（例如 function `now()`）将在发起者上执行，并在 `UPDATE` 和 `DELETE` 查询中替换为字面量。这有助于在执行带有常量非确定性函数的变更时保持副本中的数据同步。默认值：`false`。
## mutations_execute_subqueries_on_initiator {#mutations_execute_subqueries_on_initiator} 

<SettingsInfoBlock type="Bool" default_value="0" />

如果为真，标量子查询将在发起者上执行，并在 `UPDATE` 和 `DELETE` 查询中替换为字面量。默认值：`false`。
## mutations_max_literal_size_to_replace {#mutations_max_literal_size_to_replace} 

<SettingsInfoBlock type="UInt64" default_value="16384" />

在 `UPDATE` 和 `DELETE` 查询中要替换的序列化字面量的最大字节大小。仅在上面至少启用一个设置时生效。默认值：16384（16 KiB）。
## mutations_sync {#mutations_sync} 

<SettingsInfoBlock type="UInt64" default_value="0" />

允许同步执行 `ALTER TABLE ... UPDATE|DELETE|MATERIALIZE INDEX|MATERIALIZE PROJECTION|MATERIALIZE COLUMN|MATERIALIZE STATISTICS` 查询（[mutations](../../sql-reference/statements/alter/index.md/#mutations)）。

可能的值：

- 0 - 变更异步执行。
- 1 - 查询等待当前服务器上的所有变更完成。
- 2 - 查询等待所有副本上的所有变更完成（如果存在）。
## mysql_datatypes_support_level {#mysql_datatypes_support_level} 

定义如何将 MySQL 类型转换为相应的 ClickHouse 类型。以逗号分隔的列表，可以组合 `decimal`、`datetime64`、`date2Date32` 或 `date2String`。
- `decimal`: 在允许精度时将 `NUMERIC` 和 `DECIMAL` 类型转换为 `Decimal`。
- `datetime64`: 在精度不为 `0` 时将 `DATETIME` 和 `TIMESTAMP` 类型转换为 `DateTime64` 而不是 `DateTime`。
- `date2Date32`: 将 `DATE` 转换为 `Date32` 而不是 `Date`。优先于 `date2String`。
- `date2String`: 将 `DATE` 转换为 `String` 而不是 `Date`。被 `datetime64` 重写。
## mysql_map_fixed_string_to_text_in_show_columns {#mysql_map_fixed_string_to_text_in_show_columns} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.2"},{"label": "1"},{"label": "Reduce the configuration effort to connect ClickHouse with BI tools."}]}]}/>

启用时，ClickHouse 数据类型 [FixedString](../../sql-reference/data-types/fixedstring.md) 在 [SHOW COLUMNS](../../sql-reference/statements/show.md/#show_columns) 中将显示为 `TEXT`。

仅在通过 MySQL 线协议连接时生效。

- 0 - 使用 `BLOB`。
- 1 - 使用 `TEXT`。
## mysql_map_string_to_text_in_show_columns {#mysql_map_string_to_text_in_show_columns} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.2"},{"label": "1"},{"label": "Reduce the configuration effort to connect ClickHouse with BI tools."}]}]}/>

启用时，ClickHouse 数据类型 [String](../../sql-reference/data-types/string.md) 在 [SHOW COLUMNS](../../sql-reference/statements/show.md/#show_columns) 中将显示为 `TEXT`。

仅在通过 MySQL 线协议连接时生效。

- 0 - 使用 `BLOB`。
- 1 - 使用 `TEXT`。
## mysql_max_rows_to_insert {#mysql_max_rows_to_insert} 

<SettingsInfoBlock type="UInt64" default_value="65536" />

MySQL 存储引擎中 MySQL 批量插入的最大行数。
## network_compression_method {#network_compression_method} 

<SettingsInfoBlock type="String" default_value="LZ4" />

设置用于服务器之间以及服务器与 [clickhouse-client](../../interfaces/cli.md) 之间通信的数据压缩方法。

可能的值：

- `LZ4` — 设置 LZ4 压缩方法。
- `ZSTD` — 设置 ZSTD 压缩方法。

**另请参见**

- [network_zstd_compression_level](#network_zstd_compression_level)
## network_zstd_compression_level {#network_zstd_compression_level} 

<SettingsInfoBlock type="Int64" default_value="1" />

调整 ZSTD 压缩级别。仅在 [network_compression_method](#network_compression_method) 设置为 `ZSTD` 时使用。

可能的值：

- 从 1 到 15 的正整数。
## normalize_function_names {#normalize_function_names} 

<SettingsInfoBlock type="Bool" default_value="1" />

将函数名称规范化为它们的规范名称。
## number_of_mutations_to_delay {#number_of_mutations_to_delay} 

<SettingsInfoBlock type="UInt64" default_value="0" />

如果变更的表至少包含这么多未完成的变更，则人工作为变更的慢下去。0 - 禁用。
## number_of_mutations_to_throw {#number_of_mutations_to_throw} 

<SettingsInfoBlock type="UInt64" default_value="0" />

如果变更的表至少包含这么多未完成的变更，则抛出 '变更过多 ...' 异常。0 - 禁用。
## odbc_bridge_connection_pool_size {#odbc_bridge_connection_pool_size} 

<SettingsInfoBlock type="UInt64" default_value="16" />

ODBC 桥中每个连接设置字符串的连接池大小。
## odbc_bridge_use_connection_pooling {#odbc_bridge_use_connection_pooling} 

<SettingsInfoBlock type="Bool" default_value="1" />

在 ODBC 桥中使用连接池。如果设置为 false，每次都会创建一个新连接。
## offset {#offset} 

<SettingsInfoBlock type="UInt64" default_value="0" />

设置在开始返回查询的行之前要跳过的行数。它会调整由 [OFFSET](/sql-reference/statements/select/offset) 子句设置的偏移量，使这两个值相加。

可能的值：

- 0 — 不跳过任何行。
- 正整数。

**示例**

输入表：

```sql
CREATE TABLE test (i UInt64) ENGINE = MergeTree() ORDER BY i;
INSERT INTO test SELECT number FROM numbers(500);
```

查询：

```sql
SET limit = 5;
SET offset = 7;
SELECT * FROM test LIMIT 10 OFFSET 100;
```
结果：

```text
┌───i─┐
│ 107 │
│ 108 │
│ 109 │
└─────┘
```
## opentelemetry_start_trace_probability {#opentelemetry_start_trace_probability} 

<SettingsInfoBlock type="Float" default_value="0" />

设置 ClickHouse 可以为执行的查询启动跟踪的概率（如果没有提供父级 [trace context](https://www.w3.org/TR/trace-context/)）。

可能的值：

- 0 — 禁用所有执行查询的跟踪（如果没有提供父级跟踪上下文）。
- 范围在 [0..1] 之间的正浮点数。例如，如果设置值为 `0.5`，则 ClickHouse 平均可以为一半的查询启动跟踪。
- 1 — 启用所有执行查询的跟踪。
## opentelemetry_trace_processors {#opentelemetry_trace_processors} 

<SettingsInfoBlock type="Bool" default_value="0" />

为处理程序收集 OpenTelemetry spans。
## optimize_aggregation_in_order {#optimize_aggregation_in_order} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用在 [SELECT](../../sql-reference/statements/select/index.md) 查询中对 [GROUP BY](/sql-reference/statements/select/group-by) 的优化，以按对应顺序在 [MergeTree](../../engines/table-engines/mergetree-family/mergetree.md) 表中聚合数据。

可能的值：

- 0 — 禁用 `GROUP BY` 优化。
- 1 — 启用 `GROUP BY` 优化。

**另请参见**

- [GROUP BY 优化](/sql-reference/statements/select/group-by#group-by-optimization-depending-on-table-sorting-key)
## optimize_aggregators_of_group_by_keys {#optimize_aggregators_of_group_by_keys} 

<SettingsInfoBlock type="Bool" default_value="1" />

在 SELECT 部分消除 GROUP BY 键的 min/max/any/anyLast 聚合器。
## optimize_and_compare_chain {#optimize_and_compare_chain} 

<SettingsInfoBlock type="Bool" default_value="1" />

在 AND 链中填充常量比较，以增强过滤能力。支持运算符 `<`、`<=`、`>`、`>=`、`=` 和这些运算符的组合。例如，`(a < b) AND (b < c) AND (c < 5)` 将变为 `(a < b) AND (b < c) AND (c < 5) AND (b < 5) AND (a < 5)`。
## optimize_append_index {#optimize_append_index} 

<SettingsInfoBlock type="Bool" default_value="0" />

使用 [constraints](../../sql-reference/statements/create/table.md/#constraints) 以附加索引条件。 默认值为 `false`。

可能的值：

- true，false
## optimize_arithmetic_operations_in_aggregate_functions {#optimize_arithmetic_operations_in_aggregate_functions} 

<SettingsInfoBlock type="Bool" default_value="1" />

将算术运算移动到聚合函数之外。
## optimize_count_from_files {#optimize_count_from_files} 

<SettingsInfoBlock type="Bool" default_value="1" />

启用或禁用从不同输入格式文件中计数行数的优化。适用于表函数/引擎 `file`/`s3`/`url`/`hdfs`/`azureBlobStorage`。

可能的值：

- 0 — 禁用优化。
- 1 — 启用优化。
## optimize_distinct_in_order {#optimize_distinct_in_order} 

<SettingsInfoBlock type="Bool" default_value="1" />

如果某些列在 DISTINCT 中形成排序的前缀，启用 DISTINCT 优化。例如，合并树或 ORDER BY 声明的排序键的前缀。
## optimize_distributed_group_by_sharding_key {#optimize_distributed_group_by_sharding_key} 

<SettingsInfoBlock type="Bool" default_value="1" />

优化 `GROUP BY sharding_key` 查询，通过避免在发起者服务器上进行昂贵的聚合（这将减少发起者服务器上查询的内存使用）。

支持以下类型的查询（及其所有组合）：

- `SELECT DISTINCT [..., ]sharding_key[, ...] FROM dist`
- `SELECT ... FROM dist GROUP BY sharding_key[, ...]`
- `SELECT ... FROM dist GROUP BY sharding_key[, ...] ORDER BY x`
- `SELECT ... FROM dist GROUP BY sharding_key[, ...] LIMIT 1`
- `SELECT ... FROM dist GROUP BY sharding_key[, ...] LIMIT 1 BY x`

以下类型的查询不支持（可能会稍后支持其中一些）：

- `SELECT ... GROUP BY sharding_key[, ...] WITH TOTALS`
- `SELECT ... GROUP BY sharding_key[, ...] WITH ROLLUP`
- `SELECT ... GROUP BY sharding_key[, ...] WITH CUBE`
- `SELECT ... GROUP BY sharding_key[, ...] SETTINGS extremes=1`

可能的值：

- 0 — 禁用。
- 1 — 启用。

另请参见：

- [distributed_group_by_no_merge](#distributed_group_by_no_merge)
- [distributed_push_down_limit](#distributed_push_down_limit)
- [optimize_skip_unused_shards](#optimize_skip_unused_shards)

:::note
目前需要 `optimize_skip_unused_shards`（原因是将来可能会默认启用，仅在数据通过分布式表插入时才能正确工作，即数据根据 sharding_key 分布）。
:::
## optimize_extract_common_expressions {#optimize_extract_common_expressions} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.1"},{"label": "1"},{"label": "Optimize WHERE, PREWHERE, ON, HAVING and QUALIFY expressions by extracting common expressions out from disjunction of conjunctions."}]}, {"id": "row-2","items": [{"label": "24.12"},{"label": "0"},{"label": "Introduce setting to optimize WHERE, PREWHERE, ON, HAVING and QUALIFY expressions by extracting common expressions out from disjunction of conjunctions."}]}]}/>

允许从 WHERE、PREWHERE、ON、HAVING 和 QUALIFY 表达式中的析取中提取公共表达式。像 `(A AND B) OR (A AND C)` 这样的逻辑表达式可以重写为 `A AND (B OR C)`，这可能有助于利用：
- 简单过滤表达式中的索引
- 交叉到内部连接优化
## optimize_functions_to_subcolumns {#optimize_functions_to_subcolumns} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.8"},{"label": "1"},{"label": "Enabled settings by default"}]}]}/>

启用或禁用通过将某些函数转换为读取子列的优化。这会减少要读取的数据量。

可以转换的函数：

- [length](/sql-reference/functions/array-functions#length) 转换为读取 [size0](../../sql-reference/data-types/array.md/#array-size) 子列。
- [empty](/sql-reference/functions/array-functions#empty) 转换为读取 [size0](../../sql-reference/data-types/array.md/#array-size) 子列。
- [notEmpty](/sql-reference/functions/array-functions#notempty) 转换为读取 [size0](../../sql-reference/data-types/array.md/#array-size) 子列。
- [isNull](/sql-reference/functions/functions-for-nulls#isnull) 转换为读取 [null](../../sql-reference/data-types/nullable.md/#finding-null) 子列。
- [isNotNull](/sql-reference/functions/functions-for-nulls#isnotnull) 转换为读取 [null](../../sql-reference/data-types/nullable.md/#finding-null) 子列。
- [count](/sql-reference/aggregate-functions/reference/count) 转换为读取 [null](../../sql-reference/data-types/nullable.md/#finding-null) 子列。
- [mapKeys](/sql-reference/functions/tuple-map-functions#mapkeys) 转换为读取 [keys](/sql-reference/data-types/map#reading-subcolumns-of-map) 子列。
- [mapValues](/sql-reference/functions/tuple-map-functions#mapvalues) 转换为读取 [values](/sql-reference/data-types/map#reading-subcolumns-of-map) 子列。

可能的值：

- 0 — 禁用优化。
- 1 — 启用优化。
## optimize_group_by_constant_keys {#optimize_group_by_constant_keys} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "23.9"},{"label": "1"},{"label": "Optimize group by constant keys by default"}]}]}/>

在块中所有键都是常量时优化 GROUP BY。
## optimize_group_by_function_keys {#optimize_group_by_function_keys} 

<SettingsInfoBlock type="Bool" default_value="1" />

消除 GROUP BY 部分中其他键的函数。
## optimize_if_chain_to_multiif {#optimize_if_chain_to_multiif} 

<SettingsInfoBlock type="Bool" default_value="0" />

将 if(cond1, then1, if(cond2, ...)) 链替换为 multiIf。目前对数值类型没有好处。
## optimize_if_transform_strings_to_enum {#optimize_if_transform_strings_to_enum} 

<SettingsInfoBlock type="Bool" default_value="0" />

将 If 和 Transform 中的字符串类型参数替换为枚举。默认禁用，因为这可能会在分布式查询中造成不一致的改变，导致其失败。
## optimize_injective_functions_in_group_by {#optimize_injective_functions_in_group_by} 

<SettingsInfoBlock type="Bool" default_value="1" />

在 GROUP BY 部分用其参数替换注入函数。
## optimize_injective_functions_inside_uniq {#optimize_injective_functions_inside_uniq} 

删除 uniq*() 函数内部的单参数注入函数。
## optimize_min_equality_disjunction_chain_length {#optimize_min_equality_disjunction_chain_length} 

优化的最小表达式链长度 `expr = x1 OR ... expr = xN`。
## optimize_min_inequality_conjunction_chain_length {#optimize_min_inequality_conjunction_chain_length} 

优化的最小表达式链长度 `expr <> x1 AND ... expr <> xN`。
## optimize_move_to_prewhere {#optimize_move_to_prewhere} 

启用或禁用在 [SELECT](../../sql-reference/statements/select/index.md) 查询中的自动 [PREWHERE](../../sql-reference/statements/select/prewhere.md) 优化。

仅适用于 [*MergeTree](../../engines/table-engines/mergetree-family/index.md) 表。

可能的值：

- 0 — 禁用自动 `PREWHERE` 优化。
- 1 — 启用自动 `PREWHERE` 优化。
## optimize_move_to_prewhere_if_final {#optimize_move_to_prewhere_if_final} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用在带有 [FINAL](/sql-reference/statements/select/from#final-modifier) 修饰符的 [SELECT](../../sql-reference/statements/select/index.md) 查询中的自动 [PREWHERE](../../sql-reference/statements/select/prewhere.md) 优化。

仅适用于 [*MergeTree](../../engines/table-engines/mergetree-family/index.md) 表。

可能的值：

- 0 — 在带有 `FINAL` 修饰符的 `SELECT` 查询中禁用自动 `PREWHERE` 优化。
- 1 — 在带有 `FINAL` 修饰符的 `SELECT` 查询中启用自动 `PREWHERE` 优化。

**另请参见**

- [optimize_move_to_prewhere](#optimize_move_to_prewhere) 设置。
## optimize_multiif_to_if {#optimize_multiif_to_if} 

<SettingsInfoBlock type="Bool" default_value="1" />

将仅有一个条件的 'multiIf' 替换为 'if'。
## optimize_normalize_count_variants {#optimize_normalize_count_variants} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "21.3"},{"label": "1"},{"label": "Rewrite aggregate functions that semantically equals to count() as count() by default"}]}]}/>

重写在语义上等于 count() 的聚合函数为 count()。
## optimize_on_insert {#optimize_on_insert} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "21.1"},{"label": "1"},{"label": "Enable data optimization on INSERT by default for better user experience"}]}]}/>

启用或禁用插入前的数据转换，就像在此块上执行了合并一样（根据表引擎）。

可能的值：

- 0 — 禁用。
- 1 — 启用。

**示例**

启用和禁用之间的差异：

查询：

```sql
SET optimize_on_insert = 1;

CREATE TABLE test1 (`FirstTable` UInt32) ENGINE = ReplacingMergeTree ORDER BY FirstTable;

INSERT INTO test1 SELECT number % 2 FROM numbers(5);

SELECT * FROM test1;

SET optimize_on_insert = 0;

CREATE TABLE test2 (`SecondTable` UInt32) ENGINE = ReplacingMergeTree ORDER BY SecondTable;

INSERT INTO test2 SELECT number % 2 FROM numbers(5);

SELECT * FROM test2;
```

结果：

```text
┌─FirstTable─┐
│          0 │
│          1 │
└────────────┘

┌─SecondTable─┐
│           0 │
│           0 │
│           0 │
│           1 │
│           1 │
└─────────────┘
```

注意，此设置会影响 [物化视图](/sql-reference/statements/create/view#materialized-view) 的行为。
## optimize_or_like_chain {#optimize_or_like_chain} 

<SettingsInfoBlock type="Bool" default_value="0" />

将多个 OR LIKE 优化为 multiMatchAny。此优化默认不应启用，因为在某些情况下会违反索引分析。
## optimize_read_in_order {#optimize_read_in_order} 

<SettingsInfoBlock type="Bool" default_value="1" />

启用在 [SELECT](../../sql-reference/statements/select/index.md) 查询中从 [MergeTree](../../engines/table-engines/mergetree-family/mergetree.md) 表读取数据的 [ORDER BY](/sql-reference/statements/select/order-by#optimization-of-data-reading) 优化。

可能的值：

- 0 — 禁用 `ORDER BY` 优化。
- 1 — 启用 `ORDER BY` 优化。

**另请参见**

- [ORDER BY 子句](/sql-reference/statements/select/order-by#optimization-of-data-reading)
## optimize_read_in_window_order {#optimize_read_in_window_order} 

<SettingsInfoBlock type="Bool" default_value="1" />

在窗口子句中启用 ORDER BY 优化，以对应顺序读取 MergeTree 表中的数据。
## optimize_redundant_functions_in_order_by {#optimize_redundant_functions_in_order_by} 

<SettingsInfoBlock type="Bool" default_value="1" />

如果 ORDER BY 的参数也在 ORDER BY 中，则从 ORDER BY 中移除函数。
## optimize_respect_aliases {#optimize_respect_aliases} 

<SettingsInfoBlock type="Bool" default_value="1" />

如果设置为 true，那么它将尊重 WHERE/GROUP BY/ORDER BY 中的别名，这将有助于分区修剪/二级索引/ optimize_aggregation_in_order/optimize_read_in_order/optimize_trivial_count。
## optimize_rewrite_aggregate_function_with_if {#optimize_rewrite_aggregate_function_with_if} 

<SettingsInfoBlock type="Bool" default_value="1" />

重写带有 if 表达式作为参数的聚合函数，当它们逻辑等价时。例如，`avg(if(cond, col, null))` 可以重写为 `avgOrNullIf(cond, col)`。这可能提高性能。

:::note
仅在分析器启用时支持（`enable_analyzer = 1`）。
:::
## optimize_rewrite_array_exists_to_has {#optimize_rewrite_array_exists_to_has} 

<SettingsInfoBlock type="Bool" default_value="0" />

当逻辑等价时，将 arrayExists() 函数重写为 has()。例如，arrayExists(x -> x = 1, arr) 可以重写为 has(arr, 1)。
## optimize_rewrite_sum_if_to_count_if {#optimize_rewrite_sum_if_to_count_if} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.4"},{"label": "1"},{"label": "Only available for the analyzer, where it works correctly"}]}]}/>

将 sumIf() 和 sum(if()) 函数重写为 countIf() 函数，当它们逻辑等价时。
## optimize_skip_merged_partitions {#optimize_skip_merged_partitions} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用优化 [OPTIMIZE TABLE ... FINAL](../../sql-reference/statements/optimize.md) 查询，如果只有一个级别大于 0 的部分，并且没有过期的生存时间。

- `OPTIMIZE TABLE ... FINAL SETTINGS optimize_skip_merged_partitions=1`

默认情况下，`OPTIMIZE TABLE ... FINAL` 查询即使只有一个部分也会重写该部分。

可能的值：

- 1 - 启用优化。
- 0 - 禁用优化。
## optimize_skip_unused_shards {#optimize_skip_unused_shards} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用跳过未使用的分片，以用于在 `WHERE/PREWHERE` 中具有 sharding key 条件的 [SELECT](../../sql-reference/statements/select/index.md) 查询（假设数据按 sharding key 分布，否则查询将产生不正确的结果）。

可能的值：

- 0 — 禁用。
- 1 — 启用。
## optimize_skip_unused_shards_limit {#optimize_skip_unused_shards_limit} 

<SettingsInfoBlock type="UInt64" default_value="1000" />

sharding key 值的数量限制，如果达到该限制，将关闭 `optimize_skip_unused_shards`。

值过多可能需要大量处理，而收益值得怀疑，因为如果你在 `IN (...)` 中有大量值，那么查询很可能无论如何会发送到所有分片。
## optimize_skip_unused_shards_nesting {#optimize_skip_unused_shards_nesting} 

<SettingsInfoBlock type="UInt64" default_value="0" />

控制 [`optimize_skip_unused_shards`](#optimize_skip_unused_shards) 取决于分布式查询的嵌套级别（例如当你有一个 `Distributed` 表查看另一个 `Distributed` 表时）。

可能的值：

- 0 — 禁用，`optimize_skip_unused_shards` 始终有效。
- 1 — 仅对第一级启用 `optimize_skip_unused_shards`。
- 2 — 对第二级启用 `optimize_skip_unused_shards`。
## optimize_skip_unused_shards_rewrite_in {#optimize_skip_unused_shards_rewrite_in} 

<SettingsInfoBlock type="Bool" default_value="1" />

重写远程分片的查询中的 IN，以排除不属于该分片的值（需要 optimize_skip_unused_shards）。

可能的值：

- 0 — 禁用。
- 1 — 启用。
## optimize_sorting_by_input_stream_properties {#optimize_sorting_by_input_stream_properties} 

<SettingsInfoBlock type="Bool" default_value="1" />

根据输入流的排序属性优化排序。
## optimize_substitute_columns {#optimize_substitute_columns} 

<SettingsInfoBlock type="Bool" default_value="0" />

使用 [constraints](../../sql-reference/statements/create/table.md/#constraints) 进行列替换。默认值为 `false`。

可能的值：

- true，false
## optimize_syntax_fuse_functions {#optimize_syntax_fuse_functions} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用根据相同参数合并聚合函数。它重写包含至少两个相同参数的 [sum](/sql-reference/aggregate-functions/reference/sum)、[count](/sql-reference/aggregate-functions/reference/count) 或 [avg](/sql-reference/aggregate-functions/reference/avg) 聚合函数的查询为 [sumCount](/sql-reference/aggregate-functions/reference/sumcount)。

可能的值：

- 0 — 不合并相同参数的函数。
- 1 — 合并相同参数的函数。

**示例**

查询：

```sql
CREATE TABLE fuse_tbl(a Int8, b Int8) Engine = Log;
SET optimize_syntax_fuse_functions = 1;
EXPLAIN SYNTAX SELECT sum(a), sum(b), count(b), avg(b) from fuse_tbl FORMAT TSV;
```

结果：

```text
SELECT
    sum(a),
    sumCount(b).1,
    sumCount(b).2,
    (sumCount(b).1) / (sumCount(b).2)
FROM fuse_tbl
```
## optimize_throw_if_noop {#optimize_throw_if_noop} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用在 [OPTIMIZE](../../sql-reference/statements/optimize.md) 查询未执行合并时抛出异常。

默认情况下，如果未进行任何操作，`OPTIMIZE` 会成功返回。此设置允许您区分这些情况并在异常消息中获取原因。

可能的值：

- 1 — 启用抛出异常。
- 0 — 禁用抛出异常。
## optimize_time_filter_with_preimage {#optimize_time_filter_with_preimage} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.2"},{"label": "1"},{"label": "通过转换函数为等效比较来优化日期和日期时间谓词，而不进行转换（例如 `toYear(col) = 2023 -> col >= '2023-01-01' AND col <= '2023-12-31'`）"}]}]}/>

通过转换函数为等效比较来优化日期和日期时间谓词，而不进行转换（例如 `toYear(col) = 2023 -> col >= '2023-01-01' AND col <= '2023-12-31'`）。
## optimize_trivial_approximate_count_query {#optimize_trivial_approximate_count_query} 

<SettingsInfoBlock type="Bool" default_value="0" />

使用用于支持此类估计的存储的近似值进行琐碎计数优化，例如 EmbeddedRocksDB。

可能的值：

- 0 — 禁用优化。
- 1 — 启用优化。
## optimize_trivial_count_query {#optimize_trivial_count_query} 

<SettingsInfoBlock type="Bool" default_value="1" />

启用或禁用优化琐碎查询 `SELECT count() FROM table`，使用 MergeTree 的元数据。如果您需要使用行级安全性，请禁用此设置。

可能的值：

- 0 — 禁用优化。
- 1 — 启用优化。

另请参见：

- [optimize_functions_to_subcolumns](#optimize_functions_to_subcolumns)
## optimize_trivial_insert_select {#optimize_trivial_insert_select} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.7"},{"label": "0"},{"label": "The optimization does not make sense in many cases."}]}]}/>

优化琐碎的 'INSERT INTO table SELECT ... FROM TABLES' 查询。
## optimize_uniq_to_count {#optimize_uniq_to_count} 

<SettingsInfoBlock type="Bool" default_value="1" />

在子查询具有 DISTINCT 或 GROUP BY 子句时，将 uniq 及其变体（除 uniqUpTo 外）重写为 count。
## optimize_use_implicit_projections {#optimize_use_implicit_projections} 

<SettingsInfoBlock type="Bool" default_value="1" />

自动选择隐式投影以执行 SELECT 查询。
## optimize_use_projections {#optimize_use_projections} 

<SettingsInfoBlock type="Bool" default_value="1" />

启用或禁用在处理 `SELECT` 查询时的 [projection](../../engines/table-engines/mergetree-family/mergetree.md/#projections) 优化。

可能的值：

- 0 — 禁用投影优化。
- 1 — 启用投影优化。
## optimize_using_constraints {#optimize_using_constraints} 

<SettingsInfoBlock type="Bool" default_value="0" />

在查询优化时使用 [constraints](../../sql-reference/statements/create/table.md/#constraints)。默认值 `false`。

可能的值：

- true，false
## os_thread_priority {#os_thread_priority} 

<SettingsInfoBlock type="Int64" default_value="0" />

设置执行查询的线程的优先级（[nice](https://en.wikipedia.org/wiki/Nice_(Unix)））。操作系统调度器在选择要在每个可用 CPU 核心上运行的下一个线程时会考虑此优先级。

:::note
要使用此设置，您需要设置 `CAP_SYS_NICE` 权限。`clickhouse-server` 包在安装期间进行设置。某些虚拟环境不允许您设置 `CAP_SYS_NICE` 权限。在这种情况下，`clickhouse-server` 在启动时会显示相关消息。
:::

可能的值：

- 您可以在 `[-20, 19]` 范围内设置值。

较低的值意味着更高的优先级。具有低 `nice` 优先级值的线程比高值的线程执行得更频繁。高值更适合长时间运行的非交互式查询，因为可以在到达时迅速让出资源以支持短时间的交互式查询。
## output_format_compression_level {#output_format_compression_level} 

<SettingsInfoBlock type="UInt64" default_value="3" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.1"},{"label": "3"},{"label": "Allow to change compression level in the query output"}]}]}/>

如果查询输出被压缩，则默认压缩级别。当 `SELECT` 查询具有 `INTO OUTFILE` 时应用，或在写入表函数 `file`、`url`、`hdfs`、`s3` 或 `azureBlobStorage` 时。

可能的值：从 `1` 到 `22`。
## output_format_compression_zstd_window_log {#output_format_compression_zstd_window_log} 

<SettingsInfoBlock type="UInt64" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.1"},{"label": "0"},{"label": "Allow to change zstd window log in the query output when zstd compression is used"}]}]}/>

当输出压缩方法为 `zstd` 时使用。如果大于 `0`，此设置明确设置压缩窗口大小（2 的幂），并启用 zstd 压缩的长范围模式。这可以帮助获得更好的压缩比。

可能的值：非负数。请注意，如果值太小或太大，`zstdlib` 将引发异常。典型值从 `20`（窗口大小 = `1MB`）到 `30`（窗口大小 = `1GB`）。
## output_format_parallel_formatting {#output_format_parallel_formatting} 

<SettingsInfoBlock type="Bool" default_value="1" />

启用或禁用数据格式的并行格式化。仅支持 [TSV](../../interfaces/formats.md/#tabseparated) 、[TSKV](../../interfaces/formats.md/#tskv) 、[CSV](../../interfaces/formats.md/#csv) 和 [JSONEachRow](../../interfaces/formats.md/#jsoneachrow) 格式。

可能的值：

- 1 — 启用。
- 0 — 禁用。
## page_cache_block_size {#page_cache_block_size} 

<SettingsInfoBlock type="UInt64" default_value="1048576" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.5"},{"label": "1048576"},{"label": "Made this setting adjustable on a per-query level."}]}]}/>

在用户空间页面缓存中存储的文件块的大小，以字节为单位。所有经过缓存的读取都会向上取整为此大小的倍数。

此设置可以按查询级别进行调整，但具有不同块大小的缓存项不能重用。更改此设置会有效使现有缓存中的条目失效。

较高的值，例如 1 MiB 适合于高吞吐量查询，而较低的值，例如 64 KiB 适合于低延迟点查询。
## page_cache_inject_eviction {#page_cache_inject_eviction} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.3"},{"label": "0"},{"label": "Added userspace page cache"}]}]}/>

用户空间页面缓存将有时随机使某些页面失效。旨在进行测试。
## page_cache_lookahead_blocks {#page_cache_lookahead_blocks} 

<SettingsInfoBlock type="UInt64" default_value="16" />

在用户空间页面缓存未命中的情况下，尽可能从底层存储一次读取如此多连续的块，如果它们不在缓存中。每个块的大小为 page_cache_block_size 字节。

较高的值适合高吞吐量查询，而低延迟点查询则在没有前置读取的情况下工作更好。
## parallel_distributed_insert_select {#parallel_distributed_insert_select} 

<SettingsInfoBlock type="UInt64" default_value="0" />

启用并行分布式 `INSERT ... SELECT` 查询。

如果我们执行 `INSERT INTO distributed_table_a SELECT ... FROM distributed_table_b` 查询，并且两个表使用相同的集群，并且这两个表都是 [复制的](../../engines/table-engines/mergetree-family/replication.md) 或非复制的，则该查询在每个分片上本地处理。

可能的值：

- 0 — 禁用。
- 1 — `SELECT` 将在分布式引擎的底层表的每个分片上执行。
- 2 — `SELECT` 和 `INSERT` 将在分布式引擎的每个分片的底层表上执行。
## parallel_hash_join_threshold {#parallel_hash_join_threshold} 

<SettingsInfoBlock type="UInt64" default_value="100000" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.5"},{"label": "100000"},{"label": "New setting"}]}, {"id": "row-2","items": [{"label": "25.4"},{"label": "0"},{"label": "New setting"}]}, {"id": "row-3","items": [{"label": "25.3"},{"label": "0"},{"label": "New setting"}]}]}/>

应用基于哈希的连接算法时，此阈值帮助决定是否使用 `hash` 和 `parallel_hash`（仅在可用右侧表大小的估计情况下）。
在右侧表大小低于此阈值时使用前者。
## parallel_replica_offset {#parallel_replica_offset} 

<BetaBadge/>

<SettingsInfoBlock type="UInt64" default_value="0" />

这是一个内部设置，不应直接使用，并表示“并行副本”模式的实现细节。此设置将由发起服务器自动为分布式查询设置，以便在并行副本中参与查询处理的副本索引。
## parallel_replicas_allow_in_with_subquery {#parallel_replicas_allow_in_with_subquery} 

<BetaBadge/>

<SettingsInfoBlock type="Bool" default_value="1" />

如果为真，则 IN 的子查询将在每个追随者副本上执行。
## parallel_replicas_connect_timeout_ms {#parallel_replicas_connect_timeout_ms} 

<BetaBadge/>

<SettingsInfoBlock type="Milliseconds" default_value="300" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.6"},{"label": "300"},{"label": "Separate connection timeout for parallel replicas queries"}]}]}/>

在使用并行副本执行查询期间连接到远程副本的超时，以毫秒为单位。如果超时到期，则不使用对应的副本进行查询执行。
## parallel_replicas_count {#parallel_replicas_count} 

<BetaBadge/>

<SettingsInfoBlock type="UInt64" default_value="0" />

这是一个内部设置，不应直接使用，并表示“并行副本”模式的实现细节。此设置将由发起服务器自动为分布式查询设置，以便参与查询处理的并行副本数量。
## parallel_replicas_custom_key {#parallel_replicas_custom_key} 

<BetaBadge/>

可以使用的任意整数表达式，用于在特定表之间拆分副本的工作。
值可以是任意整数表达式。

简单的使用主键的表达式是首选。

如果在由多个副本组成的单个分片的集群上使用此设置，这些副本将转换为虚拟分片。
否则，它将与 `SAMPLE` 键的行为相同，它将使用每个分片的多个副本。
## parallel_replicas_custom_key_range_lower {#parallel_replicas_custom_key_range_lower} 

<BetaBadge/>

<SettingsInfoBlock type="UInt64" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.6"},{"label": "0"},{"label": "Add settings to control the range filter when using parallel replicas with dynamic shards"}]}]}/>

允许筛选类型 `range` 根据自定义范围 `[parallel_replicas_custom_key_range_lower, INT_MAX]` 在副本之间均匀分配工作。

当与 [parallel_replicas_custom_key_range_upper](#parallel_replicas_custom_key_range_upper) 一起使用时，它允许在范围 `[parallel_replicas_custom_key_range_lower, parallel_replicas_custom_key_range_upper]` 上均匀分配副本的工作。

注意：此设置在查询处理期间不会导致任何额外数据被筛选，实际上它改变了范围筛选在范围 `[0, INT_MAX]` 中进行并行处理的断点。
## parallel_replicas_custom_key_range_upper {#parallel_replicas_custom_key_range_upper} 

<BetaBadge/>

<SettingsInfoBlock type="UInt64" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.6"},{"label": "0"},{"label": "Add settings to control the range filter when using parallel replicas with dynamic shards. A value of 0 disables the upper limit"}]}]}/>

允许筛选类型 `range` 根据自定义范围 `[0, parallel_replicas_custom_key_range_upper]` 在副本之间均匀分配工作。值为 0 禁用上界，将其设置为自定义键表达式的最大值。

当与 [parallel_replicas_custom_key_range_lower](#parallel_replicas_custom_key_range_lower) 一起使用时，它允许在范围 `[parallel_replicas_custom_key_range_lower, parallel_replicas_custom_key_range_upper]` 上均匀分配副本的工作。

注意：此设置在查询处理期间不会导致任何额外数据被筛选，实际上它改变了范围筛选在范围 `[0, INT_MAX]` 中进行并行处理的断点。
## parallel_replicas_for_cluster_engines {#parallel_replicas_for_cluster_engines} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.3"},{"label": "1"},{"label": "New setting."}]}]}/>

用其 -Cluster 替代表函数引擎
## parallel_replicas_for_non_replicated_merge_tree {#parallel_replicas_for_non_replicated_merge_tree} 

<BetaBadge/>

<SettingsInfoBlock type="Bool" default_value="0" />

如果为真，ClickHouse 将对非复制的 MergeTree 表使用并行副本算法。
## parallel_replicas_index_analysis_only_on_coordinator {#parallel_replicas_index_analysis_only_on_coordinator} 

<BetaBadge/>

<SettingsInfoBlock type="Bool" default_value="1" />

仅在副本协调器上进行索引分析，其他副本上跳过。仅对启用了 parallel_replicas_local_pla 时有效。
## parallel_replicas_insert_select_local_pipeline {#parallel_replicas_insert_select_local_pipeline} 

<BetaBadge/>

<SettingsInfoBlock type="Bool" default_value="1" />

在使用并行副本的分布式 INSERT SELECT 期间使用本地管道。
## parallel_replicas_local_plan {#parallel_replicas_local_plan} 

<BetaBadge/>

<SettingsInfoBlock type="Bool" default_value="1" />

为本地副本构建本地计划。
## parallel_replicas_mark_segment_size {#parallel_replicas_mark_segment_size} 

<BetaBadge/>

<SettingsInfoBlock type="UInt64" default_value="0" />

部分虚拟划分为段，以便在副本之间并行读取。此设置控制这些段的大小。在您绝对确定自己的操作之前，不建议更改。值应在范围 [128; 16384] 之间。
## parallel_replicas_min_number_of_rows_per_replica {#parallel_replicas_min_number_of_rows_per_replica} 

<BetaBadge/>

<SettingsInfoBlock type="UInt64" default_value="0" />

限制查询中使用的副本数量为 (估算读取的行数 / min_number_of_rows_per_replica)。最大仍然受 'max_parallel_replicas' 的限制。
## parallel_replicas_mode {#parallel_replicas_mode} 

<BetaBadge/>

<SettingsInfoBlock type="ParallelReplicasMode" default_value="read_tasks" />

用于并行副本自定义键的筛选类型。 默认 - 对自定义键使用取模操作，范围 - 在自定义键上使用范围筛选，使用自定义键的所有可能值类型。
## parallel_replicas_only_with_analyzer {#parallel_replicas_only_with_analyzer} 

<BetaBadge/>

<SettingsInfoBlock type="Bool" default_value="1" />

应启用分析器以使用并行副本。当分析器禁用时，即使启用了从副本的并行读取，查询执行也会回退到本地执行。不支持在没有启用分析器的情况使用并行副本。
## parallel_replicas_prefer_local_join {#parallel_replicas_prefer_local_join} 

<BetaBadge/>

<SettingsInfoBlock type="Bool" default_value="1" />

如果为真，并且可以使用并行副本算法执行 JOIN，并且右侧 JOIN 部分的所有存储都是 *MergeTree，则将使用本地 JOIN，替代 GLOBAL JOIN。
## parallel_view_processing {#parallel_view_processing} 

<SettingsInfoBlock type="Bool" default_value="0" />

允许并发推送到附加视图，而不是顺序推送。
## parallelize_output_from_storages {#parallelize_output_from_storages} 

<SettingsInfoBlock type="Bool" default_value="1" />

并行化从存储的读取步骤输出。如果可能，它允许在从存储读取后并行化查询处理。
## parsedatetime_e_requires_space_padding {#parsedatetime_e_requires_space_padding} 

<SettingsInfoBlock type="Bool" default_value="0" />

格式器 '%e' 在函数 'parseDateTime' 中期望单数日用空格填充，例如 ' 2' 被接受，但 '2' 会引发错误。
## parsedatetime_parse_without_leading_zeros {#parsedatetime_parse_without_leading_zeros} 

<SettingsInfoBlock type="Bool" default_value="1" />

格式器 '%c'、'%l' 和 '%k' 在函数 'parseDateTime' 中解析没有前导零的月份和小时。
## partial_merge_join_left_table_buffer_bytes {#partial_merge_join_left_table_buffer_bytes} 

<SettingsInfoBlock type="UInt64" default_value="0" />

如果不为 0，则在部分合并 JOIN 中将左表块组合成更大的块。每个加入线程使用最多 2 倍指定的内存。
## partial_merge_join_rows_in_right_blocks {#partial_merge_join_rows_in_right_blocks} 

<SettingsInfoBlock type="UInt64" default_value="65536" />

限制部分合并 JOIN 算法中右侧 JOIN 数据块的大小 [JOIN](../../sql-reference/statements/select/join.md) 查询。

ClickHouse 服务器：

1. 将右侧 JOIN 数据拆分为最多指定行数的块。
2. 按其最小值和最大值为每个块创建索引。
3. 如果可能，将准备好的块卸载到磁盘。

可能的值：

- 任何正整数。 推荐值范围：[1000, 100000]。
## partial_result_on_first_cancel {#partial_result_on_first_cancel} 

<SettingsInfoBlock type="Bool" default_value="0" />

允许查询在取消后返回部分结果。
## parts_to_delay_insert {#parts_to_delay_insert} 

<SettingsInfoBlock type="UInt64" default_value="0" />

如果目标表在单个分区中至少包含那么多活动部分，则人为地延迟插入表。
## parts_to_throw_insert {#parts_to_throw_insert} 

<SettingsInfoBlock type="UInt64" default_value="0" />

如果目标表的单个分区中活动部分超过这个数量，则抛出 'Too many parts ...' 异常。
## periodic_live_view_refresh {#periodic_live_view_refresh} 

<SettingsInfoBlock type="Seconds" default_value="60" />

强制刷新周期性刷新的实时视图的间隔。
## poll_interval {#poll_interval} 

在指定的秒数内阻塞服务器上的查询等待循环。
## postgresql_connection_attempt_timeout {#postgresql_connection_attempt_timeout} 

<SettingsInfoBlock type="UInt64" default_value="2" />

连接 PostgreSQL 终端的单次连接超时（秒）。
该值作为连接 URL 的 `connect_timeout` 参数传递。
## postgresql_connection_pool_auto_close_connection {#postgresql_connection_pool_auto_close_connection} 

关闭连接，然后将连接返回到池中。
## postgresql_connection_pool_retries {#postgresql_connection_pool_retries} 

<SettingsInfoBlock type="UInt64" default_value="2" />

PostgreSQL 表引擎和数据库引擎的连接池推送/弹出重试次数。
## postgresql_connection_pool_size {#postgresql_connection_pool_size} 

PostgreSQL 表引擎和数据库引擎的连接池大小。
## postgresql_connection_pool_wait_timeout {#postgresql_connection_pool_wait_timeout} 

PostgreSQL 表引擎和数据库引擎的连接池中空池的推送/弹出超时。默认情况下将在空池上阻塞。
## postgresql_fault_injection_probability {#postgresql_fault_injection_probability} 

<SettingsInfoBlock type="Float" default_value="0" />

内部（用于复制）PostgreSQL 查询失败的近似概率。有效值在区间 [0.0f, 1.0f]。
## prefer_column_name_to_alias {#prefer_column_name_to_alias} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用在查询表达式和子句中使用原始列名而不是别名。这在别名与列名相同时尤其重要，见 [Expression Aliases](/sql-reference/syntax#notes-on-usage)。启用此设置使 ClickHouse 的别名语法规则与大多数其他数据库引擎更兼容。

可能的值：

- 0 — 列名被别名替代。
- 1 — 列名不被别名替代。

**示例**

启用和禁用之间的区别：

查询：

```sql
SET prefer_column_name_to_alias = 0;
SELECT avg(number) AS number, max(number) FROM numbers(10);
```

结果：

```text
Received exception from server (version 21.5.1):
Code: 184. DB::Exception: Received from localhost:9000. DB::Exception: Aggregate function avg(number) is found inside another aggregate function in query: While processing avg(number) AS number.
```

查询：

```sql
SET prefer_column_name_to_alias = 1;
SELECT avg(number) AS number, max(number) FROM numbers(10);
```

结果：

```text
┌─number─┬─max(number)─┐
│    4.5 │           9 │
└────────┴─────────────┘
```
## prefer_external_sort_block_bytes {#prefer_external_sort_block_bytes} 

<SettingsInfoBlock type="UInt64" default_value="16744704" />

优先最大块字节用于外部排序，降低合并过程中的内存使用。
## prefer_global_in_and_join {#prefer_global_in_and_join} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用将 `IN`/`JOIN` 操作替换为 `GLOBAL IN`/`GLOBAL JOIN`。

可能的值：

- 0 — 禁用。 `IN`/`JOIN` 操作不会被替换为 `GLOBAL IN`/`GLOBAL JOIN`。
- 1 — 启用。 `IN`/`JOIN` 操作将被替换为 `GLOBAL IN`/`GLOBAL JOIN`。

**用法**

尽管 `SET distributed_product_mode=global` 可以改变分布式表的查询行为，但这不适合本地表或来自外部资源的表。在此情况下，`prefer_global_in_and_join` 设置就发挥了作用。

例如，我们有服务节点查询的本地表，这些表不适合分布。我们需要在分布式处理期间实时地散布它们的数据，使用 `GLOBAL` 关键字 — `GLOBAL IN`/`GLOBAL JOIN`。

`prefer_global_in_and_join` 的另一个使用场景是访问由外部引擎创建的表。此设置有助于减少在连接这些表时对外部源的调用次数：每个查询只需一次调用。

**另见：**

- [Distributed subqueries](/sql-reference/operators/in#distributed-subqueries)有关如何使用 `GLOBAL IN`/`GLOBAL JOIN` 的更多信息。
## prefer_localhost_replica {#prefer_localhost_replica} 

<SettingsInfoBlock type="Bool" default_value="1" />

启用/禁用在处理分布式查询时优先使用本地主机副本。

可能的值：

- 1 — 如果存在，则 ClickHouse 会始终将查询发送到本地主机副本。
- 0 — ClickHouse 使用 [load_balancing](#load_balancing) 设置指定的均衡策略。

:::note
如果您在没有 [parallel_replicas_custom_key](#parallel_replicas_custom_key) 的情况下使用 [max_parallel_replicas](#max_parallel_replicas)，请禁用此设置。
如果设置了 [parallel_replicas_custom_key](#parallel_replicas_custom_key)，则仅在其在包含多个副本的多个分片的集群上使用时禁用此设置。
如果在包含单个分片和多个副本的集群中使用，则禁用此设置会产生负面影响。
:::
## prefer_warmed_unmerged_parts_seconds {#prefer_warmed_unmerged_parts_seconds} 

<CloudAvailableBadge/>

<SettingsInfoBlock type="Int64" default_value="0" />

仅在 ClickHouse Cloud 中有效。如果合并的部分少于此处的秒数并且未预热（见 [cache_populated_by_fetch](merge-tree-settings.md/#cache_populated_by_fetch)），但其所有源部分均可用且已预热，SELECT 查询将从这些部分读取。仅适用于 Replicated-/SharedMergeTree。请注意，这仅检查 CacheWarmer 是否处理了该部分；如果该部分由其他因素获取到缓存，它仍将被视为冷部分；如果它已被预热，然后从缓存中逐出，它依然被视为温暖的部分。
## preferred_block_size_bytes {#preferred_block_size_bytes} 

<SettingsInfoBlock type="UInt64" default_value="1000000" />

此设置调整查询处理的数据块大小，并代表额外的微调，以适应更粗糙的 'max_block_size' 设置。如果列很大，并且在 'max_block_size' 行中块的大小可能大于指定的字节数，则其大小将降低以便更好地支持 CPU 缓存的局部性。
## preferred_max_column_in_block_size_bytes {#preferred_max_column_in_block_size_bytes} 

<SettingsInfoBlock type="UInt64" default_value="0" />

在读取时限制块中最大列的大小。 有助于减少缓存缺失次数。 应接近 L2 缓存大小。
## preferred_optimize_projection_name {#preferred_optimize_projection_name} 

如果设置为非空字符串，ClickHouse 将尝试在查询中应用指定的投影。

可能的值：

- 字符串：所需投影的名称。
## prefetch_buffer_size {#prefetch_buffer_size} 

<SettingsInfoBlock type="UInt64" default_value="1048576" />

从文件系统读取的预抓取缓冲区的最大大小。
## print_pretty_type_names {#print_pretty_type_names} 

<SettingsInfoBlock type="Bool" default_value="1" />

允许在 `DESCRIBE` 查询和 `toTypeName()` 函数中以美观的方式打印深度嵌套的类型名称并进行缩进。

示例：

```sql
CREATE TABLE test (a Tuple(b String, c Tuple(d Nullable(UInt64), e Array(UInt32), f Array(Tuple(g String, h Map(String, Array(Tuple(i String, j UInt64))))), k Date), l Nullable(String))) ENGINE=Memory;
DESCRIBE TABLE test FORMAT TSVRaw SETTINGS print_pretty_type_names=1;
```

```
a   Tuple(
    b String,
    c Tuple(
        d Nullable(UInt64),
        e Array(UInt32),
        f Array(Tuple(
            g String,
            h Map(
                String,
                Array(Tuple(
                    i String,
                    j UInt64
                ))
            )
        )),
        k Date
    ),
    l Nullable(String)
)
```
## priority {#priority} 

<SettingsInfoBlock type="UInt64" default_value="0" />

查询的优先级。 1 - 最高，数值越高优先级越低； 0 - 不使用优先级。
## push_external_roles_in_interserver_queries {#push_external_roles_in_interserver_queries} 

<SettingsInfoBlock type="Bool" default_value="1" />

启用在执行查询时将用户角色从发起者推送到其他节点。
## query_cache_compress_entries {#query_cache_compress_entries} 

<SettingsInfoBlock type="Bool" default_value="1" />

压缩 [query cache](../query-cache.md) 中的条目。 减少查询缓存的内存消耗，但会使插入/读取变慢。

可能的值：

- 0 - 禁用
- 1 - 启用
## query_cache_max_entries {#query_cache_max_entries} 

当前用户在 [query cache](../query-cache.md) 中可以存储的查询结果的最大数量。 0 表示无限制。

可能的值：

- 正整数 >= 0。
## query_cache_max_size_in_bytes {#query_cache_max_size_in_bytes} 

当前用户可以在 [query cache](../query-cache.md) 中分配的最大内存量（以字节为单位）。 0 表示无限制。

可能的值：

- 正整数 >= 0。
## query_cache_min_query_duration {#query_cache_min_query_duration} 

查询需要运行的最低持续时间（以毫秒为单位），其结果才能存储在 [query cache](../query-cache.md) 中。

可能的值：

- 正整数 >= 0。
## query_cache_min_query_runs {#query_cache_min_query_runs} 

`SELECT` 查询必须运行的最少次数，才能将其结果存储在 [query cache](../query-cache.md) 中。

可能的值：

- 正整数 >= 0。
## query_cache_nondeterministic_function_handling {#query_cache_nondeterministic_function_handling} 

控制 [query cache](../query-cache.md) 如何处理带有非确定性函数（如 `rand()` 或 `now()`）的 `SELECT` 查询。

可能的值：

- `'throw'` - 抛出异常，不缓存查询结果。
- `'save'` - 缓存查询结果。
- `'ignore'` - 不缓存查询结果且不抛出异常。
## query_cache_share_between_users {#query_cache_share_between_users} 

如果开启，缓存的 `SELECT` 查询结果可以被其他用户读取。
由于安全原因，不建议启用此设置。

可能的值：

- 0 - 禁用
- 1 - 启用
## query_cache_squash_partial_results {#query_cache_squash_partial_results} 

将部分结果块压缩为大小 [max_block_size](#max_block_size) 的块。 减少插入 [query cache](../query-cache.md) 的性能，但提高缓存条目的压缩性（见 [query_cache_compress-entries](#query_cache_compress_entries)）。

可能的值：

- 0 - 禁用
- 1 - 启用
## query_cache_system_table_handling {#query_cache_system_table_handling} 

控制 [query cache](../query-cache.md) 如何处理针对系统表的 `SELECT` 查询，即数据库 `system.*` 和 `information_schema.*` 中的表。

可能的值：

- `'throw'` - 抛出异常且不缓存查询结果。
- `'save'` - 缓存查询结果。
- `'ignore'` - 不缓存查询结果且不抛出异常。
## query_cache_tag {#query_cache_tag} 

作为 [query cache](../query-cache.md) 条目的标签的字符串。
相同的查询具有不同的标签会被查询缓存视为不同。

可能的值：

- 任何字符串。
## query_cache_ttl {#query_cache_ttl} 

在此秒数后， [query cache](../query-cache.md) 中的条目变得过时。

可能的值：

- 正整数 >= 0。
## query_condition_cache_store_conditions_as_plaintext {#query_condition_cache_store_conditions_as_plaintext} 

将 [query condition cache](/operations/query-condition-cache) 的筛选条件以明文形式存储。
如果启用，system.query_condition_cache 显示逐字的筛选条件，这使得调试缓存问题更容易。
默认情况下禁用，因为明文筛选条件可能暴露敏感信息。

可能的值：

- 0 - 禁用
- 1 - 启用
## query_metric_log_interval {#query_metric_log_interval} 

收集单个查询的 [query_metric_log](../../operations/system-tables/query_metric_log.md) 的间隔（以毫秒为单位）。

如果设置为负值，将使用 [query_metric_log setting](/operations/server-configuration-parameters/settings#query_metric_log) 中的 `collect_interval_milliseconds` 值，或者如果不存在则默认为 1000。

要禁用单个查询的收集，将 `query_metric_log_interval` 设置为 0。

默认值： -1
## query_plan_aggregation_in_order {#query_plan_aggregation_in_order} 

切换按顺序聚合查询计划级优化。
仅在设置 [query_plan_enable_optimizations](#query_plan_enable_optimizations) 为 1 时生效。

:::note
这是一个专家级设置，仅供开发人员调试使用。该设置可能会在未来以不向后兼容的方式更改或被移除。
:::

可能的值：

- 0 - 禁用
- 1 - 启用
## query_plan_convert_join_to_in {#query_plan_convert_join_to_in} 

允许将 JOIN 转换为带有 IN 的子查询，如果输出列仅与左表相关。可能在非 ANY JOIN（例如 ALL JOINs 默认情况下）时导致错误结果。
## query_plan_convert_outer_join_to_inner_join {#query_plan_convert_outer_join_to_inner_join} 

允许将外连接转换为内部连接，如果连接之后的筛选始终过滤默认值。
## query_plan_enable_multithreading_after_window_functions {#query_plan_enable_multithreading_after_window_functions} 

启用在评估窗口函数后进行多线程，以允许并行流处理。
## query_plan_enable_optimizations {#query_plan_enable_optimizations} 

切换查询计划级别的查询优化。

:::note
这是一个专家级设置，仅供开发人员调试使用。该设置可能会在未来以不向后兼容的方式更改或被移除。
:::

可能的值：

- 0 - 禁用查询计划级别的所有优化。
- 1 - 启用查询计划级别的优化（但各单独优化可能仍然通过其各自设置禁用）。
## query_plan_execute_functions_after_sorting {#query_plan_execute_functions_after_sorting} 

切换查询计划级优化，将表达式移动到排序步骤之后。
仅在设置 [query_plan_enable_optimizations](#query_plan_enable_optimizations) 为 1 时生效。

:::note
这是一个专家级设置，仅供开发人员调试使用。该设置可能会在未来以不向后兼容的方式更改或被移除。
:::

可能的值：

- 0 - 禁用
- 1 - 启用
## query_plan_filter_push_down {#query_plan_filter_push_down} 

切换查询计划级优化，将筛选条件移到执行计划中。
仅在设置 [query_plan_enable_optimizations](#query_plan_enable_optimizations) 为 1 时生效。

:::note
这是一个专家级设置，仅供开发人员调试使用。该设置可能会在未来以不向后兼容的方式更改或被移除。
:::

可能的值：

- 0 - 禁用
- 1 - 启用
## query_plan_join_shard_by_pk_ranges {#query_plan_join_shard_by_pk_ranges} 

如果连接键包含两个表的主键前缀，则针对JOIN应用分片。支持哈希、并行哈希和全排序合并算法。通常不会加速查询，但可能降低内存消耗。
## query_plan_join_swap_table {#query_plan_join_swap_table} 

确定连接的哪个侧应为构建表（也称为内部表，即在哈希连接中插入到哈希表中的表），在查询计划中仅支持与 `JOIN ON` 子句的 `ALL` 连接严格性。可能的值为：
- 'auto': 让规划器决定使用哪个表作为构建表。
- 'false': 永远不交换表（右表为构建表）。
- 'true': 始终交换表（左表为构建表）。
## query_plan_lift_up_array_join {#query_plan_lift_up_array_join} 

切换查询计划级优化，将 ARRAY JOIN 提升到执行计划中。
仅在设置 [query_plan_enable_optimizations](#query_plan_enable_optimizations) 为 1 时生效。

:::note
这是一个专家级设置，仅供开发人员调试使用。该设置可能会在未来以不向后兼容的方式更改或被移除。
:::

可能的值：

- 0 - 禁用
- 1 - 启用
## query_plan_lift_up_union {#query_plan_lift_up_union} 

切换查询计划级优化，将较大的查询计划子树移动到联合中，以启用进一步优化。
仅在设置 [query_plan_enable_optimizations](#query_plan_enable_optimizations) 为 1 时生效。

:::note
这是一个专家级设置，仅供开发人员调试使用。该设置可能会在未来以不向后兼容的方式更改或被移除。
:::

可能的值：

- 0 - 禁用
- 1 - 启用
## query_plan_max_limit_for_lazy_materialization {#query_plan_max_limit_for_lazy_materialization} 

控制允许使用查询计划进行懒惰物化优化的最大限制值。如果为零，则没有限制。
## query_plan_max_optimizations_to_apply {#query_plan_max_optimizations_to_apply} 

限制应用于查询计划的总优化数量，请参见设置 [query_plan_enable_optimizations](#query_plan_enable_optimizations)。
有助于避免复杂查询的长时间优化。
在 EXPLAIN PLAN 查询中，当达到此限制后停止应用优化并按原样返回计划。
对于常规查询执行，如果实际优化数量超过此设置，则会引发异常。

:::note
这是一个专家级设置，仅供开发人员调试使用。该设置可能会在未来以不向后兼容的方式更改或被移除。
:::
## query_plan_merge_expressions {#query_plan_merge_expressions} 

切换查询计划级优化，合并连续的筛选条件。
仅在设置 [query_plan_enable_optimizations](#query_plan_enable_optimizations) 为 1 时生效。

:::note
这是一个专家级设置，仅供开发人员调试使用。该设置可能会在未来以不向后兼容的方式更改或被移除。
:::

可能的值：

- 0 - 禁用
- 1 - 启用
## query_plan_merge_filter_into_join_condition {#query_plan_merge_filter_into_join_condition} 

允许将筛选条件合并到 JOIN 条件中，并将 CROSS JOIN 转换为 INNER。
## query_plan_merge_filters {#query_plan_merge_filters} 

允许在查询计划中合并筛选条件。
## query_plan_optimize_lazy_materialization {#query_plan_optimize_lazy_materialization} 

使用查询计划进行懒惰物化优化。
## query_plan_optimize_prewhere {#query_plan_optimize_prewhere} 

允许将筛选条件推送到 PREWHERE 表达式，以支持的存储。
## query_plan_push_down_limit {#query_plan_push_down_limit} 

切换查询计划级优化，将 LIMIT 移动到执行计划中。
仅在设置 [query_plan_enable_optimizations](#query_plan_enable_optimizations) 为 1 时生效。

:::note
这是一个专家级设置，仅供开发人员调试使用。该设置可能会在未来以不向后兼容的方式更改或被移除。
:::

可能的值：

- 0 - 禁用
- 1 - 启用
## query_plan_read_in_order {#query_plan_read_in_order} 

切换按顺序读取优化查询计划级优化。
仅在设置 [query_plan_enable_optimizations](#query_plan_enable_optimizations) 为 1 时生效。

:::note
这是一个专家级设置，仅供开发人员调试使用。该设置可能会在未来以不向后兼容的方式更改或被移除。
:::

可能的值：

- 0 - 禁用
- 1 - 启用
## query_plan_remove_redundant_distinct {#query_plan_remove_redundant_distinct} 

切换查询计划级优化，去除冗余 DISTINCT 步骤。
仅在设置 [query_plan_enable_optimizations](#query_plan_enable_optimizations) 为 1 时生效。

:::note
这是一个专家级设置，仅供开发人员调试使用。该设置可能会在未来以不向后兼容的方式更改或被移除。
:::

可能的值：

- 0 - 禁用
- 1 - 启用
## query_plan_remove_redundant_sorting {#query_plan_remove_redundant_sorting} 

切换查询计划级优化，去除冗余排序步骤，例如在子查询中。
仅在设置 [query_plan_enable_optimizations](#query_plan_enable_optimizations) 为 1 时生效。

:::note
这是一个专家级设置，仅供开发人员调试使用。该设置可能会在未来以不向后兼容的方式更改或被移除。
:::

可能的值：

- 0 - 禁用
- 1 - 启用
## query_plan_reuse_storage_ordering_for_window_functions {#query_plan_reuse_storage_ordering_for_window_functions} 

切换查询计划级优化，当对窗口函数排序时使用存储排序策略。
仅在设置 [query_plan_enable_optimizations](#query_plan_enable_optimizations) 为 1 时生效。

:::note
这是一个专家级设置，仅供开发人员调试使用。该设置可能会在未来以不向后兼容的方式更改或被移除。
:::

可能的值：

- 0 - 禁用
- 1 - 启用
## query_plan_split_filter {#query_plan_split_filter} 

:::note
这是一个专家级设置，仅供开发人员调试使用。该设置可能会在未来以不向后兼容的方式更改或被移除。
:::

切换查询计划级优化，将筛选条件拆分为表达式。
仅在设置 [query_plan_enable_optimizations](#query_plan_enable_optimizations) 为 1 时生效。

可能的值：

- 0 - 禁用
- 1 - 启用
## query_plan_try_use_vector_search {#query_plan_try_use_vector_search} 

切换查询计划级优化，尝试使用向量相似度索引。
仅在设置 [query_plan_enable_optimizations](#query_plan_enable_optimizations) 为 1 时生效。

:::note
这是一个专家级设置，仅供开发人员调试使用。该设置可能会在未来以不向后兼容的方式更改或被移除。
:::

可能的值：

- 0 - 禁用
- 1 - 启用
## query_plan_use_new_logical_join_step {#query_plan_use_new_logical_join_step} 

在查询计划中使用新的逻辑连接步骤。
## query_profiler_cpu_time_period_ns {#query_profiler_cpu_time_period_ns} 

设置 [query profiler](../../operations/optimizing-performance/sampling-query-profiler.md) 的 CPU 时钟计时器的周期。该计时器仅计算 CPU 时间。

可能的值：

- 正整数的纳秒数。

    推荐值：

            - 10000000（每秒 100 次）纳秒或更多，用于单个查询。
            - 1000000000（每秒一次），用于集群范围的分析。

- 0 用于关闭计时器。

**在 ClickHouse Cloud 中暂时禁用。**

另见：

- 系统表 [trace_log](/operations/system-tables/trace_log)
## query_profiler_real_time_period_ns {#query_profiler_real_time_period_ns} 

<SettingsInfoBlock type="UInt64" default_value="1000000000" />

设置[查询分析器](../../operations/optimizing-performance/sampling-query-profiler.md)实时时钟计时器的周期。实时时钟计时器计算墙面时钟时间。

可能的值：

- 正整数，单位为纳秒。

    推荐值：

            - 10000000（每秒100次）纳秒及以下用于单个查询。
            - 1000000000（每秒一次）用于集群范围内的分析。

- 设置为0以关闭计时器。

**在 ClickHouse Cloud 中暂时禁用。**

另见：

- 系统表 [trace_log](/operations/system-tables/trace_log)
## queue_max_wait_ms {#queue_max_wait_ms} 

<SettingsInfoBlock type="Milliseconds" default_value="0" />

如果同时请求数量超过最大值，请求队列的等待时间。
## rabbitmq_max_wait_ms {#rabbitmq_max_wait_ms} 

<SettingsInfoBlock type="Milliseconds" default_value="5000" />

从 RabbitMQ 读取前的等待时间。
## read_backoff_max_throughput {#read_backoff_max_throughput} 

<SettingsInfoBlock type="UInt64" default_value="1048576" />

在读取缓慢的情况下减少线程数量的设置。当读取带宽低于一定字节数每秒时计算事件。
## read_backoff_min_concurrency {#read_backoff_min_concurrency} 

<SettingsInfoBlock type="UInt64" default_value="1" />

在读取缓慢时尝试保持最小线程数的设置。
## read_backoff_min_events {#read_backoff_min_events} 

<SettingsInfoBlock type="UInt64" default_value="2" />

在读取缓慢情况下减少线程数量的设置。减少线程数量后的事件数。
## read_backoff_min_interval_between_events_ms {#read_backoff_min_interval_between_events_ms} 

<SettingsInfoBlock type="Milliseconds" default_value="1000" />

在读取缓慢情况下减少线程数量的设置。如果前一个事件经过的时间少于一定量，则不关注该事件。
## read_backoff_min_latency_ms {#read_backoff_min_latency_ms} 

<SettingsInfoBlock type="Milliseconds" default_value="1000" />

在读取缓慢时减少线程数量的设置。仅关注耗时至少达到该时间的读取。
## read_from_filesystem_cache_if_exists_otherwise_bypass_cache {#read_from_filesystem_cache_if_exists_otherwise_bypass_cache} 

<SettingsInfoBlock type="Bool" default_value="0" />

允许以被动模式使用文件系统缓存 - 利用现有的缓存项，但不再将更多项放入缓存。如果为繁忙的临时查询设置此设置并为短期实时查询保留禁用状态，这将避免因过于繁重的查询而导致的缓存抖动，从而提高整个系统的效率。
## read_from_page_cache_if_exists_otherwise_bypass_cache {#read_from_page_cache_if_exists_otherwise_bypass_cache} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.3"},{"label": "0"},{"label": "Added userspace page cache"}]}]}/>

以被动模式使用用户空间页面缓存，类似于 read_from_filesystem_cache_if_exists_otherwise_bypass_cache。
## read_in_order_two_level_merge_threshold {#read_in_order_two_level_merge_threshold} 

<SettingsInfoBlock type="UInt64" default_value="100" />

在按主键顺序读取时，进行初步合并步骤所需读取的最小分片数量。
## read_in_order_use_buffering {#read_in_order_use_buffering} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.7"},{"label": "1"},{"label": "Use buffering before merging while reading in order of primary key"}]}]}/>

按主键顺序读取时，合并前使用缓冲。这可以增加查询执行的并行性。
## read_in_order_use_virtual_row {#read_in_order_use_virtual_row} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.11"},{"label": "0"},{"label": "Use virtual row while reading in order of primary key or its monotonic function fashion. It is useful when searching over multiple parts as only relevant ones are touched."}]}]}/>

在按主键顺序读取时使用虚拟行或其单调函数形式。当在多个分片上搜索时，只有相关分片被访问。
## read_overflow_mode {#read_overflow_mode} 

<SettingsInfoBlock type="OverflowMode" default_value="throw" />

超出限制时的处理方式。
## read_overflow_mode_leaf {#read_overflow_mode_leaf} 

<SettingsInfoBlock type="OverflowMode" default_value="throw" />

设置当读取的数据量超过叶子限制时发生的事情。

可能的选项：
- `throw`：抛出异常（默认）。
- `break`：停止执行查询并返回部分结果。
## read_priority {#read_priority} 

<SettingsInfoBlock type="Int64" default_value="0" />

从本地文件系统或远程文件系统读取数据的优先级。仅支持适用于本地文件系统的 'pread_threadpool' 方法和适用于远程文件系统的 `threadpool` 方法。
## read_through_distributed_cache {#read_through_distributed_cache} 

<CloudAvailableBadge/>

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.10"},{"label": "0"},{"label": "A setting for ClickHouse Cloud"}]}]}/>

仅在 ClickHouse Cloud 中有效。允许从分布式缓存中读取。
## readonly {#readonly} 

<SettingsInfoBlock type="UInt64" default_value="0" />

0 - 没有只读限制。1 - 仅允许读取请求，以及显式允许更改的设置。2 - 仅允许读取请求，以及更改设置，但不包括 'readonly' 设置。
## receive_data_timeout_ms {#receive_data_timeout_ms} 

<SettingsInfoBlock type="Milliseconds" default_value="2000" />

接收第一数据包或来自副本的正向进度的连接超时。
## receive_timeout {#receive_timeout} 

<SettingsInfoBlock type="Seconds" default_value="300" />

接收来自网络数据的超时，以秒为单位。如果在此时间段内没有接收到字节，则抛出异常。如果在客户端设置此设置，则对应连接端的 socket 也将设置 'send_timeout'。
## regexp_max_matches_per_row {#regexp_max_matches_per_row} 

<SettingsInfoBlock type="UInt64" default_value="1000" />

设置每行单个正则表达式的最大匹配次数。使用它可以防止在使用贪婪正则表达式时内存过载，例如在 [extractAllGroupsHorizontal](/sql-reference/functions/string-search-functions#extractallgroupshorizontal) 函数中。

可能的值：

- 正整数。
## reject_expensive_hyperscan_regexps {#reject_expensive_hyperscan_regexps} 

<SettingsInfoBlock type="Bool" default_value="1" />

拒绝可能会用 hyperscan 评估的代价昂贵的模式（由于 NFA 状态爆炸）。
## remerge_sort_lowered_memory_bytes_ratio {#remerge_sort_lowered_memory_bytes_ratio} 

<SettingsInfoBlock type="Float" default_value="2" />

如果再合并后的内存使用量未减少到该比例，则将禁用再合并。
## remote_filesystem_read_method {#remote_filesystem_read_method} 

<SettingsInfoBlock type="String" default_value="threadpool" />

从远程文件系统读取数据的方法，可以是：read, threadpool。
## remote_filesystem_read_prefetch {#remote_filesystem_read_prefetch} 

<SettingsInfoBlock type="Bool" default_value="1" />

在从远程文件系统读取数据时应使用预取。
## remote_fs_read_backoff_max_tries {#remote_fs_read_backoff_max_tries} 

<SettingsInfoBlock type="UInt64" default_value="5" />

最大尝试读取的次数，具有退避机制。
## remote_fs_read_max_backoff_ms {#remote_fs_read_max_backoff_ms} 

<SettingsInfoBlock type="UInt64" default_value="10000" />

尝试从远程磁盘读取数据时的最大等待时间。
## remote_read_min_bytes_for_seek {#remote_read_min_bytes_for_seek} 

<SettingsInfoBlock type="UInt64" default_value="4194304" />

远程读取（url, s3）时进行查找所需的最小字节数，而不是读取并忽略。
## rename_files_after_processing {#rename_files_after_processing} 

- **类型：** 字符串

- **默认值：** 空字符串

此设置允许为通过 `file` 表函数处理的文件指定重命名模式。当选项设置时，所有由 `file` 表函数读取的文件将根据指定的模式和占位符进行重命名，仅当文件处理成功时。
### 占位符

- `%a` — 完整原始文件名（例如，“sample.csv”）。
- `%f` — 原始文件名，去掉扩展名（例如，“sample”）。
- `%e` — 原始文件扩展名，并包括点（例如，“.csv”）。
- `%t` — 时间戳（以微秒为单位）。
- `%%` — 百分号（“%”）。
### 示例
- 选项：`--rename_files_after_processing="processed_%f_%t%e"`

- 查询：`SELECT * FROM file('sample.csv')`


如果读取 `sample.csv` 成功，文件将被重命名为 `processed_sample_1683473210851438.csv`。
## replace_running_query {#replace_running_query} 

<SettingsInfoBlock type="Bool" default_value="0" />

使用 HTTP 接口时，可以传递 'query_id' 参数。此为用作查询标识符的任意字符串。
如果同一用户使用相同的 'query_id' 的查询在此时已经存在，则行为取决于 'replace_running_query' 参数。

`0`（默认） – 抛出异常（如果当前正在运行相同 'query_id' 的查询，则不允许查询运行）。

`1` – 取消旧查询并开始运行新的查询。

将此参数设置为1以实现分段条件的建议。在输入下一个字符后，如果旧查询尚未完成，则应取消它。
## replace_running_query_max_wait_ms {#replace_running_query_max_wait_ms} 

<SettingsInfoBlock type="Milliseconds" default_value="5000" />

在活动[replace_running_query](#replace_running_query) 设置时，运行相同 `query_id` 的查询完成的等待时间。

可能的值：

- 正整数。
- 0 — 抛出异常，不允许运行新查询，如果服务器上已执行相同 `query_id` 的查询。
## replication_wait_for_inactive_replica_timeout {#replication_wait_for_inactive_replica_timeout} 

<SettingsInfoBlock type="Int64" default_value="120" />

指定等待非活动副本执行 [ALTER](../../sql-reference/statements/alter/index.md)、[OPTIMIZE](../../sql-reference/statements/optimize.md) 或 [TRUNCATE](../../sql-reference/statements/truncate.md) 查询的时间（以秒为单位）。

可能的值：

- 0 — 不等待。
- 负整数 — 无限期等待。
- 正整数 — 等待的秒数。
## restore_replace_external_dictionary_source_to_null {#restore_replace_external_dictionary_source_to_null} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.10"},{"label": "0"},{"label": "New setting."}]}]}/>

还原时将外部字典源替换为 Null。用于测试目的。
## restore_replace_external_engines_to_null {#restore_replace_external_engines_to_null} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.8"},{"label": "0"},{"label": "New setting."}]}]}/>

出于测试目的。将所有外部引擎替换为 Null，以避免发起外部连接。
## restore_replace_external_table_functions_to_null {#restore_replace_external_table_functions_to_null} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.8"},{"label": "0"},{"label": "New setting."}]}]}/>

出于测试目的。将所有外部表函数替换为 Null，以避免发起外部连接。
## restore_replicated_merge_tree_to_shared_merge_tree {#restore_replicated_merge_tree_to_shared_merge_tree} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.2"},{"label": "0"},{"label": "New setting."}]}]}/>

在 RESTORE 时将表引擎从 Replicated*MergeTree 替换为 Shared*MergeTree。
## result_overflow_mode {#result_overflow_mode} 

<SettingsInfoBlock type="OverflowMode" default_value="throw" />

云默认值：`throw`

设置当结果量超过任一限值时的处理方式。

可能的值：
- `throw`：抛出异常（默认）。
- `break`：停止执行查询并返回部分结果，如同源数据耗尽。

使用 'break' 类似于使用 LIMIT。`Break` 仅在块级别中中断执行。这意味着返回的行数大于[`max_result_rows`](/operations/settings/settings#max_result_rows)、[`max_block_size`](/operations/settings/settings#max_block_size)的倍数，并依赖于[`max_threads`](/operations/settings/settings#max_threads)。

**示例**

```sql title="Query"
SET max_threads = 3, max_block_size = 3333;
SET max_result_rows = 3334, result_overflow_mode = 'break';

SELECT *
FROM numbers_mt(100000)
FORMAT Null;
```

```text title="Result"
6666 rows in set. ...
```
## rewrite_count_distinct_if_with_count_distinct_implementation {#rewrite_count_distinct_if_with_count_distinct_implementation} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "23.8"},{"label": "1"},{"label": "Rewrite countDistinctIf with count_distinct_implementation configuration"}]}]}/>

允许您在使用[count_distinct_implementation](#count_distinct_implementation)设置时重写 `countDistinctIf`。

可能的值：

- true — 允许。
- false — 不允许。
## s3_allow_multipart_copy {#s3_allow_multipart_copy} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.2"},{"label": "1"},{"label": "New setting."}]}]}/>

允许 S3 中的多部分复制。
## s3_allow_parallel_part_upload {#s3_allow_parallel_part_upload} 

<SettingsInfoBlock type="Bool" default_value="1" />

使用多个线程进行 S3 多部分上传。这可能导致略高的内存使用。
## s3_check_objects_after_upload {#s3_check_objects_after_upload} 

<SettingsInfoBlock type="Bool" default_value="0" />

检查每个上传的对象，以确保上传成功。
## s3_connect_timeout_ms {#s3_connect_timeout_ms} 

<SettingsInfoBlock type="UInt64" default_value="1000" />

连接 S3 磁盘主机的超时。
## s3_create_new_file_on_insert {#s3_create_new_file_on_insert} 

<SettingsInfoBlock type="Bool" default_value="0" />

在每次插入 S3 引擎表时启用或禁用创建新文件。如果启用，每次插入将使用类似于以下模式的键创建新的 S3 对象：

初始： `data.Parquet.gz` -> `data.1.Parquet.gz` -> `data.2.Parquet.gz` 等等。

可能的值：
- 0 — `INSERT` 查询创建新文件，或者如果文件已存在，则失败，并且未设置 s3_truncate_on_insert。
- 1 — 如果未设置 s3_truncate_on_insert，则 `INSERT` 查询在每次插入时使用后缀（从第二个开始）创建新文件。

有关更多详细信息，请参见[此处](/integrations/s3#inserting-data)。
## s3_disable_checksum {#s3_disable_checksum} 

<SettingsInfoBlock type="Bool" default_value="0" />

在将文件发送到 S3 时不计算校验和。这通过避免对文件的过度处理传递来加快写入速度。这在大多数情况下都是安全的，因为 MergeTree 表的数据会由 ClickHouse 进行校验和检查，并且在通过 HTTPS 访问 S3 时，TLS 层已经在网络传输中提供完整性。然而，在 S3 上添加额外的校验和可以提供深度防御。
## s3_ignore_file_doesnt_exist {#s3_ignore_file_doesnt_exist} 

<SettingsInfoBlock type="Bool" default_value="0" />

如果在读取某些键时文件不存在，则忽略缺失。

可能的值：
- 1 — `SELECT` 返回空结果。
- 0 — `SELECT` 抛出异常。
## s3_list_object_keys_size {#s3_list_object_keys_size} 

<SettingsInfoBlock type="UInt64" default_value="1000" />

批量返回的 ListObject 请求的最大文件数量。
## s3_max_connections {#s3_max_connections} 

<SettingsInfoBlock type="UInt64" default_value="1024" />

每个服务器的最大连接数。
## s3_max_get_burst {#s3_max_get_burst} 

<SettingsInfoBlock type="UInt64" default_value="0" />

在达到每秒请求限制之前，可以同时发出的最大请求数。默认（0）等于 `s3_max_get_rps`。
## s3_max_get_rps {#s3_max_get_rps} 

<SettingsInfoBlock type="UInt64" default_value="0" />

在达到节流之前每秒 S3 GET 请求的限制。零意味着无限。
## s3_max_inflight_parts_for_one_file {#s3_max_inflight_parts_for_one_file} 

<SettingsInfoBlock type="UInt64" default_value="20" />

在多部分上传请求中并发加载的最大部分数。0 表示无限制。
## s3_max_part_number {#s3_max_part_number} 

<SettingsInfoBlock type="UInt64" default_value="10000" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.6"},{"label": "10000"},{"label": "Maximum part number number for s3 upload part"}]}]}/>

S3 上传部分的最大部分编号。
## s3_max_put_burst {#s3_max_put_burst} 

<SettingsInfoBlock type="UInt64" default_value="0" />

在超过每秒请求限制之前，可以同时发出的最大请求数。默认（0）等于 `s3_max_put_rps`。
## s3_max_put_rps {#s3_max_put_rps} 

<SettingsInfoBlock type="UInt64" default_value="0" />

在达到节流之前每秒 S3 PUT 请求的限制。零意味着无限。
## s3_max_redirects {#s3_max_redirects} 

<SettingsInfoBlock type="UInt64" default_value="10" />

允许的 S3 重定向跳数的最大数量。
## s3_max_single_operation_copy_size {#s3_max_single_operation_copy_size} 

<SettingsInfoBlock type="UInt64" default_value="33554432" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.6"},{"label": "33554432"},{"label": "Maximum size for a single copy operation in s3"}]}]}/>

S3 中单次操作复制的最大大小。此设置仅在 s3_allow_multipart_copy 为 true 时使用。
## s3_max_single_part_upload_size {#s3_max_single_part_upload_size} 

<SettingsInfoBlock type="UInt64" default_value="33554432" />

使用单独的部分上传到 S3 的最大对象大小。
## s3_max_single_read_retries {#s3_max_single_read_retries} 

<SettingsInfoBlock type="UInt64" default_value="4" />

在单次 S3 读取中的最大重试次数。
## s3_max_unexpected_write_error_retries {#s3_max_unexpected_write_error_retries} 

<SettingsInfoBlock type="UInt64" default_value="4" />

在 S3 写入期间遇到意外错误时的最大重试次数。
## s3_max_upload_part_size {#s3_max_upload_part_size} 

<SettingsInfoBlock type="UInt64" default_value="5368709120" />

在对 S3 进行多部分上传时，上传部分的最大大小。
## s3_min_upload_part_size {#s3_min_upload_part_size} 

<SettingsInfoBlock type="UInt64" default_value="16777216" />

在对 S3 进行多部分上传时，上传部分的最小大小。
## s3_request_timeout_ms {#s3_request_timeout_ms} 

<SettingsInfoBlock type="UInt64" default_value="30000" />

发送和接收数据到/从 S3 的空闲超时。如果单个 TCP 读或写调用阻塞时间过长则失败。
## s3_retry_attempts {#s3_retry_attempts} 

Aws::Client::RetryStrategy 的设置，Aws::Client 会自行重试，0 表示不重试。
## s3_skip_empty_files {#s3_skip_empty_files} 

<SettingsInfoBlock type="Bool" default_value="1" />

启用或禁用在[S3](../../engines/table-engines/integrations/s3.md)引擎表中跳过空文件的功能。

可能的值：
- 0 — 如果空文件与请求的格式不兼容，则 `SELECT` 将抛出异常。
- 1 — 对于空文件，`SELECT` 返回空结果。
## s3_slow_all_threads_after_network_error {#s3_slow_all_threads_after_network_error} 

<SettingsInfoBlock type="Bool" default_value="1" />

设置为 `true` 时，在一个 S3 请求因可重试的网络错误失败后，所有执行 S3 请求的线程都将缓慢执行一段时间。
设置为 `false` 时，每个执行 S3 请求的线程在网络错误上使用独立的退避集。
## s3_strict_upload_part_size {#s3_strict_upload_part_size} 

<SettingsInfoBlock type="UInt64" default_value="0" />

在对 S3 进行多部分上传时，上传部分的确切大小（某些实现不支持可变大小的部分）。
## s3_throw_on_zero_files_match {#s3_throw_on_zero_files_match} 

在 ListObjects 请求无法匹配任何文件时抛出错误。
## s3_truncate_on_insert {#s3_truncate_on_insert} 

启用或禁用在 S3 引擎表中插入前进行截断。如果禁用，当 S3 对象已存在时，将在插入尝试时抛出异常。

可能的值：
- 0 — `INSERT` 查询创建新文件，或者如果文件存在则失败且未设置 s3_create_new_file_on_insert。
- 1 — `INSERT` 查询用新数据替换文件的现有内容。

有关更多详细信息，请参见[此处](/integrations/s3#inserting-data)。
## s3_upload_part_size_multiply_factor {#s3_upload_part_size_multiply_factor} 

<SettingsInfoBlock type="UInt64" default_value="2" />

每次从单次写入上传到 S3 的 s3_multiply_parts_count_threshold 部件时，乘以 s3_min_upload_part_size。
## s3_upload_part_size_multiply_parts_count_threshold {#s3_upload_part_size_multiply_parts_count_threshold} 

<SettingsInfoBlock type="UInt64" default_value="500" />

每当此数量的部分被上传到 S3 时，s3_min_upload_part_size 将乘以 s3_upload_part_size_multiply_factor。
## s3_use_adaptive_timeouts {#s3_use_adaptive_timeouts} 

<SettingsInfoBlock type="Bool" default_value="1" />

设置为 `true` 时，所有 S3 请求在首次尝试时采用较低的发送和接收超时。
设置为 `false` 时，所有尝试采用相同的超时。
## s3_validate_request_settings {#s3_validate_request_settings} 

<SettingsInfoBlock type="Bool" default_value="1" />

启用 S3 请求设置的验证。

可能的值：
- 1 — 验证设置。
- 0 — 不验证设置。
## s3queue_default_zookeeper_path {#s3queue_default_zookeeper_path} 

<SettingsInfoBlock type="String" default_value="/clickhouse/s3queue/" />

S3Queue 引擎的默认 zookeeper 路径前缀。
## s3queue_enable_logging_to_s3queue_log {#s3queue_enable_logging_to_s3queue_log} 

启用写入 system.s3queue_log。该值可以通过表设置进行覆盖。
## s3queue_migrate_old_metadata_to_buckets {#s3queue_migrate_old_metadata_to_buckets} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.1"},{"label": "0"},{"label": "New setting."}]}]}/>

将 S3Queue 表的旧元数据结构迁移到新结构中。
## schema_inference_cache_require_modification_time_for_url {#schema_inference_cache_require_modification_time_for_url} 

<SettingsInfoBlock type="Bool" default_value="1" />

使用带有最后修改时间验证的 URL 的缓存模式（对于带有 Last-Modified 标头的 URL）。
## schema_inference_use_cache_for_azure {#schema_inference_use_cache_for_azure} 

<SettingsInfoBlock type="Bool" default_value="1" />

在使用 Azure 表函数时使用模式推断缓存。
## schema_inference_use_cache_for_file {#schema_inference_use_cache_for_file} 

<SettingsInfoBlock type="Bool" default_value="1" />

在使用文件表函数时使用模式推断缓存。
## schema_inference_use_cache_for_hdfs {#schema_inference_use_cache_for_hdfs} 

<SettingsInfoBlock type="Bool" default_value="1" />

在使用 HDFS 表函数时使用模式推断缓存。
## schema_inference_use_cache_for_s3 {#schema_inference_use_cache_for_s3} 

<SettingsInfoBlock type="Bool" default_value="1" />

在使用 S3 表函数时使用模式推断缓存。
## schema_inference_use_cache_for_url {#schema_inference_use_cache_for_url} 

<SettingsInfoBlock type="Bool" default_value="1" />

在使用 URL 表函数时使用模式推断缓存。
## secondary_indices_enable_bulk_filtering {#secondary_indices_enable_bulk_filtering} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.5"},{"label": "1"},{"label": "A new algorithm for filtering by data skipping indices"}]}]}/>

为索引启用批量过滤算法。预计始终更好，但我们有此设置以便于兼容性和控制。
## select_sequential_consistency {#select_sequential_consistency} 

<SettingsInfoBlock type="UInt64" default_value="0" />

:::note
此设置在 SharedMergeTree 和 ReplicatedMergeTree 之间的行为有所不同，请参见[SharedMergeTree 一致性](/cloud/reference/shared-merge-tree#consistency)以获取有关 SharedMergeTree 中 `select_sequential_consistency` 行为的更多信息。
:::

启用或禁用 `SELECT` 查询的顺序一致性。需要禁用 `insert_quorum_parallel`（默认启用）。

可能的值：

- 0 — 禁用。
- 1 — 启用。

使用

启用顺序一致性时，ClickHouse 仅允许客户端对包含所有先前使用 `insert_quorum` 执行的 `INSERT` 查询的数据的副本执行 `SELECT` 查询。如果客户端引用部分副本，ClickHouse 将生成异常。SELECT 查询将不包括尚未写入副本法定人数的数据。

启用 `insert_quorum_parallel` 时（默认设置）`select_sequential_consistency` 将无效。这是因为并行的 `INSERT` 查询可以写入不同的法定副本集合，因此无法保证某个单一副本接收了所有写入。

另见：

- [insert_quorum](#insert_quorum)
- [insert_quorum_timeout](#insert_quorum_timeout)
- [insert_quorum_parallel](#insert_quorum_parallel)
## send_logs_level {#send_logs_level} 

<SettingsInfoBlock type="LogsLevel" default_value="fatal" />

发送具有指定最低级别的服务器文本日志到客户端。有效值：'trace', 'debug', 'information', 'warning', 'error', 'fatal', 'none'。
## send_logs_source_regexp {#send_logs_source_regexp} 

发送具有匹配日志源名称的指定正则表达式的服务器文本日志。为空表示所有源。
## send_progress_in_http_headers {#send_progress_in_http_headers} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用 `clickhouse-server` 响应中的 `X-ClickHouse-Progress` HTTP 响应头。

有关更多信息，请参阅[HTTP 接口描述](../../interfaces/http.md)。

可能的值：

- 0 — 禁用。
- 1 — 启用。
## send_timeout {#send_timeout} 

<SettingsInfoBlock type="Seconds" default_value="300" />

发送数据到网络的超时，以秒为单位。如果客户端需要发送一些数据，但在此时间段内无法发送任何字节，则抛出异常。如果在客户端设置此设置，则对应连接端的 socket 也将设置 'receive_timeout'。
## serialize_query_plan {#serialize_query_plan} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.4"},{"label": "0"},{"label": "NewSetting"}]}]}/>

序列化用于分布式处理的查询计划。
## session_timezone {#session_timezone} 

<BetaBadge/>

设置当前会话或查询的隐式时区。
隐式时区是应用于未显式指定时区的 DateTime/DateTime64 类型值的时区。
此设置优先于全球配置的（服务器级别）隐式时区。
值为 ''（空字符串）表示当前会话或查询的隐式时区与[服务器时区](../server-configuration-parameters/settings.md/#timezone)相同。

您可以使用函数 `timeZone()` 和 `serverTimeZone()` 来获取会话时区和服务器时区。

可能的值：

- 任何来自 `system.time_zones` 的时区名称，例如 `Europe/Berlin`、`UTC` 或 `Zulu`。

示例：

```sql
SELECT timeZone(), serverTimeZone() FORMAT CSV

"Europe/Berlin","Europe/Berlin"
```

```sql
SELECT timeZone(), serverTimeZone() SETTINGS session_timezone = 'Asia/Novosibirsk' FORMAT CSV

"Asia/Novosibirsk","Europe/Berlin"
```

将会话时区 'America/Denver' 分配给内置的 DateTime（未显式指定时区）：

```sql
SELECT toDateTime64(toDateTime64('1999-12-12 23:23:23.123', 3), 3, 'Europe/Zurich') SETTINGS session_timezone = 'America/Denver' FORMAT TSV

1999-12-13 07:23:23.123
```

:::warning
并非所有解析 DateTime/DateTime64 的函数都遵循 `session_timezone`。这可能导致微妙的错误。
请参见以下示例和说明。
:::

```sql
CREATE TABLE test_tz (`d` DateTime('UTC')) ENGINE = Memory AS SELECT toDateTime('2000-01-01 00:00:00', 'UTC');

SELECT *, timeZone() FROM test_tz WHERE d = toDateTime('2000-01-01 00:00:00') SETTINGS session_timezone = 'Asia/Novosibirsk'
0 rows in set.

SELECT *, timeZone() FROM test_tz WHERE d = '2000-01-01 00:00:00' SETTINGS session_timezone = 'Asia/Novosibirsk'
┌───────────────────d─┬─timeZone()───────┐
│ 2000-01-01 00:00:00 │ Asia/Novosibirsk │
└─────────────────────┴──────────────────┘
```

这发生于不同的解析管道：
- 在第一个 `SELECT` 查询中使用未显式给出时区的 `toDateTime()`尊重 `session_timezone` 和全局时区。
- 在第二个查询中，从字符串解析的 DateTime 继承现有列 `d` 的类型和时区。因此，设置 `session_timezone` 和全局时区将不被尊重。

**另见**

- [timezone](../server-configuration-parameters/settings.md/#timezone)
## set_overflow_mode {#set_overflow_mode} 

<SettingsInfoBlock type="OverflowMode" default_value="throw" />

设置当数据量超过任一限值时发生的事情。

可能的值：
- `throw`：抛出异常（默认）。
- `break`：停止执行查询并返回部分结果，如同源数据耗尽。
## shared_merge_tree_sync_parts_on_partition_operations {#shared_merge_tree_sync_parts_on_partition_operations} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "24.12"},{"label": "1"},{"label": "New setting. By default parts are always synchronized"}]}]}/>

在 SMT 表中，MOVE|REPLACE|ATTACH 分区操作后自动同步数据部分集。仅在云中使用。
## short_circuit_function_evaluation {#short_circuit_function_evaluation} 

<SettingsInfoBlock type="ShortCircuitFunctionEvaluation" default_value="enable" />

允许根据[短路方案](https://en.wikipedia.org/wiki/Short-circuit_evaluation)计算 [if](../../sql-reference/functions/conditional-functions.md/#if)、[multiIf](../../sql-reference/functions/conditional-functions.md/#multiif)、[and](/sql-reference/functions/logical-functions#and) 和 [or](/sql-reference/functions/logical-functions#or) 函数。这有助于优化这些函数中复杂表达式的执行，防止可能的异常（例如，当意外发生除以零时）。

可能的值：

- `enable` — 启用适合短路功能评估的函数（可能抛出异常或计算成本高）。
- `force_enable` — 对所有函数启用短路功能评估。
- `disable` — 禁用短路功能评估。
## short_circuit_function_evaluation_for_nulls {#short_circuit_function_evaluation_for_nulls} 

<SettingsInfoBlock type="Bool" default_value="1" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "25.1"},{"label": "1"},{"label": "Allow to execute functions with Nullable arguments only on rows with non-NULL values in all arguments"}]}]}/>

优化当任何参数为 NULL 时返回 NULL 的函数的评估。当函数参数中 NULL 值的比例超过 short_circuit_function_evaluation_for_nulls_threshold 时，系统将跳过逐行评估函数。相反，它会立即返回 NULL，以避免不必要的计算。
## short_circuit_function_evaluation_for_nulls_threshold {#short_circuit_function_evaluation_for_nulls_threshold} 

<SettingsInfoBlock type="Double" default_value="1" />

在包含所有参数非 NULL 值的行上仅对带有 Nullable 参数的函数执行的 NULL 值的比率阈值。当短路功能评估设置为启用时应用。当包含 NULL 值的行的比例超过该阈值时，将不评估这些包含 NULL 值的行。
## show_table_uuid_in_table_create_query_if_not_nil {#show_table_uuid_in_table_create_query_if_not_nil} 

<SettingsInfoBlock type="Bool" default_value="0" />

<VersionHistory rows={[{"id": "row-1","items": [{"label": "20.7"},{"label": "0"},{"label": "Stop showing  UID of the table in its CREATE query for Engine=Atomic"}]}]}/>

设置 `SHOW TABLE` 查询显示。

可能的值：

- 0 — 查询将不显示表 UUID。
- 1 — 查询将显示表 UUID。
## single_join_prefer_left_table {#single_join_prefer_left_table} 

<SettingsInfoBlock type="Bool" default_value="1" />

对于单个 JOIN，在标识歧义的情况下优先选择左表。
## skip_redundant_aliases_in_udf {#skip_redundant_aliases_in_udf} 

<SettingsInfoBlock type="Bool" default_value="0" />

在用户自定义函数中不使用（不替代）冗余别名，从而简化其使用。

可能的值：

- 1 — 在 UDF 中跳过（替换）别名。
- 0 — 在 UDF 中不跳过（替换）别名。

**示例**

启用和禁用之间的差异：

查询：

```sql
SET skip_redundant_aliases_in_udf = 0;
CREATE FUNCTION IF NOT EXISTS test_03274 AS ( x ) -> ((x + 1 as y, y + 2));

EXPLAIN SYNTAX SELECT test_03274(4 + 2);
```

结果：

```text
SELECT ((4 + 2) + 1 AS y, y + 2)
```

查询：

```sql
SET skip_redundant_aliases_in_udf = 1;
CREATE FUNCTION IF NOT EXISTS test_03274 AS ( x ) -> ((x + 1 as y, y + 2));

EXPLAIN SYNTAX SELECT test_03274(4 + 2);
```

结果：

```text
SELECT ((4 + 2) + 1, ((4 + 2) + 1) + 2)
```
## skip_unavailable_shards {#skip_unavailable_shards} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用默默跳过不可用分片。

如果所有副本不可用，则该分片被视为不可用。副本在以下情况下不可用：

- 由于任何原因，ClickHouse 无法连接到副本。

    在连接到副本时，ClickHouse 会进行多次尝试。如果所有尝试均失败，则该副本被视为不可用。

- 副本无法通过 DNS 解析。

    如果副本的主机名无法通过 DNS 解析，可能表示以下情况：

    - 副本主机没有 DNS 记录。这可能发生在动态 DNS 系统中，例如 [Kubernetes](https://kubernetes.io)，在该系统中，节点在停机期间可能无法解析，这并不算错误。

    - 配置错误。ClickHouse 配置文件包含错误的主机名。

可能的值：

- 1 — 启用跳过。

    如果分片不可用，ClickHouse 返回基于部分数据的结果，并且不报告节点可用性问题。

- 0 — 禁用跳过。

    如果分片不可用，ClickHouse 将抛出异常。
## sleep_after_receiving_query_ms {#sleep_after_receiving_query_ms} 

<SettingsInfoBlock type="Milliseconds" default_value="0" />

在 TCPHandler 中接收查询后休眠的时间。
## sleep_in_send_data_ms {#sleep_in_send_data_ms} 

<SettingsInfoBlock type="Milliseconds" default_value="0" />

在 TCPHandler 中发送数据时的休眠时间。
## sleep_in_send_tables_status_ms {#sleep_in_send_tables_status_ms} 

<SettingsInfoBlock type="Milliseconds" default_value="0" />

在 TCPHandler 中发送表状态响应的休眠时间。
## sort_overflow_mode {#sort_overflow_mode} 

<SettingsInfoBlock type="OverflowMode" default_value="throw" />

设置在排序之前接收的行数超过其中之一限制时发生的事情。

可能的值：
- `throw`：抛出异常。
- `break`：停止执行查询并返回部分结果。
## split_intersecting_parts_ranges_into_layers_final {#split_intersecting_parts_ranges_into_layers_final} 

<SettingsInfoBlock type="Bool" default_value="1" />

在最终优化过程中将交集部分范围分成层。
## split_parts_ranges_into_intersecting_and_non_intersecting_final {#split_parts_ranges_into_intersecting_and_non_intersecting_final} 

<SettingsInfoBlock type="Bool" default_value="1" />

在最终优化期间将部分范围拆分为交集和非交集
## splitby_max_substrings_includes_remaining_string {#splitby_max_substrings_includes_remaining_string} 

<SettingsInfoBlock type="Bool" default_value="0" />

控制函数 [splitBy*()](../../sql-reference/functions/splitting-merging-functions.md) 在参数 `max_substrings` > 0 时，是否会将剩余字符串包含在结果数组的最后一个元素中。

可能的值：

- `0` - 剩余字符串不会包含在结果数组的最后一个元素中。
- `1` - 剩余字符串将包含在结果数组的最后一个元素中。这是 Spark 的 [`split()`](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.split.html) 函数和 Python 的 ['string.split()'](https://docs.python.org/3/library/stdtypes.html#str.split) 方法的行为。
## stop_refreshable_materialized_views_on_startup {#stop_refreshable_materialized_views_on_startup} 

<ExperimentalBadge/>

<SettingsInfoBlock type="Bool" default_value="0" />

在服务器启动时，防止调度可刷新的物化视图，仿佛执行了 SYSTEM STOP VIEWS。之后可以手动通过 `SYSTEM START VIEWS` 或 `SYSTEM START VIEW <name>` 启动它们。这也适用于新创建的视图。对非可刷新的物化视图没有影响。
## storage_file_read_method {#storage_file_read_method} 

<SettingsInfoBlock type="LocalFSReadMethod" default_value="pread" />

从存储文件读取数据的方法，可以是：`read`，`pread`，`mmap`。mmap 方法不适用于 clickhouse-server（它是为 clickhouse-local 设计的）。
## storage_system_stack_trace_pipe_read_timeout_ms {#storage_system_stack_trace_pipe_read_timeout_ms} 

<SettingsInfoBlock type="Milliseconds" default_value="100" />

从管道读取信息的最长时间，用于查询 `system.stack_trace` 表时从线程接收信息。此设置用于测试目的，不应由用户更改。
## stream_flush_interval_ms {#stream_flush_interval_ms} 

<SettingsInfoBlock type="Milliseconds" default_value="7500" />

适用于具有流式存储的表，在超时时或线程生成 [max_insert_block_size](#max_insert_block_size) 行时使用。

默认值为 7500。

值越小，数据进入表的频率越高。将值设置得过低会导致性能下降。
## stream_like_engine_allow_direct_select {#stream_like_engine_allow_direct_select} 

<SettingsInfoBlock type="Bool" default_value="0" />

允许对 Kafka、RabbitMQ、FileLog、Redis Streams 和 NATS 引擎进行直接 SELECT 查询。如果有附加的物化视图，即使启用此设置，也不允许执行 SELECT 查询。
## stream_like_engine_insert_queue {#stream_like_engine_insert_queue} 

当流式引擎从多个队列读取时，用户需要选择一个队列进行写入。由 Redis Streams 和 NATS 使用。
## stream_poll_timeout_ms {#stream_poll_timeout_ms} 

<SettingsInfoBlock type="Milliseconds" default_value="500" />

从/向流存储轮询数据的超时。
## system_events_show_zero_values {#system_events_show_zero_values} 

<SettingsInfoBlock type="Bool" default_value="0" />

允许从 [`system.events`](../../operations/system-tables/events.md) 中选择零值事件。

某些监控系统要求在每个检查点向它们传递所有度量值，即使度量值为零。

可能的值：

- 0 — 禁用。
- 1 — 启用。

**示例**

查询

```sql
SELECT * FROM system.events WHERE event='QueryMemoryLimitExceeded';
```

结果

```text
Ok.
```

查询
```sql
SET system_events_show_zero_values = 1;
SELECT * FROM system.events WHERE event='QueryMemoryLimitExceeded';
```

结果

```text
┌─event────────────────────┬─value─┬─description───────────────────────────────────────────┐
│ QueryMemoryLimitExceeded │     0 │ Number of times when memory limit exceeded for query. │
└──────────────────────────┴───────┴───────────────────────────────────────────────────────┘
```
## table_function_remote_max_addresses {#table_function_remote_max_addresses} 

<SettingsInfoBlock type="UInt64" default_value="1000" />

设置从模式生成的 [remote](../../sql-reference/table-functions/remote.md) 函数的最大地址数。

可能的值：

- 正整数。
## tcp_keep_alive_timeout {#tcp_keep_alive_timeout} 

<SettingsInfoBlock type="Seconds" default_value="290" />

连接在 TCP 开始发送保活探测之前需要保持空闲的时间（以秒计算）
## temporary_data_in_cache_reserve_space_wait_lock_timeout_milliseconds {#temporary_data_in_cache_reserve_space_wait_lock_timeout_milliseconds} 

<SettingsInfoBlock type="UInt64" default_value="600000" />

等待锁定缓存以保留临时数据在文件系统缓存中所需的时间
## temporary_files_codec {#temporary_files_codec} 

<SettingsInfoBlock type="String" default_value="LZ4" />

设置用于在磁盘上进行排序和连接操作的临时文件的压缩编解码器。

可能的值：

- LZ4 — 应用 [LZ4](https://en.wikipedia.org/wiki/LZ4_(compression_algorithm)) 压缩。
- NONE — 不应用任何压缩。
## throw_if_deduplication_in_dependent_materialized_views_enabled_with_async_insert {#throw_if_deduplication_in_dependent_materialized_views_enabled_with_async_insert} 

<SettingsInfoBlock type="Bool" default_value="1" />

在启用 `async_insert` 时，如果设置 `deduplicate_blocks_in_dependent_materialized_views` 被启用，则在 INSERT 查询时抛出异常。这保证了正确性，因为这些功能不能一起工作。
## throw_if_no_data_to_insert {#throw_if_no_data_to_insert} 

<SettingsInfoBlock type="Bool" default_value="1" />

允许或禁止空的 INSERT，默认启用（对空插入抛出错误）。仅适用于使用 [`clickhouse-client`](/interfaces/cli) 或通过 [gRPC 接口](/interfaces/grpc) 的 INSERT。
## throw_on_error_from_cache_on_write_operations {#throw_on_error_from_cache_on_write_operations} 

在写操作（INSERT，合并）时忽略来自缓存的错误
## throw_on_max_partitions_per_insert_block {#throw_on_max_partitions_per_insert_block} 

<SettingsInfoBlock type="Bool" default_value="1" />

允许您控制在达到 `max_partitions_per_insert_block` 时的行为。

可能的值：
- `true`  - 当插入块达到 `max_partitions_per_insert_block` 时，抛出异常。
- `false` - 当达到 `max_partitions_per_insert_block` 时，记录警告。

:::tip
如果您试图了解在更改 [`max_partitions_per_insert_block`](/operations/settings/settings#max_partitions_per_insert_block) 时对用户的影响，这可能会很有用。
:::
## throw_on_unsupported_query_inside_transaction {#throw_on_unsupported_query_inside_transaction} 

<ExperimentalBadge/>

<SettingsInfoBlock type="Bool" default_value="1" />

如果在事务内使用不支持的查询，则抛出异常
## timeout_before_checking_execution_speed {#timeout_before_checking_execution_speed} 

<SettingsInfoBlock type="Seconds" default_value="10" />

检查执行速度是否过慢（不少于 `min_execution_speed`），在指定的秒数到期后进行检测。
## timeout_overflow_mode {#timeout_overflow_mode} 

设置查询运行时间超过 `max_execution_time` 或估计运行时间超过 `max_estimated_execution_time` 时的处理方式。

可能的值：
- `throw`: 抛出异常（默认）。
- `break`: 停止执行查询并返回部分结果，仿佛源数据已耗尽。
## timeout_overflow_mode_leaf {#timeout_overflow_mode_leaf} 

<SettingsInfoBlock type="OverflowMode" default_value="throw" />

设置在叶节点中查询运行超过 `max_execution_time_leaf` 时的处理方式。

可能的值：
- `throw`: 抛出异常（默认）。
- `break`: 停止执行查询并返回部分结果，仿佛源数据已耗尽。
## totals_auto_threshold {#totals_auto_threshold} 

<SettingsInfoBlock type="Float" default_value="0.5" />

`totals_mode = 'auto'` 的阈值。
请参见 "WITH TOTALS 修饰符" 一节。
## totals_mode {#totals_mode} 

在存在 HAVING 时，计算 TOTALS 的方式，以及在 max_rows_to_group_by 和 group_by_overflow_mode = 'any' 时的计算方式。
请参见 "WITH TOTALS 修饰符" 一节。
## trace_profile_events {#trace_profile_events} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用或禁用在每次更新配置事件时收集堆栈跟踪，同时记录配置事件名称和增量值，并将其发送至 [trace_log](/operations/system-tables/trace_log)。

可能的值：

- 1 — 启用配置事件跟踪。
- 0 — 禁用配置事件跟踪。
## transfer_overflow_mode {#transfer_overflow_mode} 

<SettingsInfoBlock type="OverflowMode" default_value="throw" />

设置当数据量超过某一限制时的处理方式。

可能的值：
- `throw`: 抛出异常（默认）。
- `break`: 停止执行查询并返回部分结果，仿佛源数据已耗尽。
## transform_null_in {#transform_null_in} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用 [NULL](/sql-reference/syntax#null) 值在 [IN](../../sql-reference/operators/in.md) 操作符中的相等性。

默认情况下，`NULL` 值无法比较，因为 `NULL` 意味着未定义值。因此，比较 `expr = NULL` 必须始终返回 `false`。启用此设置后，`NULL = NULL` 在 `IN` 操作符中返回 `true`。

可能的值：

- 0 — 在 `IN` 操作符中，`NULL` 值比较返回 `false`。
- 1 — 在 `IN` 操作符中，`NULL` 值比较返回 `true`。

**示例**

考虑 `null_in` 表：

```text
┌──idx─┬─────i─┐
│    1 │     1 │
│    2 │  NULL │
│    3 │     3 │
└──────┴───────┘
```

查询：

```sql
SELECT idx, i FROM null_in WHERE i IN (1, NULL) SETTINGS transform_null_in = 0;
```

结果：

```text
┌──idx─┬────i─┐
│    1 │    1 │
└──────┴──────┘
```

查询：

```sql
SELECT idx, i FROM null_in WHERE i IN (1, NULL) SETTINGS transform_null_in = 1;
```

结果：

```text
┌──idx─┬─────i─┐
│    1 │     1 │
│    2 │  NULL │
└──────┴───────┘
```

**另见**

- [IN 操作符中的 NULL 处理](/sql-reference/operators/in#null-processing)
## traverse_shadow_remote_data_paths {#traverse_shadow_remote_data_paths} 

<SettingsInfoBlock type="Bool" default_value="0" />

在查询 system.remote_data_paths 时，遍历冻结的数据（影子目录）以及实际表数据
## union_default_mode {#union_default_mode} 

设置组合 `SELECT` 查询结果的模式。仅在与 [UNION](../../sql-reference/statements/select/union.md) 共享而未明确指定 `UNION ALL` 或 `UNION DISTINCT` 时使用此设置。

可能的值：

- `'DISTINCT'` — ClickHouse 在组合查询的结果时输出去重后的行。
- `'ALL'` — ClickHouse 输出所有行，包括重复行。
- `''` — ClickHouse 在与 `UNION` 一起使用时生成异常。

请参见 [UNION](../../sql-reference/statements/select/union.md) 中的示例。
## unknown_packet_in_send_data {#unknown_packet_in_send_data} 

<SettingsInfoBlock type="UInt64" default_value="0" />

发送未知数据包而不是数据的第 N 个数据包
## update_parallel_mode {#update_parallel_mode} 

<SettingsInfoBlock type="UpdateParallelMode" default_value="auto" />

确定并发更新查询的行为。

可能的值：
- `sync` - 顺序运行所有 `UPDATE` 查询。
- `auto` - 仅在列之间存在依赖关系时顺序运行 `UPDATE` 查询。
- `async` - 不同步更新查询。
## update_sequential_consistency {#update_sequential_consistency} 

<SettingsInfoBlock type="Bool" default_value="1" />

如果为 true，则在执行更新之前，将已更新的部分集更新到最新版本。
## use_async_executor_for_materialized_views {#use_async_executor_for_materialized_views} 

<SettingsInfoBlock type="Bool" default_value="0" />

使用异步和潜在的多线程执行物化视图查询，这可以加速在 INSERT 期间处理视图，但也会消耗更多内存。
## use_cache_for_count_from_files {#use_cache_for_count_from_files} 

<SettingsInfoBlock type="Bool" default_value="1" />

启用在表函数 `file`/`s3`/`url`/`hdfs`/`azureBlobStorage` 中从文件计数时的行数缓存。

默认启用。
## use_client_time_zone {#use_client_time_zone} 

使用客户端时区来解释 DateTime 字符串值，而不是采用服务器时区。
## use_compact_format_in_distributed_parts_names {#use_compact_format_in_distributed_parts_names} 

<SettingsInfoBlock type="Bool" default_value="1" />

用于以紧凑格式存储块，以用于背景（`distributed_foreground_insert`）INSERT 到具有 `Distributed` 引擎的表。

可能的值：

- 0 — 使用 `user[:password]@host:port#default_database` 目录格式。
- 1 — 使用 `[shard{shard_index}[_replica{replica_index}]]` 目录格式。

:::note
- 使用 `use_compact_format_in_distributed_parts_names=0` 时，对集群定义的更改不会应用于背景 INSERT。
- 使用 `use_compact_format_in_distributed_parts_names=1` 时，改变集群定义中的节点顺序将更改 `shard_index`/`replica_index`，请注意。
:::
## use_concurrency_control {#use_concurrency_control} 

<SettingsInfoBlock type="Bool" default_value="1" />

尊重服务器的并发控制（请参见 `concurrent_threads_soft_limit_num` 和 `concurrent_threads_soft_limit_ratio_to_cores` 全局服务器设置）。如果禁用，即使服务器超载，也允许使用更多线程（不建议在正常使用中这样做，主要用于测试）。
## use_hedged_requests {#use_hedged_requests} 

<SettingsInfoBlock type="Bool" default_value="1" />

启用远程查询的对冲请求逻辑。它允许与不同副本建立多个连接进行查询。如果当前与副本的现有连接未在 `hedged_connection_timeout` 时间内建立，或在 `receive_data_timeout` 内未接收到数据，则启用新连接。查询将使用发送非空进度包的第一个连接（或数据包，如果 `allow_changing_replica_until_first_data_packet`）；其他连接将被取消。支持 `max_parallel_replicas > 1` 的查询。

默认启用。

在 Cloud 中默认禁用。
## use_hive_partitioning {#use_hive_partitioning} 

<SettingsInfoBlock type="Bool" default_value="1" />

启用时，ClickHouse 将在路径中检测 Hive 风格的分区（`/name=value/`），在像 [File](/sql-reference/table-functions/file#hive-style-partitioning)、[S3](/sql-reference/table-functions/s3#hive-style-partitioning)、[URL](/sql-reference/table-functions/url#hive-style-partitioning)、[HDFS](/sql-reference/table-functions/hdfs#hive-style-partitioning)、[AzureBlobStorage](/sql-reference/table-functions/azureBlobStorage#hive-style-partitioning) 的文件类表引擎中，并允许在查询中使用分区列作为虚拟列。这些虚拟列的名称将与分区路径中的名称相同，但以 `_` 开头。
## use_iceberg_metadata_files_cache {#use_iceberg_metadata_files_cache} 

<SettingsInfoBlock type="Bool" default_value="1" />

如果启用，冰山表函数和冰山存储可能会利用冰山元数据文件缓存。

可能的值：

- 0 - 禁用
- 1 - 启用
## use_iceberg_partition_pruning {#use_iceberg_partition_pruning} 

<SettingsInfoBlock type="Bool" default_value="1" />

对 Iceberg 表使用冰山分区裁剪
## use_index_for_in_with_subqueries {#use_index_for_in_with_subqueries} 

当 IN 操作符的右侧是子查询或表表达式时，尝试使用索引。
## use_index_for_in_with_subqueries_max_values {#use_index_for_in_with_subqueries_max_values} 

<SettingsInfoBlock type="UInt64" default_value="0" />

IN 操作符右侧的集合最大大小，以利用表索引进行过滤。可以避免因为准备大型查询而导致性能下降和更高的内存使用。零表示不限制。
## use_json_alias_for_old_object_type {#use_json_alias_for_old_object_type} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用时，将使用 `JSON` 数据类型别名创建旧的 [Object('json')](../../sql-reference/data-types/json.md) 类型，而不是新的 [JSON](../../sql-reference/data-types/newjson.md) 类型。
## use_legacy_to_time {#use_legacy_to_time} 

<SettingsInfoBlock type="Bool" default_value="0" />

启用时，允许使用旧版 toTime 函数，该函数将带时间的日期转换为某一固定日期，同时保留时间。否则，将使用新的 toTime 函数，该函数将不同类型的数据转换为时间类型。
旧的遗留函数也始终可以作为 toTimeWithFixedDate 访问。
## use_page_cache_for_disks_without_file_cache {#use_page_cache_for_disks_without_file_cache} 

<SettingsInfoBlock type="Bool" default_value="0" />

对没有启用文件系统缓存的远程磁盘使用用户空间页面缓存。
## use_page_cache_with_distributed_cache {#use_page_cache_with_distributed_cache} 

<SettingsInfoBlock type="Bool" default_value="0" />

在使用分布式缓存时使用用户空间页面缓存。
## use_query_cache {#use_query_cache} 

如果启用，`SELECT` 查询可能会利用 [查询缓存](../query-cache.md)。参数 [enable_reads_from_query_cache](#enable_reads_from_query_cache) 和 [enable_writes_to_query_cache](#enable_writes_to_query_cache) 更详细地控制缓存的使用。

可能的值：

- 0 - 禁用
- 1 - 启用
## use_query_condition_cache {#use_query_condition_cache} 

<SettingsInfoBlock type="Bool" default_value="1" />

启用 [查询条件缓存](/operations/query-condition-cache)。缓存存储在数据部分中不满足 `WHERE` 子句条件的粒度范围，并将此信息重用为后续查询的临时索引。

可能的值：

- 0 - 禁用
- 1 - 启用
## use_skip_indexes {#use_skip_indexes} 

<SettingsInfoBlock type="Bool" default_value="1" />

在查询执行期间使用数据跳过索引。

可能的值：

- 0 — 禁用。
- 1 — 启用。
## use_skip_indexes_if_final {#use_skip_indexes_if_final} 

<SettingsInfoBlock type="Bool" default_value="0" />

控制在执行带有 FINAL 修饰符的查询时是否使用跳过索引。

默认情况下，此设置被禁用，因为跳过索引可能会排除包含最新数据的行（粒度），这可能导致不正确的结果。启用后，即使在使用 FINAL 修饰符时也会应用跳过索引，有可能提高性能，但存在遗漏最近更新的风险。

可能的值：

- 0 — 禁用。
- 1 — 启用。
## use_skip_indexes_if_final_exact_mode {#use_skip_indexes_if_final_exact_mode} 

<SettingsInfoBlock type="Bool" default_value="0" />

控制在执行带有 FINAL 修饰符的查询时，跳过索引返回的粒度是否在较新的部分中扩展，以返回正确的结果。

使用跳过索引可能会排除包含最新数据的行（粒度），这可能导致不正确的结果。此设置可以确保通过扫描与跳过索引返回的范围重叠的新部分来返回正确的结果。

可能的值：

- 0 — 禁用。
- 1 — 启用。
## use_structure_from_insertion_table_in_table_functions {#use_structure_from_insertion_table_in_table_functions} 

<SettingsInfoBlock type="UInt64" default_value="2" />

使用插入表的结构，而不是从数据推断模式。可能的值： 0 - 禁用，1 - 启用，2 - 自动
## use_uncompressed_cache {#use_uncompressed_cache} 

<SettingsInfoBlock type="Bool" default_value="0" />

是否使用未压缩块的缓存。接受 0 或 1。默认值为 0（禁用）。
使用未压缩缓存（仅适用于 MergeTree 家族的表）可以在处理大量短查询时显著降低延迟并增加吞吐量。为频繁发送短请求的用户启用此设置。还要注意 [uncompressed_cache_size](/operations/server-configuration-parameters/settings#uncompressed_cache_size) 配置参数（仅在配置文件中设置）– 未压缩缓存块的大小。默认情况下，它为 8 GiB。未压缩缓存根据需要填充，且最少使用的数据会被自动删除。

对于读取至少有一定量数据（百万行及以上）的查询，将自动禁用未压缩缓存，以便为真实的小查询节省空间。这意味着您可以始终将 'use_uncompressed_cache' 设置为 1。
## use_variant_as_common_type {#use_variant_as_common_type} 

<SettingsInfoBlock type="Bool" default_value="0" />

允许在没有通用类型的参数类型时，将 `Variant` 类型用作 [if](../../sql-reference/functions/conditional-functions.md/#if)/[multiIf](../../sql-reference/functions/conditional-functions.md/#multiif)/[array](../../sql-reference/functions/array-functions.md)/[map](../../sql-reference/functions/tuple-map-functions.md) 函数的返回类型。

示例：

```sql
SET use_variant_as_common_type = 1;
SELECT toTypeName(if(number % 2, number, range(number))) as variant_type FROM numbers(1);
SELECT if(number % 2, number, range(number)) as variant FROM numbers(5);
```

```text
┌─variant_type───────────────────┐
│ Variant(Array(UInt64), UInt64) │
└────────────────────────────────┘
┌─variant───┐
│ []        │
│ 1         │
│ [0,1]     │
│ 3         │
│ [0,1,2,3] │
└───────────┘
```

```sql
SET use_variant_as_common_type = 1;
SELECT toTypeName(multiIf((number % 4) = 0, 42, (number % 4) = 1, [1, 2, 3], (number % 4) = 2, 'Hello, World!', NULL)) AS variant_type FROM numbers(1);
SELECT multiIf((number % 4) = 0, 42, (number % 4) = 1, [1, 2, 3], (number % 4) = 2, 'Hello, World!', NULL) AS variant FROM numbers(4);
```

```text
─variant_type─────────────────────────┐
│ Variant(Array(UInt8), String, UInt8) │
└──────────────────────────────────────┘

┌─variant───────┐
│ 42            │
│ [1,2,3]       │
│ Hello, World! │
│ ᴺᵁᴸᴸ          │
└───────────────┘
```

```sql
SET use_variant_as_common_type = 1;
SELECT toTypeName(array(range(number), number, 'str_' || toString(number))) as array_of_variants_type from numbers(1);
SELECT array(range(number), number, 'str_' || toString(number)) as array_of_variants FROM numbers(3);
```

```text
┌─array_of_variants_type────────────────────────┐
│ Array(Variant(Array(UInt64), String, UInt64)) │
└───────────────────────────────────────────────┘

┌─array_of_variants─┐
│ [[],0,'str_0']    │
│ [[0],1,'str_1']   │
│ [[0,1],2,'str_2'] │
└───────────────────┘
```

```sql
SET use_variant_as_common_type = 1;
SELECT toTypeName(map('a', range(number), 'b', number, 'c', 'str_' || toString(number))) as map_of_variants_type from numbers(1);
SELECT map('a', range(number), 'b', number, 'c', 'str_' || toString(number)) as map_of_variants FROM numbers(3);
```

```text
┌─map_of_variants_type────────────────────────────────┐
│ Map(String, Variant(Array(UInt64), String, UInt64)) │
└─────────────────────────────────────────────────────┘

┌─map_of_variants───────────────┐
│ {'a':[],'b':0,'c':'str_0'}    │
│ {'a':[0],'b':1,'c':'str_1'}   │
│ {'a':[0,1],'b':2,'c':'str_2'} │
└───────────────────────────────┘
```
## use_with_fill_by_sorting_prefix {#use_with_fill_by_sorting_prefix} 

<SettingsInfoBlock type="Bool" default_value="1" />

ORDER BY 子句中以 WITH FILL 开头的列形成排序前缀。具有不同排序前缀值的行会独立填充
## validate_enum_literals_in_operators {#validate_enum_literals_in_operators} 

<SettingsInfoBlock type="Bool" default_value="0" />

如果启用，将在 `IN`、`NOT IN`、`==`、`!=` 等操作符中验证枚举字面值是否符合枚举类型，并在字面值不是有效枚举值时抛出异常。
## validate_mutation_query {#validate_mutation_query} 

<SettingsInfoBlock type="Bool" default_value="1" />

在接受变更查询之前进行验证。变更在后台执行，运行无效查询会导致变更卡住，需要手动干预。

仅在遇到向后不兼容的错误时更改此设置。
## validate_polygons {#validate_polygons} 

<SettingsInfoBlock type="Bool" default_value="1" />

启用或禁用在 [pointInPolygon](/sql-reference/functions/geo/coordinates#pointinpolygon) 函数中抛出异常，如果多边形是自交或者自切。

可能的值：

- 0 — 禁用抛出异常。 `pointInPolygon` 接受无效多边形并为其返回可能不正确的结果。
- 1 — 启用抛出异常。
## vector_search_filter_strategy {#vector_search_filter_strategy} 

<BetaBadge/>

<SettingsInfoBlock type="VectorSearchFilterStrategy" default_value="auto" />

如果向量搜索查询带有 WHERE 子句，此设置确定是先进行过滤（预过滤）还是优先检查向量相似性索引（后过滤）。可能的值：
- 'auto' - 后过滤（确切语义可能在将来发生变化）。
- 'postfilter' - 使用向量相似性索引识别最近邻，然后应用其他过滤器
- 'prefilter' - 首先评估其他过滤器，然后进行暴力搜索以识别邻居。
## vector_search_postfilter_multiplier {#vector_search_postfilter_multiplier} 

<BetaBadge/>

<SettingsInfoBlock type="Float" default_value="1" />

在对其他谓词执行后过滤之前，将从向量相似性索引获取的最近邻乘以该数字。
## wait_changes_become_visible_after_commit_mode {#wait_changes_become_visible_after_commit_mode} 

<ExperimentalBadge/>

<SettingsInfoBlock type="TransactionsWaitCSNMode" default_value="wait_unknown" />

等待已提交的更改在最新快照中变得可见
## wait_for_async_insert {#wait_for_async_insert} 

如果为 true，等待异步插入的处理
## wait_for_async_insert_timeout {#wait_for_async_insert_timeout} 

等待处理异步插入的超时
## wait_for_window_view_fire_signal_timeout {#wait_for_window_view_fire_signal_timeout} 

<ExperimentalBadge/>

<SettingsInfoBlock type="Seconds" default_value="10" />

在事件时间处理中等待窗口视图触发信号的超时
## window_view_clean_interval {#window_view_clean_interval} 

<ExperimentalBadge/>

<SettingsInfoBlock type="Seconds" default_value="60" />

窗口视图的清理间隔（以秒为单位），以释放过时数据。
## window_view_heartbeat_interval {#window_view_heartbeat_interval} 

<ExperimentalBadge/>

<SettingsInfoBlock type="Seconds" default_value="15" />

以秒为单位的心跳间隔，以指示监视查询仍然有效。
## workload {#workload} 

<SettingsInfoBlock type="String" default_value="default" />

用于访问资源的工作负载名称
## write_through_distributed_cache {#write_through_distributed_cache} 

<CloudAvailableBadge/>

<SettingsInfoBlock type="Bool" default_value="0" />

仅在 ClickHouse Cloud 中有效。允许写入分布式缓存（写入 s3 也将通过分布式缓存完成）
## zstd_window_log_max {#zstd_window_log_max} 

<SettingsInfoBlock type="Int64" default_value="0" />

允许您选择 ZSTD 的最大窗口日志（不适用于 MergeTree 家族）
