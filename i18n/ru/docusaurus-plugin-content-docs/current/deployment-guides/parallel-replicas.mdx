---
slug: /deployment-guides/parallel-replicas
title: 'Параллельные реплики'
keywords: ['параллельная реплика']
description: 'В этом руководстве мы сначала обсудим, как ClickHouse распределяет запросы между несколькими шардерами через распределённые таблицы, а затем как запрос может использовать несколько реплик для его выполнения.'
---

import Image from '@theme/IdealImage';
import BetaBadge from '@theme/badges/BetaBadge';
import image_1 from '@site/static/images/deployment-guides/parallel-replicas-1.png'
import image_2 from '@site/static/images/deployment-guides/parallel-replicas-2.png'
import image_3 from '@site/static/images/deployment-guides/parallel-replicas-3.png'
import image_4 from '@site/static/images/deployment-guides/parallel-replicas-4.png'
import image_5 from '@site/static/images/deployment-guides/parallel-replicas-5.png'
import image_6 from '@site/static/images/deployment-guides/parallel-replicas-6.png'
import image_7 from '@site/static/images/deployment-guides/parallel-replicas-7.png'
import image_8 from '@site/static/images/deployment-guides/parallel-replicas-8.png'
import image_9 from '@site/static/images/deployment-guides/parallel-replicas-9.png'

<BetaBadge/>
## Введение {#introduction}

ClickHouse выполняет запросы с невероятной скоростью, но как эти запросы 
распределяются и распараллеливаются по нескольким серверам?

> В этом руководстве мы сначала обсудим, как ClickHouse распределяет запрос
между несколькими шардерами через распределенные таблицы, а затем как запрос
может использовать несколько реплик для его выполнения.
## Шардированная архитектура {#sharded-architecture}

В бездисковой архитектуре кластеры обычно разделяются на
несколько шардов, при этом каждый шард содержит подмножество всех данных.
Распределенная таблица располагается поверх этих шардов, предоставляя
объединённый вид на все данные.

Чтения могут быть отправлены в локальную таблицу. Выполнение запроса будет
происходить только на указанном шарде, либо может быть отправлено в распределенную
таблицу, и в этом случае каждый шард выполнит указанные запросы. Сервер, где
был запрошен распределенная таблица, агрегирует данные и отвечает клиенту:

<Image img={image_1} size="md" alt="шардированная архитектура" />

На рисунке выше показано, что происходит, когда клиент запрашивает распределённую таблицу:

<ol className="docs-ordered-list">
    <li>
        Запрос select отправляется в распределённую таблицу на узел произвольно 
        (через стратегию round-robin или после маршрутизации на конкретный сервер 
        через балансировщик нагрузки). Этот узел теперь будет действовать как координатор.
    </li>
    <li>
        Узел найдет каждый шард, который должен выполнить запрос, 
        используя информацию, указанную распределённой таблицей, и запрос 
        отправляется на каждый шард.
    </li>
    <li>
        Каждый шард читает, фильтрует и агрегирует данные локально, а затем 
        отправляет обратно состояние, которое можно объединить, к координатору.
    </li>
    <li>
        Координирующий узел объединяет данные, а затем отправляет ответ 
        клиенту.
    </li>
</ol>

Когда мы добавляем реплики в процесс, он остаётся довольно схожим, с единственным
отличием, что только одна реплика из каждого шарда выполнит запрос.
Это означает, что больше запросов может быть обработано параллельно.
## Нешардированная архитектура {#non-sharded-architecture}

ClickHouse Cloud имеет очень другую архитектуру по сравнению с представленной выше.
(См. ["Архитектура ClickHouse Cloud"](https://clickhouse.com/docs/cloud/reference/architecture) 
для более детальной информации). С разделением вычислений и хранения, и практически
неограниченным количеством хранилища, потребность в шардах становится менее важной.

На картинке ниже показана архитектура ClickHouse Cloud:

<Image img={image_2} size="md" alt="нешардированная архитектура" />

Эта архитектура позволяет нам добавлять и удалять реплики почти
мгновенно, обеспечивая очень высокую масштабируемость кластера. Кластер 
ClickHouse Keeper (показан справа) обеспечивает наличие единого источника 
истины для метаданных. Реплики могут получать метаданные из кластера 
ClickHouse Keeper и поддерживать одинаковые данные. Сами данные хранятся в
объектном хранилище, и SSD-кэш позволяет ускорять запросы.

Но как теперь распределить выполнение запроса по нескольким серверам? В
шардированной архитектуре это было довольно очевидно, поскольку каждый шард
мог фактически выполнить запрос на подмножестве данных. Как это работает, когда
шардирования нет?
## Введение в параллельные реплики {#introducing-parallel-replicas}

Для параллельного выполнения запросов через несколько серверов, сначала нужно 
назначить один из наших серверов координатором. Координатор создает 
список задач, которые необходимо выполнить, гарантирует их выполнения,
агрегирует результаты и возвращает их клиенту. Как и в большинстве распределённых систем,
это будет ролью узла, который получил изначальный запрос. Также необходимо 
определить единицу работы. В шардированной архитектуре единицей работы является шард,
подмножество данных. С параллельными репликами мы будем использовать небольшую часть таблицы,
называемую [гранулы](/docs/guides/best-practices/sparse-primary-indexes#data-is-organized-into-granules-for-parallel-data-processing),
как единицу работы.

Теперь давайте посмотрим, как это работает на практике, с помощью рисунка ниже:

<Image img={image_3} size="md" alt="Параллельные реплики" />

С параллельными репликами:

<ol className="docs-ordered-list">
    <li>
        Запрос от клиента отправляется на один узел после прохождения через
        балансировщик нагрузки. Этот узел становится координатором для этого
        запроса.
    </li>
    <li>
        Узел анализирует индекс каждой части и выбирает правильные части и
        гранулы для обработки.
    </li>
    <li>
        Координатор делит рабочую нагрузку на набор гранул, которые могут быть
        назначены различным репликам.
    </li>
    <li>
        Каждый набор гранул обрабатывается соответствующими репликами, и когда
        они закончат, будет отправлено состояние, которое можно
        объединить, к координатору.
    </li>
    <li>
        Наконец, координатор объединяет все результаты от реплик и 
        возвращает ответ клиенту.
    </li>
</ol>

Вышеперечисленные шаги показывают, как в теории работают параллельные реплики. 
Однако на практике существует множество факторов, которые могут 
помешать работе такой логики:

<ol className="docs-ordered-list">
    <li>
        Некоторые реплики могут быть недоступны.
    </li>
    <li>
        Репликация в ClickHouse асинхронная, некоторые реплики могут не
        иметь одинаковых частей в какой-то момент времени.
    </li>
    <li>
        Задержка между репликами должна как-то обрабатываться.
    </li>
    <li>
        Кэш файловой системы отличается от реплики к реплике в зависимости от
        активности на каждой реплике, что означает, что случайное назначение
        задач может привести к менее оптимальной производительности, учитывая
        локальность кэша.
    </li>
</ol>

Мы рассмотрим, как преодолеваются эти факторы в следующих разделах.
### Объявления {#announcements}

Для решения вопросов (1) и (2) из вышеприведённого списка мы ввели понятие
объявления. Давайте визуализируем, как это работает, используя рисунок ниже:

<Image img={image_4} size="md" alt="Объявления" />

<ol className="docs-ordered-list">
    <li>
        Запрос от клиента отправляется на один узел после прохождения через
        балансировщик нагрузки. Узел становится координатором для этого
        запроса.
    </li>
    <li>
        Координатор отправляет запрос на получение объявлений от
        всех реплик в кластере. Реплики могут иметь слегка разные
        представления текущего набора частей для таблицы. В результате, необходимо
        собрать эту информацию, чтобы избежать неверных решений по
        планированию.
    </li>
    <li>
        Координатор затем использует объявления для определения набора
        гранул, которые могут быть назначены различным репликам. Здесь, например,
        видно, что никакие гранулы из части 3 не были назначены реплике 2,
        потому что эта реплика не предоставила данную часть в своём
        объявлении. Также обратите внимание, что ни одна задача не была назначена
        реплике 3, потому что эта реплика не предоставила объявление.
    </li>
    <li>
        После того, как каждая реплика обработала запрос на своём подмножестве
        гранул, и состояние, которое можно объединить, было отправлено к
        координатору, координатор объединяет результаты и 
        ответ отправляется клиенту.
    </li>
</ol>
### Динамическая координация {#dynamic-coordination}

Для решения проблемы задержки, мы добавили динамическую координацию. Это означает,
что все гранулы не отправляются реплике одним запросом, но каждая реплика
сможет запросить новую задачу (набор гранул для обработки) у
координатора. Координатор предоставит реплике набор гранул на основе
полученного объявления.

Предположим, что мы находимся на стадии процесса, где все реплики отправили 
объявление со всеми частями.

Рисунок ниже визуализирует, как работает динамическая координация:

<Image img={image_5} size="md" alt="Динамическая координация - часть 1" />

<ol className="docs-ordered-list">
    <li>
        Реплики уведомляют координаторский узел, что они способны обработать
        задачи, они также могут указать, сколько работы они могут обработать.
    </li>
    <li>
        Координатор назначает задачи репликам.
    </li>
</ol>

<Image img={image_6} size="md" alt="Динамическая координация - часть 2" />

<ol className="docs-ordered-list">
    <li>
        Реплики 1 и 2 смогли быстро завершить свои задачи. Они
        запрашивают другую задачу у координаторского узла.
    </li>
    <li>
        Координатор назначает новые задачи репликам 1 и 2.
    </li>
</ol>

<Image img={image_7} size="md" alt="Динамическая координация - часть 3" />

<ol className="docs-ordered-list">
    <li>
        Теперь все реплики завершили обработку своих задач. Они
        запрашивают больше задач.
    </li>
    <li>
        Координатор, используя объявления, проверяет, какие задачи остались
        для обработки, но оставшихся задач нет.
    </li>
    <li>
        Координатор сообщает репликам, что все было обработано.
        Теперь он объединит все состояния, которые можно объединить, и
        ответит на запрос.
    </li>
</ol>
### Управление локальностью кэша {#managing-cache-locality}

Последней оставшейся потенциальной проблемой является управление локальностью кэша. Если запрос
выполняется несколько раз, как можно обеспечить, что одна и та же задача будет маршрутизироваться 
к одной и той же реплике? В предыдущем примере у нас были следующие назначенные задачи:

<table>
    <thead>
        <tr>
            <th></th>
            <th>Реплика 1</th>
            <th>Реплика 2</th>
            <th>Реплика 3</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Часть 1</td>
            <td>g1, g6, g7</td>
            <td>g2, g4, g5</td>
            <td>g3</td>
        </tr>
        <tr>
            <td>Часть 2</td>
            <td>g1</td>
            <td>g2, g4, g5</td>
            <td>g3</td>
        </tr>
        <tr>
            <td>Часть 3</td>
            <td>g1, g6</td>
            <td>g2, g4, g5</td>
            <td>g3</td>
        </tr>
    </tbody>
</table>

Чтобы гарантировать, что одни и те же задачи назначаются одним и тем же репликам и могут
воспользоваться кэшем, происходят две вещи. Хэш из части + набор гранул
(задача) вычисляется. Модуль числа реплик для назначения задачи
применяется.

На бумаге это звучит хорошо, но на практике, внезапная нагрузка на одну реплику или 
ухудшение сети, может вводить задержки, если одна и та же реплика постоянно используется
для выполнения определенных задач. Если `max_parallel_replicas` меньше
числа реплик, то для выполнения запроса выбираются случайные реплики.
### Кража задач {#task-stealing}

если какая-то реплика обрабатывает задачи медленнее других, другие реплики попытаются
'украсть' задачи, которые по идее принадлежат этой реплике по хэшу, чтобы уменьшить
задержку.
### Ограничения {#limitations}

Эта функция имеет известные ограничения, основные из которых задокументированы в
этом разделе.

:::note
Если вы обнаружили проблему, которая не является одной из перечисленных ниже
ограничений и подозреваете, что параллельная реплика может быть причиной, пожалуйста,
откройте проблему на GitHub, используя метку `comp-parallel-replicas`.
:::

| Ограничение                                    | Описание                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
|-----------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Сложные запросы                                 | В настоящее время параллельная реплика работает довольно хорошо для простых запросов. Слои сложности, такие как общие табличные выражения (CTE), подзапросы, JOIN, не плоские запросы и др., могут отрицательно сказываться на производительности запроса.                                                                                                                                                                                                                   |
| Маленькие запросы                                 | Если вы выполняете запрос, который не обрабатывает много строк, то его выполнение на нескольких репликах может не привести к улучшению времени выполнения, учитывая, что сетевое время для координации между репликами может привести к дополнительным циклам при выполнении запроса. Вы можете уменьшить эти проблемы, используя настройку: [`parallel_replicas_min_number_of_rows_per_replica`](/docs/operations/settings/settings#parallel_replicas_min_number_of_rows_per_replica).  |
| Параллельные реплики отключены с FINAL      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| Данные с высокой кардинальностью и сложная агрегация | Агрегация с высокой кардинальностью, которая нуждается в отправке большого количества данных, может значительно замедлить ваши запросы.                                                                                                                                                                                                                                                                                                                                                                     |
| Совместимость с новым анализатором           | Новый анализатор может значительно замедлить или ускорить выполнение запросов в конкретных сценариях.                                                                                                                                                                                                                                                                                                                                                                       |
## Настройки, связанные с параллельными репликами {#settings-related-to-parallel-replicas}

| Настройка                                            | Описание                                                                                                                                                                                                                                                         |
|----------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `enable_parallel_replicas`                         | `0`: отключено<br/> `1`: включено <br/>`2`: Принудительное использование параллельной реплики, выбрасывает исключение, если не используется.                                                                                                                                                          |
| `cluster_for_parallel_replicas`                    | Имя кластера для использования параллельной репликации; если вы используете ClickHouse Cloud, используйте `default`.                                                                                                                                                                 |
| `max_parallel_replicas`                            | Максимальное количество реплик, используемых для выполнения запроса на несколько реплик, если указано число меньше, чем количество реплик в кластере, узлы будут выбраны случайно. Это значение также может быть переполнено для учёта горизонтального масштабирования. |
| `parallel_replicas_min_number_of_rows_per_replica` | Помогает ограничить количество используемых реплик на основе количества строк, которые нужно обработать. Количество реплик определяется: <br/> `примерное количество строк для чтения` / `min_number_of_rows_per_replica`.                                                               |
| `allow_experimental_analyzer`                      | `0`: использовать старый анализатор<br/> `1`: использовать новый анализатор. <br/><br/>Поведение параллельных реплик может изменяться в зависимости от используемого анализатора.                                                                                                                                    |
## Исследование проблем с параллельными репликами {#investigating-issues-with-parallel-replicas}

Вы можете проверить, какие настройки используются для каждого запроса в таблице [`system.query_log`](/docs/operations/system-tables/query_log). Вы также можете посмотреть таблицу [`system.events`](/docs/operations/system-tables/events), чтобы увидеть все события, которые произошли на сервере, и использовать табличную функцию [`clusterAllReplicas`](/docs/sql-reference/table-functions/cluster) для просмотра таблиц на всех репликах (если вы пользователь облака, используйте `default`).

```sql title="Query"
SELECT
   hostname(),
   *
FROM clusterAllReplicas('default', system.events)
WHERE event ILIKE '%ParallelReplicas%'
```
<details>
<summary>Ответ</summary>
```response title="Response"
┌─hostname()───────────────────────┬─event──────────────────────────────────────────┬─value─┬─description──────────────────────────────────────────────────────────────────────────────────────────┐
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasHandleRequestMicroseconds      │   438 │ Время, затраченное на обработку запросов на метки от реплик                                             │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasHandleAnnouncementMicroseconds │   558 │ Время, затраченное на обработку объявлений реплик                                                        │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasReadUnassignedMarks            │   240 │ Сумма по всем репликам, сколько несвязанных меток было запланировано                                    │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasReadAssignedForStealingMarks   │     4 │ Сумма по всем репликам, сколько из запланированных меток было назначено для захвата по согласованному хешу │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasStealingByHashMicroseconds     │     5 │ Время, затраченное на сбор сегментов, предназначенных для захвата по хешу                               │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasProcessingPartsMicroseconds    │     5 │ Время, затраченное на обработку частей данных                                                           │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasStealingLeftoversMicroseconds  │     3 │ Время, затраченное на сбор сиротских сегментов                                                          │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasUsedCount                      │     2 │ Количество реплик, использованных для выполнения запроса с параллельными репликами на основе задач      │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasAvailableCount                 │     6 │ Количество реплик, доступных для выполнения запроса с параллельными репликами на основе задач           │
└──────────────────────────────────┴────────────────────────────────────────────────┴───────┴──────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─hostname()───────────────────────┬─event──────────────────────────────────────────┬─value─┬─description──────────────────────────────────────────────────────────────────────────────────────────┐
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasHandleRequestMicroseconds      │   698 │ Время, затраченное на обработку запросов на метки от реплик                                             │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasHandleAnnouncementMicroseconds │   644 │ Время, затраченное на обработку объявлений реплик                                                        │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasReadUnassignedMarks            │   190 │ Сумма по всем репликам, сколько несвязанных меток было запланировано                                    │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasReadAssignedForStealingMarks   │    54 │ Сумма по всем репликам, сколько из запланированных меток было назначено для захвата по согласованному хешу │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasStealingByHashMicroseconds     │     8 │ Время, затраченное на сбор сегментов, предназначенных для захвата по хешу                               │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasProcessingPartsMicroseconds    │     4 │ Время, затраченное на обработку частей данных                                                           │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasStealingLeftoversMicroseconds  │     2 │ Время, затраченное на сбор сиротских сегментов                                                          │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasUsedCount                      │     2 │ Количество реплик, использованных для выполнения запроса с параллельными репликами на основе задач      │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasAvailableCount                 │     6 │ Количество реплик, доступных для выполнения запроса с параллельными репликами на основе задач           │
└──────────────────────────────────┴────────────────────────────────────────────────┴───────┴──────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─hostname()───────────────────────┬─event──────────────────────────────────────────┬─value─┬─description──────────────────────────────────────────────────────────────────────────────────────────┐
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasHandleRequestMicroseconds      │   620 │ Время, затраченное на обработку запросов на метки от реплик                                             │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasHandleAnnouncementMicroseconds │   656 │ Время, затраченное на обработку объявлений реплик                                                        │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasReadUnassignedMarks            │     1 │ Сумма по всем репликам, сколько несвязанных меток было запланировано                                    │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasReadAssignedForStealingMarks   │     1 │ Сумма по всем репликам, сколько из запланированных меток было назначено для захвата по согласованному хешу │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasStealingByHashMicroseconds     │     4 │ Время, затраченное на сбор сегментов, предназначенных для захвата по хешу                               │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasProcessingPartsMicroseconds    │     3 │ Время, затраченное на обработку частей данных                                                           │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasStealingLeftoversMicroseconds  │     1 │ Время, затраченное на сбор сиротских сегментов                                                          │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasUsedCount                      │     2 │ Количество реплик, использованных для выполнения запроса с параллельными репликами на основе задач      │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasAvailableCount                 │    12 │ Количество реплик, доступных для выполнения запроса с параллельными репликами на основе задач           │
└──────────────────────────────────┴────────────────────────────────────────────────┴───────┴──────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─hostname()───────────────────────┬─event──────────────────────────────────────────┬─value─┬─description──────────────────────────────────────────────────────────────────────────────────────────┐
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasHandleRequestMicroseconds      │   696 │ Время, затраченное на обработку запросов на метки от реплик                                             │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasHandleAnnouncementMicroseconds │   717 │ Время, затраченное на обработку объявлений реплик                                                        │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasReadUnassignedMarks            │     2 │ Сумма по всем репликам, сколько несвязанных меток было запланировано                                    │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasReadAssignedForStealingMarks   │     2 │ Сумма по всем репликам, сколько из запланированных меток было назначено для захвата по согласованному хешу │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasStealingByHashMicroseconds     │    10 │ Время, затраченное на сбор сегментов, предназначенных для захвата по хешу                               │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasProcessingPartsMicroseconds    │     6 │ Время, затраченное на обработку частей данных                                                           │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasStealingLeftoversMicroseconds  │     2 │ Время, затраченное на сбор сиротских сегментов                                                          │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasUsedCount                      │     2 │ Количество реплик, использованных для выполнения запроса с параллельными репликами на основе задач      │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasAvailableCount                 │    12 │ Количество реплик, доступных для выполнения запроса с параллельными репликами на основе задач           │
└──────────────────────────────────┴────────────────────────────────────────────────┴───────┴──────────────────────────────────────────────────────────────────────────────────────────────────────┘
```
</details>

Таблица [`system.text_log`](/docs/operations/system-tables/text_log) также содержит информацию о выполнении запросов с использованием параллельных реплик:

```sql title="Query"
SELECT message
FROM clusterAllReplicas('default', system.text_log)
WHERE query_id = 'ad40c712-d25d-45c4-b1a1-a28ba8d4019c'
ORDER BY event_time_microseconds ASC
```

<details>
<summary>Ответ</summary>
```response title="Response"
┌─message────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ (from 54.218.178.249:59198) SELECT * FROM session_events WHERE type='type2' LIMIT 10 SETTINGS allow_experimental_parallel_reading_from_replicas=2; (stage: Complete)                                                                                       │
│ Query SELECT __table1.clientId AS clientId, __table1.sessionId AS sessionId, __table1.pageId AS pageId, __table1.timestamp AS timestamp, __table1.type AS type FROM default.session_events AS __table1 WHERE __table1.type = 'type2' LIMIT _CAST(10, 'UInt64') SETTINGS allow_experimental_parallel_reading_from_replicas = 2 to stage Complete │
│ Access granted: SELECT(clientId, sessionId, pageId, timestamp, type) ON default.session_events                                                                                                                                                             │
│ Query SELECT __table1.clientId AS clientId, __table1.sessionId AS sessionId, __table1.pageId AS pageId, __table1.timestamp AS timestamp, __table1.type AS type FROM default.session_events AS __table1 WHERE __table1.type = 'type2' LIMIT _CAST(10, 'UInt64') to stage WithMergeableState only analyze │
│ Access granted: SELECT(clientId, sessionId, pageId, timestamp, type) ON default.session_events                                                                                                                                                             │
│ Query SELECT __table1.clientId AS clientId, __table1.sessionId AS sessionId, __table1.pageId AS pageId, __table1.timestamp AS timestamp, __table1.type AS type FROM default.session_events AS __table1 WHERE __table1.type = 'type2' LIMIT _CAST(10, 'UInt64') from stage FetchColumns to stage WithMergeableState only analyze │
│ Query SELECT __table1.clientId AS clientId, __table1.sessionId AS sessionId, __table1.pageId AS pageId, __table1.timestamp AS timestamp, __table1.type AS type FROM default.session_events AS __table1 WHERE __table1.type = 'type2' LIMIT _CAST(10, 'UInt64') SETTINGS allow_experimental_parallel_reading_from_replicas = 2 to stage WithMergeableState only analyze │
│ Access granted: SELECT(clientId, sessionId, pageId, timestamp, type) ON default.session_events                                                                                                                                                             │
│ Query SELECT __table1.clientId AS clientId, __table1.sessionId AS sessionId, __table1.pageId AS pageId, __table1.timestamp AS timestamp, __table1.type AS type FROM default.session_events AS __table1 WHERE __table1.type = 'type2' LIMIT _CAST(10, 'UInt64') SETTINGS allow_experimental_parallel_reading_from_replicas = 2 from stage FetchColumns to stage WithMergeableState only analyze │
│ Query SELECT __table1.clientId AS clientId, __table1.sessionId AS sessionId, __table1.pageId AS pageId, __table1.timestamp AS timestamp, __table1.type AS type FROM default.session_events AS __table1 WHERE __table1.type = 'type2' LIMIT _CAST(10, 'UInt64') SETTINGS allow_experimental_parallel_reading_from_replicas = 2 from stage WithMergeableState to stage Complete │
│ The number of replicas requested (100) is bigger than the real number available in the cluster (6). Will use the latter number to execute the query.                                                                                                       │
│ Initial request from replica 4: 2 parts: [part all_0_2_1 with ranges [(0, 182)], part all_3_3_0 with ranges [(0, 62)]]----------
Received from 4 replica
                                                                                                   │
│ Reading state is fully initialized: part all_0_2_1 with ranges [(0, 182)] in replicas [4]; part all_3_3_0 with ranges [(0, 62)] in replicas [4]                                                                                                            │
│ Sent initial requests: 1 Replicas count: 6                                                                                                                                                                                                                 │
│ Initial request from replica 2: 2 parts: [part all_0_2_1 with ranges [(0, 182)], part all_3_3_0 with ranges [(0, 62)]]----------
Received from 2 replica
                                                                                                   │
│ Sent initial requests: 2 Replicas count: 6                                                                                                                                                                                                                 │
│ Handling request from replica 4, minimal marks size is 240                                                                                                                                                                                                 │
│ Going to respond to replica 4 with 1 parts: [part all_0_2_1 with ranges [(128, 182)]]. Finish: false; mine_marks=0, stolen_by_hash=54, stolen_rest=0                                                                                                       │
│ Initial request from replica 1: 2 parts: [part all_0_2_1 with ranges [(0, 182)], part all_3_3_0 with ranges [(0, 62)]]----------
Received from 1 replica
                                                                                                   │
│ Sent initial requests: 3 Replicas count: 6                                                                                                                                                                                                                 │
│ Handling request from replica 4, minimal marks size is 240                                                                                                                                                                                                 │
│ Going to respond to replica 4 with 2 parts: [part all_0_2_1 with ranges [(0, 128)], part all_3_3_0 with ranges [(0, 62)]]. Finish: false; mine_marks=0, stolen_by_hash=0, stolen_rest=190                                                                  │
│ Initial request from replica 0: 2 parts: [part all_0_2_1 with ranges [(0, 182)], part all_3_3_0 with ranges [(0, 62)]]----------
Received from 0 replica
                                                                                                   │
│ Sent initial requests: 4 Replicas count: 6                                                                                                                                                                                                                 │
│ Initial request from replica 5: 2 parts: [part all_0_2_1 with ranges [(0, 182)], part all_3_3_0 with ranges [(0, 62)]]----------
Received from 5 replica
                                                                                                   │
│ Sent initial requests: 5 Replicas count: 6                                                                                                                                                                                                                 │
│ Handling request from replica 2, minimal marks size is 240                                                                                                                                                                                                 │
│ Going to respond to replica 2 with 0 parts: []. Finish: true; mine_marks=0, stolen_by_hash=0, stolen_rest=0                                                                                                                                                │
│ Initial request from replica 3: 2 parts: [part all_0_2_1 with ranges [(0, 182)], part all_3_3_0 with ranges [(0, 62)]]----------
Received from 3 replica
                                                                                                   │
│ Sent initial requests: 6 Replicas count: 6                                                                                                                                                                                                                 │
│ Total rows to read: 2000000                                                                                                                                                                                                                                │
│ Handling request from replica 5, minimal marks size is 240                                                                                                                                                                                                 │
│ Going to respond to replica 5 with 0 parts: []. Finish: true; mine_marks=0, stolen_by_hash=0, stolen_rest=0                                                                                                                                                │
│ Handling request from replica 0, minimal marks size is 240                                                                                                                                                                                                 │
│ Going to respond to replica 0 with 0 parts: []. Finish: true; mine_marks=0, stolen_by_hash=0, stolen_rest=0                                                                                                                                                │
│ Handling request from replica 1, minimal marks size is 240                                                                                                                                                                                                 │
│ Going to respond to replica 1 with 0 parts: []. Finish: true; mine_marks=0, stolen_by_hash=0, stolen_rest=0                                                                                                                                                │
│ Handling request from replica 3, minimal marks size is 240                                                                                                                                                                                                 │
│ Going to respond to replica 3 with 0 parts: []. Finish: true; mine_marks=0, stolen_by_hash=0, stolen_rest=0                                                                                                                                                │
│ (c-crimson-vd-86-server-rdhnsx3-0.c-crimson-vd-86-server-headless.ns-crimson-vd-86.svc.cluster.local:9000) Cancelling query because enough data has been read                                                                                              │
│ Read 81920 rows, 5.16 MiB in 0.013166 sec., 6222087.194288318 rows/sec., 391.63 MiB/sec.                                                                                                                                                                   │
│ Coordination done: Statistics: replica 0 - {requests: 2 marks: 0 assigned_to_me: 0 stolen_by_hash: 0 stolen_unassigned: 0}; replica 1 - {requests: 2 marks: 0 assigned_to_me: 0 stolen_by_hash: 0 stolen_unassigned: 0}; replica 2 - {requests: 2 marks: 0 assigned_to_me: 0 stolen_by_hash: 0 stolen_unassigned: 0}; replica 3 - {requests: 2 marks: 0 assigned_to_me: 0 stolen_by_hash: 0 stolen_unassigned: 0}; replica 4 - {requests: 3 marks: 244 assigned_to_me: 0 stolen_by_hash: 54 stolen_unassigned: 190}; replica 5 - {requests: 2 marks: 0 assigned_to_me: 0 stolen_by_hash: 0 stolen_unassigned: 0} │
│ Peak memory usage (for query): 1.81 MiB.                                                                                                                                                                                                                   │
│ Processed in 0.024095586 sec.                                                                                                                                                                                                                              │
└────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
```
</details>

Наконец, вы можете использовать `EXPLAIN PIPELINE`. Он показывает, как ClickHouse будет выполнять запрос и какие ресурсы будут использованы для выполнения запроса. Возьмём, например, следующий запрос:

```sql
SELECT count(), uniq(pageId) , min(timestamp), max(timestamp) 
FROM session_events 
WHERE type='type3' 
GROUP BY toYear(timestamp) LIMIT 10
```

Давайте посмотрим на конвейер запросов без параллельной реплики:

```sql title="EXPLAIN PIPELINE (без параллельной реплики)"
EXPLAIN PIPELINE graph = 1, compact = 0 
SELECT count(), uniq(pageId) , min(timestamp), max(timestamp) 
FROM session_events 
WHERE type='type3' 
GROUP BY toYear(timestamp) 
LIMIT 10 
SETTINGS allow_experimental_parallel_reading_from_replicas=0 
FORMAT TSV;
```

<Image img={image_8} size="lg" alt="EXPLAIN without parallel_replica" />

А теперь с параллельной репликой:

```sql title="EXPLAIN PIPELINE (с параллельной репликой)"
EXPLAIN PIPELINE graph = 1, compact = 0 
SELECT count(), uniq(pageId) , min(timestamp), max(timestamp) 
FROM session_events 
WHERE type='type3' 
GROUP BY toYear(timestamp) 
LIMIT 10 
SETTINGS allow_experimental_parallel_reading_from_replicas=2 
FORMAT TSV;
```

<Image img={image_9} size="lg" alt="EXPLAIN with parallel_replica"/>
