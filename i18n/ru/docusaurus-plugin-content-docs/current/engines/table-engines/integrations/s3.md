---
description: 'Этот движок предоставляет интеграцию с экосистемой Amazon S3. Похож на движок HDFS, но предлагает специфические функции для S3.'
sidebar_label: 'S3'
sidebar_position: 180
slug: /engines/table-engines/integrations/s3
title: 'Движок таблиц S3'
---


# Движок таблиц S3

Этот движок предоставляет интеграцию с экосистемой [Amazon S3](https://aws.amazon.com/s3/). Этот движок похож на движок [HDFS](/engines/table-engines/integrations/hdfs), но предлагает функции, специфичные для S3.

## Пример {#example}

```sql
CREATE TABLE s3_engine_table (name String, value UInt32)
    ENGINE=S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/test-data.csv.gz', 'CSV', 'gzip')
    SETTINGS input_format_with_names_use_header = 0;

INSERT INTO s3_engine_table VALUES ('one', 1), ('two', 2), ('three', 3);

SELECT * FROM s3_engine_table LIMIT 2;
```

```text
┌─name─┬─value─┐
│ one  │     1 │
│ two  │     2 │
└──────┴───────┘
```

## Создание таблицы {#creating-a-table}

```sql
CREATE TABLE s3_engine_table (name String, value UInt32)
    ENGINE = S3(path [, NOSIGN | aws_access_key_id, aws_secret_access_key,] format, [compression])
    [PARTITION BY expr]
    [SETTINGS ...]
```

### Параметры движка {#parameters}

- `path` — URL корзины с путем к файлу. Поддерживаются следующие подстановочные знаки в режиме только для чтения: `*`, `**`, `?`, `{abc,def}` и `{N..M}`, где `N`, `M` — числа, `'abc'`, `'def'` — строки. Дополнительную информацию см. [ниже](#wildcards-in-path).
- `NOSIGN` - Если это ключевое слово указано вместо учетных данных, все запросы не будут подписаны.
- `format` — [формат](/sql-reference/formats#formats-overview) файла.
- `aws_access_key_id`, `aws_secret_access_key` - Долгосрочные учетные данные для пользователя аккаунта [AWS](https://aws.amazon.com/). Вы можете использовать их для аутентификации ваших запросов. Параметр является необязательным. Если учетные данные не указаны, они используются из файла конфигурации. Дополнительную информацию см. [Использование S3 для хранения данных](../mergetree-family/mergetree.md#table_engine-mergetree-s3).
- `compression` — Тип сжатия. Поддерживаемые значения: `none`, `gzip/gz`, `brotli/br`, `xz/LZMA`, `zstd/zst`. Параметр является необязательным. По умолчанию сжатие будет автоматически определяться по расширению файла.

### Кэш данных {#data-cache}

Движок таблицы `S3` поддерживает кэширование данных на локальном диске. 
Смотрите параметры конфигурации кэша файловой системы и использование в этом [разделе](/operations/storing-data.md/#using-local-cache). 
Кэширование осуществляется в зависимости от пути и ETag объекта хранения, поэтому ClickHouse не будет читать устаревшую версию кэша.

Чтобы включить кэширование, используйте настройку `filesystem_cache_name = '<name>'` и `enable_filesystem_cache = 1`.

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_3.csv', 'minioadmin', 'minioadminpassword', 'CSV')
SETTINGS filesystem_cache_name = 'cache_for_s3', enable_filesystem_cache = 1;
```

Существует два способа определить кэш в файле конфигурации.

1. Добавьте следующий раздел в файл конфигурации ClickHouse:

```xml
<clickhouse>
    <filesystem_caches>
        <cache_for_s3>
            <path>путь к директории кэша</path>
            <max_size>10Gi</max_size>
        </cache_for_s3>
    </filesystem_caches>
</clickhouse>
```

2. Повторно используйте конфигурацию кэша (а значит, и хранилище кэша) из раздела `storage_configuration` ClickHouse, [описанного здесь](/operations/storing-data.md/#using-local-cache)

### PARTITION BY {#partition-by}

`PARTITION BY` — Необязательный. В большинстве случаев вам не нужен ключ партиционирования, и если он нужен, то обычно не нужен более детальный ключ партиционирования, чем по месяцу. Партиционирование не ускоряет запросы (в отличие от выражения ORDER BY). Никогда не используйте слишком детальное партиционирование. Не партиционируйте ваши данные по идентификаторам клиентов или именам (вместо этого сделайте идентификатор клиента или имя первым столбцом в выражении ORDER BY).

Для партиционирования по месяцам используйте выражение `toYYYYMM(date_column)`, где `date_column` — это колонка с датой типа [Date](/sql-reference/data-types/date.md). Имена партиций имеют формат `"YYYYMM"`.

### Запрос данных из партиционированных таблиц {#querying-partitioned-data}

В этом примере используется [рецепт docker compose](https://github.com/ClickHouse/examples/tree/5fdc6ff72f4e5137e23ea075c88d3f44b0202490/docker-compose-recipes/recipes/ch-and-minio-S3), который интегрирует ClickHouse и MinIO. Вы должны быть в состоянии воспроизвести те же запросы, используя S3, заменив значения конечной точки и аутентификации.

Обратите внимание, что конечная точка S3 в конфигурации `ENGINE` использует параметр token `{_partition_id}` как часть объекта S3 (имя файла), и что запросы SELECT работают с этими именами объектов (например, `test_3.csv`).

:::note
Как показано в примере, прямой запрос из партиционированных таблиц S3 в настоящее время не поддерживается, но может быть выполнен путем запроса отдельных партиций с использованием функции таблицы S3.

Основное применение для записи партиционированных данных в S3 — это возможность переноса этих данных в другую систему ClickHouse (например, перемещение из локальных систем в ClickHouse Cloud). Поскольку наборы данных ClickHouse часто очень большие, а надежность сети иногда несовершенна, имеет смысл переносить наборы данных по частям, таким образом выполняя партиционированные записи.
:::

#### Создание таблицы {#create-the-table}
```sql
CREATE TABLE p
(
    `column1` UInt32,
    `column2` UInt32,
    `column3` UInt32
)
ENGINE = S3(
-- highlight-next-line
           'http://minio:10000/clickhouse//test_{_partition_id}.csv',
           'minioadmin',
           'minioadminpassword',
           'CSV')
PARTITION BY column3
```

#### Вставка данных {#insert-data}
```sql
insert into p values (1, 2, 3), (3, 2, 1), (78, 43, 45)
```

#### Запрос из партиции 3 {#select-from-partition-3}

:::tip
Этот запрос использует функцию таблицы s3
:::

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_3.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```
```response
┌─c1─┬─c2─┬─c3─┐
│  1 │  2 │  3 │
└────┴────┴────┘
```

#### Запрос из партиции 1 {#select-from-partition-1}
```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_1.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```
```response
┌─c1─┬─c2─┬─c3─┐
│  3 │  2 │  1 │
└────┴────┴────┘
```

#### Запрос из партиции 45 {#select-from-partition-45}
```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_45.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```
```response
┌─c1─┬─c2─┬─c3─┐
│ 78 │ 43 │ 45 │
└────┴────┴────┘
```

#### Ограничение {#limitation}

Вы, возможно, попытаетесь выполнить `Select * from p`, но, как было отмечено выше, этот запрос завершится ошибкой; используйте предыдущий запрос.

```sql
SELECT * FROM p
```
```response
Получено исключение от сервера (версия 23.4.1):
Код: 48. DB::Exception: Получено от localhost:9000. DB::Exception: Чтение из партиционированного S3 хранилища еще не реализовано. (NOT_IMPLEMENTED)
```

## Вставка данных {#inserting-data}

Обратите внимание, что строки могут быть вставлены только в новые файлы. Нет циклов слияния или операций разделения файлов. Как только файл записан, последующие вставки завершатся с ошибкой. Чтобы избежать этого, вы можете использовать настройки `s3_truncate_on_insert` и `s3_create_new_file_on_insert`. Смотрите больше деталей [здесь](/integrations/s3#inserting-data).

## Виртуальные колонки {#virtual-columns}

- `_path` — Путь к файлу. Тип: `LowCardinality(String)`.
- `_file` — Имя файла. Тип: `LowCardinality(String)`.
- `_size` — Размер файла в байтах. Тип: `Nullable(UInt64)`. Если размер неизвестен, значение будет `NULL`.
- `_time` — Время последнего изменения файла. Тип: `Nullable(DateTime)`. Если время неизвестно, значение будет `NULL`.
- `_etag` — ETag файла. Тип: `LowCardinality(String)`. Если etag неизвестен, значение будет `NULL`.

Для получения дополнительной информации о виртуальных колонках смотрите [здесь](../../../engines/table-engines/index.md#table_engines-virtual_columns).

## Детали реализации {#implementation-details}

- Чтения и записи могут выполняться параллельно
- Не поддерживается:
    - Операции `ALTER` и `SELECT...SAMPLE`.
    - Индексы.
    - [Репликация без копирования](../../../operations/storing-data.md#zero-copy) возможна, но не поддерживается.

  :::note Репликация без копирования не готова для продакшена
  Репликация без копирования отключена по умолчанию в ClickHouse версии 22.8 и выше. Эта функция не рекомендуется для использования в продуктивной среде.
  :::

## Подстановочные знаки в пути {#wildcards-in-path}

Аргумент `path` может указывать на несколько файлов, используя подстановочные знаки в стиле bash. Для обработки файл должен существовать и соответствовать полному шаблону пути. Перечисление файлов определяется во время `SELECT` (не в момент `CREATE`).

- `*` — Заменяет любое количество любых символов, кроме `/`, включая пустую строку.
- `**` — Заменяет любое количество любых символов, включая `/`, включая пустую строку.
- `?` — Заменяет любой один символ.
- `{some_string,another_string,yet_another_one}` — Заменяет любую из строк `'some_string', 'another_string', 'yet_another_one'`.
- `{N..M}` — Заменяет любое число в диапазоне от N до M, включая оба границы. N и M могут иметь ведущие нули, например, `000..078`.

Конструкции с `{}` аналогичны функции таблицы [remote](../../../sql-reference/table-functions/remote.md).

:::note
Если перечисление файлов содержит числовые диапазоны с ведущими нулями, используйте конструкцию с фигурными скобками для каждой цифры отдельно или используйте `?`.
:::

**Пример с подстановочными знаками 1**

Создайте таблицу с файлами, названными `file-000.csv`, `file-001.csv`, ... , `file-999.csv`:

```sql
CREATE TABLE big_table (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/my_folder/file-{000..999}.csv', 'CSV');
```

**Пример с подстановочными знаками 2**

Предположим, у нас есть несколько файлов в формате CSV с следующими URI на S3:

- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_1.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_2.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_3.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_1.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_2.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_3.csv'

Существует несколько способов создать таблицу, состоящую из всех шести файлов:

1. Укажите диапазон постфиксов файлов:

```sql
CREATE TABLE table_with_range (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/some_file_{1..3}', 'CSV');
```

2. Возьмите все файлы с префиксом `some_file_` (в обеих папках не должно быть лишних файлов с таким префиксом):

```sql
CREATE TABLE table_with_question_mark (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/some_file_?', 'CSV');
```

3. Возьмите все файлы в обеих папках (все файлы должны соответствовать формату и схеме, описанным в запросе):

```sql
CREATE TABLE table_with_asterisk (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/*', 'CSV');
```

## Параметры хранения {#storage-settings}

- [s3_truncate_on_insert](/operations/settings/settings.md#s3_truncate_on_insert) - позволяет обрезать файл перед вставкой в него. Отключен по умолчанию.
- [s3_create_new_file_on_insert](/operations/settings/settings.md#s3_create_new_file_on_insert) - позволяет создавать новый файл при каждой вставке, если формат имеет суффикс. Отключен по умолчанию.
- [s3_skip_empty_files](/operations/settings/settings.md#s3_skip_empty_files) - позволяет пропускать пустые файлы при чтении. Включен по умолчанию.

## Настройки, связанные с S3 {#settings}

Следующие настройки могут быть установлены перед выполнением запроса или размещены в файле конфигурации.

- `s3_max_single_part_upload_size` — Максимальный размер объекта для загрузки с использованием одной загрузки на S3. Значение по умолчанию: `32Mb`.
- `s3_min_upload_part_size` — Минимальный размер части, которую нужно загрузить при многокомпонентной загрузке на [S3 Multipart upload](https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html). Значение по умолчанию: `16Mb`.
- `s3_max_redirects` — Максимальное количество перенаправлений S3, допустимых. Значение по умолчанию: `10`.
- `s3_single_read_retries` — Максимальное количество попыток во время однократного чтения. Значение по умолчанию: `4`.
- `s3_max_put_rps` — Максимальная скорость запросов PUT в секунду до ограничения. Значение по умолчанию: `0` (неограничено).
- `s3_max_put_burst` — Максимальное количество запросов, которые могут быть сделаны одновременно до достижения лимита запросов в секунду. По умолчанию (`0`) равно `s3_max_put_rps`.
- `s3_max_get_rps` — Максимальная скорость запросов GET в секунду до ограничения. Значение по умолчанию: `0` (неограничено).
- `s3_max_get_burst` — Максимальное количество запросов, которые могут быть сделаны одновременно до достижения лимита запросов в секунду. По умолчанию (`0`) равно `s3_max_get_rps`.
- `s3_upload_part_size_multiply_factor` - Умножьте `s3_min_upload_part_size` на этот коэффициент каждый раз, когда `s3_upload_part_size_multiply_parts_count_threshold` частей были загружены из одной записи в S3. Значение по умолчанию: `2`.
- `s3_upload_part_size_multiply_parts_count_threshold` - Каждый раз, когда это количество частей было загружено на S3, `s3_min_upload_part_size` умножается на `s3_upload_part_size_multiply_factor`. Значение по умолчанию: `500`.
- `s3_max_inflight_parts_for_one_file` - Ограничивает количество запросов PUT, которые могут выполняться одновременно для одного объекта. Это число должно быть ограничено. Значение `0` обозначает неограниченное количество. Значение по умолчанию: `20`. Каждая часть в процессе имеет буфер размером `s3_min_upload_part_size` для первых `s3_upload_part_size_multiply_factor` частей и больше, когда файл достаточно велик, см. `upload_part_size_multiply_factor`. При настройках по умолчанию один загружаемый файл потребляет не более `320Mb` для файла, который меньше `8G`. Потребление больше для более крупных файлов.

Соображения безопасности: если злонамеренный пользователь может указать произвольные URL S3, необходимо установить `s3_max_redirects` в ноль, чтобы избежать [SSRF](https://en.wikipedia.org/wiki/Server-side_request_forgery) атак; или, альтернативно, `remote_host_filter` должен быть указан в конфигурации сервера.

## Настройки на основе конечной точки {#endpoint-settings}

Следующие настройки могут быть указаны в файле конфигурации для данной конечной точки (которая будет соответствовать точному префиксу URL):

- `endpoint` — Указывает префикс конечной точки. Обязательный.
- `access_key_id` и `secret_access_key` — Указывают учетные данные для использования с данной конечной точкой. Необязательный.
- `use_environment_credentials` — Если установлено в `true`, клиент S3 попытается получить учетные данные из переменных окружения и [метаданных Amazon EC2](https://en.wikipedia.org/wiki/Amazon_Elastic_Compute_Cloud) для данной конечной точки. Необязательный, значение по умолчанию: `false`.
- `region` — Указывает название региона S3. Необязательный.
- `use_insecure_imds_request` — Если установлено в `true`, клиент S3 будет использовать небезопасный IMDS запрос при получении учетных данных из метаданных Amazon EC2. Необязательный, значение по умолчанию: `false`.
- `expiration_window_seconds` — Период грации для проверки, истекли ли учетные данные, основанные на времени действия. Необязательный, значение по умолчанию: `120`.
- `no_sign_request` - Игнорировать все учетные данные, чтобы запросы не были подписаны. Полезно для доступа к публичным корзинам.
- `header` — Добавляет указанный HTTP заголовок к запросу к данной конечной точке. Необязательный, может быть указан несколько раз.
- `access_header` - Добавляет указанный HTTP заголовок к запросу к данной конечной точке, в случаях, когда нет других учетных данных из другого источника.
- `server_side_encryption_customer_key_base64` — Если указано, будут установлены необходимые заголовки для доступа к объектам S3 с шифрованием SSE-C. Необязательный.
- `server_side_encryption_kms_key_id` - Если указано, будут установлены необходимые заголовки для доступа к объектам S3 с [шифрованием SSE-KMS](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html). Если указана пустая строка, будет использован управляемый AWS ключ S3. Необязательный.
- `server_side_encryption_kms_encryption_context` - Если указано вместе с `server_side_encryption_kms_key_id`, будет установлен данный заголовок контекста шифрования для SSE-KMS. Необязательный.
- `server_side_encryption_kms_bucket_key_enabled` - Если указано вместе с `server_side_encryption_kms_key_id`, будет установлен заголовок для включения ключей корзины S3 для SSE-KMS. Необязательный, может быть значением `true` или `false`, по умолчанию ничего не установлено (соответствует настройке на уровне корзины).
- `max_single_read_retries` — Максимальное количество попыток во время однократного чтения. Значение по умолчанию: `4`. Необязательный.
- `max_put_rps`, `max_put_burst`, `max_get_rps` и `max_get_burst` - Параметры ограничения (см. выше) для использования конкретной конечной точки вместо запроса. Необязательный.

**Пример:**

```xml
<s3>
    <endpoint-name>
        <endpoint>https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/</endpoint>
        <!-- <access_key_id>ACCESS_KEY_ID</access_key_id> -->
        <!-- <secret_access_key>SECRET_ACCESS_KEY</secret_access_key> -->
        <!-- <region>us-west-1</region> -->
        <!-- <use_environment_credentials>false</use_environment_credentials> -->
        <!-- <use_insecure_imds_request>false</use_insecure_imds_request> -->
        <!-- <expiration_window_seconds>120</expiration_window_seconds> -->
        <!-- <no_sign_request>false</no_sign_request> -->
        <!-- <header>Authorization: Bearer SOME-TOKEN</header> -->
        <!-- <server_side_encryption_customer_key_base64>BASE64-ENCODED-KEY</server_side_encryption_customer_key_base64> -->
        <!-- <server_side_encryption_kms_key_id>KMS_KEY_ID</server_side_encryption_kms_key_id> -->
        <!-- <server_side_encryption_kms_encryption_context>KMS_ENCRYPTION_CONTEXT</server_side_encryption_kms_encryption_context> -->
        <!-- <server_side_encryption_kms_bucket_key_enabled>true</server_side_encryption_kms_bucket_key_enabled> -->
        <!-- <max_single_read_retries>4</max_single_read_retries> -->
    </endpoint-name>
</s3>
```

## Работа с архивами {#working-with-archives}

Допустим, у нас есть несколько архивных файлов с следующими URI на S3:

- 'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-10.csv.zip'
- 'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-11.csv.zip'
- 'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-12.csv.zip'

Извлечение данных из этих архивов возможно с помощью ::. Подстановочные знаки могут использоваться как в части URL, так и в части после :: (отвечающей за имя файла внутри архива).

```sql
SELECT *
FROM s3(
   'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-1{0..2}.csv.zip :: *.csv'
);
```

:::note
ClickHouse поддерживает три формата архивов:
ZIP
TAR
7Z
В то время как архивы ZIP и TAR могут быть доступны из любого поддерживаемого места хранения, архивы 7Z могут быть прочитаны только из локальной файловой системы, где установлен ClickHouse.
:::


## Доступ к публичным корзинам {#accessing-public-buckets}

ClickHouse пытается получить учетные данные из многих различных источников. Иногда это может вызвать проблемы при доступе к некоторым корзинам, которые являются публичными, что приводит к возврату клиентом кода ошибки `403`. 
Эта проблема может быть устранена с помощью ключевого слова `NOSIGN`, принуждающего клиента игнорировать все учетные данные и не подписывать запросы.

```sql
CREATE TABLE big_table (name String, value UInt32)
    ENGINE = S3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/aapl_stock.csv', NOSIGN, 'CSVWithNames');
```

## Оптимизация производительности {#optimizing-performance}

Для получения подробной информации о оптимизации производительности функции s3 смотрите [наше подробное руководство](/integrations/s3/performance).

## Смотрите также {#see-also}

- [s3 функция таблицы](../../../sql-reference/table-functions/s3.md)
- [Интеграция S3 с ClickHouse](/integrations/s3)
