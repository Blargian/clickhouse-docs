---
slug: /deployment-guides/parallel-replicas
title: '並列レプリカ'
keywords: ['parallel replica']
description: 'このガイドでは、ClickHouseが分散テーブルを通じて複数のシャードにクエリを分配する方法と、その後クエリが複数のレプリカを活用して実行される方法について説明します。'
---

import Image from '@theme/IdealImage';
import BetaBadge from '@theme/badges/BetaBadge';
import image_1 from '@site/static/images/deployment-guides/parallel-replicas-1.png'
import image_2 from '@site/static/images/deployment-guides/parallel-replicas-2.png'
import image_3 from '@site/static/images/deployment-guides/parallel-replicas-3.png'
import image_4 from '@site/static/images/deployment-guides/parallel-replicas-4.png'
import image_5 from '@site/static/images/deployment-guides/parallel-replicas-5.png'
import image_6 from '@site/static/images/deployment-guides/parallel-replicas-6.png'
import image_7 from '@site/static/images/deployment-guides/parallel-replicas-7.png'
import image_8 from '@site/static/images/deployment-guides/parallel-replicas-8.png'
import image_9 from '@site/static/images/deployment-guides/parallel-replicas-9.png'

<BetaBadge/>
## はじめに {#introduction}

ClickHouseはクエリを非常に迅速に処理しますが、これらのクエリはどのように複数のサーバーに分配され、並列化されているのでしょうか？

> このガイドでは、ClickHouseが分散テーブルを通じて複数のシャードにクエリを分配する方法と、その後クエリが複数のレプリカを活用して実行される方法について説明します。
## シャーディングアーキテクチャ {#sharded-architecture}

共有Nothingアーキテクチャでは、クラスタは通常、複数のシャードに分割され、各シャードは全体データのサブセットを含みます。分散テーブルはこれらのシャードの上に存在し、完全なデータの統一ビューを提供します。

読み取りはローカルテーブルに送信できます。クエリの実行は指定されたシャードでのみ行われるか、分散テーブルに送信され、この場合、各シャードが与えられたクエリを実行します。分散テーブルにクエリがあったサーバーはデータを集約し、クライアントに応答します：

<Image img={image_1} size="md" alt="シャーディングアーキテクチャ" />

上の図は、クライアントが分散テーブルにクエリを送信したときに何が起こるかを視覚化しています：

<ol className="docs-ordered-list">
    <li>
        セレクトクエリは、ノード上の分散テーブルに送信されます（ラウンドロビン戦略またはロードバランサーによって特定のサーバーにルーティングされた後）。このノードは現在、コーディネーターとして機能します。
    </li>
    <li>
        ノードは、分散テーブルによって指定された情報に基づいて、クエリを実行する必要がある各シャードを特定し、クエリが各シャードに送信されます。
    </li>
    <li>
        各シャードはローカルでデータを読み取り、フィルタリングおよび集約を行い、マージ可能な状態をコーディネーターに返します。
    </li>
    <li>
        コーディネーターはデータをマージし、クライアントに応答を返します。
    </li>
</ol>

レプリカを混ぜると、プロセスは非常に似ていますが、唯一の違いは、各シャードの単一のレプリカのみがクエリを実行することです。これは、より多くのクエリが並列に処理できることを意味します。
## 非シャーディングアーキテクチャ {#non-sharded-architecture}

ClickHouse Cloudのアーキテクチャは、上記のものとは非常に異なります。（詳細については、["ClickHouse Cloud アーキテクチャ"](https://clickhouse.com/docs/cloud/reference/architecture)を参照してください）。計算とストレージの分離、および事実上無限のストレージを持つことで、シャードの必要性はそれほど重要ではなくなります。

下の図はClickHouse Cloudのアーキテクチャを示しています：

<Image img={image_2} size="md" alt="非シャーディングアーキテクチャ" />

このアーキテクチャにより、レプリカをほぼ瞬時に追加および削除でき、高いクラスタのスケーラビリティが保証されます。ClickHouse Keeperクラスタ（右側に示されている）は、メタデータの単一の真実のソースを確保します。レプリカはClickHouse Keeperクラスタからメタデータを取得し、すべて同じデータを保持します。データ自体はオブジェクトストレージに保存されており、SSDキャッシュによりクエリを迅速化できます。

では、複数のサーバーにクエリ実行をどのように分散できるでしょうか？シャーディングアーキテクチャでは、各シャードがデータのサブセットでクエリを実行できるため、このプロセスは非常に明白でした。シャーディングがない場合はどうなりますか？
## 並列レプリカの導入 {#introducing-parallel-replicas}

複数のサーバーによるクエリ実行を並列化するためには、最初に我々のサーバーの1つをコーディネーターとして割り当てる必要があります。コーディネーターは、実行する必要があるタスクのリストを作成し、それらがすべて実行され、集約され、結果がクライアントに戻ることを保証します。ほとんどの分散システムと同様に、これが最初のクエリを受け取ったノードの役割になります。また、作業の単位を定義する必要もあります。シャーディングアーキテクチャでは、作業の単位はシャード（データのサブセット）です。並列レプリカでは、作業の単位としてテーブルの小さな部分（[グラニュール](/docs/guides/best-practices/sparse-primary-indexes#data-is-organized-into-granules-for-parallel-data-processing））を使用します。

では、以下の図を使って実際にどうなるのか見てみましょう：

<Image img={image_3} size="md" alt="並列レプリカ" />

並列レプリカの場合：

<ol className="docs-ordered-list">
    <li>
        クライアントからのクエリは、ロードバランサーを通過した後、1つのノードに送信されます。このノードがこのクエリのコーディネーターになります。
    </li>
    <li>
        ノードは各パーツのインデックスを分析し、処理するための適切なパーツとグラニュールを選択します。
    </li>
    <li>
        コーディネーターはワークロードをさまざまなレプリカに割り当てることができるグラニュールのセットに分割します。
    </li>
    <li>
        各グラニュールのセットは対応するレプリカによって処理され、処理が完了するとコーディネーターにマージ可能な状態が送信されます。
    </li>
    <li>
        最後に、コーディネーターはレプリカからのすべての結果をマージし、クライアントに応答を返します。
    </li>
</ol>

上記のステップは理論的には並列レプリカがどのように機能するかを概説しています。しかし、実際には、こうした論理が完全に機能することを妨げる多くの要因があります：

<ol className="docs-ordered-list">
    <li>
        一部のレプリカが利用できないことがあります。
    </li>
    <li>
        ClickHouseのレプリケーションは非同期であるため、一部のレプリカは一時的に異なるパーツを保持しているかもしれません。
    </li>
    <li>
        レプリカ間の待機遅延を何らかの形で対処する必要があります。
    </li>
    <li>
        各レプリカの活動に基づいてファイルシステムキャッシュが異なるため、ランダムなタスク割り当てがキャッシュローカリティの観点で最適なパフォーマンスを妨げる可能性があります。
    </li>
</ol>

これらの要因がどのように克服されるかについては、次のセクションで説明します。
### アナウンスメント {#announcements}

上記のリストの（1）および（2）に対処するために、我々はアナウンスメントという概念を導入しました。以下の図を使ってその仕組みを視覚化してみましょう：

<Image img={image_4} size="md" alt="アナウンスメント" />

<ol className="docs-ordered-list">
    <li>
        クライアントからのクエリは、ロードバランサーを通過した後、1つのノードに送信されます。このノードがこのクエリのコーディネーターになります。
    </li>
    <li>
        コーディネーティングノードは、クラスタ内のすべてのレプリカからアナウンスメントを取得するためのリクエストを送信します。レプリカはテーブルの現在のパーツセットについてわずかに異なるビューを持つ場合があります。そのため、不正確なスケジューリング決定を避けるために、この情報を収集する必要があります。
    </li>
    <li>
        コーディネーティングノードは、アナウンスメントを使用して、異なるレプリカに割り当てることができるグラニュールのセットを定義します。ここでは、たとえば、レプリカ2にパート3のグラニュールが割り当てられていないのは、このレプリカがそのアナウンスメントにこのパートを提供していないためです。また、レプリカ3にはタスクが割り当てられていないことにも注意してください。これはそのレプリカがアナウンスメントを提供していないためです。
    </li>
    <li>
        各レプリカが自分のサブセットのグラニュールに対してクエリを処理し、マージ可能な状態がコーディネーターに戻されると、コーディネーターは結果をマージし、応答がクライアントに送信されます。
    </li>
</ol>
### 動的コーディネーション {#dynamic-coordination}

待機遅延の問題に対処するために、動的コーディネーションを追加しました。これは、すべてのグラニュールが1つのリクエストでレプリカに送信されるのではなく、各レプリカがコーディネーターに新しいタスク（処理されるグラニュールのセット）をリクエストできるようにすることを意味します。コーディネーターは受信したアナウンスメントに基づいてレプリカにグラニュールのセットを提供します。

すべてのレプリカがすべてのパーツを含むアナウンスメントを送信した段階にいると仮定しましょう。

以下の図は、動的コーディネーションがどのように機能するかを視覚化しています：

<Image img={image_5} size="md" alt="動的コーディネーション - パート1" />

<ol className="docs-ordered-list">
    <li>
        レプリカはコーディネーターにタスクを処理できることを知らせ、処理できる作業量も指定できます。
    </li>
    <li>
        コーディネーターはレプリカにタスクを割り当てます。
    </li>
</ol>

<Image img={image_6} size="md" alt="動的コーディネーション - パート2" />

<ol className="docs-ordered-list">
    <li>
        レプリカ1およびレプリカ2はタスクを非常に早く完了できます。彼らはコーディネーターから別のタスクをリクエストします。
    </li>
    <li>
        コーディネーターはレプリカ1とレプリカ2に新しいタスクを割り当てます。
    </li>
</ol>

<Image img={image_7} size="md" alt="動的コーディネーション - パート3" />

<ol className="docs-ordered-list">
    <li>
        すべてのレプリカが自己のタスク処理を完了しました。彼らはさらにタスクをリクエストします。
    </li>
    <li>
        コーディネーターはアナウンスメントを使用して、処理すべき残りのタスクを確認しますが、残っているタスクはありません。
    </li>
    <li>
        コーディネーターはレプリカにすべての処理が完了したことを伝えます。これからマージ可能な状態をすべてマージし、クエリに応答します。
    </li>
</ol>
### キャッシュローカリティの管理 {#managing-cache-locality}

最後に残る潜在的な問題は、キャッシュローカリティをどのように扱うかです。クエリが複数回実行される場合、同じタスクが同じレプリカにルーティングされることをどう確保できますか？前の例では、次のタスクが割り当てられました：

<table>
    <thead>
        <tr>
            <th></th>
            <th>レプリカ1</th>
            <th>レプリカ2</th>
            <th>レプリカ3</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>パート1</td>
            <td>g1, g6, g7</td>
            <td>g2, g4, g5</td>
            <td>g3</td>
        </tr>
        <tr>
            <td>パート2</td>
            <td>g1</td>
            <td>g2, g4, g5</td>
            <td>g3</td>
        </tr>
        <tr>
            <td>パート3</td>
            <td>g1, g6</td>
            <td>g2, g4, g5</td>
            <td>g3</td>
        </tr>
    </tbody>
</table>

同じタスクが同じレプリカに割り当てられ、キャッシュからの利益を得ることを確実にするために、2つのことが行われます。パートとグラニュールのセット（タスク）のハッシュが計算されます。タスク割り当てに対してはレプリカの数のモジュロが適用されます。

理論的にはこれは良さそうですが、実際には、1つのレプリカに突然の負荷がかかると待機遅延が発生したり、ネットワークの劣化が生じたりする可能性があるため、特定のタスクの実行に一貫して同じレプリカが使用されると問題になります。`max_parallel_replicas`がレプリカの数未満の場合、クエリ実行のためにランダムなレプリカが選択されます。
### タスクの奪取 {#task-stealing}

あるレプリカが他のレプリカよりもタスク処理が遅い場合、他のレプリカはそのレプリカに本来属するタスクをハッシュに基づいて「奪取」し、待機遅延を減らすことを試みます。
### 制限事項 {#limitations}

この機能には既知の制限があり、その主要なものはこのセクションに記載されています。

:::note
以下に挙げる制限事項のいずれでもない問題を見つけ、並列レプリカが原因だと疑われる場合は、`comp-parallel-replicas`ラベルを使用してGitHubで問題を報告してください。
:::

| 制限事項                                       | 説明                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
|-----------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 複雑なクエリ                                   | 現在、並列レプリカは比較的シンプルなクエリに対してうまく機能しています。CTE、サブクエリ、JOIN、非フラットクエリなどの複雑さの層はクエリのパフォーマンスに悪影響を及ぼす可能性があります。                                                                                                                                                                                                                                                                                   |
| 小さなクエリ                                   | 多くの行を処理しないクエリを実行する場合、複数のレプリカで実行しても、レプリカ間の調整に必要なネットワーク時間がクエリ実行の追加サイクルを引き起こす可能性があるため、パフォーマンスが向上しないことがあります。`parallel_replicas_min_number_of_rows_per_replica`設定を使用することでこれらの問題を制限できます。  |
| FINALを使用した場合は並列レプリカが無効       |                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| 高い基数データと複雑な集計                  | 多くのデータを送信する必要がある高基数の集計は、クエリを大幅に遅くする可能性があります。                                                                                                                                                                                                                                                                                                                                                                     |
| 新しいアナライザーとの互換性                  | 新しいアナライザーは特定のシナリオでクエリ実行を大幅に遅くしたり速くしたりする可能性があります。                                                                                                                                                                                                                                                                                                                                                                       |
## 並列レプリカに関連する設定 {#settings-related-to-parallel-replicas}

| 設定                                                  | 説明                                                                                                                                                                                                                                                         |
|----------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `enable_parallel_replicas`                         | `0`: 無効<br/> `1`: 有効 <br/>`2`: 並列レプリカの使用を強制します。使用されていない場合、例外がスローされます。                                                                                                                                                |
| `cluster_for_parallel_replicas`                    | 並列レプリケーションに使用するクラスタ名。ClickHouse Cloudを使用している場合は、`default`を使用してください。                                                                                                                                                     |
| `max_parallel_replicas`                            | 複数のレプリカでのクエリ実行に使用する最大レプリカ数。クラスタ内のレプリカの数よりも少ない数字が指定されると、ノードがランダムに選択されます。この値は水平スケーリングに考慮してオーバーコミットすることもできます。                                                                        |
| `parallel_replicas_min_number_of_rows_per_replica` | 処理する必要のある行の数に基づいて使用するレプリカの数を制限するのを助けます。使用されるレプリカの数は次のように定義されます：<br/> `読み取る推定行数` / `各レプリカの最小行数`。  |
| `allow_experimental_analyzer`                      | `0`: 古いアナライザーを使用<br/> `1`: 新しいアナライザーを使用。<br/><br/>並列レプリカの動作は、使用されるアナライザーによって変更される可能性があります。                                                                                                                                                          |
## パラレルレプリカに関する問題の調査 {#investigating-issues-with-parallel-replicas}

各クエリで使用されている設定を確認するには、 
[`system.query_log`](/docs/operations/system-tables/query_log) テーブルをチェックできます。また、 
[`system.events`](/docs/operations/system-tables/events) テーブルを見て、サーバー上で発生したすべてのイベントを確認することができ、 
[`clusterAllReplicas`](/docs/sql-reference/table-functions/cluster) テーブル関数を使用して、すべてのレプリカ上のテーブルを表示できます 
(クラウドユーザーの場合は、`default`を使用します)。

```sql title="クエリ"
SELECT
   hostname(),
   *
FROM clusterAllReplicas('default', system.events)
WHERE event ILIKE '%ParallelReplicas%'
```
<details>
<summary>レスポンス</summary>
```response title="レスポンス"
┌─hostname()───────────────────────┬─event──────────────────────────────────────────┬─value─┬─description──────────────────────────────────────────────────────────────────────────────────────────┐
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasHandleRequestMicroseconds      │   438 │ レプリカからのマークのリクエストを処理するのにかかった時間                                               │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasHandleAnnouncementMicroseconds │   558 │ レプリカの発表を処理するのにかかった時間                                                         │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasReadUnassignedMarks            │   240 │ すべてのレプリカにおける未割り当てマークの合計数                                  │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasReadAssignedForStealingMarks   │     4 │ 一貫性ハッシュされて盗まれるために予定されたマークの合計数                                  │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasStealingByHashMicroseconds     │     5 │ ハッシュによる盗みを意図したセグメントを収集するのにかかった時間                                            │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasProcessingPartsMicroseconds    │     5 │ データパーツを処理するのにかかった時間                                                                     │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasStealingLeftoversMicroseconds  │     3 │ 孤立したセグメントを収集するのにかかった時間                                                              │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasUsedCount                      │     2 │ タスクベースのパラレルレプリカを使用してクエリを実行するために使用されたレプリカの数                         │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasAvailableCount                 │     6 │ タスクベースのパラレルレプリカを使用してクエリを実行するために使用可能なレプリカの数                    │
└──────────────────────────────────┴────────────────────────────────────────────────┴───────┴──────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─hostname()───────────────────────┬─event──────────────────────────────────────────┬─value─┬─description──────────────────────────────────────────────────────────────────────────────────────────┐
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasHandleRequestMicroseconds      │   698 │ レプリカからのマークのリクエストを処理するのにかかった時間                                               │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasHandleAnnouncementMicroseconds │   644 │ レプリカの発表を処理するのにかかった時間                                                         │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasReadUnassignedMarks            │   190 │ すべてのレプリカにおける未割り当てマークの合計数                                  │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasReadAssignedForStealingMarks   │    54 │ 一貫性ハッシュされて盗まれるために予定されたマークの合計数                                  │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasStealingByHashMicroseconds     │     8 │ ハッシュによる盗みを意図したセグメントを収集するのにかかった時間                                            │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasProcessingPartsMicroseconds    │     4 │ データパーツを処理するのにかかった時間                                                                     │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasStealingLeftoversMicroseconds  │     2 │ 孤立したセグメントを収集するのにかかった時間                                                              │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasUsedCount                      │     2 │ タスクベースのパラレルレプリカを使用してクエリを実行するために使用されたレプリカの数                         │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasAvailableCount                 │     6 │ タスクベースのパラレルレプリカを使用してクエリを実行するために使用可能なレプリカの数                    │
└──────────────────────────────────┴────────────────────────────────────────────────┴───────┴──────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─hostname()───────────────────────┬─event──────────────────────────────────────────┬─value─┬─description──────────────────────────────────────────────────────────────────────────────────────────┐
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasHandleRequestMicroseconds      │   620 │ レプリカからのマークのリクエストを処理するのにかかった時間                                               │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasHandleAnnouncementMicroseconds │   656 │ レプリカの発表を処理するのにかかった時間                                                         │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasReadUnassignedMarks            │     1 │ すべてのレプリカにおける未割り当てマークの合計数                                  │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasReadAssignedForStealingMarks   │     1 │ 一貫性ハッシュされて盗まれるために予定されたマークの合計数                                  │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasStealingByHashMicroseconds     │     4 │ ハッシュによる盗みを意図したセグメントを収集するのにかかった時間                                            │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasProcessingPartsMicroseconds    │     3 │ データパーツを処理するのにかかった時間                                                                     │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasStealingLeftoversMicroseconds  │     1 │ 孤立したセグメントを収集するのにかかった時間                                                              │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasUsedCount                      │     2 │ タスクベースのパラレルレプリカを使用してクエリを実行するために使用されたレプリカの数                         │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasAvailableCount                 │    12 │ タスクベースのパラレルレプリカを使用してクエリを実行するために使用可能なレプリカの数                    │
└──────────────────────────────────┴────────────────────────────────────────────────┴───────┴──────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─hostname()───────────────────────┬─event──────────────────────────────────────────┬─value─┬─description──────────────────────────────────────────────────────────────────────────────────────────┐
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasHandleRequestMicroseconds      │   696 │ レプリカからのマークのリクエストを処理するのにかかった時間                                               │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasHandleAnnouncementMicroseconds │   717 │ レプリカの発表を処理するのにかかった時間                                                         │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasReadUnassignedMarks            │     2 │ すべてのレプリカにおける未割り当てマークの合計数                                  │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasReadAssignedForStealingMarks   │     2 │ 一貫性ハッシュされて盗まれるために予定されたマークの合計数                                  │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasStealingByHashMicroseconds     │    10 │ ハッシュによる盗みを意図したセグメントを収集するのにかかった時間                                            │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasProcessingPartsMicroseconds    │     6 │ データパーツを処理するのにかかった時間                                                                     │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasStealingLeftoversMicroseconds  │     2 │ 孤立したセグメントを収集するのにかかった時間                                                              │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasUsedCount                      │     2 │ タスクベースのパラレルレプリカを使用してクエリを実行するために使用されたレプリカの数                         │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasAvailableCount                 │    12 │ タスクベースのパラレルレプリカを使用してクエリを実行するために使用可能なレプリカの数                    │
└──────────────────────────────────┴────────────────────────────────────────────────┴───────┴──────────────────────────────────────────────────────────────────────────────────────────────────────┘
```
</details>

また、[`system.text_log`](/docs/operations/system-tables/text_log) テーブルには、パラレルレプリカを使用したクエリの実行に関する情報も含まれています：

```sql title="クエリ"
SELECT message
FROM clusterAllReplicas('default', system.text_log)
WHERE query_id = 'ad40c712-d25d-45c4-b1a1-a28ba8d4019c'
ORDER BY event_time_microseconds ASC
```

<details>
<summary>レスポンス</summary>
```response title="レスポンス"
┌─message────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ (from 54.218.178.249:59198) SELECT * FROM session_events WHERE type='type2' LIMIT 10 SETTINGS allow_experimental_parallel_reading_from_replicas=2; (stage: Complete)                                                                                       │
│ クエリ SELECT __table1.clientId AS clientId, __table1.sessionId AS sessionId, __table1.pageId AS pageId, __table1.timestamp AS timestamp, __table1.type AS type FROM default.session_events AS __table1 WHERE __table1.type = 'type2' LIMIT _CAST(10, 'UInt64') SETTINGS allow_experimental_parallel_reading_from_replicas = 2 to stage Complete │
│ アクセス許可: SELECT(clientId, sessionId, pageId, timestamp, type) ON default.session_events                                                                                                                                                             │
│ クエリ SELECT __table1.clientId AS clientId, __table1.sessionId AS sessionId, __table1.pageId AS pageId, __table1.timestamp AS timestamp, __table1.type AS type FROM default.session_events AS __table1 WHERE __table1.type = 'type2' LIMIT _CAST(10, 'UInt64') to stage WithMergeableState only analyze │
│ アクセス許可: SELECT(clientId, sessionId, pageId, timestamp, type) ON default.session_events                                                                                                                                                             │
│ クエリ SELECT __table1.clientId AS clientId, __table1.sessionId AS sessionId, __table1.pageId AS pageId, __table1.timestamp AS timestamp, __table1.type AS type FROM default.session_events AS __table1 WHERE __table1.type = 'type2' LIMIT _CAST(10, 'UInt64') from stage FetchColumns to stage WithMergeableState only analyze │
│ クエリ SELECT __table1.clientId AS clientId, __table1.sessionId AS sessionId, __table1.pageId AS pageId, __table1.timestamp AS timestamp, __table1.type AS type FROM default.session_events AS __table1 WHERE __table1.type = 'type2' LIMIT _CAST(10, 'UInt64') SETTINGS allow_experimental_parallel_reading_from_replicas = 2 to stage WithMergeableState only analyze │
│ アクセス許可: SELECT(clientId, sessionId, pageId, timestamp, type) ON default.session_events                                                                                                                                                             │
│ クエリ SELECT __table1.clientId AS clientId, __table1.sessionId AS sessionId, __table1.pageId AS pageId, __table1.timestamp AS timestamp, __table1.type AS type FROM default.session_events AS __table1 WHERE __table1.type = 'type2' LIMIT _CAST(10, 'UInt64') SETTINGS allow_experimental_parallel_reading_from_replicas = 2 from stage FetchColumns to stage WithMergeableState only analyze │
│ クエリ SELECT __table1.clientId AS clientId, __table1.sessionId AS sessionId, __table1.pageId AS pageId, __table1.timestamp AS timestamp, __table1.type AS type FROM default.session_events AS __table1 WHERE __table1.type = 'type2' LIMIT _CAST(10, 'UInt64') SETTINGS allow_experimental_parallel_reading_from_replicas = 2 from stage WithMergeableState to stage Complete │
│ リクエストされたレプリカの数（100）は、クラスタ内で使用可能な実際の数（6）を超えています。クエリの実行には後者の数を使用します。                                                                                                       │
│ レプリカ4からの初期リクエスト: 2 パーツ: [part all_0_2_1 with ranges [(0, 182)], part all_3_3_0 with ranges [(0, 62)]]----------
レプリカ4から受信                                                                                                   │
│ 読み取る状態が完全に初期化されました: part all_0_2_1 with ranges [(0, 182)] in replicas [4]; part all_3_3_0 with ranges [(0, 62)] in replicas [4]                                                                                                            │
│ 初期リクエストが送信されました: 1 レプリカ数: 6                                                                                                                                                                                                                 │
│ レプリカ2からの初期リクエスト: 2 パーツ: [part all_0_2_1 with ranges [(0, 182)], part all_3_3_0 with ranges [(0, 62)]]----------
レプリカ2から受信                                                                                                   │
│ 初期リクエストが送信されました: 2 レプリカ数: 6                                                                                                                                                                                                                 │
│ レプリカ4からのリクエスト処理中、最小マークサイズは240です                                                                                                                                                                                                 │
│ レプリカ4に次のパーツを返答します: [part all_0_2_1 with ranges [(128, 182)]]. 終了: false; mine_marks=0, stolen_by_hash=54, stolen_rest=0                                                                                                       │
│ レプリカ1からの初期リクエスト: 2 パーツ: [part all_0_2_1 with ranges [(0, 182)], part all_3_3_0 with ranges [(0, 62)]]----------
レプリカ1から受信                                                                                                   │
│ 初期リクエストが送信されました: 3 レプリカ数: 6                                                                                                                                                                                                                 │
│ レプリカ4からのリクエスト処理中、最小マークサイズは240です                                                                                                                                                                                                 │
│ レプリカ4に次の2パーツを返答します: [part all_0_2_1 with ranges [(0, 128)], part all_3_3_0 with ranges [(0, 62)]]. 終了: false; mine_marks=0, stolen_by_hash=0, stolen_rest=190                                                                  │
│ レプリカ0からの初期リクエスト: 2 パーツ: [part all_0_2_1 with ranges [(0, 182)], part all_3_3_0 with ranges [(0, 62)]]----------
レプリカ0から受信                                                                                                   │
│ 初期リクエストが送信されました: 4 レプリカ数: 6                                                                                                                                                                                                                 │
│ レプリカ5からの初期リクエスト: 2 パーツ: [part all_0_2_1 with ranges [(0, 182)], part all_3_3_0 with ranges [(0, 62)]]----------
レプリカ5から受信                                                                                                   │
│ 初期リクエストが送信されました: 5 レプリカ数: 6                                                                                                                                                                                                                 │
│ レプリカ2からのリクエスト処理中、最小マークサイズは240です                                                                                                                                                                                                 │
│ レプリカ2に部分数: [] を返答します: []. 終了: true; mine_marks=0, stolen_by_hash=0, stolen_rest=0                                                                                                                                                │
│ レプリカ3からの初期リクエスト: 2 パーツ: [part all_0_2_1 with ranges [(0, 182)], part all_3_3_0 with ranges [(0, 62)]]----------
レプリカ3から受信                                                                                                   │
│ 初期リクエストが送信されました: 6 レプリカ数: 6                                                                                                                                                                                                                 │
│ 読み取る行の合計: 2000000                                                                                                                                                                                                                                │
│ レプリカ5からのリクエスト処理中、最小マークサイズは240です                                                                                                                                                                                                 │
│ レプリカ5に部分数: []. 終了: true; mine_marks=0, stolen_by_hash=0, stolen_rest=0                                                                                                                                                │
│ レプリカ0からのリクエスト処理中、最小マークサイズは240です                                                                                                                                                                                                 │
│ レプリカ0に部分数: []. 終了: true; mine_marks=0, stolen_by_hash=0, stolen_rest=0                                                                                                                                                │
│ レプリカ1からのリクエスト処理中、最小マークサイズは240です                                                                                                                                                                                                 │
│ レプリカ1に部分数: []. 終了: true; mine_marks=0, stolen_by_hash=0, stolen_rest=0                                                                                                                                                │
│ レプリカ3からのリクエスト処理中、最小マークサイズは240です                                                                                                                                                                                                 │
│ レプリカ3に部分数: []. 終了: true; mine_marks=0, stolen_by_hash=0, stolen_rest=0                                                                                                                                                │
│ (c-crimson-vd-86-server-rdhnsx3-0.c-crimson-vd-86-server-headless.ns-crimson-vd-86.svc.cluster.local:9000) データが十分に読み取られたためクエリをキャンセルします。                                                                                              │
│ 81920 行を読み取った、5.16 MiBを0.013166秒で、6222087.194288318 行/秒、391.63 MiB/秒。                                                                                                                                                                   │
│ 調整完了: 統計: レプリカ0 - {リクエスト: 2 マーク: 0 assigned_to_me: 0 stolen_by_hash: 0 stolen_unassigned: 0}; レプリカ1 - {リクエスト: 2 マーク: 0 assigned_to_me: 0 stolen_by_hash: 0 stolen_unassigned: 0}; レプリカ2 - {リクエスト: 2 マーク: 0 assigned_to_me: 0 stolen_by_hash: 0 stolen_unassigned: 0}; レプリカ3 - {リクエスト: 2 マーク: 0 assigned_to_me: 0 stolen_by_hash: 0 stolen_unassigned: 0}; レプリカ4 - {リクエスト: 3 マーク: 244 assigned_to_me: 0 stolen_by_hash: 54 stolen_unassigned: 190}; レプリカ5 - {リクエスト: 2 マーク: 0 assigned_to_me: 0 stolen_by_hash: 0 stolen_unassigned: 0} │
│ クエリに対するメモリ使用量のピーク: 1.81 MiB。                                                                                                                                                                                                                   │
│ 0.024095586秒で処理されました。                                                                                                                                                                                                                              │
└────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
```
</details>

最後に、`EXPLAIN PIPELINE`も使用できます。これにより、ClickHouseがクエリを実行する方法や、クエリの実行に使用されるリソースが強調表示されます。以下のクエリを例として見てみましょう：

```sql
SELECT count(), uniq(pageId) , min(timestamp), max(timestamp) 
FROM session_events 
WHERE type='type3' 
GROUP BY toYear(timestamp) LIMIT 10
```

パラレルレプリカなしでのクエリパイプラインを見てみましょう：

```sql title="EXPLAIN PIPELINE (パラレルレプリカなし)"
EXPLAIN PIPELINE graph = 1, compact = 0 
SELECT count(), uniq(pageId) , min(timestamp), max(timestamp) 
FROM session_events 
WHERE type='type3' 
GROUP BY toYear(timestamp) 
LIMIT 10 
SETTINGS allow_experimental_parallel_reading_from_replicas=0 
FORMAT TSV;
```

<Image img={image_8} size="lg" alt="EXPLAIN without parallel_replica" />

次に、パラレルレプリカを使用した場合は：

```sql title="EXPLAIN PIPELINE (パラレルレプリカあり)"
EXPLAIN PIPELINE graph = 1, compact = 0 
SELECT count(), uniq(pageId) , min(timestamp), max(timestamp) 
FROM session_events 
WHERE type='type3' 
GROUP BY toYear(timestamp) 
LIMIT 10 
SETTINGS allow_experimental_parallel_reading_from_replicas=2 
FORMAT TSV;
```

<Image img={image_9} size="lg" alt="EXPLAIN with parallel_replica"/>
