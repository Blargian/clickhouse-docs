---
slug: /deployment-guides/parallel-replicas
title: 'Параллельные реплики'
keywords: ['parallel replica']
description: 'В этом руководстве мы сначала обсудим, как ClickHouse распределяет запрос по нескольким шардам через распределённые таблицы, а затем как запрос может использовать несколько реплик для выполнения.'
---

import Image from '@theme/IdealImage';
import BetaBadge from '@theme/badges/BetaBadge';
import image_1 from '@site/static/images/deployment-guides/parallel-replicas-1.png'
import image_2 from '@site/static/images/deployment-guides/parallel-replicas-2.png'
import image_3 from '@site/static/images/deployment-guides/parallel-replicas-3.png'
import image_4 from '@site/static/images/deployment-guides/parallel-replicas-4.png'
import image_5 from '@site/static/images/deployment-guides/parallel-replicas-5.png'
import image_6 from '@site/static/images/deployment-guides/parallel-replicas-6.png'
import image_7 from '@site/static/images/deployment-guides/parallel-replicas-7.png'
import image_8 from '@site/static/images/deployment-guides/parallel-replicas-8.png'
import image_9 from '@site/static/images/deployment-guides/parallel-replicas-9.png'

<BetaBadge/>
## Введение {#introduction}

ClickHouse обрабатывает запросы крайне быстро, но как эти запросы 
распределяются и параллелизируются по нескольким серверам? 

> В этом руководстве мы сначала обсудим, как ClickHouse распределяет запрос по
нескольким шардам через распределённые таблицы, а затем как запрос может использовать 
несколько реплик для выполнения.
## Шардированная архитектура {#sharded-architecture}

В архитектуре shared-nothing кластеры обычно разделяются на
несколько шардов, каждый из которых содержит подмножество общих данных. 
Распределённая таблица находится поверх этих шардов, предоставляя единое представление 
всех данных.

Запросы могут быть отправлены в локальную таблицу. Выполнение запроса будет
происходить только на указанном шарде или он может быть отправлен в распределённую таблицу,
и в этом случае каждый шард выполнит данные запросы. Сервер, на котором выполнялся запрос к
распределённой таблице, агрегирует данные и отвечает клиенту:

<Image img={image_1} size="md" alt="Шардированная архитектура" />

В приведённой выше схеме показано, что происходит, когда клиент выполняет запрос к распределённой таблице:

<ol className="docs-ordered-list">
    <li>
        Select-запрос отправляется в распределённую таблицу на узле случайным
        образом (через метод round-robin или после маршрутизации на определённый сервер
        балансировщиком нагрузки). Этот узел теперь будет действовать как координатор.
    </li>
    <li>
        Узел определяет каждый шард, который должен выполнить запрос, 
        через информацию, указанную распределённой таблицей, и запрос
        отправляется на каждый шард.
    </li>
    <li>
        Каждый шард локально читает, фильтрует и агрегирует данные, а затем
        отправляет состояние, которое можно объединить, обратно координатору.
    </li>
    <li>
        Координирующий узел объединяет данные и затем отправляет ответ
        клиенту.
    </li>
</ol>

Когда мы добавляем реплики в смесь, процесс довольно схож, с той лишь разницей, что только
одна реплика из каждого шарда выполнит запрос.
Это означает, что большее количество запросов может быть обработано параллельно.
## Нешардированная архитектура {#non-sharded-architecture}

ClickHouse Cloud имеет совершенно другую архитектуру по сравнению с представленной выше.
(См. ["Архитектуру ClickHouse Cloud"](https://clickhouse.com/docs/cloud/reference/architecture)
для получения дополнительной информации). С разделением вычислений и хранения, а также с практически
бесконечным объёмом хранения, потребность в шардах становится менее значительной.

На рисунке ниже показана архитектура ClickHouse Cloud:

<Image img={image_2} size="md" alt="Нешардированная архитектура" />

Эта архитектура позволяет нам добавлять и удалять реплики практически
мгновенно, обеспечивая очень высокую масштабируемость кластера. 
Кластер ClickHouse Keeper (показан справа) обеспечивает единый источник истинных
метаданных. Реплики могут получать метаданные из кластера ClickHouse Keeper
и все поддерживают одинаковые данные. Сами данные хранятся в
объектном хранилище, и кеш SSD позволяет ускорить запросы.

Но как теперь можно распределять выполнение запросов по нескольким серверам? В 
шардированной архитектуре это было довольно очевидно, учитывая, что каждый шард мог фактически
выполнять запрос на подмножестве данных. Как это работает, если шардирование отсутствует?
## Введение параллельных реплик {#introducing-parallel-replicas}

Чтобы параллелизировать выполнение запросов через несколько серверов, сначала нам 
нужно назначить один из наших серверов координатором. Координатор — это тот,
кто создаёт список задач, которые нужно выполнить, обеспечивает их выполнение, 
агрегацию и возвращение результата клиенту. Как и во многих распределённых системах,
это будет роль узла, который получает изначальный запрос. Нам также нужно определить
единицу работы. В шардированной архитектуре единица работы — это шард, подмножество данных. 
С параллельными репликами мы будем использовать небольшую часть таблицы, называемую 
[гранулы](/docs/guides/best-practices/sparse-primary-indexes#data-is-organized-into-granules-for-parallel-data-processing),
в качестве единицы работы.

Теперь давайте посмотрим, как это работает на практике, с помощью рисунка ниже:

<Image img={image_3} size="md" alt="Параллельные реплики" />

С параллельными репликами:

<ol className="docs-ordered-list">
    <li>
        Запрос от клиента отправляется на один узел после прохождения через балансировщик нагрузки. 
        Этот узел становится координатором для этого запроса.
    </li>
    <li>
        Узел анализирует индекс каждой части и выбирает правильные части и гранулы для обработки.
    </li>
    <li>
        Координатор разделяет рабочую нагрузку на набор гранул, которые могут быть назначены различным репликам.
    </li>
    <li>
        Каждый набор гранул обрабатывается соответствующими репликами, и 
        в случае завершения обработки координатору отправляется состояние, которое можно объединить.
    </li>
    <li>
        Наконец, координатор объединяет все результаты от реплик и
        затем возвращает ответ клиенту.
    </li>
</ol>

Вышеописанные шаги описывают, как работают параллельные реплики в теории.
Однако на практике есть множество факторов, которые могут помешать данной логике
работать идеально:

<ol className="docs-ordered-list">
    <li>
        Некоторые реплики могут быть недоступны.
    </li>
    <li>
        Репликация в ClickHouse является асинхронной, на момент времени у некоторых реплик
        могут отсутствовать одинаковые части.
    </li>
    <li>
        "Хвостовая" задержка между репликами требует какого-то решения.
    </li>
    <li>
        Кэш файловой системы различается от реплики к реплике в зависимости от 
        активности на каждой реплике, что означает, что случайное назначение задач может 
        привести к менее оптимальной производительности с учетом локальности кэша.
    </li>
</ol>

Мы исследуем, как эти факторы преодолеваются в следующих разделах.
### Объявления {#announcements}

Для решения вопросов (1) и (2) из приведённого выше списка мы ввели концепцию
объявления. Давайте посмотрим, как это работает, используя рисунок ниже:

<Image img={image_4} size="md" alt="Объявления" />

<ol className="docs-ordered-list">
    <li>
        Запрос от клиента отправляется на один узел после прохождения через балансировщик нагрузки. Узел становится координатором для этого запроса.
    </li>
    <li>
        Координирующий узел отправляет запрос, чтобы получить объявления от
        всех реплик в кластере. У реплик могут быть немного разные
        представления текущего набора частей для таблицы. В результате нам необходимо
        собирать эту информацию, чтобы избежать некорректных решений о планировании.
    </li>
    <li>
        Координирующий узел затем использует объявления для определения набора
        гранул, которые могут быть назначены различным репликам. Здесь, к примеру,
        видно, что ни одна из гранул части 3 не была назначена реплике 2
        потому что эта реплика не предоставила эту часть в своём объявлении.
        Также обратите внимание, что задачи не были назначены реплике 3 потому что 
        реплика не предоставила объявление.
    </li>
    <li>
        После того, как каждая реплика обработала запрос на своём подмножестве гранул
        и состояние, которое можно объединить, было отправлено обратно координирующему узлу,
        координатор объединяет результаты и ответ отправляется клиенту.
    </li>
</ol>
### Динамическая координация {#dynamic-coordination}

Для решения проблемы хвостовой задержки мы добавили динамическую координацию. Это означает,
что все гранулы не отправляются реплике в одном запросе, но каждая реплика
может запросить новую задачу (набор гранул для обработки) у
координирующего узла. Координатор предоставит реплике набор гранул, основываясь
на полученном объявлении.

Предположим, что мы находимся на этапе процесса, когда все реплики отправили
объявление со всеми частями.

Рисунок ниже визуализирует, как работает динамическая координация:

<Image img={image_5} size="md" alt="Динамическая координация - часть 1" />

<ol className="docs-ordered-list">
    <li>
        Реплики сообщают координирующему узлу, что они могут обрабатывать
        задачи, они также могут указать, сколько работы они могут выполнить.
    </li>
    <li>
        Координатор назначает задачи репликам.
    </li>
</ol>

<Image img={image_6} size="md" alt="Динамическая координация - часть 2" />

<ol className="docs-ordered-list">
    <li>
        Реплика 1 и 2 завершают свои задачи очень быстро. Они
        запросят другую задачу у координирующего узла.
    </li>
    <li>
        Координатор назначает новые задачи репликам 1 и 2.
    </li>
</ol>

<Image img={image_7} size="md" alt="Динамическая координация - часть 3" />

<ol className="docs-ordered-list">
    <li>
        Все реплики завершили обработку своих задач. Они
        запрашивают больше задач.
    </li>
    <li>
        Координатор, используя объявления, проверяет, какие задачи остаются 
        для обработки, но никаких оставшихся задач нет.
    </li>
    <li>
        Координатор даёт знать репликам, что всё было обработано.
        Теперь он объединит все состояния, которые можно объединить, и ответит на запрос.
    </li>
</ol>
### Управление локальностью кеша {#managing-cache-locality}

Последняя оставшаяся потенциальная проблема — это как мы управляем локальностью кеша. Если запрос
выполняется несколько раз, как мы можем обеспечить, чтобы одна и та же задача направлялась на
ту же реплику? В предыдущем примере у нас были назначены следующие задачи:

<table>
    <thead>
        <tr>
            <th></th>
            <th>Реплика 1</th>
            <th>Реплика 2</th>
            <th>Реплика 3</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Часть 1</td>
            <td>g1, g6, g7</td>
            <td>g2, g4, g5</td>
            <td>g3</td>
        </tr>
        <tr>
            <td>Часть 2</td>
            <td>g1</td>
            <td>g2, g4, g5</td>
            <td>g3</td>
        </tr>
        <tr>
            <td>Часть 3</td>
            <td>g1, g6</td>
            <td>g2, g4, g5</td>
            <td>g3</td>
        </tr>
    </tbody>
</table>

Чтобы гарантировать, что те же задачи назначаются тем же репликам и могут
использовать преимущества кеша, происходит два действия. Рассчитывается хеш от части + набора 
гранул (задача). Применяется модуль по количеству реплик для назначения задачи.

На бумаге это звучит хорошо, но на практике, внезапная нагрузка на одну реплику или 
ухудшение работы сети, могут привести к "хвостовой" задержке, если одна и та же реплика 
постоянно используется для выполнения определённых задач. Если `max_parallel_replicas` меньше 
чем количество реплик, то для выполнения запроса выбираются случайные реплики.
### Кража задач {#task-stealing}

если какая-то реплика обрабатывает задачи медленнее, чем другие, другие реплики попытаются
"украсть" задачи, которые, в принципе, принадлежат этой реплике по хешу, чтобы уменьшить 
"хвостовую" задержку.
### Ограничения {#limitations}

Эта функция имеет известные ограничения, о которых основные из них задокументированы в
этом разделе.

:::note
Если вы нашли проблему, которая не относится к приведённым ниже ограничениям, и
подозреваете, что причиной может быть параллельная реплика, пожалуйста, откройте тикет на GitHub, используя 
метку `comp-parallel-replicas`.
:::

| Ограничение                                      | Описание                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
|--------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Сложные запросы                                  | В настоящее время параллельная реплика работает довольно хорошо для простых запросов. Сложные слои, такие как CTE, подзапросы, JOIN, неструктурированное выполнение запроса и т.д., могут негативно сказаться на производительности запроса.                                                                                                                                                                                                                                                               |
| Небольшие запросы                                | Если вы выполняете запрос, который не обрабатывает много строк, выполнение его на нескольких репликах может не привести к лучшему времени выполнения, учитывая, что сетевое время для координации между репликами может привести к дополнительным временным интервалам выполнения запроса. Вы можете ограничить эти проблемы, используя настройку: [`parallel_replicas_min_number_of_rows_per_replica`](/docs/operations/settings/settings#parallel_replicas_min_number_of_rows_per_replica).  |
| Параллельные реплики отключены с FINAL                     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| Высокая кардинальность данных и сложная агрегация | Высокая кардинальность агрегации, которая нуждается в отправке большого объема данных, может значительно замедлить ваши запросы.                                                                                                                                                                                                                                                                                                                                                                     |
| Совместимость с новым анализатором              | Новый анализатор может значительно замедлить или ускорить выполнение запросов в определённых сценариях.                                                                                                                                                                                                                                                                                                                                                                       |

## Параметры, связанные с параллельными репликами {#settings-related-to-parallel-replicas}

| Настройка                                          | Описание                                                                                                                                                                                                                                                         |
|----------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `enable_parallel_replicas`                         | `0`: отключено<br/> `1`: включено <br/>`2`: Форсировать использование параллельной реплики, вызовет исключение, если не используется.                                                                                                                                                          |
| `cluster_for_parallel_replicas`                    | Имя кластера, используемое для параллельной репликации; если вы используете ClickHouse Cloud, используйте `default`.                                                                                                                                                                 |
| `max_parallel_replicas`                            | Максимальное количество реплик, используемых для выполнения запроса на нескольких репликах, если указано количество, меньшее чем количество реплик в кластере, узлы будут выбраны случайным образом. Это значение также может быть превышено для учёта горизонтального масштабирования. |
| `parallel_replicas_min_number_of_rows_per_replica` | Помогает ограничить количество реплик, используемых на основе количества строк, которые нужно обработать. Количество реплик определяется: <br/> `оценочное количество строк для чтения` / `min_number_of_rows_per_replica`.                                                               |
| `allow_experimental_analyzer`                      | `0`: использовать старый анализатор<br/> `1`: использовать новый анализатор. <br/><br/>Поведение параллельных реплик может измениться в зависимости от используемого анализатора.                                                                                                                                    |
## Исследование проблем с параллельными репликами {#investigating-issues-with-parallel-replicas}

Вы можете проверить, какие настройки используются для каждого запроса в таблице [`system.query_log`](/docs/operations/system-tables/query_log). Вы также можете взглянуть на таблицу [`system.events`](/docs/operations/system-tables/events), чтобы увидеть все события, которые произошли на сервере, и вы можете использовать табличную функцию [`clusterAllReplicas`](/docs/sql-reference/table-functions/cluster), чтобы увидеть таблицы на всех репликах (если вы облачный пользователь, используйте `default`).

```sql title="Запрос"
SELECT
   hostname(),
   *
FROM clusterAllReplicas('default', system.events)
WHERE event ILIKE '%ParallelReplicas%'
```
<details>
<summary>Ответ</summary>
```response title="Ответ"
┌─hostname()───────────────────────┬─event──────────────────────────────────────────┬─value─┬─description──────────────────────────────────────────────────────────────────────────────────────────┐
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasHandleRequestMicroseconds      │   438 │ Time spent processing requests for marks from replicas                                               │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasHandleAnnouncementMicroseconds │   558 │ Time spent processing replicas announcements                                                         │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasReadUnassignedMarks            │   240 │ Sum across all replicas of how many unassigned marks were scheduled                                  │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasReadAssignedForStealingMarks   │     4 │ Sum across all replicas of how many of scheduled marks were assigned for stealing by consistent hash │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasStealingByHashMicroseconds     │     5 │ Time spent collecting segments meant for stealing by hash                                            │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasProcessingPartsMicroseconds    │     5 │ Time spent processing data parts                                                                     │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasStealingLeftoversMicroseconds  │     3 │ Time spent collecting orphaned segments                                                              │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasUsedCount                      │     2 │ Number of replicas used to execute a query with task-based parallel replicas                         │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasAvailableCount                 │     6 │ Number of replicas available to execute a query with task-based parallel replicas                    │
└──────────────────────────────────┴────────────────────────────────────────────────┴───────┴──────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─hostname()───────────────────────┬─event──────────────────────────────────────────┬─value─┬─description──────────────────────────────────────────────────────────────────────────────────────────┐
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasHandleRequestMicroseconds      │   698 │ Time spent processing requests for marks from replicas                                               │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasHandleAnnouncementMicroseconds │   644 │ Time spent processing replicas announcements                                                         │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasReadUnassignedMarks            │   190 │ Sum across all replicas of how many unassigned marks were scheduled                                  │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasReadAssignedForStealingMarks   │    54 │ Sum across all replicas of how many of scheduled marks were assigned for stealing by consistent hash │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasStealingByHashMicroseconds     │     8 │ Time spent collecting segments meant for stealing by hash                                            │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasProcessingPartsMicroseconds    │     4 │ Time spent processing data parts                                                                     │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasStealingLeftoversMicroseconds  │     2 │ Time spent collecting orphaned segments                                                              │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasUsedCount                      │     2 │ Number of replicas used to execute a query with task-based parallel replicas                         │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasAvailableCount                 │     6 │ Number of replicas available to execute a query with task-based parallel replicas                    │
└──────────────────────────────────┴────────────────────────────────────────────────┴───────┴──────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─hostname()───────────────────────┬─event──────────────────────────────────────────┬─value─┬─description──────────────────────────────────────────────────────────────────────────────────────────┐
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasHandleRequestMicroseconds      │   620 │ Time spent processing requests for marks from replicas                                               │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasHandleAnnouncementMicroseconds │   656 │ Time spent processing replicas announcements                                                         │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasReadUnassignedMarks            │     1 │ Sum across all replicas of how many unassigned marks were scheduled                                  │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasReadAssignedForStealingMarks   │     1 │ Sum across all replicas of how many of scheduled marks were assigned for stealing by consistent hash │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasStealingByHashMicroseconds     │     4 │ Time spent collecting segments meant for stealing by hash                                            │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasProcessingPartsMicroseconds    │     3 │ Time spent processing data parts                                                                     │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasStealingLeftoversMicroseconds  │     1 │ Time spent collecting orphaned segments                                                              │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasUsedCount                      │     2 │ Number of replicas used to execute a query with task-based parallel replicas                         │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasAvailableCount                 │    12 │ Number of replicas available to execute a query with task-based parallel replicas                    │
└──────────────────────────────────┴────────────────────────────────────────────────┴───────┴──────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─hostname()───────────────────────┬─event──────────────────────────────────────────┬─value─┬─description──────────────────────────────────────────────────────────────────────────────────────────┐
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasHandleRequestMicroseconds      │   696 │ Time spent processing requests for marks from replicas                                               │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasHandleAnnouncementMicroseconds │   717 │ Time spent processing replicas announcements                                                         │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasReadUnassignedMarks            │     2 │ Sum across all replicas of how many unassigned marks were scheduled                                  │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasReadAssignedForStealingMarks   │     2 │ Sum across all replicas of how many of scheduled marks were assigned for stealing by consistent hash │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasStealingByHashMicroseconds     │    10 │ Time spent collecting segments meant for stealing by hash                                            │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasProcessingPartsMicroseconds    │     6 │ Time spent processing data parts                                                                     │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasStealingLeftoversMicroseconds  │     2 │ Time spent collecting orphaned segments                                                              │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasUsedCount                      │     2 │ Number of replicas used to execute a query with task-based parallel replicas                         │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasAvailableCount                 │    12 │ Number of replicas available to execute a query with task-based parallel replicas                    │
└──────────────────────────────────┴────────────────────────────────────────────────┴───────┴──────────────────────────────────────────────────────────────────────────────────────────────────────┘
```
</details>

Таблица [`system.text_log`](/docs/operations/system-tables/text_log) также содержит информацию о выполнении запросов с использованием параллельных реплик:

```sql title="Запрос"
SELECT message
FROM clusterAllReplicas('default', system.text_log)
WHERE query_id = 'ad40c712-d25d-45c4-b1a1-a28ba8d4019c'
ORDER BY event_time_microseconds ASC
```

<details>
<summary>Ответ</summary>
```response title="Ответ"
┌─message────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ (from 54.218.178.249:59198) SELECT * FROM session_events WHERE type='type2' LIMIT 10 SETTINGS allow_experimental_parallel_reading_from_replicas=2; (stage: Complete)                                                                                       │
│ Query SELECT __table1.clientId AS clientId, __table1.sessionId AS sessionId, __table1.pageId AS pageId, __table1.timestamp AS timestamp, __table1.type AS type FROM default.session_events AS __table1 WHERE __table1.type = 'type2' LIMIT _CAST(10, 'UInt64') SETTINGS allow_experimental_parallel_reading_from_replicas = 2 to stage Complete │
│ Access granted: SELECT(clientId, sessionId, pageId, timestamp, type) ON default.session_events                                                                                                                                                             │
│ Query SELECT __table1.clientId AS clientId, __table1.sessionId AS sessionId, __table1.pageId AS pageId, __table1.timestamp AS timestamp, __table1.type AS type FROM default.session_events AS __table1 WHERE __table1.type = 'type2' LIMIT _CAST(10, 'UInt64') to stage WithMergeableState only analyze │
│ Access granted: SELECT(clientId, sessionId, pageId, timestamp, type) ON default.session_events                                                                                                                                                             │
│ Query SELECT __table1.clientId AS clientId, __table1.sessionId AS sessionId, __table1.pageId AS pageId, __table1.timestamp AS timestamp, __table1.type AS type FROM default.session_events AS __table1 WHERE __table1.type = 'type2' LIMIT _CAST(10, 'UInt64') from stage FetchColumns to stage WithMergeableState only analyze │
│ Query SELECT __table1.clientId AS clientId, __table1.sessionId AS sessionId, __table1.pageId AS pageId, __table1.timestamp AS timestamp, __table1.type AS type FROM default.session_events AS __table1 WHERE __table1.type = 'type2' LIMIT _CAST(10, 'UInt64') SETTINGS allow_experimental_parallel_reading_from_replicas = 2 to stage WithMergeableState only analyze │
│ Access granted: SELECT(clientId, sessionId, pageId, timestamp, type) ON default.session_events                                                                                                                                                             │
│ Query SELECT __table1.clientId AS clientId, __table1.sessionId AS sessionId, __table1.pageId AS pageId, __table1.timestamp AS timestamp, __table1.type AS type FROM default.session_events AS __table1 WHERE __table1.type = 'type2' LIMIT _CAST(10, 'UInt64') SETTINGS allow_experimental_parallel_reading_from_replicas = 2 from stage FetchColumns to stage WithMergeableState only analyze │
│ Query SELECT __table1.clientId AS clientId, __table1.sessionId AS sessionId, __table1.pageId AS pageId, __table1.timestamp AS timestamp, __table1.type AS type FROM default.session_events AS __table1 WHERE __table1.type = 'type2' LIMIT _CAST(10, 'UInt64') SETTINGS allow_experimental_parallel_reading_from_replicas = 2 from stage WithMergeableState to stage Complete │
│ The number of replicas requested (100) is bigger than the real number available in the cluster (6). Will use the latter number to execute the query.                                                                                                       │
│ Initial request from replica 4: 2 parts: [part all_0_2_1 with ranges [(0, 182)], part all_3_3_0 with ranges [(0, 62)]]----------
Received from 4 replica
                                                                                                   │
│ Reading state is fully initialized: part all_0_2_1 with ranges [(0, 182)] in replicas [4]; part all_3_3_0 with ranges [(0, 62)] in replicas [4]                                                                                                            │
│ Sent initial requests: 1 Replicas count: 6                                                                                                                                                                                                                 │
│ Initial request from replica 2: 2 parts: [part all_0_2_1 with ranges [(0, 182)], part all_3_3_0 with ranges [(0, 62)]]----------
Received from 2 replica
                                                                                                   │
│ Sent initial requests: 2 Replicas count: 6                                                                                                                                                                                                                 │
│ Handling request from replica 4, minimal marks size is 240                                                                                                                                                                                                 │
│ Going to respond to replica 4 with 1 parts: [part all_0_2_1 with ranges [(128, 182)]]. Finish: false; mine_marks=0, stolen_by_hash=54, stolen_rest=0                                                                                                       │
│ Initial request from replica 1: 2 parts: [part all_0_2_1 with ranges [(0, 182)], part all_3_3_0 with ranges [(0, 62)]]----------
Received from 1 replica
                                                                                                   │
│ Sent initial requests: 3 Replicas count: 6                                                                                                                                                                                                                 │
│ Handling request from replica 4, minimal marks size is 240                                                                                                                                                                                                 │
│ Going to respond to replica 4 with 2 parts: [part all_0_2_1 with ranges [(0, 128)], part all_3_3_0 with ranges [(0, 62)]]. Finish: false; mine_marks=0, stolen_by_hash=0, stolen_rest=190                                                                  │
│ Initial request from replica 0: 2 parts: [part all_0_2_1 with ranges [(0, 182)], part all_3_3_0 with ranges [(0, 62)]]----------
Received from 0 replica
                                                                                                   │
│ Sent initial requests: 4 Replicas count: 6                                                                                                                                                                                                                 │
│ Initial request from replica 5: 2 parts: [part all_0_2_1 with ranges [(0, 182)], part all_3_3_0 with ranges [(0, 62)]]----------
Received from 5 replica
                                                                                                   │
│ Sent initial requests: 5 Replicas count: 6                                                                                                                                                                                                                 │
│ Handling request from replica 2, minimal marks size is 240                                                                                                                                                                                                 │
│ Going to respond to replica 2 with 0 parts: []. Finish: true; mine_marks=0, stolen_by_hash=0, stolen_rest=0                                                                                                                                                │
│ Initial request from replica 3: 2 parts: [part all_0_2_1 with ranges [(0, 182)], part all_3_3_0 with ranges [(0, 62)]]----------
Received from 3 replica
                                                                                                   │
│ Sent initial requests: 6 Replicas count: 6                                                                                                                                                                                                                 │
│ Total rows to read: 2000000                                                                                                                                                                                                                                │
│ Handling request from replica 5, minimal marks size is 240                                                                                                                                                                                                 │
│ Going to respond to replica 5 with 0 parts: []. Finish: true; mine_marks=0, stolen_by_hash=0, stolen_rest=0                                                                                                                                                │
│ Handling request from replica 0, minimal marks size is 240                                                                                                                                                                                                 │
│ Going to respond to replica 0 with 0 parts: []. Finish: true; mine_marks=0, stolen_by_hash=0, stolen_rest=0                                                                                                                                                │
│ Handling request from replica 1, minimal marks size is 240                                                                                                                                                                                                 │
│ Going to respond to replica 1 with 0 parts: []. Finish: true; mine_marks=0, stolen_by_hash=0, stolen_rest=0                                                                                                                                                │
│ Handling request from replica 3, minimal marks size is 240                                                                                                                                                                                                 │
│ Going to respond to replica 3 with 0 parts: []. Finish: true; mine_marks=0, stolen_by_hash=0, stolen_rest=0                                                                                                                                                │
│ (c-crimson-vd-86-server-rdhnsx3-0.c-crimson-vd-86-server-headless.ns-crimson-vd-86.svc.cluster.local:9000) Cancelling query because enough data has been read                                                                                              │
│ Read 81920 rows, 5.16 MiB in 0.013166 sec., 6222087.194288318 rows/sec., 391.63 MiB/sec.                                                                                                                                                                   │
│ Coordination done: Statistics: replica 0 - {requests: 2 marks: 0 assigned_to_me: 0 stolen_by_hash: 0 stolen_unassigned: 0}; replica 1 - {requests: 2 marks: 0 assigned_to_me: 0 stolen_by_hash: 0 stolen_unassigned: 0}; replica 2 - {requests: 2 marks: 0 assigned_to_me: 0 stolen_by_hash: 0 stolen_unassigned: 0}; replica 3 - {requests: 2 marks: 0 assigned_to_me: 0 stolen_by_hash: 0 stolen_unassigned: 0}; replica 4 - {requests: 3 marks: 244 assigned_to_me: 0 stolen_by_hash: 54 stolen_unassigned: 190}; replica 5 - {requests: 2 marks: 0 assigned_to_me: 0 stolen_by_hash: 0 stolen_unassigned: 0} │
│ Peak memory usage (for query): 1.81 MiB.                                                                                                                                                                                                                   │
│ Processed in 0.024095586 sec.                                                                                                                                                                                                                              │
└────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
```
</details>

Наконец, вы также можете использовать `EXPLAIN PIPELINE`. Это подчеркнёт, как ClickHouse собирается выполнять запрос и какие ресурсы будут использоваться для выполнения запроса. Давайте рассмотрим следующий пример запроса:

```sql
SELECT count(), uniq(pageId) , min(timestamp), max(timestamp) 
FROM session_events 
WHERE type='type3' 
GROUP BY toYear(timestamp) LIMIT 10
```

Посмотрим на конвейер запроса без параллельной реплики:

```sql title="EXPLAIN PIPELINE (без параллельной реплики)"
EXPLAIN PIPELINE graph = 1, compact = 0 
SELECT count(), uniq(pageId) , min(timestamp), max(timestamp) 
FROM session_events 
WHERE type='type3' 
GROUP BY toYear(timestamp) 
LIMIT 10 
SETTINGS allow_experimental_parallel_reading_from_replicas=0 
FORMAT TSV;
```

<Image img={image_8} size="lg" alt="EXPLAIN без parallel_replica" />

А теперь с параллельной репликой:

```sql title="EXPLAIN PIPELINE (с параллельной репликой)"
EXPLAIN PIPELINE graph = 1, compact = 0 
SELECT count(), uniq(pageId) , min(timestamp), max(timestamp) 
FROM session_events 
WHERE type='type3' 
GROUP BY toYear(timestamp) 
LIMIT 10 
SETTINGS allow_experimental_parallel_reading_from_replicas=2 
FORMAT TSV;
```

<Image img={image_9} size="lg" alt="EXPLAIN с parallel_replica"/>
