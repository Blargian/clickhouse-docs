---
slug: /data-modeling/denormalization
title: 'Денормализация данных'
description: 'Как использовать денормализацию для улучшения производительности запросов'
keywords: ['денормализация данных', 'денормализовать', 'оптимизация запросов']
---

import denormalizationDiagram from '@site/static/images/data-modeling/denormalization-diagram.png';
import denormalizationSchema from '@site/static/images/data-modeling/denormalization-schema.png';
import Image from '@theme/IdealImage';


# Денормализация данных

Денормализация данных — это метод в ClickHouse, который использует плоские таблицы для минимизации задержки запросов за счет избегания соединений.

## Сравнение нормализованных и денормализованных схем {#comparing-normalized-vs-denormalized-schemas}

Денормализация данных включает в себя намеренное обратное преобразование процесса нормализации с целью оптимизации производительности базы данных для конкретных паттернов запросов. В нормализованных базах данных данные разбиваются на несколько взаимосвязанных таблиц для минимизации избыточности и обеспечения целостности данных. Денормализация вновь вводит избыточность, комбинируя таблицы, дублируя данные и включая вычисляемые поля в одну таблицу или меньшее количество таблиц — фактически перемещая любые соединения из времени выполнения запроса в время вставки.

Этот процесс сокращает необходимость в сложных соединениях во время выполнения запросов и может значительно ускорить операции чтения, что делает его идеальным для приложений с высокой нагрузкой на чтение и сложными запросами. Однако это может увеличить сложность операций записи и обслуживания, поскольку любые изменения в дублированных данных должны быть распространены по всем экземплярам для поддержания согласованности.

<Image img={denormalizationDiagram} size="lg" alt="Денормализация в ClickHouse"/>

<br />

Распространенной техникой, популяризированной решениями NoSQL, является денормализация данных в отсутствие поддержки `JOIN`, фактически храня все статистические данные или связанные строки на родительской строке в виде столбцов и вложенных объектов. Например, в примере схемы блога мы можем хранить все `Comments` как `Array` объектов на соответствующих постах.

## Когда использовать денормализацию {#when-to-use-denormalization}

В общем, мы рекомендуем денормализацию в следующих случаях:

- Денормализуйте таблицы, которые меняются нечасто или для которых допустима задержка перед тем, как данные станут доступны для аналитических запросов, т.е. данные могут быть полностью перезагружены в пакетном режиме.
- Избегайте денормализации отношений многие-ко-многим. Это может привести к необходимости обновления многих строк, если меняется одна исходная строка.
- Избегайте денормализации отношений с высокой кардинальностью. Если в каждой строке таблицы есть тысячи связанных записей в другой таблице, их необходимо представить в виде `Array` — либо примитивного типа, либо кортежей. Обычно, массивы с более чем 1000 кортежей не рекомендуется использовать.
- Вместо того чтобы денормализовывать все столбцы как вложенные объекты, рассмотрите возможность денормализации только статистики с помощью материализованных представлений (см. ниже).

Не вся информация должна быть денормализована — только ключевая информация, к которой необходимо получать доступ часто.

Работы по денормализации могут выполняться как в ClickHouse, так и upstream, например, с использованием Apache Flink.

## Избегайте денормализации для часто обновляемых данных {#avoid-denormalization-on-frequently-updated-data}

Для ClickHouse денормализация является одним из нескольких вариантов, которые пользователи могут использовать для оптимизации производительности запросов, но ее следует использовать осторожно. Если данные обновляются часто и необходимо обновление в почти реальном времени, этот подход следует избегать. Используйте это, если основная таблица в основном добавляет записи или может периодически перезагружаться в пакетном режиме, например, ежедневно.

Как подход он имеет одну основную проблему — производительность записи и обновление данных. Более конкретно, денормализация фактически переносит ответственность за соединение данных с времени выполнения запроса на время интеграции. Хотя это может значительно улучшить производительность запроса, это усложняет интеграцию и означает, что конвейеры данных должны повторно вставлять строку в ClickHouse, если изменяется какая-либо из строк, которые были использованы для ее составления. Это может означать, что изменение в одной исходной строке потенциально требует обновления многих строк в ClickHouse. В сложных схемах, где строки были составлены из сложных соединений, изменение одной строки в вложенном компоненте соединения потенциально может означать, что нужно обновить миллионы строк.

Достижение этого в реальном времени часто нереалистично и требует значительных усилий со стороны разработчиков из-за двух проблем:

1. Применение правильных операторов соединения, когда меняется строка таблицы. Это не должно вызывать обновление всех объектов для соединения — только тех, на которые это повлияло. Модификация соединений для фильтрации правильных строк эффективно, а также достижение этого при высокой пропускной способности требует внешних инструментов или инженерного решения.
2. Обновление строк в ClickHouse необходимо внимательно управлять, что вводит дополнительную сложность.

<br />

Процесс пакетного обновления становится более распространенным, когда все денормализованные объекты периодически перезагружаются.

## Практические случаи использования денормализации {#practical-cases-for-denormalization}

Рассмотрим несколько практических примеров, где денормализация может иметь смысл, и другие случаи, где более желательно использовать альтернативные подходы.

Предположим, у нас есть таблица `Posts`, которая уже была денормализована со статистикой, такой как `AnswerCount` и `CommentCount` — исходные данные предоставлены в этой форме. На самом деле, возможно, мы захотим нормализовать эту информацию, так как она, вероятно, будет часто меняться. Многие из этих столбцов также доступны через другие таблицы, например, комментарии к посту доступны через столбец `PostId` и таблицу `Comments`. Для целей примера предположим, что посты перезагружаются в пакетном процессе.

Мы также рассматриваем только денормализацию других таблиц в таблицу `Posts`, поскольку считаем ее нашей основной таблицей для аналитики. Денормализация в другом направлении также была бы уместной для некоторых запросов, при этом те же самые вышеуказанные соображения применимы.

*Для каждого из следующих примеров предположим, что существует запрос, который требует использования обеих таблиц в соединении.*

### Посты и голоса {#posts-and-votes}

Голоса для постов представлены в виде отдельных таблиц. Оптимизированная схема для этого представлена ниже, а также команда вставки для загрузки данных:

```sql
CREATE TABLE votes
(
        `Id` UInt32,
        `PostId` Int32,
        `VoteTypeId` UInt8,
        `CreationDate` DateTime64(3, 'UTC'),
        `UserId` Int32,
        `BountyAmount` UInt8
)
ENGINE = MergeTree
ORDER BY (VoteTypeId, CreationDate, PostId)

INSERT INTO votes SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/stackoverflow/parquet/votes/*.parquet')

0 rows in set. Elapsed: 26.272 sec. Processed 238.98 million rows, 2.13 GB (9.10 million rows/s., 80.97 MB/s.)
```

На первый взгляд, это могут быть кандидаты для денормализации в таблицу постов. Однако у данного подхода есть несколько проблем.

Голоса добавляются к постам часто. Хотя это может снизиться со временем, следующий запрос показывает, что у нас около 40k голосов в час на 30k постах.

```sql
SELECT round(avg(c)) AS avg_votes_per_hr, round(avg(posts)) AS avg_posts_per_hr
FROM
(
        SELECT
        toStartOfHour(CreationDate) AS hr,
        count() AS c,
        uniq(PostId) AS posts
        FROM votes
        GROUP BY hr
)

┌─avg_votes_per_hr─┬─avg_posts_per_hr─┐
│               41759 │         33322 │
└──────────────────┴──────────────────┘
```

Это можно решить пакетной обработкой, если задержка допустима, но это все равно требует обработки обновлений, если мы не хотим периодически перезагружать все посты (что вряд ли желательно).

Более проблематично то, что у некоторых постов очень много голосов:

```sql
SELECT PostId, concat('https://stackoverflow.com/questions/', PostId) AS url, count() AS c
FROM votes
GROUP BY PostId
ORDER BY c DESC
LIMIT 5

┌───PostId─┬─url──────────────────────────────────────────┬─────c─┐
│ 11227902 │ https://stackoverflow.com/questions/11227902 │ 35123 │
│   927386 │ https://stackoverflow.com/questions/927386   │ 29090 │
│ 11227809 │ https://stackoverflow.com/questions/11227809 │ 27475 │
│   927358 │ https://stackoverflow.com/questions/927358   │ 26409 │
│  2003515 │ https://stackoverflow.com/questions/2003515  │ 25899 │
└──────────┴──────────────────────────────────────────────┴───────┘
```

Основное наблюдение здесь заключается в том, что агрегированные статистические данные о голосах для каждого поста были бы достаточны для большинства анализов — нам не нужно денормализовать всю информацию о голосах. Например, текущий столбец `Score` представляет собой такую статистику, т.е. общее количество голосов "за" минус "против". В идеале мы просто хотели бы получить эти статистические данные во время выполнения запроса с помощью простого поиска (см. [словарь](/dictionary)).

### Пользователи и награды {#users-and-badges}

Теперь давайте рассмотрим наши `Users` и `Badges`:

<Image img={denormalizationSchema} size="lg" alt="Схема пользователей и наград"/>

<p></p>
Сначала вставляем данные следующей командой:
<p></p>

```sql
CREATE TABLE users
(
    `Id` Int32,
    `Reputation` LowCardinality(String),
    `CreationDate` DateTime64(3, 'UTC') CODEC(Delta(8), ZSTD(1)),
    `DisplayName` String,
    `LastAccessDate` DateTime64(3, 'UTC'),
    `AboutMe` String,
    `Views` UInt32,
    `UpVotes` UInt32,
    `DownVotes` UInt32,
    `WebsiteUrl` String,
    `Location` LowCardinality(String),
    `AccountId` Int32
)
ENGINE = MergeTree
ORDER BY (Id, CreationDate)
```

```sql
CREATE TABLE badges
(
    `Id` UInt32,
    `UserId` Int32,
    `Name` LowCardinality(String),
    `Date` DateTime64(3, 'UTC'),
    `Class` Enum8('Gold' = 1, 'Silver' = 2, 'Bronze' = 3),
    `TagBased` Bool
)
ENGINE = MergeTree
ORDER BY UserId

INSERT INTO users SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/stackoverflow/parquet/users.parquet')

0 rows in set. Elapsed: 26.229 sec. Processed 22.48 million rows, 1.36 GB (857.21 thousand rows/s., 51.99 MB/s.)

INSERT INTO badges SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/stackoverflow/parquet/badges.parquet')

0 rows in set. Elapsed: 18.126 sec. Processed 51.29 million rows, 797.05 MB (2.83 миллиона строк/с., 43.97 МБ/с.)
```

Хотя пользователи могут часто получать награды, вряд ли это набор данных, который необходимо обновлять более одного раза в день. Связь между наградами и пользователями — один-ко-многим. Возможно, мы можем просто денормализовать награды на пользователей в виде списка кортежей? Хотя это возможно, быстрый проверочный запрос, подтверждающий наибольшее количество наград на пользователя, предполагает, что это не идеально:

```sql
SELECT UserId, count() AS c FROM badges GROUP BY UserId ORDER BY c DESC LIMIT 5

┌─UserId─┬─────c─┐
│  22656 │ 19334 │
│   6309 │ 10516 │
│ 100297 │  7848 │
│ 157882 │  7574 │
│  29407 │  6512 │
└────────┴───────┘
```

Вряд ли реалистично денормализовать 19k объектов на одну строку. Эта связь, вероятно, лучше оставить в виде отдельных таблиц или с добавленными статистическими данными.

> Мы можем захотеть денормализовать статистику по наградам на пользователей, например, количество наград. Мы рассматриваем такой пример при использовании словарей для этого набора данных во время вставки.

### Посты и связанные посты {#posts-and-postlinks}

`PostLinks` соединяют `Posts`, которые пользователи считают связанными или дублирующимися. Следующий запрос показывает схему и команду загрузки:

```sql
CREATE TABLE postlinks
(
  `Id` UInt64,
  `CreationDate` DateTime64(3, 'UTC'),
  `PostId` Int32,
  `RelatedPostId` Int32,
  `LinkTypeId` Enum('Linked' = 1, 'Duplicate' = 3)
)
ENGINE = MergeTree
ORDER BY (PostId, RelatedPostId)

INSERT INTO postlinks SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/stackoverflow/parquet/postlinks.parquet')

0 rows in set. Elapsed: 4.726 sec. Processed 6.55 million rows, 129.70 MB (1.39 миллиона строк/с., 27.44 МБ/с.)
```

Мы можем подтвердить, что ни один пост не имеет чрезмерного числа ссылок, препятствующего денормализации:

```sql
SELECT PostId, count() AS c
FROM postlinks
GROUP BY PostId
ORDER BY c DESC LIMIT 5

┌───PostId─┬───c─┐
│ 22937618 │ 125 │
│  9549780 │ 120 │
│  3737139 │ 109 │
│ 18050071 │ 103 │
│ 25889234 │  82 │
└──────────┴─────┘
```

Точно так же эти ссылки не являются событиями, которые происходят слишком часто:

```sql
SELECT
  round(avg(c)) AS avg_votes_per_hr,
  round(avg(posts)) AS avg_posts_per_hr
FROM
(
  SELECT
  toStartOfHour(CreationDate) AS hr,
  count() AS c,
  uniq(PostId) AS posts
  FROM postlinks
  GROUP BY hr
)

┌─avg_votes_per_hr─┬─avg_posts_per_hr─┐
│                54 │                    44     │
└──────────────────┴──────────────────┘
```

Используем это как наш пример денормализации ниже.

### Простой пример статистики {#simple-statistic-example}

В большинстве случаев денормализация требует добавления одного столбца или статистики к родительской строке. Например, мы можем захотеть просто обогатить наши посты количеством дублирующихся постов, нам просто нужно добавить столбец.

```sql
CREATE TABLE posts_with_duplicate_count
(
  `Id` Int32 CODEC(Delta(4), ZSTD(1)),
   ... -другие столбцы
   `DuplicatePosts` UInt16
) ENGINE = MergeTree
ORDER BY (PostTypeId, toDate(CreationDate), CommentCount)
```

Для заполнения этой таблицы мы используем `INSERT INTO SELECT`, объединяя нашу статистику дублирования с постами.

```sql
INSERT INTO posts_with_duplicate_count SELECT
    posts.*,
    DuplicatePosts
FROM posts AS posts
LEFT JOIN
(
    SELECT PostId, countIf(LinkTypeId = 'Duplicate') AS DuplicatePosts
    FROM postlinks
    GROUP BY PostId
) AS postlinks ON posts.Id = postlinks.PostId
```

### Использование сложных типов для отношений один-ко-многим {#exploiting-complex-types-for-one-to-many-relationships}

Для выполнения денормализации мы часто должны использовать сложные типы. Если денормализуется связь один-к-одному с небольшим количеством столбцов, пользователи могут просто добавлять их в виде строк с их исходными типами, как показано выше. Однако это часто нежелательно для больших объектов и невозможно для отношений один-ко-многим.

В случаях сложных объектов или отношений один-ко-многим пользователи могут использовать:

- Именованные кортежи — эти кортежи позволяют представлять связанную структуру в виде набора столбцов.
- Array(Tuple) или Nested — массив именованных кортежей, также известных как вложенные, где каждый элемент представляет объект. Применимо к отношениям один-ко-многим.

В качестве примера мы демонстрируем денормализацию `PostLinks` на `Posts` ниже.

Каждый пост может содержать несколько ссылок на другие посты, как показано в схеме `PostLinks` ранее. Как вложенный тип, мы можем представить эти связанные и дублирующиеся посты следующим образом:

```sql
SET flatten_nested=0
CREATE TABLE posts_with_links
(
  `Id` Int32 CODEC(Delta(4), ZSTD(1)),
   ... -другие столбцы
   `LinkedPosts` Nested(CreationDate DateTime64(3, 'UTC'), PostId Int32),
   `DuplicatePosts` Nested(CreationDate DateTime64(3, 'UTC'), PostId Int32),
) ENGINE = MergeTree
ORDER BY (PostTypeId, toDate(CreationDate), CommentCount)
```

> Обратите внимание на использование параметра `flatten_nested=0`. Рекомендуется отключить уплотнение вложенных данных.

Мы можем выполнить эту денормализацию с помощью `INSERT INTO SELECT` с запросом `OUTER JOIN`:

```sql
INSERT INTO posts_with_links
SELECT
    posts.*,
    arrayMap(p -> (p.1, p.2), arrayFilter(p -> p.3 = 'Linked' AND p.2 != 0, Related)) AS LinkedPosts,
    arrayMap(p -> (p.1, p.2), arrayFilter(p -> p.3 = 'Duplicate' AND p.2 != 0, Related)) AS DuplicatePosts
FROM posts
LEFT JOIN (
    SELECT
         PostId,
         groupArray((CreationDate, RelatedPostId, LinkTypeId)) AS Related
    FROM postlinks
    GROUP BY PostId
) AS postlinks ON posts.Id = postlinks.PostId

0 rows in set. Elapsed: 155.372 sec. Processed 66.37 million rows, 76.33 GB (427.18 thousand rows/s., 491.25 MB/s.)
Peak memory usage: 6.98 GiB.
```

> Обратите внимание на время выполнения. Мы смогли денормализовать 66 миллионов строк примерно за 2 минуты. Как мы увидим позже, это операция, которую мы можем запланировать.

Обратите внимание на использование функций `groupArray` для объединения `PostLinks` в массив для каждого `PostId` перед объединением. Этот массив затем фильтруется на два подсписка: `LinkedPosts` и `DuplicatePosts`, которые также исключают любые пустые результаты из внешнего соединения.

Мы можем выбрать некоторые строки, чтобы увидеть нашу новую денормализованную структуру:

```sql
SELECT LinkedPosts, DuplicatePosts
FROM posts_with_links
WHERE (length(LinkedPosts) > 2) AND (length(DuplicatePosts) > 0)
LIMIT 1
FORMAT Vertical

Row 1:
──────
LinkedPosts:    [('2017-04-11 11:53:09.583',3404508),('2017-04-11 11:49:07.680',3922739),('2017-04-11 11:48:33.353',33058004)]
DuplicatePosts: [('2017-04-11 12:18:37.260',3922739),('2017-04-11 12:18:37.260',33058004)]
```

## Оркестрация и планирование денормализации {#orchestrating-and-scheduling-denormalization}

### Пакет {#batch}

Эксплуатация денормализации требует процесса трансформации, в котором она может быть выполнена и оркестрирована.

Мы показали выше, как ClickHouse может использоваться для выполнения этой трансформации после загрузки данных через `INSERT INTO SELECT`. Это подходящее решение для периодических пакетных трансформаций.

У пользователей есть несколько вариантов для оркестрации этого в ClickHouse, предполагая, что периодический пакетный процесс загрузки приемлем:

- **[Обновляемые материализованные представления](/materialized-view/refreshable-materialized-view)** — обновляемые материализованные представления можно использовать для периодического планирования запроса, результаты которого отправляются в целевую таблицу. При выполнении запроса представление обеспечивает атомарное обновление целевой таблицы. Это предоставляет нативное средство ClickHouse для планирования этой работы.
- **Внешние инструменты** — использование таких инструментов, как [dbt](https://www.getdbt.com/) и [Airflow](https://airflow.apache.org/), для периодического планирования трансформации. [Интеграция ClickHouse для dbt](/integrations/dbt) обеспечивает выполнение этого атомарно с созданием новой версии целевой таблицы, которая затем атомарно обменита с версией, получающей запросы (через команду [EXCHANGE](/sql-reference/statements/exchange)).

### Стриминг {#streaming}

Пользователи также могут захотеть выполнить это вне ClickHouse, до вставки, с использованием таких технологий стриминга, как [Apache Flink](https://flink.apache.org/). В качестве альтернативы можно использовать инкрементные [материализованные представления](/guides/developer/cascading-materialized-views) для выполнения этого процесса по мере вставки данных.
