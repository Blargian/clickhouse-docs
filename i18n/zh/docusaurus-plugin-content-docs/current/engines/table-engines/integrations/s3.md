---
'description': '这个引擎与 Amazon S3 生态系统集成。类似于 HDFS 引擎，但提供 S3 特定功能。'
'sidebar_label': 'S3'
'sidebar_position': 180
'slug': '/engines/table-engines/integrations/s3'
'title': 'S3 表引擎'
---


# S3 表引擎

该引擎提供与 [Amazon S3](https://aws.amazon.com/s3/) 生态系统的集成。该引擎类似于 [HDFS](/engines/table-engines/integrations/hdfs) 引擎，但提供特定于 S3 的特性。

## 示例 {#example}

```sql
CREATE TABLE s3_engine_table (name String, value UInt32)
    ENGINE=S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/test-data.csv.gz', 'CSV', 'gzip')
    SETTINGS input_format_with_names_use_header = 0;

INSERT INTO s3_engine_table VALUES ('one', 1), ('two', 2), ('three', 3);

SELECT * FROM s3_engine_table LIMIT 2;
```

```text
┌─name─┬─value─┐
│ one  │     1 │
│ two  │     2 │
└──────┴───────┘
```

## 创建表 {#creating-a-table}

```sql
CREATE TABLE s3_engine_table (name String, value UInt32)
    ENGINE = S3(path [, NOSIGN | aws_access_key_id, aws_secret_access_key,] format, [compression])
    [PARTITION BY expr]
    [SETTINGS ...]
```

### 引擎参数 {#parameters}

- `path` — 存储桶 URL 及文件路径。在只读模式下支持以下通配符：`*`，`**`，`?`，`{abc,def}` 和 `{N..M}`，其中 `N` 和 `M` 为数字，`'abc'` 和 `'def'` 为字符串。有关更多信息，请参见 [下文](#wildcards-in-path)。
- `NOSIGN` - 如果该关键字在凭证位置提供，则所有请求将不被签名。
- `format` — 文件的 [格式](/sql-reference/formats#formats-overview)。
- `aws_access_key_id`, `aws_secret_access_key` - AWS 账户用户的长期凭证。您可以使用这些凭证来进行请求认证。该参数为可选项。如果未指定凭证，则使用配置文件中的凭证。有关更多信息，请参见 [使用 S3 进行数据存储](../mergetree-family/mergetree.md#table_engine-mergetree-s3)。
- `compression` — 压缩类型。支持的值：`none`，`gzip/gz`，`brotli/br`，`xz/LZMA`，`zstd/zst`。该参数为可选项。默认情况下，它将根据文件扩展名自动检测压缩类型。

### 数据缓存 {#data-cache}

`S3` 表引擎支持在本地磁盘上缓存数据。请参见 [这一节](/operations/storing-data.md/#using-local-cache) 中的文件系统缓存配置选项和使用。缓存的创建依赖于存储对象的路径和 ETag，因此 ClickHouse 不会读取过期的缓存版本。

要启用缓存，请使用设置 `filesystem_cache_name = '<name>'` 和 `enable_filesystem_cache = 1`。

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_3.csv', 'minioadmin', 'minioadminpassword', 'CSV')
SETTINGS filesystem_cache_name = 'cache_for_s3', enable_filesystem_cache = 1;
```

有两种方式在配置文件中定义缓存。

1. 将以下部分添加到 ClickHouse 配置文件中：

```xml
<clickhouse>
    <filesystem_caches>
        <cache_for_s3>
            <path>path to cache directory</path>
            <max_size>10Gi</max_size>
        </cache_for_s3>
    </filesystem_caches>
</clickhouse>
```

2. 从 ClickHouse 的 `storage_configuration` 部分重用缓存配置（因此缓存存储），[详述如下](/operations/storing-data.md/#using-local-cache)

### PARTITION BY {#partition-by}

`PARTITION BY` — 可选。在大多数情况下，您不需要分区键，如果需要，则通常不需要比按月更细粒度的分区键。分区不会加速查询（与 ORDER BY 表达式相反）。您不应该使用过于细粒度的分区。不要按客户标识符或名称分区您的数据（相反，将客户标识符或名称作为 ORDER BY 表达式中的第一列）。

要按月分区，请使用 `toYYYYMM(date_column)` 表达式，其中 `date_column` 是类型为 [Date](/sql-reference/data-types/date.md) 的日期列。此处的分区名称具有 `"YYYYMM"` 格式。

### 查询分区数据 {#querying-partitioned-data}

该示例使用 [docker compose recipe](https://github.com/ClickHouse/examples/tree/5fdc6ff72f4e5137e23ea075c88d3f44b0202490/docker-compose-recipes/recipes/ch-and-minio-S3)，集成了 ClickHouse 和 MinIO。您应该能够通过替换端点和认证值来重现相同的查询。

请注意，在 `ENGINE` 配置中，S3 端点使用参数 token `{_partition_id}` 作为 S3 对象（文件名）的一部分，并且 SELECT 查询是针对这些生成的对象名称进行选择的（例如，`test_3.csv`）。

:::note
如示例所示，当前不直接支持从分区的 S3 表中查询，但可以通过使用 S3 表函数查询单个分区来实现。

在 S3 中写入分区数据的主要用例是为了将数据转移到另一个 ClickHouse 系统（例如，从本地系统迁移到 ClickHouse Cloud）。由于 ClickHouse 数据集通常非常大，并且网络可靠性有时不完美，因此以子集的形式传输数据集是有意义的，因此需要进行分区写入。
:::

#### 创建表 {#create-the-table}
```sql
CREATE TABLE p
(
    `column1` UInt32,
    `column2` UInt32,
    `column3` UInt32
)
ENGINE = S3(
-- highlight-next-line
           'http://minio:10000/clickhouse//test_{_partition_id}.csv',
           'minioadmin',
           'minioadminpassword',
           'CSV')
PARTITION BY column3
```

#### 插入数据 {#insert-data}
```sql
insert into p values (1, 2, 3), (3, 2, 1), (78, 43, 45)
```

#### 从分区 3 查询 {#select-from-partition-3}

:::tip
该查询使用了 s3 表函数
:::

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_3.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```
```response
┌─c1─┬─c2─┬─c3─┐
│  1 │  2 │  3 │
└────┴────┴────┘
```

#### 从分区 1 查询 {#select-from-partition-1}
```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_1.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```
```response
┌─c1─┬─c2─┬─c3─┐
│  3 │  2 │  1 │
└────┴────┴────┘
```

#### 从分区 45 查询 {#select-from-partition-45}
```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_45.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```
```response
┌─c1─┬─c2─┬─c3─┐
│ 78 │ 43 │ 45 │
└────┴────┴────┘
```

#### 限制 {#limitation}

您可能会尝试 `Select * from p`，但如上所述，该查询将失败；请使用前面的查询。

```sql
SELECT * FROM p
```
```response
Received exception from server (version 23.4.1):
Code: 48. DB::Exception: Received from localhost:9000. DB::Exception: Reading from a partitioned S3 storage is not implemented yet. (NOT_IMPLEMENTED)
```

## 插入数据 {#inserting-data}

请注意，行只能插入到新文件中。没有合并周期或文件拆分操作。一旦写入文件，后续插入将失败。为避免这种情况，您可以使用 `s3_truncate_on_insert` 和 `s3_create_new_file_on_insert` 设置。有关更多细节，请参见 [这里](/integrations/s3#inserting-data)。

## 虚拟列 {#virtual-columns}

- `_path` — 文件路径。类型：`LowCardinality(String)`。
- `_file` — 文件名。类型：`LowCardinality(String)`。
- `_size` — 文件大小（以字节为单位）。类型：`Nullable(UInt64)`。如果大小未知，则值为 `NULL`。
- `_time` — 文件的最后修改时间。类型：`Nullable(DateTime)`。如果时间未知，则值为 `NULL`。
- `_etag` — 文件的 ETag。类型：`LowCardinality(String)`。如果 etag 未知，则值为 `NULL`。

有关虚拟列的更多信息，请见 [这里](../../../engines/table-engines/index.md#table_engines-virtual_columns)。

## 实现细节 {#implementation-details}

- 读取和写入可以并行进行。
- 不支持：
    - `ALTER` 和 `SELECT...SAMPLE` 操作。
    - 索引。
    - [零拷贝](../../../operations/storing-data.md#zero-copy) 复制是可能的，但不被支持。

  :::note 零拷贝复制尚未准备好投入生产
  在 ClickHouse 版本 22.8 及更高版本中，零拷贝复制默认是禁用的。此功能不建议在生产环境中使用。
  :::

## 路径中的通配符 {#wildcards-in-path}

`path` 参数可以使用类似 bash 的通配符指定多个文件。要进行处理，文件必须存在并匹配整个路径模式。文件列出在 `SELECT` 时确定（而不是在 `CREATE` 之时）。

- `*` — 替代任何数量的任意字符，包括空字符串，但不包括 `/`。
- `**` — 替代任何数量的任意字符（包括 `/`）及空字符串。
- `?` — 替代任意单个字符。
- `{some_string,another_string,yet_another_one}` — 替代任何字符串 `'some_string', 'another_string', 'yet_another_one'`。
- `{N..M}` — 替代范围从 N 到 M 的任意数字，包括边界。 N 和 M 可以有前导零，例如 `000..078`。

带有 `{}` 的构造与 [remote](../../../sql-reference/table-functions/remote.md) 表函数类似。

:::note
如果文件列表包含带有前导零的数字范围，请为每个数字单独使用棱形构造，或使用 `?`。
:::

**通配符示例 1**

创建一个表，文件命名为 `file-000.csv`，`file-001.csv`，...，`file-999.csv`：

```sql
CREATE TABLE big_table (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/my_folder/file-{000..999}.csv', 'CSV');
```

**通配符示例 2**

假设我们在 S3 上有几个 CSV 格式的文件，URI 如下：

- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_1.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_2.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_3.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_1.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_2.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_3.csv'


有几种方法可以创建一个由所有六个文件组成的表：

1. 指定文件后缀范围：

```sql
CREATE TABLE table_with_range (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/some_file_{1..3}', 'CSV');
```

2. 获取所有以 `some_file_` 为前缀的文件（在两个文件夹中不应存在其他具有该前缀的额外文件）：

```sql
CREATE TABLE table_with_question_mark (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/some_file_?', 'CSV');
```

3. 获取两个文件夹中的所有文件（所有文件应满足查询中描述的格式和架构）：

```sql
CREATE TABLE table_with_asterisk (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/*', 'CSV');
```

## 存储设置 {#storage-settings}

- [s3_truncate_on_insert](/operations/settings/settings.md#s3_truncate_on_insert) - 允许在插入前截断文件。默认情况下禁用。
- [s3_create_new_file_on_insert](/operations/settings/settings.md#s3_create_new_file_on_insert) - 允许在每次插入时创建新文件（如果格式带有后缀）。默认情况下禁用。
- [s3_skip_empty_files](/operations/settings/settings.md#s3_skip_empty_files) - 允许在读取时跳过空文件。默认情况下启用。

## 与 S3 相关的设置 {#settings}

以下设置可以在查询执行前设定或放置到配置文件中。

- `s3_max_single_part_upload_size` — 使用单部分上传到 S3 的对象的最大大小。默认值为 `32Mb`。
- `s3_min_upload_part_size` — 在进行 [S3 分段上传](https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html) 时上传的部分的最小大小。默认值为 `16Mb`。
- `s3_max_redirects` — 允许的最大 S3 重定向跳数。默认值为 `10`。
- `s3_single_read_retries` — 单次读取的最大尝试次数。默认值为 `4`。
- `s3_max_put_rps` — 在限制之前的每秒最大 PUT 请求数。默认值为 `0`（无限制）。
- `s3_max_put_burst` — 在达到每秒请求限制之前可以同时发出的最大请求数。默认值为 `0`（与 `s3_max_put_rps` 相同）。
- `s3_max_get_rps` — 在限制之前的每秒最大 GET 请求数。默认值为 `0`（无限制）。
- `s3_max_get_burst` — 在达到每秒请求限制之前可以同时发出的最大请求数。默认值为 `0`（与 `s3_max_get_rps` 相同）。
- `s3_upload_part_size_multiply_factor` - 每次从 S3 上传 `s3_multiply_parts_count_threshold` 个部分时，将此因子乘以 `s3_min_upload_part_size`。默认值为 `2`。
- `s3_upload_part_size_multiply_parts_count_threshold` - 每次上传此数量的部分到 S3 时，`s3_min_upload_part_size` 将乘以 `s3_upload_part_size_multiply_factor`。默认值为 `500`。
- `s3_max_inflight_parts_for_one_file` - 限制可以同时运行的对单个对象的 PUT 请求数量。其数量应受到限制。值为 `0` 意味着无限制。默认值为 `20`。每个飞行的部分都有一个大小为 `s3_min_upload_part_size` 的缓冲区，前 `s3_upload_part_size_multiply_factor` 个部分的缓冲区较小，对于更大文件则会更多，见 `upload_part_size_multiply_factor`。在默认设置下，一个上传文件占用不超过 `320Mb` 对于小于 `8G` 的文件。在更大的文件中，消耗会更高。

安全考虑：如果恶意用户可以指定任意 S3 URLs，则必须将 `s3_max_redirects` 设置为零，以避免 [SSRF](https://en.wikipedia.org/wiki/Server-side_request_forgery) 攻击；或者，在服务器配置中指定 `remote_host_filter`。

## 基于端点的设置 {#endpoint-settings}

可以在配置文件中为特定端点指定以下设置（将通过 URL 的确切前缀匹配）：

- `endpoint` — 指定端点的前缀。必填。
- `access_key_id` 和 `secret_access_key` — 指定用于给定端点的凭证。可选。
- `use_environment_credentials` — 如果设置为 `true`，则 S3 客户端将尝试从环境变量和 [Amazon EC2](https://en.wikipedia.org/wiki/Amazon_Elastic_Compute_Cloud) 元数据中获取凭证。可选，默认值为 `false`。
- `region` — 指定 S3 区域名称。可选。
- `use_insecure_imds_request` — 如果设置为 `true`，S3 客户端将使用不安全的 IMDS 请求从 Amazon EC2 元数据中获取凭证。可选，默认值为 `false`。
- `expiration_window_seconds` — 检查基于过期的凭证是否过期的宽限期。可选，默认值为 `120`。
- `no_sign_request` - 忽略所有凭证，因此请求不会被签名。用于访问公共存储桶。
- `header` — 向给定端点的请求中添加指定的 HTTP 头。可选，可以指定多次。
- `access_header` - 向给定端点的请求中添加指定的 HTTP 头，当没有其他来源的凭证时。
- `server_side_encryption_customer_key_base64` — 如果指定，则设置访问 S3 对象所需的 SSE-C 加密的头。可选。
- `server_side_encryption_kms_key_id` - 如果指定，将设置访问具有 [SSE-KMS 加密](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html) 的 S3 对象所需的头。如果指定空字符串，将使用 AWS 管理的 S3 密钥。可选。
- `server_side_encryption_kms_encryption_context` - 如果与 `server_side_encryption_kms_key_id` 一起指定，将为 SSE-KMS 设置给定的加密上下文头。可选。
- `server_side_encryption_kms_bucket_key_enabled` - 如果与 `server_side_encryption_kms_key_id` 一起指定，将为 SSE-KMS 设置启用 S3 存储桶密钥的头。可选，可以是 `true` 或 `false`，默认没有设置（与存储桶级别设置匹配）。
- `max_single_read_retries` — 单次读取的最大尝试次数。默认值为 `4`。可选。
- `max_put_rps`、`max_put_burst`、`max_get_rps` 和 `max_get_burst` - 供特定端点使用的限流设置（见上文描述），而不是逐查询。可选。

**示例：**

```xml
<s3>
    <endpoint-name>
        <endpoint>https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/</endpoint>
        <!-- <access_key_id>ACCESS_KEY_ID</access_key_id> -->
        <!-- <secret_access_key>SECRET_ACCESS_KEY</secret_access_key> -->
        <!-- <region>us-west-1</region> -->
        <!-- <use_environment_credentials>false</use_environment_credentials> -->
        <!-- <use_insecure_imds_request>false</use_insecure_imds_request> -->
        <!-- <expiration_window_seconds>120</expiration_window_seconds> -->
        <!-- <no_sign_request>false</no_sign_request> -->
        <!-- <header>Authorization: Bearer SOME-TOKEN</header> -->
        <!-- <server_side_encryption_customer_key_base64>BASE64-ENCODED-KEY</server_side_encryption_customer_key_base64> -->
        <!-- <server_side_encryption_kms_key_id>KMS_KEY_ID</server_side_encryption_kms_key_id> -->
        <!-- <server_side_encryption_kms_encryption_context>KMS_ENCRYPTION_CONTEXT</server_side_encryption_kms_encryption_context> -->
        <!-- <server_side_encryption_kms_bucket_key_enabled>true</server_side_encryption_kms_bucket_key_enabled> -->
        <!-- <max_single_read_retries>4</max_single_read_retries> -->
    </endpoint-name>
</s3>
```

## 处理归档 {#working-with-archives}

假设我们在 S3 上有几个归档文件，URI 如下：

- 'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-10.csv.zip'
- 'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-11.csv.zip'
- 'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-12.csv.zip'

从这些归档中提取数据是可能的，可以使用 ::。通配符可以用于 URL 部分以及 :: 之后的部分（负责归档内部文件的名称）。

```sql
SELECT *
FROM s3(
   'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-1{0..2}.csv.zip :: *.csv'
);
```

:::note
ClickHouse 支持三种归档格式：
ZIP
TAR
7Z
尽管 ZIP 和 TAR 归档可以从任何支持的存储位置访问，但 7Z 归档只能从 ClickHouse 安装的本地文件系统中读取。
:::


## 访问公共存储桶 {#accessing-public-buckets}

ClickHouse 尝试从许多不同类型的来源获取凭证。有时，在访问某些公共存储桶时会出现问题，导致客户端返回 `403` 错误代码。可以通过使用 `NOSIGN` 关键字来避免此问题，强制客户端忽略所有凭证，不对请求进行签名。

```sql
CREATE TABLE big_table (name String, value UInt32)
    ENGINE = S3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/aapl_stock.csv', NOSIGN, 'CSVWithNames');
```

## 优化性能 {#optimizing-performance}

有关优化 s3 函数性能的详细信息，请参见 [我们的详细指南](/integrations/s3/performance)。

## 另请参见 {#see-also}

- [s3 表函数](../../../sql-reference/table-functions/s3.md)
- [将 S3 与 ClickHouse 集成](/integrations/s3)
