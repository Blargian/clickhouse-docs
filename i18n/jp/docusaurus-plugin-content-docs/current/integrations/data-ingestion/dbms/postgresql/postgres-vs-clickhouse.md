---
slug: /integrations/postgresql/postgres-vs-clickhouse
title: PostgreSQL と ClickHouse の比較
keywords: [postgres, postgresql, comparison]
---

import postgresReplicas from '@site/static/images/integrations/data-ingestion/dbms/postgres-replicas.png';

## Postgres と ClickHouse: 同等および異なる概念 {#postgres-vs-clickhouse-equivalent-and-different-concepts}

ACID トランザクションに慣れた OLTP システムから来たユーザーは、ClickHouse がパフォーマンスのためにこれを完全には提供しない意図的な妥協をしていることを理解する必要があります。 ClickHouse の意味論は、十分に理解されていれば高い耐久性保証と高い書き込みスループットを提供できます。 以下に、Postgres から ClickHouse に移行する前に、ユーザーが理解しておくべきいくつかの重要な概念を示します。

### シャードとレプリカ {#shards-vs-replicas}

シャーディングとレプリケーションは、ストレージや計算がパフォーマンスのボトルネックになるときに、1つの Postgres インスタンスを超えてスケールするために使用される2つの戦略です。 Postgres におけるシャーディングは、大きなデータベースを複数のノードに分割して、より管理可能な小さな部分にすることを含みます。しかし、Postgres はネイティブにシャーディングをサポートしていません。代わりに、Postgres を水平方向にスケールできる分散データベースにする [Citus](https://www.citusdata.com/) のような拡張機能を使用して、シャーディングを実現できます。このアプローチにより、Postgres はトランザクションレートとデータセットを増やし、負荷を複数のマシンに分散させることが可能になります。シャードは、トランザクショナルまたは分析的なワークロードタイプに柔軟性を提供するために、行ベースまたはスキーマベースであることができます。シャーディングは、複数のマシン間での調整と一貫性の保証が必要なため、データ管理およびクエリ実行の面で重要な複雑さをもたらす可能性があります。

シャードとは異なり、レプリカはプライマリノードからのすべてまたは一部のデータを含む追加の Postgres インスタンスです。レプリカは、読み取り性能の向上や HA（High Availability）シナリオなど、さまざまな理由で使用されます。物理レプリケーションは、Postgres のネイティブ機能で、データベース全体または重要な部分を別のサーバーにコピーすることを含みます。これには、プライマリノードからレプリカへ WAL セグメントを TCP/IP 経由でストリーミングすることが含まれます。対照的に、論理レプリケーションは、`INSERT`、`UPDATE`、および `DELETE` 操作に基づいて変更をストリーミングする高レベルの抽象化です。物理レプリケーションと同様の結果が適用される場合がありますが、特定のテーブルや操作を対象にした、またデータ変換や異なる Postgres バージョンをサポートするための柔軟性が大きくなります。

**対照的に、ClickHouse のシャードとレプリカは、データ分散と冗長性に関連する2つの主要概念です**。ClickHouse のレプリカは Postgres のレプリカに類似していると考えられますが、レプリケーションは最終的に一貫性があり、プライマリの概念はありません。シャーディングは、Postgres とは異なり、ネイティブでサポートされています。

シャードは、テーブルデータの一部です。常に少なくとも1つのシャードがあります。複数のサーバーにデータをシャーディングすることで、単一のサーバーの容量を超える場合に負荷を分散させることができます。ユーザーは、異なるサーバー上で手動でテーブルのシャードを作成し、データを直接挿入できます。あるいは、分散テーブルを使用して、データがルーティングされるシャードを定義するシャーディングキーを指定できます。シャーディングキーは、ランダムにすることも、ハッシュ関数の出力として使用することもできます。重要なのは、シャードは複数のレプリカで構成される場合があることです。

レプリカはデータのコピーです。ClickHouse には常にデータのコピーが少なくとも1つあるため、レプリカの最小数は1です。データの2つ目のレプリカを追加すると、障害耐性が向上し、さらに多くのクエリを処理するための計算リソースが得られる可能性があります（[Parallel Replicas](https://clickhouse.com/blog/clickhouse-release-23-03#parallel-replicas-for-utilizing-the-full-power-of-your-replicas-nikita-mikhailov) を使用して、単一のクエリの計算を分散させることでレイテンシを低下させることもできます）。レプリカは、[ReplicatedMergeTree テーブルエンジン](/engines/table-engines/mergetree-family/replication)を使用して実現され、ClickHouse が異なるサーバー間でデータの複数コピーを同期させることを可能にします。レプリケーションは物理的で、ノード間で転送されるのは圧縮されたパーツのみであり、クエリは転送されません。

要約すると、レプリカは冗長性と信頼性を提供するデータのコピーであり（および潜在的に分散処理）、シャードは分散処理と負荷分散を可能にするデータのサブセットです。

> ClickHouse Cloud は、複数の計算レプリカでサポートされた S3 にバックアップされたデータの単一コピーを使用します。データは各レプリカノードで利用でき、それぞれにローカル SSD キャッシュがあります。これは、ClickHouse Keeper を通じたメタデータのレプリケーションにのみ依存しています。

## 最終的な一貫性 {#eventual-consistency}

ClickHouse は、内部レプリケーションメカニズムを管理するために ClickHouse Keeper（C++ の ZooKeeper 実装、ZooKeeper も使用可能）を使用しており、主にメタデータのストレージに焦点を当て、最終的な一貫性を保証します。 Keeper は、分散環境内の各挿入のためのユニークな連続番号を割り当てるために使用されます。これは、操作間の順序と一貫性を維持するために重要です。このフレームワークは、マージやミューテーションなどのバックグラウンド操作も処理し、これらの作業を分散させつつ、すべてのレプリカで同じ順序で実行されることを保証します。メタデータに加えて、Keeper は、ストレージされたデータパーツのチェックサムを追跡するための包括的なコントロールセンターとして機能し、レプリカ間の分散通知システムとして作用します。

ClickHouse におけるレプリケーションプロセスは、(1) データが任意のレプリカに挿入されると開始されます。このデータは、(2) チェックサムと共にディスクに書き込まれます。書き込まれた後、レプリカは (3) この新しいデータパートを Keeper に登録し、ユニークなブロック番号を割り当て、新しいパートの詳細をログに記録しようとします。他のレプリカは、(4) レプリケーションログに新しいエントリーを検出すると、(5) 内部 HTTP プロトコル経由で対応するデータパートをダウンロードし、ZooKeeper にリストされたチェックサムに対して確認します。この方法により、すべてのレプリカは、処理速度や遅延の違いにかかわらず、最終的に一貫性があり最新のデータを保持することが保証されます。さらに、このシステムは、複数の操作を同時に処理する能力があり、データ管理プロセスの最適化やシステムのスケーラビリティとハードウェアの不整合に対する強靭性を実現しています。

<br />

<img src={postgresReplicas}    
  class="image"
  alt="NEEDS ALT"
  style={{width: '500px'}} />

<br />

ClickHouse Cloud は、データを共有オブジェクトストレージに保存することで、単一のコピーのデータを利用する [クラウド最適化されたレプリケーションメカニズム](https://clickhouse.com/blog/clickhouse-cloud-boosts-performance-with-sharedmergetree-and-lightweight-updates)を使用しています。これにより、データがすべての計算ノードに自動的に利用可能になり、ノード間でデータを物理的にレプリケートする必要がなくなります。代わりに、Keeper は計算ノード間でメタデータ（どのデータがオブジェクトストレージに存在するか）のみを共有するために使用されます。

PostgreSQL は、主にプライマリレプリカモデルを使用してデータを継続的にストリーミングするストリーミングレプリケーションを利用する点で、ClickHouse とは異なるレプリケーション戦略を採用しています。このタイプのレプリケーションは、ほぼリアルタイムの一貫性を保証し、同期または非同期であり、管理者に可用性と一貫性のバランスを制御することを提供します。ClickHouse とは異なり、PostgreSQL は、ノード間でデータオブジェクトや変更をストリーミングするために、論理レプリケーションとデコーディングを伴う WAL（Write-Ahead Logging）に依存します。このアプローチは PostgreSQL ではより単純ですが、ClickHouse が Keeper を複雑に使用して分散操作の調整と最終的一貫性を実現することによって達成される高度に分散した環境でのスケーラビリティや障害耐性のレベルを提供しないかもしれません。

## ユーザーへの影響 {#user-implications}

ClickHouse では、ユーザーが一つのレプリカにデータを書き込んだ後、別のレプリカから潜在的に未レプリケートのデータを読み取ることができるダーティリードの可能性があります。これは、Keeper を介して管理される最終的な一貫性のあるレプリケーションモデルから生じます。このモデルは、分散システム全体でのパフォーマンスとスケーラビリティを強調しており、レプリカが独立して動作し非同期に同期することを可能にします。その結果、新しく挿入されたデータは、レプリケーションの遅延や変更がシステム全体に伝播するのにかかる時間に応じて、すべてのレプリカに即座に表示されない可能性があります。

逆に、PostgreSQL のストリーミングレプリケーションモデルは、通常、プライマリがトランザクションをコミットする前に、少なくとも一つのレプリカがデータの受け取りを確認するのを待つ同期レプリケーションオプションを採用することで、ダーティリードを防ぐことができます。これにより、トランザクションがコミットされた後、そのデータが別のレプリカで利用可能であることが保証されます。プライマリに障害が発生した場合、レプリカはクエリがコミットされたデータを表示し、より厳格な一貫性レベルを維持します。

## 推奨事項 {#recommendations}

ClickHouse に初めて触れるユーザーは、これらの違いを認識しておく必要があり、これらはレプリケートされた環境で現れます。通常、最終的な一貫性は、数十億、場合によっては数兆のデータポイントに関する分析には十分であり、新しいデータが高レートで継続的に挿入されるため、メトリクスがより安定しているか、推定が十分である場合があります。

これを必要とする場合、読み取りの一貫性を高めるためのいくつかのオプションが存在します。これらの例は、通常、複雑さまたはオーバーヘッドが増加し、クエリパフォーマンスの低下につながり、ClickHouse のスケーリングをより難しくすることになります。**このアプローチは、本当に必要な場合にのみお勧めします。**

## 一貫したルーティング {#consistent-routing}

最終的な一貫性のいくつかの制限を克服するために、ユーザーはクライアントが同じレプリカにルーティングされるようにすることができます。これは、複数のユーザーが ClickHouse にクエリを実行し、リクエスト間で結果が決定論的である必要がある場合に役立ちます。結果は異なる場合がありますが、新しいデータが挿入されるにつれて、同じレプリカをクエリすることで一貫したビューが確保されます。

これは、あなたのアーキテクチャと ClickHouse OSS または ClickHouse Cloud を使用しているかどうかに応じて、いくつかのアプローチを通じて達成できます。

## ClickHouse Cloud {#clickhouse-cloud}

ClickHouse Cloud は、複数の計算レプリカによってバックアップされたデータの単一コピーを S3 に使用します。データは各レプリカノードで利用可能であり、それぞれにローカルの SSD キャッシュがあります。したがって、一貫した結果を保証するために、ユーザーは同じノードに一貫してルーティングされるようにする必要があります。

ClickHouse Cloud サービスのノードへの通信はプロキシ経由で行われます。HTTP とネイティブプロトコルの接続は、オープン状態が保持されている期間中に同じノードにルーティングされます。ほとんどのクライアントからの HTTP 1.1 接続では、これは Keep-Alive ウィンドウによって決まります。これは、ほとんどのクライアントで設定可能です。例えば Node Js です。また、これはクライアントよりも高いサーバー側の設定が必要で、ClickHouse Cloud では 10 秒に設定されています。

接続が切れる場合や接続プールを使用している場合など、接続を横断して一貫したルーティングを確保するために、ユーザーは同じ接続を使用する（ネイティブの場合は簡単）か、スティッキーエンドポイントの表示を要求することができます。これにより、クラスター内の各ノードのためのエンドポイントのセットが提供され、クライアントがクエリを決定論的にルーティングできるようになります。

> スティッキーエンドポイントへのアクセスについてはサポートにお問い合わせください。

## ClickHouse OSS {#clickhouse-oss}

OSS でこの動作を実現するには、シャードとレプリカのトポロジー、またはクエリのために [Distributed table](/engines/table-engines/special/distributed) を使用しているかどうかに依存します。

シャードとレプリカが1つしかない場合（ClickHouse が垂直スケーリングするため一般的）、ユーザーはクライアント層でノードを選択し、直接レプリカをクエリして、これが決定論的に選択されていることを確認します。

複数のシャードとレプリカを持つトポロジーも、分散テーブルなしで可能ですが、これらの高度なデプロイメントでは通常独自のルーティングインフラストラクチャがあります。したがって、1つ以上のシャードを持つデプロイメントは、分散テーブルを使用していると見なします（分散テーブルは単一シャードのデプロイメントでも使用できますが、通常必要ありません）。

この場合、ユーザーは、プロパティ（例: `session_id` や `user_id`）に基づいて、一貫したノードルーティングが実行されることを確認する必要があります。設定 [`prefer_localhost_replica=0`](/operations/settings/settings#prefer-localhost-replica)、[`load_balancing=in_order`](/operations/settings/settings#load_balancing) を [クエリ内で設定するべきです](/operations/settings/query-level)。これにより、シャードのローカルレプリカが優先され、そうでなければ設定にリストされたレプリカが優先されます - エラーの数が同じであれば、ランダム選択でフェイルオーバーが発生します。 [`load_balancing=nearest_hostname`](/operations/settings/settings#load_balancing) も、この決定論的なシャード選択の代替手段として使用できます。

> Distributed table を作成する際、ユーザーはクラスターを指定します。このクラスター定義は config.xml に指定され、シャード（およびそのレプリカ）をリストします。これにより、各ノードからの使用順序を制御できます。これを使用して、ユーザーは選択が決定論的であることを確認できます。

## 整序一貫性 {#sequential-consistency}

例外的な場合、ユーザーは整序一貫性が必要なことがあります。

データベースにおける整序一貫性とは、データベース上の操作がある整序で実行されるかのように見え、その順序がデータベースと相互作用するすべてのプロセスにおいて一貫性があることを指します。これは、すべての操作が呼び出しと完了の間に瞬時に効果を発揮し、すべての操作から観測される単一の合意された順序が存在することを意味します。

ユーザーの視点から見ると、これは通常、ClickHouse にデータを書き込み、データを読み取る際に最新の挿入行が返されることを保証する必要があるという形で現れます。
これは、いくつかの方法で達成できます（好ましい順序で）：

1. **同じノードへの読み取り/書き込み** - ネイティブプロトコルを使用している場合、または HTTP を介して書き込み/読み込みを行う [セッション](/interfaces/http#default-database)を使用している場合、同じレプリカに接続されるべきです。このシナリオでは、書き込みを行っているノードから直接読み取っているため、常に一貫した読み取りが保証されます。
2. **手動でレプリカを同期** - 一つのレプリカに書き込んで別のレプリカから読み取る場合、読み取り前に `SYSTEM SYNC REPLICA LIGHTWEIGHT` を発行できます。
3. **整序一貫性を有効にする** - クエリ設定 [`select_sequential_consistency = 1`](/operations/settings/settings#select_sequential_consistency) を介して。また、OSS では `insert_quorum = 'auto'` の設定も指定する必要があります。

<br />

これらの設定を有効にする詳細については [こちら](/cloud/reference/shared-merge-tree#consistency)をご覧ください。

> 整序一貫性の使用は、ClickHouse Keeper に対して大きな負荷をかけます。その結果、挿入や読み取りが遅くなる可能性があります。ClickHouse Cloud で主要なテーブルエンジンとして使用されている SharedMergeTree は、整序一貫性 [によるオーバーヘッドが少なく、よりスケール可能](https://cloud/reference/shared-merge-tree#consistency)です。OSS ユーザーは、このアプローチを慎重に使用し、Keeper の負荷を測定する必要があります。

## トランザクショナル（ACID）サポート {#transactional-acid-support}

PostgreSQL から移行してきたユーザーは、その ACID（Atomicity, Consistency, Isolation, Durability）プロパティに対する堅牢なサポートに慣れているかもしれません。これにより、信頼性の高いトランザクショナルデータベースとしての選択肢となっています。PostgreSQL の原子性は、各トランザクションが単一の単位として扱われ、完全に成功するか、完全にロールバックされることを保証し、部分的な更新を防ぎます。一貫性は、すべてのデータベーストランザクションが有効な状態になるように、制約、トリガー、ルールを強制することによって維持されます。隔離レベルは、PostgreSQL でサポートされると、Read Committed から Serializable の間で、同時トランザクションによって行われる変更の可視性を細かく制御できるようになります。最後に、持続性は、書き込み先行ログ（WAL）によって確保され、トランザクションがコミットされると、システム障害が発生してもその状態が維持されます。

これらのプロパティは、真実のソースとして機能する OLTP データベースでは一般的です。

強力ではありますが、これには固有の制限があり、PB スケールでの課題となります。ClickHouse は、高スループットな書き込みを維持しながらスケールでの高速な分析クエリを提供するために、これらのプロパティに妥協しています。

ClickHouse は [制限された構成のもとで](https://guides/developer/transactional) ACID プロパティを提供します - 最も単純なのは、1 つのパーティションを持つ MergeTree テーブルエンジンのレプリケーションされていないインスタンスを使用している場合です。ユーザーは、これらのケース以外でこれらのプロパティを期待せず、それらが必要ではないことを確認する必要があります。

## ClickPipes（PeerDBによる）を用いた Postgres データのレプリケーションまたは移行 {#replicating-or-migrating-postgres-data-with-clickpipes-powered-by-peerdb}

:::info
PeerDB は、ClickHouse Cloud にネイティブに利用可能です - 新しい ClickPipe コネクタを使用したブレイジングファストな Postgres から ClickHouse への CDC - 現在パブリックベータ中です。
:::

[PeerDB](https://www.peerdb.io/) を使用すると、Postgres から ClickHouse へのデータをシームレスにレプリケートできます。このツールを使用して、
1. CDC を使用した継続的なレプリケーションを行い、Postgres と ClickHouse を共存させることができます - Postgres は OLTP 用、ClickHouse は OLAP 用です。 
2. Postgres から ClickHouse への移行を行います。
