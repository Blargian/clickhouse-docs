---
description: 'Таблицы с распределенным движком не хранят собственные данные, но
  позволяют выполнять распределенную обработку запросов на нескольких серверах. Чтение
  автоматически параллелизуется. В процессе чтения используются индексы таблицы на
  удаленных серверах, если таковые имеются.'
sidebar_label: 'Распределенные'
sidebar_position: 10
slug: /engines/table-engines/special/distributed
title: 'Распределенный Двигатель Таблиц'
---


# Распределенный Двигатель Таблиц

:::warning
Чтобы создать распределенный движок таблиц в облаке, вы можете использовать функции таблиц [remote и remoteSecure](../../../sql-reference/table-functions/remote). Синтаксис `Distributed(...)` не может быть использован в ClickHouse Cloud.
:::

Таблицы с распределенным движком не хранят собственные данные, но позволяют выполнять распределенную обработку запросов на нескольких серверах. Чтение автоматически параллелизуется. В процессе чтения используются индексы таблицы на удаленных серверах, если таковые имеются.

## Создание Таблицы {#distributed-creating-a-table}

```sql
CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]
(
    name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1],
    name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2],
    ...
) ENGINE = Distributed(cluster, database, table[, sharding_key[, policy_name]])
[SETTINGS name=value, ...]
```

### Из Таблицы {#distributed-from-a-table}

Когда `Distributed` таблица ссылается на таблицу на текущем сервере, вы можете заимствовать схему этой таблицы:

```sql
CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] AS [db2.]name2 ENGINE = Distributed(cluster, database, table[, sharding_key[, policy_name]]) [SETTINGS name=value, ...]
```

### Параметры Распределения {#distributed-parameters}

#### cluster {#cluster}

`cluster` - имя кластера в конфигурационном файле сервера

#### database {#database}

`database` - имя удаленной базы данных

#### table {#table}

`table` - имя удаленной таблицы

#### sharding_key {#sharding_key}

`sharding_key` - (опционально) ключ шардирования

Указание `sharding_key` необходимо для следующего:

- Для `INSERT` в распределенную таблицу (так как движок таблицы нуждается в `sharding_key`, чтобы определить, как разделить данные). Однако, если включена настройка `insert_distributed_one_random_shard`, то `INSERT` не нуждаются в ключе шардирования.
- Для использования с `optimize_skip_unused_shards`, так как `sharding_key` необходим для определения, какие шард должны быть опрошены.

#### policy_name {#policy_name}

`policy_name` - (опционально) имя политики, будет использовано для хранения временных файлов для фоновой пересылки.

**Смотрите Также**

 - Настройка [distributed_foreground_insert](../../../operations/settings/settings.md#distributed_foreground_insert)
 - [MergeTree](../../../engines/table-engines/mergetree-family/mergetree.md#table_engine-mergetree-multiple-volumes) для примеров

### Настройки Распределения {#distributed-settings}

#### fsync_after_insert {#fsync_after_insert}

`fsync_after_insert` - выполнять `fsync` для данных файла после фоновой вставки в Distributed. Гарантирует, что ОС сбросила все вставленные данные на диск **инициирующего узла**.

#### fsync_directories {#fsync_directories}

`fsync_directories` - выполнять `fsync` для каталогов. Гарантирует, что ОС обновила метаданные каталога после операций, связанных с фоновыми вставками в распределенной таблице (после вставки, после отправки данных на шард и т.д.).

#### skip_unavailable_shards {#skip_unavailable_shards}

`skip_unavailable_shards` - Если истинно, ClickHouse тихо пропускает недоступные шард. Шард отмечается как недоступный, когда: 1) Шард недоступен по причине ошибки соединения. 2) Шард неразрешим через DNS. 3) Таблица не существует на шарде. По умолчанию false.

#### bytes_to_throw_insert {#bytes_to_throw_insert}

`bytes_to_throw_insert` - если более этого количества сжатых байтов будет ожидать фоновой вставки, будет выброшено исключение. 0 - не выбрасывать. По умолчанию 0.

#### bytes_to_delay_insert {#bytes_to_delay_insert}

`bytes_to_delay_insert` - если более этого количества сжатых байтов будет ожидать фоновой вставки, запрос будет задержан. 0 - не задерживать. По умолчанию 0.

#### max_delay_to_insert {#max_delay_to_insert}

`max_delay_to_insert` - максимальная задержка вставки данных в распределенную таблицу в секундах, если имеется много ожидающих байтов для фоновой пересылки. По умолчанию 60.

#### background_insert_batch {#background_insert_batch}

`background_insert_batch` - то же самое, что и [distributed_background_insert_batch](../../../operations/settings/settings.md#distributed_background_insert_batch).

#### background_insert_split_batch_on_failure {#background_insert_split_batch_on_failure}

`background_insert_split_batch_on_failure` - то же самое, что и [distributed_background_insert_split_batch_on_failure](../../../operations/settings/settings.md#distributed_background_insert_split_batch_on_failure).

#### background_insert_sleep_time_ms {#background_insert_sleep_time_ms}

`background_insert_sleep_time_ms` - то же самое, что и [distributed_background_insert_sleep_time_ms](../../../operations/settings/settings.md#distributed_background_insert_sleep_time_ms).

#### background_insert_max_sleep_time_ms {#background_insert_max_sleep_time_ms}

`background_insert_max_sleep_time_ms` - то же самое, что и [distributed_background_insert_max_sleep_time_ms](../../../operations/settings/settings.md#distributed_background_insert_max_sleep_time_ms).

#### flush_on_detach {#flush_on_detach}

`flush_on_detach` - Сбрасывать данные на удаленные узлы при DETACH/DROP/выключении сервера. По умолчанию true.

:::note
**Настройки долговечности** (`fsync_...`):

- Влияют только на фоновые вставки (т.е. `distributed_foreground_insert=false`), когда данные сначала хранятся на диске инициирующего узла, а затем в фоновом режиме отправляются на шарды.
- Могут значительно снизить производительность вставок.
- Влияют на запись данных, хранящихся внутри папки распределенной таблицы, в **узел, который принял вашу вставку**. Если вам нужны гарантии записи данных в подлежащие таблицы MergeTree - смотрите настройки долговечности (`...fsync...`) в `system.merge_tree_settings`.

Для **Настроек предела вставки** (`..._insert`) смотрите также:

- Настройка [distributed_foreground_insert](../../../operations/settings/settings.md#distributed_foreground_insert).
- Настройка [prefer_localhost_replica](/operations/settings/settings#prefer_localhost_replica).
- `bytes_to_throw_insert` обрабатывается перед `bytes_to_delay_insert`, поэтому вы не должны устанавливать его на значение меньше чем `bytes_to_delay_insert`.
:::

**Пример**

```sql
CREATE TABLE hits_all AS hits
ENGINE = Distributed(logs, default, hits[, sharding_key[, policy_name]])
SETTINGS
    fsync_after_insert=0,
    fsync_directories=0;
```

Данные будут читаться со всех серверов в кластере `logs`, из таблицы `default.hits`, расположенной на каждом сервере в кластере. Данные не только считываются, но и частично обрабатываются на удаленных серверах (насколько это возможно). Например, для запроса с `GROUP BY`, данные будут агрегироваться на удаленных серверах, а промежуточные состояния агрегатных функций будут отправлены на сервер-запросчик. Затем данные будут дополнительно агрегированы.

Вместо имени базы данных вы можете использовать постоянное выражение, возвращающее строку. Например: `currentDatabase()`.

## Кластеры {#distributed-clusters}

Кластеры настраиваются в [файле конфигурации сервера](../../../operations/configuration-files.md):

```xml
<remote_servers>
    <logs>
        <!-- Межсерверный секрет для распределенных запросов по кластерам
             по умолчанию: без секрета (аутентификация не будет выполнена)

             Если задан, то распределенные запросы будут проверяться на шард, так как, по меньшей мере:
             - такой кластер должен существовать на шарде,
             - такой кластер должен иметь один и тот же секрет.

             Также (и это более важно), начальный пользователь будет
             использоваться как текущий пользователь для запроса.
        -->
        <!-- <secret></secret> -->
        
        <!-- Опционально. Разрешены ли распределенные DDL-запросы (клауза ON CLUSTER) для этого кластера. По умолчанию: true (разрешено). -->
        <!-- <allow_distributed_ddl_queries>true</allow_distributed_ddl_queries> -->
        
        <shard>
            <!-- Опционально. Вес шарда при записи данных. По умолчанию: 1. -->
            <weight>1</weight>
            <!-- Опционально. Имя шарда. Должно быть непустым и уникальным среди шард в кластере. Если не указано, то будет пустым. -->
            <name>shard_01</name>
            <!-- Опционально. Записывать данные только в одну из реплик. По умолчанию: false (записывать данные во все реплики). -->
            <internal_replication>false</internal_replication>
            <replica>
                <!-- Опционально. Приоритет реплики для балансировки нагрузки (см. также настройку load_balancing). По умолчанию: 1 (меньшее значение имеет больший приоритет). -->
                <priority>1</priority>
                <host>example01-01-1</host>
                <port>9000</port>
            </replica>
            <replica>
                <host>example01-01-2</host>
                <port>9000</port>
            </replica>
        </shard>
        <shard>
            <weight>2</weight>
            <name>shard_02</name>
            <internal_replication>false</internal_replication>
            <replica>
                <host>example01-02-1</host>
                <port>9000</port>
            </replica>
            <replica>
                <host>example01-02-2</host>
                <secure>1</secure>
                <port>9440</port>
            </replica>
        </shard>
    </logs>
</remote_servers>
```

Здесь определяется кластер с именем `logs`, который состоит из двух шард, каждый из которых содержит две реплики. Шарды ссылаются на серверы, содержащие разные части данных (чтобы прочитать все данные, необходимо обратиться ко всем шарам). Реплики - это дублирующие серверы (чтобы прочитать все данные, вы можете получить доступ к данным на любой из реплик).

Имена кластеров не могут содержать точки.

Параметры `host`, `port`, и опционально `user`, `password`, `secure`, `compression` указаны для каждого сервера:

- `host` – адрес удаленного сервера. Вы можете использовать либо доменное имя, либо адрес IPv4 или IPv6. Если вы укажете доменное имя, сервер сделает DNS-запрос при запуске, и результат сохраняется, пока сервер работает. Если DNS-запрос не удался, сервер не запускается. Если вы измените DNS-запись, перезапустите сервер.
- `port` – TCP порт для активного обмена (`tcp_port` в конфигурации, обычно 9000). Не путать с `http_port`.
- `user` – имя пользователя для подключения к удаленному серверу. Значение по умолчанию - пользователь `default`. Этот пользователь должен иметь доступ для подключения к указанному серверу. Доступ настраивается в файле `users.xml`. Для получения дополнительной информации смотрите раздел [Права доступа](../../../guides/sre/user-management/index.md).
- `password` – пароль для подключения к удаленному серверу (не замаскирован). Значение по умолчанию: пустая строка.
- `secure` - использовать ли безопасное SSL/TLS соединение. Обычно также требует указать порт (по умолчанию безопасный порт `9440`). Сервер должен слушать на `<tcp_port_secure>9440</tcp_port_secure>` и быть настроен с правильными сертификатами.
- `compression` - использовать сжатие данных. Значение по умолчанию: `true`.

При указании реплик одна из доступных реплик будет выбрана для каждой из шард при чтении. Вы можете настроить алгоритм балансировки нагрузки (предпочтение, к какой реплике обращаться) – смотрите настройку [load_balancing](../../../operations/settings/settings.md#load_balancing). Если соединение с сервером не установлено, будет совершена попытка подключиться с коротким таймаутом. Если соединение не удалось, будет выбрана следующая реплика, и так далее для всех реплик. Если попытка соединения не удалась для всех реплик, попытка будет повторена таким же образом несколько раз. Это работает в пользу устойчивости, но не обеспечивает полной отказоустойчивости: удаленный сервер может принять соединение, но может не работать или работать плохо.

Вы можете указать только одну из шард (в этом случае обработка запроса должна быть удаленной, а не распределенной) или любое количество шард. В каждой шарде вы можете указать от одной до любого количества реплик. Для каждой шард можно указать различное количество реплик.

Вы можете указать столько кластеров, сколько вам нужно в конфигурации.

Для просмотра ваших кластеров используйте таблицу `system.clusters`.

Движок `Distributed` позволяет работать с кластером так же, как с локальным сервером. Однако конфигурацию кластера нельзя указать динамически, она должна быть настроена в конфигурационном файле сервера. Обычно все серверы в кластере имеют одну и ту же конфигурацию кластера (хотя это не обязательно). Кластеры из конфигурационного файла обновляются на лету, без перезапуска сервера.

Если вам нужно отправлять запрос к неизвестному набору шард и реплик каждый раз, вам не обязательно создавать `Distributed` таблицу – используйте вместо этого функцию таблицы `remote`. Смотрите раздел [Функции таблиц](../../../sql-reference/table-functions/index.md).

## Запись данных {#distributed-writing-data}

Существует два метода записи данных в кластер:

Во-первых, вы можете определить, какие серверы записывают какие данные, и выполнять запись непосредственно на каждом шарде. Иными словами, выполнять прямые `INSERT` команды на удаленных таблицах в кластере, на которые ссылается `Distributed` таблица. Это наиболее гибкое решение, так как вы можете использовать любую схему шардирования, даже ту, которая не является тривиальной из-за требований предметной области. Это также наиболее оптимальное решение, поскольку данные могут записываться в разные шард совершенно независимо.

Во-вторых, вы можете выполнять `INSERT` команды на `Distributed` таблице. В этом случае таблица сама распределит вставленные данные по серверам. Чтобы записать в `Distributed` таблицу, необходимо настроить параметр `sharding_key` (если шардов больше одного).

Каждый шард может иметь указан параметр `<weight>` в конфигурационном файле. По умолчанию вес составляет `1`. Данные распределяются по шард в зависимости от веса шард. Все веса шард складываются, затем вес каждого шарда делится на общий вес, чтобы определить долю каждого шард. Например, если существует два шард, и первый имеет вес 1, в то время как второй имеет вес 2, первый получит одну треть (1 / 3) вставленных строк, а второй получит две трети (2 / 3).

Каждый шард может иметь параметр `internal_replication`, указанный в конфигурационном файле. Если этот параметр установлен в `true`, операция записи выбирает первую здоровую реплику и записывает данные в нее. Используйте это, если таблицы, лежащие в основе `Distributed` таблицы, являются реплицированными таблицами (например, любые из движков таблиц `Replicated*MergeTree`). Одна из реплик таблицы примет запись, и она будет автоматически реплицирована на другие реплики.

Если `internal_replication` установлен в `false` (по умолчанию), данные записываются во все реплики. В этом случае таблица `Distributed` сама реплицирует данные. Это хуже, чем использовать реплицированные таблицы, поскольку консистентность реплик не проверяется и со временем они будут содержать немного разные данные.

Чтобы выбрать шар, к которому будет отправлена строка данных, анализируется выражение шардирования, и берется его остаток от деления на общий вес шард. Строка отправляется в шард, который соответствует полупромежутку остатков от `prev_weights` до `prev_weights + weight`, где `prev_weights` - это общий вес шард с наименьшими номерами, а `weight` - это вес этого шарда. Например, если есть два шард, и первый имеет вес 9, а второй - 10, строка будет отправлена на первый шард для остатков из диапазона \[0, 9), и на второй для остатков из диапазона \[9, 19).

Выражение шардирования может быть любым выражением из констант и столбцов таблицы, возвращающим целое число. Например, вы можете использовать выражение `rand()` для случайного распределения данных или `UserID` для распределения по остатку от деления идентификатора пользователя (в этом случае данные одного пользователя будут находиться на одном шарде, что упрощает выполнение `IN` и `JOIN` по пользователям). Если один из столбцов не распределен достаточно равномерно, вы можете обернуть его в хеш-функцию, например, `intHash64(UserID)`.

Простое вычисление остатка от деления является ограниченным решением для шардирования и не всегда подходит. Оно эффективно работает для средних и больших объемов данных (десятки серверов), но не для очень больших объемов данных (сотни серверов и более). В последнем случае используйте схему шардирования, необходимую для предметной области, а не записи в `Distributed` таблицах.

Вам следует обратить внимание на схему шардирования в следующих случаях:

- Используются запросы, требующие объединения данных (`IN` или `JOIN`) по конкретному ключу. Если данные шардируются по этому ключу, вы можете использовать локальный `IN` или `JOIN` вместо `GLOBAL IN` или `GLOBAL JOIN`, что гораздо более эффективно.
- Используется большое количество серверов (сотни и более) с большим количеством небольших запросов, например, запросов на данные отдельных клиентов (например, веб-сайтов, рекламодателей или партнеров). Чтобы небольшие запросы не влияли на весь кластер, имеет смысл размещать данные для одного клиента на одном шарде. В качестве альтернативы вы можете настроить двухуровневое шардирование: разделить весь кластер на "слои", состоящие из нескольких шард. Данные для одного клиента расположены на одном слое, но к слою могут добавляться шард, по мере необходимости, и данные распределяются случайным образом внутри них. `Distributed` таблицы создаются для каждого слоя, а одна общая распределенная таблица создается для глобальных запросов.

Данные записываются в фоновом режиме. При вставке в таблицу блок данных просто записывается на локальную файловую систему. Данные отправляются на удаленные серверы в фоновом режиме, как можно скорее. Периодичность отправки данных регулируется настройками [distributed_background_insert_sleep_time_ms](../../../operations/settings/settings.md#distributed_background_insert_sleep_time_ms) и [distributed_background_insert_max_sleep_time_ms](../../../operations/settings/settings.md#distributed_background_insert_max_sleep_time_ms). Движок `Distributed` отправляет каждый файл с вставленными данными отдельно, но вы можете включить пакетную отправку файлов с помощью настройки [distributed_background_insert_batch](../../../operations/settings/settings.md#distributed_background_insert_batch). Эта настройка повышает производительность кластера за счет лучшего использования локальных серверных и сетевых ресурсов. Вы должны проверить, были ли данные успешно отправлены, проверяя список файлов (данные, ожидающие отправки) в каталоге таблицы: `/var/lib/clickhouse/data/database/table/`. Количество потоков, выполняющих фоновые задачи, можно задать с помощью настройки [background_distributed_schedule_pool_size](/operations/server-configuration-parameters/settings#background_distributed_schedule_pool_size).

Если сервер перестал существовать или произошел резкий перезапуск (например, из-за сбоя оборудования) после `INSERT` в `Distributed` таблицу, вставленные данные могут быть потеряны. Если в каталоге таблицы обнаружена поврежденная часть данных, она переносится в подсDirectory `broken` и больше не используется.

## Чтение данных {#distributed-reading-data}

При запросе `Distributed` таблицы запросы `SELECT` отправляются на все шард и работают независимо от того, как данные распределены между шард (они могут быть распределены совершенно случайно). При добавлении нового шарда вам не нужно переносить в него старые данные. Вместо этого вы можете записывать новые данные, используя более тяжелый вес – данные будут распределены немного неравномерно, но запросы будут работать правильно и эффективно.

Когда включена опция `max_parallel_replicas`, обработка запросов параллелизуется по всем репликам внутри одного шарда. Для получения дополнительной информации смотрите раздел [max_parallel_replicas](../../../operations/settings/settings.md#max_parallel_replicas).

Чтобы узнать больше о том, как обрабатываются распределенные запросы `in` и `global in`, обратитесь к [этой](/sql-reference/operators/in#distributed-subqueries) документации.

## Виртуальные Колонки {#virtual-columns}

#### _shard_num {#_shard_num}

`_shard_num` — Содержит значение `shard_num` из таблицы `system.clusters`. Тип: [UInt32](../../../sql-reference/data-types/int-uint.md).

:::note
Поскольку функции таблиц [remote](../../../sql-reference/table-functions/remote.md) и [cluster](../../../sql-reference/table-functions/cluster.md) внутренне создают временную распределенную таблицу, `_shard_num` доступен и там.
:::

**Смотрите Также**

- Описание [Виртуальных колонок](../../../engines/table-engines/index.md#table_engines-virtual_columns)
- Настройка [background_distributed_schedule_pool_size](/operations/server-configuration-parameters/settings#background_distributed_schedule_pool_size)
- Функции [shardNum()](../../../sql-reference/functions/other-functions.md#shardnum) и [shardCount()](../../../sql-reference/functions/other-functions.md#shardcount)
