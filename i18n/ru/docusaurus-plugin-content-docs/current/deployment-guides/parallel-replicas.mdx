---
slug: /deployment-guides/parallel-replicas
title: 'Параллельные Реплики'
keywords: ['параллельная реплика']
description: 'В этом руководстве мы сначала обсудим, как ClickHouse распределяет запрос через несколько шардов с помощью распределенных таблиц, а затем, как запрос может использовать несколько реплик для выполнения.'
---

import Image from '@theme/IdealImage';
import BetaBadge from '@theme/badges/BetaBadge';
import image_1 from '@site/static/images/deployment-guides/parallel-replicas-1.png'
import image_2 from '@site/static/images/deployment-guides/parallel-replicas-2.png'
import image_3 from '@site/static/images/deployment-guides/parallel-replicas-3.png'
import image_4 from '@site/static/images/deployment-guides/parallel-replicas-4.png'
import image_5 from '@site/static/images/deployment-guides/parallel-replicas-5.png'
import image_6 from '@site/static/images/deployment-guides/parallel-replicas-6.png'
import image_7 from '@site/static/images/deployment-guides/parallel-replicas-7.png'
import image_8 from '@site/static/images/deployment-guides/parallel-replicas-8.png'
import image_9 from '@site/static/images/deployment-guides/parallel-replicas-9.png'

<BetaBadge/>
## Введение {#introduction}

ClickHouse обрабатывает запросы крайне быстро, но как эти запросы распределяются и параллелизируются между несколькими серверами? 

> В этом руководстве мы сначала обсудим, как ClickHouse распределяет запрос между
несколькими шарами с помощью распределенных таблиц, а затем, как запрос может использовать 
несколько реплик для его выполнения.
## Шардированная архитектура {#sharded-architecture}

В архитектуре с разделением ничего, кластеры обычно делятся на
несколько шардов, при этом каждый шард содержит подмножество общей информации. Распределенная таблица существует поверх этих шардов, обеспечивая единый вид на
полные данные.

Чтения могут быть отправлены на локальную таблицу. Выполнение запроса будет происходить только
на указанном шарде или может быть отправлено в распределенную таблицу, и в этом
случае каждый шард выполнит данные запросы. Сервер, на который был выполнен запрос к распределенной
таблице, агрегирует данные и отвечает клиенту:

<Image img={image_1} size="md" alt="шардированная архитектура" />

На рисунке выше визуализируется, что происходит, когда клиент запрашивает распределенную таблицу:

<ol className="docs-ordered-list">
    <li>
        Запрос SELECT отправляется на распределенную таблицу на узле произвольно
        (через стратегию кругового распределения или после маршрутизации на конкретный сервер 
        балансировщиком нагрузки). Этот узел будет выступать в роли координатора.
    </li>
    <li>
        Узел найдет каждый шард, который необходимо выполнить для запроса
        на основе информации, указанной в распределенной таблице, и запрос отправляется
        на каждый шард.
    </li>
    <li>
        Каждый шард читает, фильтрует и аггрегирует данные локально и затем
        отправляет обратно объединяемое состояние координатору.
    </li>
    <li>
        Координирующий узел объединяет данные и затем отправляет ответ обратно
        клиенту.
    </li>
</ol>

Когда мы добавляем реплики, процесс довольно похож, единственное
отличие в том, что только одна реплика из каждого шара будет выполнять запрос.
Это означает, что больше запросов может быть обработано параллельно.
## Не шардированная архитектура {#non-sharded-architecture}

ClickHouse Cloud имеет совершенно другую архитектуру по сравнению с представленной выше.
(См. ["Архитектура ClickHouse Cloud"](https://clickhouse.com/docs/cloud/reference/architecture)
для получения дополнительной информации). При разделении вычислений и хранения, и с практически бесконечным 
объемом хранения необходимость в шардировании становится менее важной.

На рисунке ниже показана архитектура ClickHouse Cloud:

<Image img={image_2} size="md" alt="не шардированная архитектура" />

Эта архитектура позволяет нам добавлять и удалять реплики почти
мгновенно, что обеспечивает очень высокую масштабируемость кластера. Кластер ClickHouse
Keeper (показан справа) гарантирует, что у нас есть единый источник правды для метаданных. 
Реплики могут извлекать метаданные из кластера ClickHouse Keeper и все поддерживают одинаковые данные. 
Сами данные хранятся в объектном хранилище, и кэш SSD позволяет ускорить запросы.

Но как теперь мы можем распределять выполнение запросов между несколькими серверами? В 
шардированной архитектуре это было довольно очевидно, поскольку каждый шард мог фактически
выполнить запрос на подмножестве данных. Как это работает, когда нет шардирования?
## Введение в параллельные реплики {#introducing-parallel-replicas}

Чтобы параллелизовать выполнение запросов через несколько серверов, нам сначала нужно
назначить один из наших серверов координатором. Координатор создает список задач, которые необходимо выполнить, 
обеспечивает их выполнение, агрегацию и то, чтобы результат был возвращен клиенту. Как
и в большинстве распределенных систем, эту роль выполняет узел, который получает 
начальный запрос. Нам также нужно определить единицу работы. В шардированной архитектуре
единицей работы является шард, подмножество данных. С параллельными репликами мы будем использовать 
небольшую часть таблицы, называемую [гранулами](/docs/guides/best-practices/sparse-primary-indexes#data-is-organized-into-granules-for-parallel-data-processing),
в качестве единицы работы.

Теперь давайте рассмотрим, как это работает на практике с помощью рисунка ниже:

<Image img={image_3} size="md" alt="Параллельные реплики" />

С параллельными репликами:

<ol className="docs-ordered-list">
    <li>
        Запрос от клиента отправляется на один узел после прохождения через
        балансировщик нагрузки. Этот узел становится координатором для этого запроса.
    </li>
    <li>
        Узел анализирует индекс каждой части и выбирает нужные части и
        гранулы для обработки.
    </li>
    <li>
        Координатор разбивает рабочую нагрузку на набор гранул, которые можно
        назначить разным репликам.
    </li>
    <li>
        Каждый набор гранул обрабатывается соответствующими репликами, и 
        объединяемое состояние отправляется координатору, когда они заканчивают.
    </li>
    <li>
        Наконец, координатор объединяет все результаты от реплик и
        затем возвращает ответ клиенту.
    </li>
</ol>

Шаги выше описывают, как работают параллельные реплики в теории.
Однако на практике существует множество факторов, которые могут помешать такой логике 
работать идеально:

<ol className="docs-ordered-list">
    <li>
        Некоторые реплики могут быть недоступны.
    </li>
    <li>
        Репликация в ClickHouse асинхронная, некоторые реплики могут не
        иметь одинаковых частей в определенный момент времени.
    </li>
    <li>
        Необходимо каким-то образом управлять задержкой между репликами.
    </li>
    <li>
        Кэш файловой системы варьируется от реплики к реплике в зависимости от 
        активности на каждой реплике, что означает, что случайное назначение задач может 
        привести к менее оптимальной производительности ввиду локальности кэша.
    </li>
</ol>

Мы изучим, как преодолеваются эти факторы в следующих разделах.
### Объявления {#announcements}

Чтобы решить (1) и (2) из списка выше, мы ввели концепцию объявления. Давайте визуализируем, как это работает, 
используя рисунок ниже:

<Image img={image_4} size="md" alt="Объявления" />

<ol className="docs-ordered-list">
    <li>
        Запрос от клиента отправляется на один узел после прохождения через
        балансировщик нагрузки. Узел становится координатором для этого запроса.
    </li>
    <li>
        Координирующий узел отправляет запрос, чтобы получить объявления 
        от всех реплик в кластере. Реплики могут иметь несколько разные
        представления текущего набора частей для таблицы. В результате нам необходимо собрать эту информацию, чтобы избежать неправильных решений о распределении задач.
    </li>
    <li>
        Координирующий узел затем использует объявления, чтобы определить набор 
        гранул, которые могут быть назначены различным репликам. Например, здесь
        мы видим, что ни одна гранула из части 3 не была назначена реплике 2,
        потому что эта реплика не предоставила эту часть в своем объявлении.
        Также обратите внимание, что задачи не были назначены реплике 3, потому что 
        реплика не предоставила объявление.
    </li>
    <li>
        После того как каждая реплика обработала запрос на своем подмножестве гранул
        и объединяемое состояние было отправлено обратно координатору, координатор объединяет результаты, и ответ 
        отправляется клиенту.
    </li>
</ol>
### Динамическое координирование {#dynamic-coordination}

Чтобы решить проблему задержки, мы добавили динамическое координирование. Это означает,
что все гранулы не отправляются реплике за один запрос, но каждая реплика
сможет запросить новую задачу (набор гранул для обработки) у
координатора. Координатор предоставит реплике набор гранул на основе
полученного объявления.

Предположим, что мы находимся на этапе процесса, где все реплики отправили
объявление со всеми частями.

На рисунке ниже визуализируется, как работает динамическое координирование:

<Image img={image_5} size="md" alt="Динамическое координирование - часть 1" />

<ol className="docs-ordered-list">
    <li>
        Реплики дают знать узлу-координатору, что они способны обрабатывать
        задачи, они также могут указать, сколько работы они могут обработать.
    </li>
    <li>
        Координатор назначает задачи репликам.
    </li>
</ol>

<Image img={image_6} size="md" alt="Динамическое координирование - часть 2" />

<ol className="docs-ordered-list">
    <li>
        Реплики 1 и 2 могут очень быстро завершить свою задачу. Они
        запросит новые задачи у узла-координатора.
    </li>
    <li>
        Координатор назначает новые задачи реплике 1 и 2.
    </li>
</ol>

<Image img={image_7} size="md" alt="Динамическое координирование - часть 3" />

<ol className="docs-ordered-list">
    <li>
        Все реплики теперь завершили обработку своей задачи. Они
        запрашивают больше задач.
    </li>
    <li>
        Координатор, используя объявления, проверяет, какие задачи 
        остаются для обработки, но не осталось никаких оставшихся задач.
    </li>
    <li>
        Координатор сообщает репликам, что все было обработано.
        Он теперь объединит все объединяемые состояния и ответит на запрос.
    </li>
</ol>
### Управление локальностью кэша {#managing-cache-locality}

Последней потенциальной проблемой является то, как мы управляем локальностью кэша. Если запрос
выполняется несколько раз, как мы можем гарантировать, что одна и та же задача будет направлена на
одну и ту же реплику? В предыдущем примере мы имели следующие назначенные задачи:

<table>
    <thead>
        <tr>
            <th></th>
            <th>Реплика 1</th>
            <th>Реплика 2</th>
            <th>Реплика 3</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Часть 1</td>
            <td>g1, g6, g7</td>
            <td>g2, g4, g5</td>
            <td>g3</td>
        </tr>
        <tr>
            <td>Часть 2</td>
            <td>g1</td>
            <td>g2, g4, g5</td>
            <td>g3</td>
        </tr>
        <tr>
            <td>Часть 3</td>
            <td>g1, g6</td>
            <td>g2, g4, g5</td>
            <td>g3</td>
        </tr>
    </tbody>
</table>

Чтобы гарантировать, что одни и те же задачи назначаются одними и теми же репликам и могут
воспользоваться кэшом, происходит две вещи. Вычисляется хеш части + набор гранул (задача). 
Применяется деление по модулю на количество реплик для назначения задач.

На бумаге это звучит хорошо, но на деле внезапная нагрузка на одну реплику или
деградация сети могут вызвать задержку, если одна и та же реплика постоянно используется
для выполнения определенных задач. Если `max_parallel_replicas` меньше
числа реплик, то случайные реплики выбираются для выполнения запросов.
### Похищение задач {#task-stealing}

Если какая-либо реплика обрабатывает задачи медленнее, чем другие, другие реплики попытаются
'похитить' задачи, которые в принципе принадлежат этой реплике по хешу, чтобы уменьшить
задержку.
### Ограничения {#limitations}

У этой функции есть известные ограничения, из которых основные задокументированы в
этом разделе.

:::note
Если вы обнаружите проблему, которая не является одной из нижеперечисленных
ограничений и подозреваете, что параллельные реплики являются причиной, пожалуйста, создайте проблему на GitHub, используя метку `comp-parallel-replicas`.
:::

| Ограничение                                    | Описание                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
|-----------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Сложные запросы                                 | В настоящее время параллельные реплики довольно хорошо работают для простых запросов. Слои сложности, такие как CTE, подзапросы, JOIN, не плоский запрос и т. д., могут негативно повлиять на производительность запроса.                                                                                                                                                                                                                                                                                   |
| Малые запросы                                 | Если вы выполняете запрос, который не обрабатывает много строк, выполнение его на нескольких репликах может не дать лучшего времени выполнения, учитывая, что время сети для координации между репликами может привести к дополнительным циклам в выполнении запроса. Вы можете ограничить эти проблемы, используя настройку: [`parallel_replicas_min_number_of_rows_per_replica`](/docs/operations/settings/settings#parallel_replicas_min_number_of_rows_per_replica).  |
| Параллельные реплики отключены с FINAL      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| Данные с высокой кардинальностью и сложная агрегация | Высокая кардинальность агрегации, требующая передачи больших объемов данных, может значительно замедлить ваши запросы.                                                                                                                                                                                                                                                                                                                                                                     |
| Совместимость с новым анализатором           | Новый анализатор может значительно замедлить или ускорить выполнение запроса в конкретных сценариях.                                                                                                                                                                                                                                                                                                                                                                       |
## Настройки, связанные с параллельными репликами {#settings-related-to-parallel-replicas}

| Настройка                                            | Описание                                                                                                                                                                                                                                                         |
|----------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `enable_parallel_replicas`                         | `0`: отключено<br/> `1`: включено <br/>`2`: Принудительное использование параллельной реплики, вызовет исключение, если не используется.                                                                                                                                                          |
| `cluster_for_parallel_replicas`                    | Имя кластера для использования для параллельной репликации; если вы используете ClickHouse Cloud, используйте `default`.                                                                                                                                                                 |
| `max_parallel_replicas`                            | Максимальное количество реплик, которые можно использовать для выполнения запроса на нескольких репликах, если указано число меньшего, чем количество реплик в кластере, узлы будут выбираться случайным образом. Это значение также может быть превышено для учета горизонтального масштабирования. |
| `parallel_replicas_min_number_of_rows_per_replica` | Помогает ограничить количество используемых реплик на основе числа строк, которые необходимо обработать; количество используемых реплик определяется: <br/> `предполагаемые строки для чтения` / `min_number_of_rows_per_replica`.                                                               |
| `allow_experimental_analyzer`                      | `0`: используйте старый анализатор<br/> `1`: используйте новый анализатор. <br/><br/>Поведение параллельных реплик может изменяться в зависимости от используемого анализатора.                                                                                                                                    |
## Исследование проблем с параллельными репликами {#investigating-issues-with-parallel-replicas}

Вы можете проверить, какие настройки используются для каждого запроса в таблице 
[`system.query_log`](/docs/operations/system-tables/query_log). Также вы можете 
ознакомиться с таблицей [`system.events`](/docs/operations/system-tables/events), 
чтобы увидеть все события, которые произошли на сервере, и можете использовать 
табличную функцию [`clusterAllReplicas`](/docs/sql-reference/table-functions/cluster) для просмотра таблиц на всех репликах 
(если вы пользователь облака, используйте `default`).

```sql title="Запрос"
SELECT
   hostname(),
   *
FROM clusterAllReplicas('default', system.events)
WHERE event ILIKE '%ParallelReplicas%'
```
<details>
<summary>Ответ</summary>
```response title="Ответ"
┌─hostname()───────────────────────┬─event──────────────────────────────────────────┬─value─┬─description──────────────────────────────────────────────────────────────────────────────────────────┐
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasHandleRequestMicroseconds      │   438 │ Время, затраченное на обработку запросов на отметки от реплик                                               │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasHandleAnnouncementMicroseconds │   558 │ Время, затраченное на обработку объявлений реплик                                                         │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasReadUnassignedMarks            │   240 │ Сумма по всем репликам: количество назначенных отметок                                                   │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasReadAssignedForStealingMarks   │     4 │ Сумма по всем репликам: количество назначенных для кражи отметок по согласованному хешу                    │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasStealingByHashMicroseconds     │     5 │ Время, затраченное на сбор сегментов, предназначенных для кражи по хешу                                    │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasProcessingPartsMicroseconds    │     5 │ Время, затраченное на обработку частей данных                                                             │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasStealingLeftoversMicroseconds  │     3 │ Время, затраченное на сбор сиротливых сегментов                                                           │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasUsedCount                      │     2 │ Количество реплик, использованных для выполнения запроса с параллельными репликами на основе задач         │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasAvailableCount                 │     6 │ Количество реплик, доступных для выполнения запроса с параллельными репликами на основе задач              │
└──────────────────────────────────┴────────────────────────────────────────────────┴───────┴──────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─hostname()───────────────────────┬─event──────────────────────────────────────────┬─value─┬─description──────────────────────────────────────────────────────────────────────────────────────────┐
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasHandleRequestMicroseconds      │   698 │ Время, затраченное на обработку запросов на отметки от реплик                                               │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasHandleAnnouncementMicroseconds │   644 │ Время, затраченное на обработку объявлений реплик                                                         │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasReadUnassignedMarks            │   190 │ Сумма по всем репликам: количество назначенных отметок                                                   │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasReadAssignedForStealingMarks   │    54 │ Сумма по всем репликам: количество назначенных для кражи отметок по согласованному хешу                    │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasStealingByHashMicroseconds     │     8 │ Время, затраченное на сбор сегментов, предназначенных для кражи по хешу                                    │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasProcessingPartsMicroseconds    │     4 │ Время, затраченное на обработку частей данных                                                             │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasStealingLeftoversMicroseconds  │     2 │ Время, затраченное на сбор сиротливых сегментов                                                           │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasUsedCount                      │     2 │ Количество реплик, использованных для выполнения запроса с параллельными репликами на основе задач         │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasAvailableCount                 │     6 │ Количество реплик, доступных для выполнения запроса с параллельными репликами на основе задач              │
└──────────────────────────────────┴────────────────────────────────────────────────┴───────┴──────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─hostname()───────────────────────┬─event──────────────────────────────────────────┬─value─┬─description──────────────────────────────────────────────────────────────────────────────────────────┐
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasHandleRequestMicroseconds      │   620 │ Время, затраченное на обработку запросов на отметки от реплик                                               │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasHandleAnnouncementMicroseconds │   656 │ Время, затраченное на обработку объявлений реплик                                                         │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasReadUnassignedMarks            │     1 │ Сумма по всем репликам: количество назначенных отметок                                                   │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasReadAssignedForStealingMarks   │     1 │ Сумма по всем репликам: количество назначенных для кражи отметок по согласованному хешу                    │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasStealingByHashMicroseconds     │     4 │ Время, затраченное на сбор сегментов, предназначенных для кражи по хешу                                    │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasProcessingPartsMicroseconds    │     3 │ Время, затраченное на обработку частей данных                                                             │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasStealingLeftoversMicroseconds  │     1 │ Время, затраченное на сбор сиротливых сегментов                                                           │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasUsedCount                      │     2 │ Количество реплик, использованных для выполнения запроса с параллельными репликами на основе задач         │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasAvailableCount                 │    12 │ Количество реплик, доступных для выполнения запроса с параллельными репликами на основе задач              │
└──────────────────────────────────┴────────────────────────────────────────────────┴───────┴──────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─hostname()───────────────────────┬─event──────────────────────────────────────────┬─value─┬─description──────────────────────────────────────────────────────────────────────────────────────────┐
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasHandleRequestMicroseconds      │   696 │ Время, затраченное на обработку запросов на отметки от реплик                                               │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasHandleAnnouncementMicroseconds │   717 │ Время, затраченное на обработку объявлений реплик                                                         │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasReadUnassignedMarks            │     2 │ Сумма по всем репликам: количество назначенных отметок                                                   │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasReadAssignedForStealingMarks   │     2 │ Сумма по всем репликам: количество назначенных для кражи отметок по согласованному хешу                    │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasStealingByHashMicroseconds     │    10 │ Время, затраченное на сбор сегментов, предназначенных для кражи по хешу                                    │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasProcessingPartsMicroseconds    │     6 │ Время, затраченное на обработку частей данных                                                             │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasStealingLeftoversMicroseconds  │     2 │ Время, затраченное на сбор сиротливых сегментов                                                           │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasUsedCount                      │     2 │ Количество реплик, использованных для выполнения запроса с параллельными репликами на основе задач         │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasAvailableCount                 │    12 │ Количество реплик, доступных для выполнения запроса с параллельными репликами на основе задач              │
└──────────────────────────────────┴────────────────────────────────────────────────┴───────┴──────────────────────────────────────────────────────────────────────────────────────────────────────┘
```
</details>

Таблица [`system.text_log`](/docs/operations/system-tables/text_log) также 
содержит информацию об исполнении запросов с использованием параллельных реплик:

```sql title="Запрос"
SELECT message
FROM clusterAllReplicas('default', system.text_log)
WHERE query_id = 'ad40c712-d25d-45c4-b1a1-a28ba8d4019c'
ORDER BY event_time_microseconds ASC
```

<details>
<summary>Ответ</summary>
```response title="Ответ"
┌─message────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ (from 54.218.178.249:59198) SELECT * FROM session_events WHERE type='type2' LIMIT 10 SETTINGS allow_experimental_parallel_reading_from_replicas=2; (stage: Complete)                                                                                       │
│ Запрос SELECT __table1.clientId AS clientId, __table1.sessionId AS sessionId, __table1.pageId AS pageId, __table1.timestamp AS timestamp, __table1.type AS type FROM default.session_events AS __table1 WHERE __table1.type = 'type2' LIMIT _CAST(10, 'UInt64') SETTINGS allow_experimental_parallel_reading_from_replicas = 2 до стадии Complete │
│ Доступ разрешен: SELECT(clientId, sessionId, pageId, timestamp, type) ON default.session_events                                                                                                                                                             │
│ Запрос SELECT __table1.clientId AS clientId, __table1.sessionId AS sessionId, __table1.pageId AS pageId, __table1.timestamp AS timestamp, __table1.type AS type FROM default.session_events AS __table1 WHERE __table1.type = 'type2' LIMIT _CAST(10, 'UInt64') до стадии WithMergeableState только анализ │
│ Доступ разрешен: SELECT(clientId, sessionId, pageId, timestamp, type) ON default.session_events                                                                                                                                                             │
│ Запрос SELECT __table1.clientId AS clientId, __table1.sessionId AS sessionId, __table1.pageId AS pageId, __table1.timestamp AS timestamp, __table1.type AS type FROM default.session_events AS __table1 WHERE __table1.type = 'type2' LIMIT _CAST(10, 'UInt64') из стадии FetchColumns к стадии WithMergeableState только анализ │
│ Запрос SELECT __table1.clientId AS clientId, __table1.sessionId AS sessionId, __table1.pageId AS pageId, __table1.timestamp AS timestamp, __table1.type AS type FROM default.session_events AS __table1 WHERE __table1.type = 'type2' LIMIT _CAST(10, 'UInt64') SETTINGS allow_experimental_parallel_reading_from_replicas = 2 до стадии WithMergeableState только анализ │
│ Доступ разрешен: SELECT(clientId, sessionId, pageId, timestamp, type) ON default.session_events                                                                                                                                                             │
│ Запрос SELECT __table1.clientId AS clientId, __table1.sessionId AS sessionId, __table1.pageId AS pageId, __table1.timestamp AS timestamp, __table1.type AS type FROM default.session_events AS __table1 WHERE __table1.type = 'type2' LIMIT _CAST(10, 'UInt64') SETTINGS allow_experimental_parallel_reading_from_replicas = 2 из стадии FetchColumns к стадии WithMergeableState только анализ │
│ Запрос SELECT __table1.clientId AS clientId, __table1.sessionId AS sessionId, __table1.pageId AS pageId, __table1.timestamp AS timestamp, __table1.type AS type FROM default.session_events AS __table1 WHERE __table1.type = 'type2' LIMIT _CAST(10, 'UInt64') SETTINGS allow_experimental_parallel_reading_from_replicas = 2 из стадии WithMergeableState к стадии Complete │
│ Запрашиваемое количество реплик (100) больше, чем фактическое количество доступных в кластере (6). Будет использоваться последнее число для выполнения запроса.                                                                                                       │
│ Начальный запрос от реплики 4: 2 части: [part all_0_2_1 с диапазонами [(0, 182)], part all_3_3_0 с диапазонами [(0, 62)]]----------
Получено от 4 реплики
                                                                                                   │
│ Состояние чтения полностью инициализировано: часть all_0_2_1 с диапазонами [(0, 182)] в репликах [4]; часть all_3_3_0 с диапазонами [(0, 62)] в репликах [4]                                                                                                            │
│ Отправлены первоначальные запросы: 1 количество реплик: 6                                                                                                                                                                                                                 │
│ Начальный запрос от реплики 2: 2 части: [part all_0_2_1 с диапазонами [(0, 182)], part all_3_3_0 с диапазонами [(0, 62)]]----------
Получено от 2 реплики
                                                                                                   │
│ Отправлены первоначальные запросы: 2 количество реплик: 6                                                                                                                                                                                                                 │
│ Обрабатывается запрос от реплики 4, минимальный размер отметок 240                                                                                                                                                                                                 │
│ Буду отвечать реплике 4 с 1 частью: [part all_0_2_1 с диапазонами [(128, 182)]]. Завершить: false; mine_marks=0, stolen_by_hash=54, stolen_rest=0                                                                                                       │
│ Начальный запрос от реплики 1: 2 части: [part all_0_2_1 с диапазонами [(0, 182)], part all_3_3_0 с диапазонами [(0, 62)]]----------
Получено от 1 реплики
                                                                                                   │
│ Отправлены первоначальные запросы: 3 количество реплик: 6                                                                                                                                                                                                                 │
│ Обрабатывается запрос от реплики 4, минимальный размер отметок 240                                                                                                                                                                                                 │
│ Буду отвечать реплике 4 с 2 частями: [part all_0_2_1 с диапазонами [(0, 128)], part all_3_3_0 с диапазонами [(0, 62)]]. Завершить: false; mine_marks=0, stolen_by_hash=0, stolen_rest=190                                                                  │
│ Начальный запрос от реплики 0: 2 части: [part all_0_2_1 с диапазонами [(0, 182)], part all_3_3_0 с диапазонами [(0, 62)]]----------
Получено от 0 реплики
                                                                                                   │
│ Отправлены первоначальные запросы: 4 количество реплик: 6                                                                                                                                                                                                                 │
│ Начальный запрос от реплики 5: 2 части: [part all_0_2_1 с диапазонами [(0, 182)], part all_3_3_0 с диапазонами [(0, 62)]]----------
Получено от 5 реплики
                                                                                                   │
│ Отправлены первоначальные запросы: 5 количество реплик: 6                                                                                                                                                                                                                 │
│ Обрабатывается запрос от реплики 2, минимальный размер отметок 240                                                                                                                                                                                                 │
│ Буду отвечать реплике 2 с 0 частями: []. Завершить: true; mine_marks=0, stolen_by_hash=0, stolen_rest=0                                                                                                                                                │
│ Начальный запрос от реплики 3: 2 части: [part all_0_2_1 с диапазонами [(0, 182)], part all_3_3_0 с диапазонами [(0, 62)]]----------
Получено от 3 реплики
                                                                                                   │
│ Отправлены первоначальные запросы: 6 количество реплик: 6                                                                                                                                                                                                                 │
│ Всего строк для чтения: 2000000                                                                                                                                                                                                                                │
│ Обрабатывается запрос от реплики 5, минимальный размер отметок 240                                                                                                                                                                                                 │
│ Буду отвечать реплике 5 с 0 частями: []. Завершить: true; mine_marks=0, stolen_by_hash=0, stolen_rest=0                                                                                                                                                │
│ Обрабатывается запрос от реплики 0, минимальный размер отметок 240                                                                                                                                                                                                 │
│ Буду отвечать реплике 0 с 0 частями: []. Завершить: true; mine_marks=0, stolen_by_hash=0, stolen_rest=0                                                                                                                                                │
│ Обрабатывается запрос от реплики 1, минимальный размер отметок 240                                                                                                                                                                                                 │
│ Буду отвечать реплике 1 с 0 частями: []. Завершить: true; mine_marks=0, stolen_by_hash=0, stolen_rest=0                                                                                                                                                │
│ Обрабатывается запрос от реплики 3, минимальный размер отметок 240                                                                                                                                                                                                 │
│ Буду отвечать реплике 3 с 0 частями: []. Завершить: true; mine_marks=0, stolen_by_hash=0, stolen_rest=0                                                                                                                                                │
│ (c-crimson-vd-86-server-rdhnsx3-0.c-crimson-vd-86-server-headless.ns-crimson-vd-86.svc.cluster.local:9000) Отмена запроса, так как достаточное количество данных было прочитано                                                                                              │
│ Прочитано 81920 строк, 5.16 MiB за 0.013166 сек., 6222087.194288318 строк/сек., 391.63 Мб/сек.                                                                                                                                                                   │
│ Координация завершена: Статистика: реплика 0 - {запросы: 2 отметки: 0 назначенные мне: 0 украденные по хешу: 0 не назначенные: 0}; реплика 1 - {запросы: 2 отметки: 0 назначенные мне: 0 украденные по хешу: 0 не назначенные: 0}; реплика 2 - {запросы: 2 отметки: 0 назначенные мне: 0 украденные по хешу: 0 не назначенные: 0}; реплика 3 - {запросы: 2 отметки: 0 назначенные мне: 0 украденные по хешу: 0 не назначенные: 0}; реплика 4 - {запросы: 3 отметки: 244 назначенные мне: 0 украденные по хешу: 54 не назначенные: 190}; реплика 5 - {запросы: 2 отметки: 0 назначенные мне: 0 украденные по хешу: 0 не назначенные: 0} │
│ Пиковое использование памяти (для запроса): 1.81 MiB.                                                                                                                                                                                                                   │
│ Обработано за 0.024095586 сек.                                                                                                                                                                                                                              │
└────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
```
</details>

Наконец, вы также можете использовать `EXPLAIN PIPELINE`. Он демонстрирует, как ClickHouse 
будет выполнять запрос и какие ресурсы будут использоваться для 
выполнения запроса. Рассмотрим следующий запрос в качестве примера:

```sql
SELECT count(), uniq(pageId) , min(timestamp), max(timestamp) 
FROM session_events 
WHERE type='type3' 
GROUP BY toYear(timestamp) LIMIT 10
```

Давайте посмотрим на конвейер запроса без параллельной реплики:

```sql title="EXPLAIN PIPELINE (без параллельной реплики)"
EXPLAIN PIPELINE graph = 1, compact = 0 
SELECT count(), uniq(pageId) , min(timestamp), max(timestamp) 
FROM session_events 
WHERE type='type3' 
GROUP BY toYear(timestamp) 
LIMIT 10 
SETTINGS allow_experimental_parallel_reading_from_replicas=0 
FORMAT TSV;
```

<Image img={image_8} size="lg" alt="EXPLAIN без parallel_replica" />

А теперь с параллельной репликой:

```sql title="EXPLAIN PIPELINE (с параллельной репликой)"
EXPLAIN PIPELINE graph = 1, compact = 0 
SELECT count(), uniq(pageId) , min(timestamp), max(timestamp) 
FROM session_events 
WHERE type='type3' 
GROUP BY toYear(timestamp) 
LIMIT 10 
SETTINGS allow_experimental_parallel_reading_from_replicas=2 
FORMAT TSV;
```

<Image img={image_9} size="lg" alt="EXPLAIN с parallel_replica"/>
