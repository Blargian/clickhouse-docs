---
description: 'Этот движок обеспечивает интеграцию с экосистемой Amazon S3. Похож на движок HDFS, но предлагает специфические функции для S3.'
sidebar_label: 'S3'
sidebar_position: 180
slug: /engines/table-engines/integrations/s3
title: 'Движок таблиц S3'
---


# Движок таблиц S3

Этот движок обеспечивает интеграцию с экосистемой [Amazon S3](https://aws.amazon.com/s3/). Этот движок похож на движок [HDFS](/engines/table-engines/integrations/hdfs), но предлагает специфические функции для S3.

## Пример {#example}

```sql
CREATE TABLE s3_engine_table (name String, value UInt32)
    ENGINE=S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/test-data.csv.gz', 'CSV', 'gzip')
    SETTINGS input_format_with_names_use_header = 0;

INSERT INTO s3_engine_table VALUES ('one', 1), ('two', 2), ('three', 3);

SELECT * FROM s3_engine_table LIMIT 2;
```

```text
┌─name─┬─value─┐
│ one  │     1 │
│ two  │     2 │
└──────┴───────┘
```

## Создание таблицы {#creating-a-table}

```sql
CREATE TABLE s3_engine_table (name String, value UInt32)
    ENGINE = S3(path [, NOSIGN | aws_access_key_id, aws_secret_access_key,] format, [compression])
    [PARTITION BY expr]
    [SETTINGS ...]
```

### Параметры движка {#parameters}

- `path` — URL бакета с путем к файлу. Поддерживает следующие символы подстановки в режиме только для чтения: `*`, `**`, `?`, `{abc,def}` и `{N..M}`, где `N`, `M` — числа, `'abc'`, `'def'` — строки. Для получения дополнительной информации смотрите [ниже](#wildcards-in-path).
- `NOSIGN` - Если это ключевое слово указано вместо учетных данных, все запросы не будут подписаны.
- `format` — [формат](/sql-reference/formats#formats-overview) файла.
- `aws_access_key_id`, `aws_secret_access_key` - Долгосрочные учетные данные для пользователя AWS. Вы можете использовать их для аутентификации ваших запросов. Параметр является необязательным. Если учетные данные не указаны, используются из файла конфигурации. Для получения дополнительной информации смотрите [Использование S3 для хранения данных](../mergetree-family/mergetree.md#table_engine-mergetree-s3).
- `compression` — Тип сжатия. Поддерживаемые значения: `none`, `gzip/gz`, `brotli/br`, `xz/LZMA`, `zstd/zst`. Параметр является необязательным. По умолчанию будет автоматически определяться сжатие по расширению файла.

### Кэширование данных {#data-cache}

Движок таблиц `S3` поддерживает кэширование данных на локальном диске. Смотрите параметры конфигурации кэша файловой системы и использование в этом [разделе](/operations/storing-data.md/#using-local-cache). Кэширование происходит в зависимости от пути и ETag объекта хранения, поэтому ClickHouse не будет читать устаревшую версию кэша.

Чтобы включить кэширование, используйте настройку `filesystem_cache_name = '<name>'` и `enable_filesystem_cache = 1`.

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_3.csv', 'minioadmin', 'minioadminpassword', 'CSV')
SETTINGS filesystem_cache_name = 'cache_for_s3', enable_filesystem_cache = 1;
```

Существует два способа определения кэша в файле конфигурации.

1. добавьте следующий раздел в файл конфигурации ClickHouse:

```xml
<clickhouse>
    <filesystem_caches>
        <cache_for_s3>
            <path>путь к директории кэша</path>
            <max_size>10Gi</max_size>
        </cache_for_s3>
    </filesystem_caches>
</clickhouse>
```

2. повторно используйте конфигурацию кэша (и, следовательно, хранилище кэша) из раздела `storage_configuration` ClickHouse, [описанного здесь](/operations/storing-data.md/#using-local-cache)

### PARTITION BY {#partition-by}

`PARTITION BY` — Необязательный. В большинстве случаев вам не нужен ключ раздела, и если он необходим, обычно вам не нужен более детализированный ключ, чем по месяцу. Разделение не ускоряет запросы (в отличие от выражения ORDER BY). Никогда не используйте слишком детализированное разделение. Не разделяйте свои данные по идентификаторам клиентов или именам (вместо этого сделайте идентификатор клиента или имя первым столбцом в выражении ORDER BY).

Для разделения по месяцу используйте выражение `toYYYYMM(date_column)`, где `date_column` — столбец с датой типа [Date](/sql-reference/data-types/date.md). Имена разделов здесь имеют формат `"YYYYMM"`.

### Запросы к разделённым данным {#querying-partitioned-data}

Этот пример использует [рецепт docker compose](https://github.com/ClickHouse/examples/tree/5fdc6ff72f4e5137e23ea075c88d3f44b0202490/docker-compose-recipes/recipes/ch-and-minio-S3), который интегрирует ClickHouse и MinIO. Вы должны иметь возможность воспроизвести те же запросы с помощью S3, заменив значения конечной точки и аутентификации.

Обратите внимание, что конечная точка S3 в конфигурации `ENGINE` использует параметр токена `{_partition_id}` как часть объекта S3 (имя файла), и что SELECT запросы выбирают по этим именам объектов (например, `test_3.csv`).

:::note
Как показано в примере, запросы из S3 таблиц, которые разделены, в данный момент не поддерживаются напрямую, но могут быть достигнуты путем запроса отдельных разделов с использованием функции таблицы S3.

Основным сценарием использования для записи разделенных данных в S3 является возможность передавать эти данные в другую систему ClickHouse (например, перемещение из локальных систем в ClickHouse Cloud). Поскольку наборы данных ClickHouse часто очень большие, а надежность сети иногда несовершенна, имеет смысл передавать наборы данных частями, поэтому записи разделяются.
:::

#### Создание таблицы {#create-the-table}
```sql
CREATE TABLE p
(
    `column1` UInt32,
    `column2` UInt32,
    `column3` UInt32
)
ENGINE = S3(
-- highlight-next-line
           'http://minio:10000/clickhouse//test_{_partition_id}.csv',
           'minioadmin',
           'minioadminpassword',
           'CSV')
PARTITION BY column3
```

#### Вставка данных {#insert-data}
```sql
insert into p values (1, 2, 3), (3, 2, 1), (78, 43, 45)
```

#### Выбор из раздела 3 {#select-from-partition-3}

:::tip
Этот запрос использует функцию таблицы s3
:::

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_3.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```
```response
┌─c1─┬─c2─┬─c3─┐
│  1 │  2 │  3 │
└────┴────┴────┘
```

#### Выбор из раздела 1 {#select-from-partition-1}
```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_1.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```
```response
┌─c1─┬─c2─┬─c3─┐
│  3 │  2 │  1 │
└────┴────┴────┘
```

#### Выбор из раздела 45 {#select-from-partition-45}
```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_45.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```
```response
┌─c1─┬─c2─┬─c3─┐
│ 78 │ 43 │ 45 │
└────┴────┴────┘
```

#### Ограничение {#limitation}

Вы можете естественным образом попытаться выполнить `Select * from p`, но, как отмечалось выше, этот запрос завершится неудачей; используйте предыдущий запрос.

```sql
SELECT * FROM p
```
```response
Получено исключение от сервера (версия 23.4.1):
Code: 48. DB::Exception: Получено от localhost:9000. DB::Exception: Чтение из разделенного S3-хранилища еще не реализовано. (NOT_IMPLEMENTED)
```

## Вставка данных {#inserting-data}

Обратите внимание, что строки можно вставлять только в новые файлы. Нет циклов слияния или операций разделения файла. Как только файл записан, последующие вставки завершатся неудачей. Чтобы избежать этого, вы можете использовать настройки `s3_truncate_on_insert` и `s3_create_new_file_on_insert`. Смотрите подробнее [здесь](/integrations/s3#inserting-data).

## Виртуальные столбцы {#virtual-columns}

- `_path` — Путь к файлу. Тип: `LowCardinality(String)`.
- `_file` — Имя файла. Тип: `LowCardinality(String)`.
- `_size` — Размер файла в байтах. Тип: `Nullable(UInt64)`. Если размер неизвестен, значение — `NULL`.
- `_time` — Время последнего изменения файла. Тип: `Nullable(DateTime)`. Если время неизвестно, значение — `NULL`.
- `_etag` — ETag файла. Тип: `LowCardinality(String)`. Если etag неизвестен, значение — `NULL`.

Для получения дополнительной информации о виртуальных столбцах смотрите [здесь](../../../engines/table-engines/index.md#table_engines-virtual_columns).

## Подробности реализации {#implementation-details}

- Чтение и запись могут быть параллельными
- Не поддерживается:
    - Операции `ALTER` и `SELECT...SAMPLE`.
    - Индексы.
    - [Репликация без копирования](../../../operations/storing-data.md#zero-copy) возможна, но не поддерживается.

  :::note Репликация без копирования не готова для производства
  Репликация без копирования отключена по умолчанию в ClickHouse версии 22.8 и выше. Эта функция не рекомендуется для использования в производственной среде.
  :::

## Символы подстановки в пути {#wildcards-in-path}

Аргумент `path` может указывать несколько файлов, используя символы подстановки в стиле bash. Для обработки файл должен существовать и соответствовать шаблону всего пути. Перечень файлов определяется во время `SELECT` (не в момент `CREATE`).

- `*` — Заменяет любое количество любых символов, кроме `/`, включая пустую строку.
- `**` — Заменяет любое количество любых символов, включая `/`, включая пустую строку.
- `?` — Заменяет любой один символ.
- `{some_string,another_string,yet_another_one}` — Заменяет любую из строк `'some_string', 'another_string', 'yet_another_one'`.
- `{N..M}` — Заменяет любое число в диапазоне от N до M, включая обе границы. N и M могут иметь ведущие нули, например, `000..078`.

Конструкции с `{}` аналогичны функции таблицы [remote](../../../sql-reference/table-functions/remote.md).

:::note
Если в перечне файлов присутствуют диапазоны чисел с ведущими нулями, используйте конструкцию с фигурными скобками для каждой цифры отдельно или используйте `?`.
:::

**Пример с символами подстановки 1**

Создайте таблицу с файлами, названными `file-000.csv`, `file-001.csv`, ... , `file-999.csv`:

```sql
CREATE TABLE big_table (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/my_folder/file-{000..999}.csv', 'CSV');
```

**Пример с символами подстановки 2**

Предположим, у нас есть несколько файлов в формате CSV со следующими URI на S3:

- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_1.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_2.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_3.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_1.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_2.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_3.csv'

Существует несколько способов создать таблицу, которая состоит из всех шести файлов:

1. Укажите диапазон постфиксов файлов:

```sql
CREATE TABLE table_with_range (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/some_file_{1..3}', 'CSV');
```

2. Возьмите все файлы с префиксом `some_file_` (не должно быть дополнительных файлов с таким префиксом в обоих папках):

```sql
CREATE TABLE table_with_question_mark (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/some_file_?', 'CSV');
```

3. Возьмите все файлы в обеих папках (все файлы должны удовлетворять формату и схеме, описанным в запросе):

```sql
CREATE TABLE table_with_asterisk (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/*', 'CSV');
```

## Настройки хранения {#storage-settings}

- [s3_truncate_on_insert](/operations/settings/settings.md#s3_truncate_on_insert) - позволяет обрезать файл перед вставкой в него. По умолчанию отключен.
- [s3_create_new_file_on_insert](/operations/settings/settings.md#s3_create_new_file_on_insert) - позволяет создавать новый файл при каждой вставке, если формат имеет суффикс. По умолчанию отключен.
- [s3_skip_empty_files](/operations/settings/settings.md#s3_skip_empty_files) - позволяет пропускать пустые файлы при чтении. Включен по умолчанию.

## Настройки, связанные с S3 {#settings}

Следующие настройки могут быть указаны перед выполнением запроса или размещены в файле конфигурации.

- `s3_max_single_part_upload_size` — Максимальный размер объекта для загрузки с использованием единственного загрузки в S3. Значение по умолчанию — `32Mb`.
- `s3_min_upload_part_size` — Минимальный размер части для загрузки во время многократной загрузки в [S3 Multipart upload](https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html). Значение по умолчанию — `16Mb`.
- `s3_max_redirects` — Максимальное количество перенаправлений S3. Значение по умолчанию — `10`.
- `s3_single_read_retries` — Максимальное количество попыток во время одного чтения. Значение по умолчанию — `4`.
- `s3_max_put_rps` — Максимальная скорость запросов PUT в секунду перед ограничением. Значение по умолчанию — `0` (неограничено).
- `s3_max_put_burst` — Максимальное количество запросов, которые могут быть выполнены одновременно, прежде чем будет достигнут лимит запросов в секунду. По умолчанию (`0`) соответствует `s3_max_put_rps`.
- `s3_max_get_rps` — Максимальная скорость запросов GET в секунду перед ограничением. Значение по умолчанию — `0` (неограничено).
- `s3_max_get_burst` — Максимальное количество запросов, которые могут быть выполнены одновременно, прежде чем будет достигнут лимит запросов в секунду. По умолчанию (`0`) соответствует `s3_max_get_rps`.
- `s3_upload_part_size_multiply_factor` - Умножает `s3_min_upload_part_size` на этот множитель каждый раз, когда `s3_upload_part_size_multiply_parts_count_threshold` частей были загружены из одной записи в S3. Значение по умолчанию — `2`.
- `s3_upload_part_size_multiply_parts_count_threshold` - Каждый раз, когда это количество частей загружается в S3, `s3_min_upload_part_size` умножается на `s3_upload_part_size_multiply_factor`. Значение по умолчанию — `500`.
- `s3_max_inflight_parts_for_one_file` - Ограничивает количество запросов на PUT, которые могут выполняться одновременно для одного объекта. Его число должно быть ограничено. Значение `0` означает неограниченное количество. Значение по умолчанию — `20`. Каждая часть в полете имеет буфер размером `s3_min_upload_part_size` для первых `s3_upload_part_size_multiply_factor` частей и больше, когда файл достаточно большой, см. `upload_part_size_multiply_factor`. С учетом настроек по умолчанию один загруженный файл потребляет не более `320Mb` для файла, который меньше `8G`. Потребление больше для более крупного файла.

Учитывая соображения безопасности: если злонамеренный пользователь может указывать произвольные URL-адреса S3, `s3_max_redirects` должен быть установлен в ноль для предотвращения [SSRF](https://en.wikipedia.org/wiki/Server-side_request_forgery) атак; или, в качестве альтернативы, в конфигурации сервера должен быть указан `remote_host_filter`.

## Настройки на основе конечной точки {#endpoint-settings}

Следующие настройки могут быть указаны в файле конфигурации для данной конечной точки (которая будет соответствовать точному префиксу URL):

- `endpoint` — Указывает префикс конечной точки. Обязательно.
- `access_key_id` и `secret_access_key` — Указывают учетные данные для использования с данной конечной точкой. Необязательно.
- `use_environment_credentials` — Если установлено в `true`, клиент S3 попытается получить учетные данные из переменных окружения и [метаданных Amazon EC2](https://en.wikipedia.org/wiki/Amazon_Elastic_Compute_Cloud) для данной конечной точки. Необязательно, значение по умолчанию — `false`.
- `region` — Указывает имя региона S3. Необязательно.
- `use_insecure_imds_request` — Если установлено в `true`, клиент S3 будет использовать небезопасный запрос IMDS, получая учетные данные из метаданных Amazon EC2. Необязательно, значение по умолчанию — `false`.
- `expiration_window_seconds` — Период грации для проверки, истекли ли учетные данные, основанные на сроке действия. Необязательно, значение по умолчанию — `120`.
- `no_sign_request` - Игнорируйте все учетные данные, чтобы запросы не подписывались. Полезно для доступа к публичным бакетам.
- `header` — Добавляет указанный HTTP заголовок к запросу к данной конечной точке. Необязательно, может быть указан несколько раз.
- `access_header` - Добавляет указанный HTTP заголовок к запросу к данной конечной точке, в случаях, где нет других учетных данных из другого источника.
- `server_side_encryption_customer_key_base64` — Если указано, будут установлены необходимые заголовки для доступа к объектам S3 с шифрованием SSE-C. Необязательно.
- `server_side_encryption_kms_key_id` - Если указано, будут установлены необходимые заголовки для доступа к объектам S3 со [шифрованием SSE-KMS](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html). Если указана пустая строка, будет использоваться управляемый ключ S3 от AWS. Необязательно.
- `server_side_encryption_kms_encryption_context` - Если указано вместе с `server_side_encryption_kms_key_id`, будет установлен заданный заголовок контекста шифрования для SSE-KMS. Необязательно.
- `server_side_encryption_kms_bucket_key_enabled` - Если указано вместе с `server_side_encryption_kms_key_id`, будет установлен заголовок для включения ключей бакетов S3 для SSE-KMS. Необязательно, может быть `true` или `false`, по умолчанию соответствует ничему (совпадает с настройками на уровне бакета).
- `max_single_read_retries` — Максимальное количество попыток во время одного чтения. Значение по умолчанию — `4`. Необязательно.
- `max_put_rps`, `max_put_burst`, `max_get_rps` и `max_get_burst` - Настройки ограничения (см. описание выше), которые используются для конкретной конечной точки вместо запроса. Необязательно.

**Пример:**

```xml
<s3>
    <endpoint-name>
        <endpoint>https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/</endpoint>
        <!-- <access_key_id>ACCESS_KEY_ID</access_key_id> -->
        <!-- <secret_access_key>SECRET_ACCESS_KEY</secret_access_key> -->
        <!-- <region>us-west-1</region> -->
        <!-- <use_environment_credentials>false</use_environment_credentials> -->
        <!-- <use_insecure_imds_request>false</use_insecure_imds_request> -->
        <!-- <expiration_window_seconds>120</expiration_window_seconds> -->
        <!-- <no_sign_request>false</no_sign_request> -->
        <!-- <header>Authorization: Bearer SOME-TOKEN</header> -->
        <!-- <server_side_encryption_customer_key_base64>BASE64-ENCODED-KEY</server_side_encryption_customer_key_base64> -->
        <!-- <server_side_encryption_kms_key_id>KMS_KEY_ID</server_side_encryption_kms_key_id> -->
        <!-- <server_side_encryption_kms_encryption_context>KMS_ENCRYPTION_CONTEXT</server_side_encryption_kms_encryption_context> -->
        <!-- <server_side_encryption_kms_bucket_key_enabled>true</server_side_encryption_kms_bucket_key_enabled> -->
        <!-- <max_single_read_retries>4</max_single_read_retries> -->
    </endpoint-name>
</s3>
```

## Работа с архивами {#working-with-archives}

Предположим, у нас есть несколько архивных файлов со следующими URI на S3:

- 'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-10.csv.zip'
- 'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-11.csv.zip'
- 'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-12.csv.zip'

Извлечение данных из этих архивов возможно с использованием ::. Символы подстановки могут использоваться как в части URL, так и в части после :: (отвечающей за имя файла внутри архива).

```sql
SELECT *
FROM s3(
   'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-1{0..2}.csv.zip :: *.csv'
);
```

:::note
ClickHouse поддерживает три формата архивов:
ZIP
TAR
7Z
В то время как ZIP и TAR архивы могут быть доступны из любого поддерживаемого хранилища, 7Z архивы могут быть прочитаны только из локальной файловой системы, где установлен ClickHouse.
:::


## Доступ к публичным бакетам {#accessing-public-buckets}

ClickHouse пытается получить учетные данные из многих разных типов источников. Иногда это может вызвать проблемы при доступе к некоторым бакетам, которые являются публичными, причиной чего является возврат клиентом кода ошибки `403`. Эта проблема может быть избегнута с помощью ключевого слова `NOSIGN`, заставляющего клиента игнорировать все учетные данные и не подписывать запросы.

```sql
CREATE TABLE big_table (name String, value UInt32)
    ENGINE = S3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/aapl_stock.csv', NOSIGN, 'CSVWithNames');
```

## Оптимизация производительности {#optimizing-performance}

Для получения подробной информации о том, как оптимизировать производительность функции S3, смотрите [наш подробный гид](/integrations/s3/performance).

## См. также {#see-also}

- [Функция таблицы s3](../../../sql-reference/table-functions/s3.md)
- [Интеграция S3 с ClickHouse](/integrations/s3)
