---
'slug': '/academic_overview'
'title': '架构概述'
'description': '我们 2024 年 VLDB 论文的文档版本'
'keywords':
- 'architecture'
---

import useBrokenLinks from "@docusaurus/useBrokenLinks";
import image_01 from '@site/static/images/managing-data/core-concepts/_vldb2024_1_Figure_0.png'
import image_02 from '@site/static/images/managing-data/core-concepts/_vldb2024_2_Figure_0.png'
import image_03 from '@site/static/images/managing-data/core-concepts/_vldb2024_2_Figure_5.png'
import image_04 from '@site/static/images/managing-data/core-concepts/_vldb2024_3_Figure_7.png'
import image_05 from '@site/static/images/managing-data/core-concepts/_vldb2024_4_Figure_6.png'
import image_06 from '@site/static/images/managing-data/core-concepts/_vldb2024_5_Figure_8.png'
import image_07 from '@site/static/images/managing-data/core-concepts/_vldb2024_6_Figure_0.png'
import image_08 from '@site/static/images/managing-data/core-concepts/_vldb2024_7_Figure_1.png'
import image_09 from '@site/static/images/managing-data/core-concepts/_vldb2024_8_Figure_7.png'
import image_10 from '@site/static/images/managing-data/core-concepts/_vldb2024_10_Figure_14.png'
import image_11 from '@site/static/images/managing-data/core-concepts/_vldb2024_10_Figure_0.png'
import image_12 from '@site/static/images/managing-data/core-concepts/_vldb2024_10_Figure_12.png'
import image_13 from '@site/static/images/managing-data/core-concepts/_vldb2024_10_Figure_13.png'
import Image from '@theme/IdealImage';

<!-- needed as docusaurus cannot resolve links to span ids, we need a custom span -->
export function Anchor(props) {
    useBrokenLinks().collectAnchor(props.id);
    return <span style={{scrollMarginTop: "var(--ifm-navbar-height)"}} {...props}/>;
}

这是我们 [VLDB 2024 科学论文](https://www.vldb.org/pvldb/vol17/p3731-schulze.pdf) 的网络版本。我们还 [博客](https://clickhouse.com/blog/first-clickhouse-research-paper-vldb-lightning-fast-analytics-for-everyone) 关于它的背景和历程，并推荐观看 ClickHouse 董事长兼创造者 Alexey Milovidov 在 VLDB 2024 的演讲：

<iframe width="1024" height="576" src="https://www.youtube.com/embed/7QXKBKDOkJE?si=5uFerjqPSXQWqDkF" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
## 摘要 {#abstract}

在过去的几十年里，存储和分析的数据量呈指数增长。各行各业的企业开始依靠这些数据来改善产品、评估绩效并做出对业务至关重要的决策。然而，随着数据量日益增大到互联网级别，企业需要以既经济又可扩展的方式管理历史数据和新数据，同时使用大量的并发查询和实时延迟的预期进行分析（例如，依赖于用例，通常少于一秒）。

本文概述了 ClickHouse，这是一款流行的开源 OLAP 数据库，旨在对具有高摄取率的 PB 级数据集进行高性能分析。其存储层结合了基于传统日志结构合并（LSM）树的数据格式以及用于在后台连续转化（例如聚合、归档）历史数据的新技术。查询使用方便的 SQL 方言编写，并通过最先进的矢量化查询执行引擎进行处理，可选的代码编译。ClickHouse 积极使用剪枝技术，避免在查询中评估无关数据。可以在表函数、表引擎或数据库引擎级别集成其他数据管理系统。实际基准测试表明，ClickHouse 是市场上最快的分析数据库之一。
## 1 介绍 {#1-introduction}

本文描述了 ClickHouse，一种专为高性能分析查询而设计的列式 OLAP 数据库，能够处理具有数万亿行和数百列的表格。ClickHouse 于 2009 年 [启动](https://clickhou.se/evolution)，作为 web 级日志文件数据的过滤和聚合运算符，并于 2016 年开源。[图 1](#page-1-0) 说明了本文所述的主要特性何时被引入 ClickHouse。

ClickHouse 旨在解决现代分析数据管理的五大关键挑战：

1. **大量数据集与高摄取率**。许多行业（如网络分析、金融和电子商务）中以数据驱动的应用程序，具有庞大且持续增长的数据量。为了处理大型数据集，分析数据库必须不仅提供有效的索引和压缩策略，还必须允许跨多个节点分发数据（向外扩展），因为单个服务器的存储限于数十 TB。此外，最近的数据往往比历史数据更与实时洞察相关。因此，分析数据库必须能够以持续高速度或突发方式摄取新数据，同时在不减慢并行报告查询的情况下，持续“降级”（例如聚合、归档）历史数据。

2. **许多并发查询并期望低延迟**。查询通常可以分为临时查询（例如探索性数据分析）或定期查询（例如定期仪表板查询）。用例越互动，预期的查询延迟就越低，导致查询优化和执行面临挑战。定期查询还提供了一个调整物理数据库布局以适应工作负载的机会。因此，数据库应该提供允许优化频繁查询的剪枝技术。根据查询优先级，数据库还必须确保对共享系统资源（如 CPU、内存、磁盘和网络 I/O）的平等或优先访问，即使在大量查询同时运行的情况下。

3. **多样化的数据存储、存储位置和格式**。为了与现有数据架构集成，现代分析数据库应展现高度的开放性，能够在任何系统、位置或格式中读取和写入外部数据。

4. **方便的查询语言，支持性能分析**。现实使用 OLAP 数据库会提出附加的“软”需求。例如，用户通常希望以一种富有表现力的 SQL 方言与数据库进行交互，而不是使用小众的编程语言，并且希望支持嵌套数据类型和广泛的常规、聚合和窗口函数。分析数据库还应提供复杂的工具，以分析系统或单个查询的性能。

5. **行业级的稳健性和多功能部署**。由于商品硬件不可靠，数据库必须提供数据复制，以确保针对节点故障的稳健性。此外，数据库应能够在任何硬件上运行，从旧笔记本电脑到强大的服务器。最后，为了避免 JVM 基于程序的垃圾回收开销并支持裸机性能（例如 SIMD），数据库最好以原生二进制形式部署到目标平台。

<Anchor id="page-1-0"/><Image img={image_01} size="lg" alt="图像 01"/>

图 1：ClickHouse 时间线。
## 2 架构 {#2-architecture}

<Anchor id="page-2-0"/><Image img={image_02} size="lg" alt="图像 02"/>

图 2：ClickHouse 数据库引擎的高层架构。

如 [图 2](#page-2-0) 所示，ClickHouse 引擎分为三个主要层次：查询处理层（在[第 4 节](#page-6-0)描述），存储层（[第 3 节](#page-1-1)）和集成层（[第 5 节](#page-9-0)）。此外，访问层管理用户会话和通过不同协议与应用程序的通信。还有用于线程管理、缓存、基于角色的访问控制、备份和持续监控的正交组件。ClickHouse 是用 C++ 构建的一个单一、静态链接的二进制文件，没有依赖关系。

查询处理遵循传统范式：解析输入查询、构建和优化逻辑和物理查询计划以及执行。ClickHouse 使用类似 MonetDB/X100 的矢量化执行模型 [\[11\]](#page-12-0)，结合机会性代码编译 [\[53\]](#page-13-0)。查询可以使用功能丰富的 SQL 方言、PRQL [\[76\]](#page-13-1) 或 Kusto 的 KQL [\[50\]](#page-13-2) 编写。

存储层由不同的表引擎组成，封装表数据的格式和位置。表引擎分为三类：第一类是 MergeTree* 家族的表引擎，它们代表 ClickHouse 中的主要持久性格式。基于 LSM 树的构想 [\[60\]](#page-13-3)，表按照水平的、已排序的部分划分，并由后台过程不断合并。不同的 MergeTree* 表引擎在合并输入部分的方式上有所不同。例如，可以对行进行聚合或替换（如果过时）。

第二类是特殊目的的表引擎，用于加速或分发查询执行。该类别包括称为字典的内存中键值表引擎。一个 [字典](https://clickhou.se/dictionaries) 定期缓存针对内部或外部数据源执行的查询结果。这显著减少了那些可以容忍一定数据过时场景下的访问延迟。其他特殊目的表引擎的例子包括用于临时表的纯内存引擎和用于透明数据分片的分布式表引擎（见下文）。

第三类表引擎是用于与外部系统（如关系数据库（例如 PostgreSQL、MySQL）、发布/订阅系统（例如 Kafka、RabbitMQ [\[24\]](#page-12-1)）或键值存储（例如 Redis）进行双向数据交换的虚拟表引擎。虚拟引擎还可以与数据湖（例如 Iceberg、DeltaLake、Hudi [\[36\]](#page-12-2)）或对象存储中的文件（例如 AWS S3、Google GCP）进行交互。

ClickHouse 支持在多个集群节点之间分片和复制表格，以实现可扩展性和可用性。分片根据分片表达式将表划分为一组表分片。各个分片是相互独立的表，通常位于不同的节点上。客户端可以直接读取和写入分片，即将它们视为独立的表，或使用分布式特殊表引擎来提供对所有表分片的全局视图。分片的主要目的是处理超出单个节点容量的数据集（通常是几十 TB 的数据）。分片的另一个用途是将表的读写负载分配到多个节点，即负载均衡。与此正交，一项分片可以在多个节点之间复制，以抵抗节点故障。为此，每个 MergeTree* 表引擎都有一个相应的 ReplicatedMergeTree* 引擎，它使用基于 Raft 一致性（\[59\]](#page-13-4) 的多主协调方案（由 [Keeper](https://clickhou.se/keeper) 实现，这是一个用 C++ 编写的 Apache Zookeeper 的替代品），以确保每个分片始终有可配置数量的副本。[第 3.6 节](#page-5-0) 详细讨论了复制机制。作为一个例子，[图 2](#page-2-0) 显示了一个表，包含两个分片，每个分片复制到两个节点。

最后，ClickHouse 数据库引擎可以在本地、云、独立或进程模式下运行。在本地模式下，用户在单台服务器或具有分片和/或复制的多节点集群上本地设置 ClickHouse。客户端通过本地协议、MySQL 或 PostgreSQL 的二进制协议或 HTTP REST API 与数据库通信。云模式由 ClickHouse Cloud 代表，这是一种完全托管和自动扩展的 DBaaS 产品。虽然本文重点关注本地模式，但我们计划在后续出版物中描述 ClickHouse Cloud 的架构。[独立模式](https://clickhou.se/local-fastest-tool) 将 ClickHouse 变成一种命令行实用工具，用于分析和转换文件，使其成为 Unix 工具（如 cat 和 grep）的 SQL 基替代品。虽然这不需要事先配置，但独立模式限制为单个服务器。最近，开发了一种称为 chDB 的进程模式 [\[15\]](#page-12-3)，用于交互式数据分析用例，如与 Pandas 数据框 [\[61\]](#page-13-5) 的 Jupyter 笔记本结合使用。受 DuckDB [\[67\]](#page-13-6) 启发， [chDB](https://clickhou.se/chdb-rocket-engine) 将 ClickHouse 嵌入为高性能 OLAP 引擎到主机进程中。与其他模式相比，这种方式可以有效地在数据库引擎和应用程序之间传递源数据和结果数据，而无需拷贝，因为它们在同一地址空间中运行。
## <Anchor id="page-1-1"/>3 存储层 {#3-storage-layer}

本节讨论 MergeTree* 表引擎作为 ClickHouse 的本地存储格式。我们描述其磁盘上的表示，并讨论 ClickHouse 中的三种数据修剪技术。之后，我们介绍合并策略，这些策略在不影响并行插入的情况下持续转化数据。最后，我们解释更新和删除的实现方式，以及数据去重、数据复制和 ACID 合规性。
### <Anchor id="page-2-2"/>3.1 磁盘格式 {#3-1-on-disk-format}

每个 MergeTree* 表引擎中的表被组织为不可变表部分的集合。当一组行被插入到表中时，就会创建一个部分。部分是自包含的，意味着它们包含了解释其内容所需的所有元数据，而无需额外查询中央目录。为了保持每个表的部分数量较低，后台合并作业会定期将多个较小的部分合并为一个更大的部分，直到达到可配置的部分大小（默认 150 GB）。由于部分是根据表的主键列进行排序的（见 [第 3.2 节](#page-3-0)），因此用于合并的高效 k-way 合并排序 [\[40\]](#page-12-5)被使用。源部分标记为非活动状态，并最终在其引用计数降为零后被删除，即没有进一步的查询从中读取。

可以以两种模式插入行：在同步插入模式中，每个 INSERT 语句创建一个新部分并将其附加到表中。为了最小化合并的开销，数据库客户端被鼓励以批量方式插入元组，例如一次插入 20,000 行。然而，如果数据应以实时方式进行分析，则客户端批处理导致的延迟通常是不可接受的。例如，观察性用例通常涉及数千个监控代理持续发送少量事件和指标数据。此类场景可以利用异步插入模式，其中 ClickHouse 将来自多个传入 INSERT 的行缓冲在同一个表中，并只在缓冲区大小超过可配置阈值或超时到期后创建新部分。

<Anchor id="page-2-1"/><Image img={image_03} size="lg" alt="图像 03"/>

图 3：MergeTree*-引擎表的插入和合并。

[图 3](#page-2-1) 显示了对 MergeTree*-引擎表的四个同步插入和两个异步插入。两个合并将活动部分的数量从初始的五减少到两个。

与 LSM 树的实现 [\[58\]](#page-13-7) 和各种数据库 [\[13,](#page-12-6) [26,](#page-12-7) [56\]](#page-13-8) 相比，ClickHouse 将所有部分视为平等，而不是将它们排列在层次结构中。因此，合并不再限于同一级别的部分。因为这也放弃了部分之间的隐式时间顺序，所以需要基于非墓碑的更新和删除的替代机制（见 [第 3.4 节](#page-4-0)）。ClickHouse 直接将插入写入磁盘，而其他基于 LSM 树的存储通常使用预写日志（见 [第 3.7 节](#page-5-1)）。

一个部分对应于磁盘上的一个目录，包含每一列一个文件。作为优化，较小部分（默认小于 10 MB）的列按顺序存储在一个文件中，以提高读取和写入的空间局部性。部分的行进一步在逻辑上被划分为 8192 条记录的组，称为颗粒。颗粒代表 ClickHouse 中被扫描和索引查找运算符处理的最小不可分割的数据单元。然而，磁盘数据的读取和写入不是在颗粒级别进行的，而是在块的粒度上进行的，块结合了列中多个相邻的颗粒。新块的形成基于每个块可配置的字节大小（默认 1 MB），即块中的颗粒数量是可变的，并取决于列的数据类型和分布。块还会进行压缩，以减少其大小和 I/O 成本。默认情况下，ClickHouse 使用 LZ4 [\[75\]](#page-13-9) 作为通用压缩算法，但用户也可以指定专用编解码器，如针对浮点数据的 Gorilla [\[63\]](#page-13-10) 或 FPC [\[12\]](#page-12-8)。压缩算法还可以链式组合。例如，可以先使用增量编码降低数值中的逻辑冗余 [\[23\]](#page-12-9)，然后进行重量压缩，最后使用 AES 编解码器对数据进行加密。块在从磁盘加载到内存时会在专门的解压缩处理上完成。为了实现对个别颗粒的快速随机访问，尽管进行了压缩，ClickHouse 还为每列存储一个映射，将每个颗粒 ID 与其包含的压缩块在列文件中的偏移量及未压缩块中颗粒的偏移量相关联。

列还可以使用两种特殊的包装数据类型进行字典编码 [\[2,](#page-12-10) [77,](#page-13-11) [81\]](#page-13-12) 或 Nullable。一种 LowCardinality(T) 用整数 ID 替代原始列值，从而显著减少具有少量唯一值的数据的存储开销。Nullable(T) 为列 T 添加一个内部位图，表示列值是否为 NULL。

最后，表可以使用任意的分区表达式进行范围、哈希或轮训分区。为了启用分区修剪，ClickHouse 还会为每个分区存储分区表达式的最小和最大值。用户可以选择性地创建更高级的列统计信息（例如，HyperLogLog [\[30\]](#page-12-11) 或 t-digest [\[28\]](#page-12-12) 统计），这些统计信息还提供基数估计。
### <Anchor id="page-3-0"/>3.2 数据修剪 {#3-2-data-pruning}

在大多数用例中，仅为了回答一个查询而扫描 PB 级的数据是太慢和昂贵的。ClickHouse 支持三种数据修剪技术，允许在搜索期间跳过大多数行，从而显著加快查询速度。

首先，用户可以为表定义一个 **主键索引**。主键列确定每个部分内行的排序顺序，也就是说，索引是局部聚集的。ClickHouse 还为每个部分存储一种映射，将每个颗粒的第一行的主键列值映射到颗粒 ID，也就是说，索引是稀疏的 [\[31\]](#page-12-13)。生成的数据结构通常足够小，可以保持完全在内存中，例如，仅需 1000 个条目即可对 810 万行进行索引。主键的主要目的是使用二进制搜索评估频繁过滤列的相等性和范围谓词，而不是使用顺序扫描（[第 4.4 节](#page-7-0)）。局部排序还可以被利用于部分的合并和查询优化，例如基于排序的聚合或在主键列形成排序列的前缀时，将排序运算符从物理执行计划中移除。

[图 4](#page-3-1) 显示了在具有页面印象统计的表上对列 EventTime 的主键索引。查询中与范围谓词匹配的颗粒可以通过在主键索引中进行二进制搜索找到，而不是顺序扫描 EventTime。

<Anchor id="page-3-1"/><Image img={image_04} size="lg" alt="图像 04"/>

图 4：使用主键索引评估筛选器。

其次，用户可以创建 **表投影**，即包含按不同主键排序的相同行的表的替代版本 [\[71\]](#page-13-13)。投影允许加速在与主表的主键不同的列上进行过滤的查询，而以增加插入、合并和空间消耗的额外开销为代价。默认情况下，投影仅从新插入主表的部分懒惰填充，而不会从现有部分填充，除非用户将投影完全实例化。查询优化器根据估计 I/O 成本在从主表读取或投影之间进行选择。如果某个部分没有对应的投影，查询执行将回退到相应的主表部分。

第三，**跳过索引** 提供了一种轻量级替代方案来代替投影。跳过索引的想法是在多个连续颗粒的层面存储少量元数据，这使得能够避免扫描无关的行。跳过索引可以为任意索引表达式创建，并使用可配置的粒度，即跳过索引块中的颗粒数量。可用的跳过索引类型包括：1. 最小-最大索引 [\[51\]](#page-13-14)，存储每个索引块的索引表达式的最小值和最大值。该索引类型对绝对范围小的局部聚集数据效果很好，例如松散排序的数据。2. 集合索引，存储可配置数量的唯一索引块值。这些索引最适用于具有小局部基数的数据，即“聚集在一起”的值。3. Bloom 过滤器索引 [\[9\]](#page-12-14)，为行、标记或 n-gram 值构建，具有可配置的误报率。这些索引支持文本搜索 [\[73\]](#page-13-15)，但与最小-最大和集合索引不同，它们不能用于范围或负面谓词。
### <Anchor id="page-4-3"/>3.3 合并时数据转换 {#3-3-merge-time-data-transformation}

商业智能和观察性用例通常需要处理以不断的高速度或突发方式生成的数据。此外，最近生成的数据通常比历史数据更具现实意义。因此，这类用例要求数据库在持续高数据摄取率的同时，持续通过聚合或数据老化等技术减少历史数据的体积。ClickHouse 允许对现有数据进行持续增量转换，使用不同的合并策略。合并时的数据转换不会妨碍 INSERT 语句的性能，但它无法保证表中不会包含不需要的（例如过时或未聚合的）值。如果需要，所有合并时的转换可以在查询时通过在 SELECT 语句中指定关键字 FINAL 来应用。

**替换合并** 仅保留基于其包含部分的创建时间戳的元组中最近插入的版本，较旧的版本将被删除。如果它们具有相同的主键列值，则元组被视为等效。为了明确控制保留哪个元组，还可以为比较指定一个特殊版本列。替换合并通常用作合并时的更新机制（通常在更新频繁的用例中），或者作为插入时数据去重的替代方案（[第 3.5 节](#page-5-2)）。

**聚合合并** 将具有相同主键列值的行折叠为一行聚合行。非主键列必须为保持汇总值的部分聚合状态。两个部分聚合状态，例如 avg() 的和（sum）和计数（count），会合并为一个新的部分聚合状态。聚合合并通常在物化视图中使用，而不是普通表。物化视图是根据对源表的转换查询填充的。与其他数据库不同，ClickHouse 不会定期使用源表的全部内容刷新物化视图。物化视图是当新部分插入源表时，Incrementally 基于转换查询的结果进行更新。

[图 5](#page-4-1) 显示了在具有页面印象统计的表上定义的物化视图。对于新插入源表的部分，转化查询计算按地区分组的最大和平均延迟，并将结果插入到物化视图中。聚合函数 avg() 和 max() 采用扩展 -State 返回部分聚合状态，而不是实际结果。为物化视图定义的聚合合并在不同的部分中持续组合部分聚合状态。为了获得最终结果，用户使用 avg() 和 max() 在物化视图中合并部分聚合状态，采用 -Merge 扩展。

<Anchor id="page-4-1"/><Image img={image_05} size="lg" alt="图像 05"/>

图 5：物化视图中的聚合合并。

**TTL（生存时间）合并** 提供历史数据的老化功能。与删除和聚合合并不同，TTL 合并一次仅处理一个部分。TTL 合并根据触发器和操作定义规则。触发器是在每行上计算时间戳的表达式，与 TTL 合并运行的时间进行比较。虽然这使用户能够以行粒度控制操作，但我们发现检查所有行是否满足给定条件并在整个部分上运行操作就足够了。可能的操作包括 1. 将部分移动到另一卷（例如更便宜、但更慢的存储），2. 重新压缩部分（例如，使用更高性能的编解码器），3. 删除部分，以及 4. 汇总，即使用分组键和聚合函数汇总行。

例如，参考 [列表 1](#page-4-2) 中的日志表定义。ClickHouse 将把时间戳列值超过一周的部分移动到缓慢但便宜的 S3 对象存储中。
<Anchor id="page-4-2"/>
```
1 CREATE TABLE tab ( ts DateTime , msg String )
2 ENGINE MergeTree PRIMARY KEY ts
3 TTL ( ts + INTERVAL 1 WEEK ) TO VOLUME 's3 '
```
列表 1：移动部分到对象存储的定义，当它超过一周时。
### <Anchor id="page-4-0"/>3.4 更新和删除 {#3-4-updates-and-deletes}

MergeTree* 表引擎的设计偏向于追加工作负载，但某些用例偶尔需要修改现有数据，例如出于合规原因。更新或删除数据有两种方法，但都不会阻塞并行插入。

**变更** 在原地重写表的所有部分。为了防止表（删除）或列（更新）临时大小翻倍，此操作是非原子性的，即并行的 SELECT 语句可以读取已变更和未变更的部分。变更保证在操作结束时物理数据被更改。删除变更仍然是昂贵的，因为它们会重写所有部分中的所有列。

作为替代方案，**轻量级删除** 仅更新一个内部位图列，以指示某一行是否被删除。ClickHouse 会在 SELECT 查询中添加对位图列的额外过滤器，以从结果中排除已删除的行。已删除的行仅在未来某个不确定的时间通过常规合并物理删除。根据列的数量，轻量级删除可能比变更更快，但代价是 SELECT 语句的速度较慢。

对同一表执行的更新和删除操作预计会是罕见的，并被序列化以避免逻辑冲突。
### <Anchor id="page-5-2"/>3.5 幂等插入 {#3-5-idempotent-inserts}

在实践中，经常遇到的问题是客户端在将数据发送到服务器以插入表时如何处理连接超时。在这种情况下，客户端很难判断数据是否成功插入。这个问题通常通过从客户端重新发送数据到服务器并依赖主键或唯一约束来拒绝重复插入来解决。数据库使用基于二叉树 [\[39,](#page-12-15) [68\]](#page-13-16)、基数树 [\[45\]](#page-13-17) 或哈希表 [\[29\]](#page-12-16) 的索引结构快速执行所需的点查找。由于这些数据结构会对每个元组进行索引，它们的空间和更新开销在大数据集和高摄取率下变得昂贵。

ClickHouse 提供了一种更轻量级的替代方案，因为每次插入最终都会创建一个部分。更具体地说，服务器维护最近插入的 N 个部分的哈希（例如 N=100），并忽略已知哈希的部分的重插入。对于不复制和复制的表，哈希分别存储在 Keeper 中。因此，插入变得幂等，即客户端只需在超时后重新发送同一批行，并假定服务器会处理去重。为了更好地控制去重过程，客户端可以选择性地提供一个插入令牌，它作为部分哈希。虽然基于哈希的去重会产生与新行哈希有关的开销，但存储和比较哈希的成本微乎其微。
```
### <Anchor id="page-5-0"/>3.6 数据复制 {#3-6-data-replication}

复制是高可用性（对节点故障的容忍）的前提，但也用于负载平衡和零停机升级 [\[14\]](#page-12-17)。在 ClickHouse 中，复制基于表状态的概念，这些状态由一组表片段（第 [3.1 节](#page-2-2)）和表元数据（如列名和类型）组成。节点通过三种操作来推进表的状态：1. 插入操作向状态添加一个新片段，2. 合并操作向状态添加一个新片段，并从状态中删除现有片段，3. 变更和 DDL 语句根据具体操作添加片段、删除片段和/或更改表元数据。操作是在单个节点上本地执行，并记录为全局复制日志中的一系列状态转换。

复制日志由通常三个 ClickHouse Keeper 进程的集群维护，这些进程使用 Raft 共识算法 [\[59\]](#page-13-4) 提供对 ClickHouse 节点集群的分布式和容错协调层。所有集群节点最初指向复制日志中的相同位置。当节点执行本地插入、合并、变更和 DDL 语句时，复制日志在所有其他节点上异步重放。因此，复制表仅最终一致，即节点在收敛到最新状态时，可能会暂时读取旧的表状态。大多数上述操作可以替代性地同步执行，直到部分节点（例如，节点的多数或所有节点）采用新状态。

作为示例，[图 6](#page-5-3) 显示了在三 ClickHouse 节点的集群中，最初为空的复制表。节点 1 首先接收两个插入语句，并在 Keeper 集群中记录它们（1 2）。接下来，节点 2 通过获取它（3）并从节点 1 下载新片段（4）重放第一个日志条目，而节点 3 则重放两个日志条目（3 4 5 6）。最后，节点 3 将两个片段合并为一个新片段，删除输入片段，并在复制日志中记录合并条目（7）。

<Anchor id="page-5-3"/><Image img={image_06} size="lg" alt="Image 06"/>

图 6：三节点集群中的复制。

为了加快同步，存在三项优化：首先，新添加到集群的节点会从头重放复制日志，而不是简单地复制写入最后复制日志条目的节点的状态。其次，合并操作通过在本地重复执行或从另一个节点获取结果片段来重放。确切的行为是可配置的，允许在 CPU 消耗和网络 I/O 之间进行平衡。例如，跨数据中心的复制通常倾向于本地合并，以最小化操作成本。第三，节点可以并行重放相互独立的复制日志条目。这包括例如，对连续插入相同表的新片段的获取，或对不同表的操作。
### <Anchor id="page-5-1"/>3.7 ACID 兼容性 {#3-7-acid-compliance}

为了最大化并发读写操作的性能，ClickHouse 尽可能避免锁定。查询是针对在查询开始时创建的包含所有参与表中所有片段的快照执行的。这确保了由并行 INSERT 或合并（第 [3.1 节](#page-2-2)）插入的新片段不会参与执行。为了防止片段在查询期间被同时修改或移除（第 [3.4 节](#page-4-0)），处理片段的引用计数在查询持续期间内递增。从形式上讲，这与基于版本化片段的 MVCC 变体所实现的快照隔离相对应 [\[6\]](#page-12-18)。因此，语句通常不符合 ACID 标准，除非在获取快照时并发写入每个只影响单个片段的事件非常罕见。

在实践中，ClickHouse 的大多数重写决策使用用例甚至可以容忍在电力故障情况下丢失新数据的风险。默认情况下，数据库利用这一点，通过不强制提交（fsync）新插入的片段到磁盘，允许内核批量写入，以牺牲原子性为代价。
## <Anchor id="page-6-0"/>4 查询处理层 {#4-query-processing-layer}

<Anchor id="page-6-1"/><Image img={image_07} size="lg" alt="Image 07"/>

图 7：在 SIMD 单元、内核和节点之间的并行化。

如 [图 7](#page-6-1) 所示，ClickHouse 在数据元素、数据块和表分片级别上对查询进行并行处理。多个数据元素可以使用 SIMD 指令在运算符内同时处理。在单个节点上，查询引擎使用多个线程同时执行运算符。ClickHouse 使用与 MonetDB/X100 相同的向量化模型 [\[11\]](#page-12-0)，即运算符生成、传递和消费多个行（数据块），而不是单行，以减少虚拟函数调用的开销。如果源表被拆分成互不重叠的表分片，多个节点可以同时扫描这些分片。因此，所有硬件资源都被充分利用，通过添加节点实现横向扩展，通过添加内核实现纵向扩展。

本节的其余部分首先更详细地描述数据元素、数据块和分片粒度下的并行处理。然后，我们介绍若干关键优化，以最大化查询性能。最后，我们讨论 ClickHouse 如何在存在同时查询时管理共享系统资源。
### 4.1 SIMD 并行化 {#4-1-simd-parallelization}

在运算符之间传递多行数据为向量化提供了机会。向量化可以基于手动编写的内联函数 [\[64,](#page-13-18) [80\]](#page-13-19) 或编译器的自动向量化 [\[25\]](#page-12-19)。受益于向量化的代码被编译为不同的计算内核。例如，查询运算符的内部热循环可以用非向量化内核、自动向量化的 AVX2 内核和手动向量化的 AVX-512 内核来实现。基于 cpuid 指令，运行时选择最快的内核 [选择在运行时](https://clickhou.se/cpu-dispatch)。这种方法允许 ClickHouse 在长期使用 S前提条件以15年（要求至少支持 SSE 4.2）的系统上运行，同时对最新硬件提供显著加速。
### 4.2 多核并行化 {#4-2-multi-core-parallelization}

<Anchor id="page-7-1"/><Image img={image_08} size="lg" alt="Image 08"/>

图 8：具有三个通道的物理运算符计划。

ClickHouse 遵循传统方法 [\[31\]](#page-12-13) 将 SQL 查询转换为物理计划运算符的有向图。运算符计划的输入由特殊的源运算符表示，这些运算符以原生或任何受支持的第三方格式读取数据（见第 [5 节](#page-9-0)）。同样，特殊的汇运算符将结果转换为所需的输出格式。物理运算符计划在查询编译时根据可配置的最大工作线程数（默认是核心数）和源表大小展开发成独立的执行通道。通道将由并行运算符处理的数据分解为不重叠的范围。为了最大化并行处理的机会，通道尽可能晚地合并。

例如， [图 8](#page-7-1) 中的节点 1 框显示了针对具有页面展示统计信息的表的典型 OLAP 查询的运算符图。在第一个阶段，源表的三个不重叠范围同时进行过滤。重新分配交换运算符动态路由结果块在第一和第二阶段之间，以保持处理线程的均匀利用。如果扫描的范围具有显著不同的选择性，则在第一阶段后，通道可能会变得不平衡。在第二阶段，将经过过滤的行按 RegionID 分组。聚合运算符以 RegionID 作为分组列，并以每组的总和和计数作为 avg() 的局部聚合状态。局部聚合结果最终通过 GroupStateMerge 运算符合并为全局聚合结果。该运算符也是一个管道破坏者，即，第三阶段只能在聚合结果完全计算后开始。在第三阶段，结果组首先通过一个分布交换运算符划分为三个相等的大不重叠分区，然后按 AvgLatency 排序。排序分为三个步骤进行：首先，ChunkSort 运算符对每个分区的单独块进行排序。其次，StreamSort 运算符保持本地排序结果，并使用2路合并排序将其与传入的有序块组合。最后，MergeSort 运算符使用 k-way 排序合并本地结果以获得最终结果。

运算符是状态机，通过输入和输出端口互相连接。运算符可能的三种状态是需要块、就绪和完成。在从需要块到就绪的状态转换中，块被放入运算符的输入端口中。在从就绪到完成的状态转换中，运算符处理输入块并生成输出块。在从完成到需要块的状态转换中，输出块从运算符的输出端口中移除。连接的两个运算符的第一和第三状态转换只能在一个组合步骤中执行。源运算符（汇运算符）仅具有就绪和完成（需要块和完成）状态。

工作线程持续遍历物理运算符计划并执行状态转换。为了保持 CPU 缓存热，计划包含提示，指示同一线程应在同一通道中处理连续的运算符。并行处理同时发生在一个阶段内各不相同的输入之间（例如在 [图 8](#page-7-1) 中，聚合运算符同时执行）和在不同阶段之间没有由管道破坏器隔开的时候（例如在 [图 8](#page-7-1) 中，同一通道内的过滤和聚合运算符可以同时运行）。为了避免当新查询启动或并发查询结束时发生过度和不足的订阅，可以在查询过程中在查询开始时指定的查询工作线程的一个数目与最大工作线程数之间改变并行程度（见第 [4.5 节](#page-9-1)）。

运算符还可以在运行时通过两种方式进一步影响查询执行。首先，运算符可以动态创建和连接新运算符。这主要用于在内存消耗超过可配置阈值时切换到外部聚合、排序或连接算法，而不是取消查询。其次，运算符可以请求工作线程进入异步队列。这样，在等待远程数据时，可以更有效地利用工作线程。

ClickHouse 的查询执行引擎和小块驱动的并行性 [\[44\]](#page-12-20) 相似，因为通道通常在不同的核心/NUMA 插槽上执行，并且工作线程可以从其他通道窃取任务。此外，没有中央调度组件；相反，工作线程通过持续遍历运算符计划单独选择任务。与小块驱动的并行性不同，ClickHouse 在计划中确定最大并行度，并相比于默认大小 ca. 100,000 行，使用更大的范围来对源表进行分区。虽然在某些情况下可能会导致停滞（例如，当不同通道中过滤器运算符的运行时差异很大时），但我们发现，广泛使用的交换运算符，如重新分配至少可以避免这种不平衡在各阶段之间累积。
### 4.3 多节点并行化 {#4-3-multi-node-parallelization}

如果查询的源表被分片，则接收查询的节点（发起节点）上的查询优化器会尽量在其他节点上执行尽可能多的工作。来自其他节点的结果可以在查询计划的不同点集成。根据查询，远程节点可以选择：1. 将原始源表列流式传输到发起节点，2. 过滤源列并发送存活行，3. 执行过滤和聚合步骤，并发送具有局部聚合状态的局部结果组，或 4. 运行整个查询，包括过滤、聚合和排序。

[图 8](#page-7-1) 中的第 2 ... N 节点显示在持有命中表分片的其他节点上执行的计划片段。这些节点过滤和分组本地数据，并将结果发送到发起节点。节点 1 上的 GroupStateMerge 运算符在结果组最终排序之前合并本地和远程结果。
### <Anchor id="page-7-0"/>4.4 整体性能优化 {#4-4-holistic-performance-optimization}

本节介绍在查询执行不同阶段应用的关键性能优化。

**查询优化**。第一组优化是在从查询的 AST 获得的语义查询表示上应用的。此类优化的示例包括常量折叠（例如，concat(lower('a'), upper('b')) 变为 'aB'），从某些聚合函数中提取标量（例如，sum(a*2) 变为 2 * sum(a)），公共子表达式消除，以及将相等过滤器的析取转换为 IN 列表（例如，x=c 或 x=d 变为 x IN (c,d)）。优化后的语义查询表示随后被转换为逻辑运算符计划。对逻辑计划的优化包括过滤下推、重新排序函数评估和排序步骤，具体取决于哪一个被估计为更昂贵。最后，逻辑查询计划被转换为物理运算符计划。此转换可以利用所涉及表引擎的特性。例如，在 MergeTree* 表引擎的情况下，如果 ORDER BY 列形成主键的前缀，则可以按磁盘顺序读取数据，并且可以从计划中移除排序运算符。还可以使用排序聚合 [\[33\]](#page-12-21)，即直接聚合在排序输入中具有相同值的运行。如果聚合的分组列形成主键的前缀，与 hash 聚合相比，排序聚合显著减少了内存消耗，同时可以在运行处理后立即将聚合值传递给下一个运算符。

**查询编译**。ClickHouse 利用 [基于 LLVM 的查询编译](https://clickhou.se/jit) 动态融合相邻的计划运算符 [\[38,](#page-12-22) [53\]](#page-13-0)。例如，表达式 a * b + c + 1 可以合并为一个运算符，而不是三个运算符。除了表达式外，ClickHouse 还利用编译一次对多个聚合函数进行评估（即，对于 GROUP BY）和对多个排序键进行排序。查询编译减少了虚拟调用的数量，将数据保留在寄存器或 CPU 缓存中，并有助于分支预测，因为需要执行的代码较少。此外，运行时编译启用了一组丰富的优化，例如在编译器中实现的逻辑优化和局部视窗优化，并可以访问最快的本地 CPU 指令。仅当同一常规、聚合或排序表达式被不同查询执行超过可配置次数时，才会启动编译。编译后的查询运算符会被缓存，并且可以被后续查询重用。[7]

**主键索引评估**。如果条件的合取范式中某些过滤子句构成主键列的前缀，则 ClickHouse 会使用主键索引评估 WHERE 条件。主键索引是从按字典顺序排序的键值范围中按照从左到右的方式分析的。对应于主键列的过滤子句使用三元逻辑进行评估——对于范围内的值，都是正确、都是错误，或混合的正确/错误。在后者情况下，范围被拆分成子范围并递归分析。对过滤条件中函数的额外优化存在。首先，函数具有描述其单调性的特征，例如，toDayOfMonth(date) 在一个月内是分段单调的。单调特征能够推断一个函数在已排序输入键值范围内生成排序结果的能力。其次，某些函数可以计算给定函数结果的前像。这用于在比较常数与关键列上的函数调用时，通过将键列值与前像进行比较来替换比较。例如，toYear(k) = 2024 可以用 k >= 2024-01-01 && k < 2025-01-01 代替。

**数据跳过**。ClickHouse 尝试在查询运行时避免数据读取，使用第 [3.2 节](#page-3-0) 中介绍的数据结构。此外，基于启发式和（可选）列统计信息，基于估计的选择性顺序顺序地评估不同列上的过滤器。只有包含至少一行匹配数据的块才会传递给下一个谓词。这逐渐减少了从谓词到谓词所需读取的数据量和计算量。此优化仅在至少存在一个高选择性的谓词时应用；否则，查询的延迟将较串行分析所有谓词时更差。

**哈希表**。哈希表是聚合和哈希连接的重要数据结构。选择合适类型的哈希表对性能至关重要。ClickHouse [实例化](https://clickhou.se/hashtables) 各种哈希表（截至2024年3月有超过30种）来自于一个通用的哈希表模板，哈希函数、分配器、单元类型和调整策略作为变体点。根据分组列的数据类型、估计哈希表基数和其他因素，为每个查询运算符选择最快的哈希表。哈希表的其他实现优化包括：

- 256个子表的两级布局（基于哈希的第一个字节）以支持巨大的键集，
- 针对不同字符串长度具有四个子表和不同哈希函数的字符串哈希表 [\[79\]](#page-13-20)，
- 当键较少时，直接将键用作桶索引的查找表（即不哈希），
- 值中嵌入哈希以更快冲突解决，当比较开销昂贵时（例如字符串，AST），
- 根据运行时统计预测的大小创建哈希表，以避免不必要的调整，
- 在单个内存块上分配多个具有相同创建/销毁生命周期的小哈希表，
- 通过每个哈希图和每个单元版本计数器的即时清除哈希表以供重用，
- 使用 CPU 预取（__builtin_prefetch）加速在哈希键后检索值的过程。

**连接**。由于 ClickHouse 最初只支持哈希连接，许多用例历史上诉诸于非规范化表。如今，数据库 [提供](https://clickhou.se/joins) SQL 中的所有连接类型（内部、左/右/全外部、交叉、按时间切换），以及不同的连接算法，如哈希连接（天真的、格雷斯）、排序-合并连接和索引连接，适用于快速键值查找的表引擎（通常是字典）。

鉴于连接是最昂贵的数据库操作之一，因此提供经典连接算法的并行变体是很重要的，理想情况下具有可配置的空间/时间权衡。对于哈希连接，ClickHouse 实现了来自 [\[7\]](#page-12-23) 的无阻塞共享分区算法。例如，[图 9](#page-8-3) 中的查询计算用户间迁移 URL 的自连接，基于页面命中统计表的结果。连接的构建阶段分为三个通道，覆盖源表的三个不重叠范围。使用分区哈希表，而不是全局哈希表。工人线程（通常为三个）通过计算哈希函数的模数确定构建一侧每个输入行的目标分区。访问哈希表分区使用汇聚交换运算符进行同步。探查阶段以类似的方式找到输入元组的目标分区。虽然此算法为每个元组引入了两个额外的哈希计算，但它大大减少了构建阶段的锁争用，具体取决于哈希表分区数量。

<Anchor id="page-8-3"/><Image img={image_09} size="lg" alt="Image 09"/>

图 9：带有三个哈希表分区的并行哈希连接。
### <Anchor id="page-9-1"/>4.5 工作负载隔离 {#4-5-workload-isolation}

ClickHouse 提供并发控制、内存使用限制和 I/O 调度，使用户能够将查询隔离到工作负载类。通过为特定工作负载类设置共享资源（CPU 核心、DRAM、磁盘和网络 I/O）上的限制，确保这些查询不会影响其他关键业务查询。

并发控制防止在线程过度订阅的情况下出现所面临的问题，尤其是并发查询的数量很高。具体来说，每个查询的工作线程数量根据可用的 CPU 核心数调整动态比例。

ClickHouse 在服务器、用户和查询级别跟踪内存分配的字节大小，从而允许设置灵活的内存使用限制。内存过度承诺使查询能够使用保证内存之外的额外空闲内存，同时确保其他查询有内存限制。此外，可以限制聚合、排序和连接子句的内存使用，从而在超出内存限制时导致回退到外部算法。

最后，I/O 调度允许用户根据最大带宽、正交请求和策略（例如 FIFO、SFC [\[32\]](#page-12-24)）限制工作负载类的本地和远程磁盘访问。
### <Anchor id="page-9-0"/>5 集成层 {#5-integration-layer}

实时决策应用通常依赖于对多个位置的数据进行高效和低延迟的访问。将外部数据在 OLAP 数据库中提供的有两种方法。使用基于推送的数据访问，第三方组件将数据库与外部数据存储连接。一个示例是专门的提取-转换-加载（ETL）工具，它将远程数据推送到目标系统。在基于拉模式的模型中，数据库本身连接到远程数据源，并将数据拉取到本地表中进行查询，或将数据导出到远程系统。虽然基于推送的方法更灵活和常见，但它们的架构占用空间更大且具有可扩展性瓶颈。相比之下，数据库中的远程连接提供了有趣的功能，例如本地和远程数据之间的连接，同时保持整体架构简单，并减少洞察时间。

本节的其余部分探讨 ClickHouse 中的基于拉的数据集成方法，旨在访问远程位置的数据。我们注意到 SQL 数据库中的远程连接的概念并不新颖。例如，自 2011 年以来实施的 SQL/MED 标准 [\[35\]](#page-12-25) 提出外部数据包装器作为管理外部数据的统一接口。与其他数据存储和存储格式的最大互操作性是 ClickHouse 的设计目标之一。截至 2024 年 3 月，ClickHouse 在我们所知的所有分析数据库中提供了最丰富的内置数据集成选项。

外部连接。ClickHouse 提供超过 [50+](https://clickhou.se/query-integrations) 的集成表函数和引擎，以便与外部系统和存储位置进行连接，包括 ODBC、MySQL、PostgreSQL、SQLite、Kafka、Hive、MongoDB、Redis、S3/GCP/Azure 对象存储和各种数据湖。我们进一步将其细分为以下附加图中所示的类别（并不包含在原来的 VLDB 论文中）。

<Anchor id="bonus-figure"/><Image img={image_10} size="lg" alt="Image 10"/>

附加图：ClickBench 的互操作性选项。

通过集成 **表函数** 的临时访问。表函数可以在 SELECT 查询的 FROM 子句中调用，以读取远程数据以进行探索性临时查询。或者，它们可用于通过 INSERT INTO TABLE FUNCTION 语句将数据写入远程存储。

持久访问。创建与远程数据存储和处理系统的永久连接有三种方法。

首先，集成 **表引擎** 将远程数据源，如 MySQL 表，表示为持久的本地表。用户使用 CREATE TABLE AS 语法存储表定义，并结合 SELECT 查询和表函数。可以指定自定义模式，例如，仅引用远程列的子集，或使用模式推断自动确定列名和等效的 ClickHouse 类型。我们进一步区分被动和主动的运行时行为：被动表引擎将查询转发给远程系统，并用结果填充本地代理表。而主动表引擎则定期从远程系统拉取数据或订阅远程更改，例如通过 PostgreSQL 的逻辑复制协议。因此，本地表包含远程表的完整副本。

其二，集成 **数据库引擎** 将远程数据存储中的表模式中的所有表映射到 ClickHouse。与前者不同，它们一般要求远程数据存储为关系数据库，并且附加提供有限支持的 DDL 语句。

第三，**字典** 可以使用针对几乎所有可能数据源的任意查询填充，其对应的集成表函数或引擎。运行时行为是主动的，因为数据在恒定时间间隔从远程存储中拉取。

数据格式。为了与第三方系统交互，现代分析数据库还必须能够处理任何格式的数据。除了其本机格式外，ClickHouse 还支持 [90+](https://clickhou.se/query-formats) 种格式，包括 CSV、JSON、Parquet、Avro、ORC、Arrow 和 Protobuf。每种格式可以是输入格式（ClickHouse 可以读取），输出格式（ClickHouse 可以导出），或两者均是。某些面向分析的格式，例如 Parquet，甚至与查询处理集成，即优化器可以利用嵌入的统计信息，过滤器直接在压缩数据上进行评估。

兼容性接口。除了其本机二进制传输协议和 HTTP 外，客户端还可以通过与 MySQL 或 PostgreSQL 传输协议兼容的接口与 ClickHouse 进行交互。此兼容性特征对于启用来自尚未实现原生 ClickHouse 连接的专有应用程序（例如某些商业智能工具）的访问非常有用。
## 6 性能作为特性 {#6-performance-as-a-feature}

本节介绍用于性能分析的内置工具，并使用现实世界和基准查询评估性能。
### 6.1 内置性能分析工具 {#6-1-built-in-performance-analysis-tools}

提供一系列广泛的工具，以调查单个查询或后台操作中的性能瓶颈。用户通过基于系统表的统一接口与所有工具进行交互。

**服务器和查询指标**。服务器级统计数据，例如活动片段计数、网络吞吐量和缓存命中率，结合每个查询统计信息，如读取的块数或索引使用统计数据进行补充。度量是在可配置的时间间隔内同步（应请求）或异步计算的。

**采样分析器**。可以使用采样分析器收集服务器线程的调用栈。结果可选地导出到外部工具，例如火焰图可视化工具。

**OpenTelemetry 集成**。OpenTelemetry 是一个跨多个数据处理系统跟踪数据行的开放标准 [\[8\]](#page-12-26)。ClickHouse 可以生成所有查询处理步骤的 OpenTelemetry 日志跨度，具有可配置的粒度，并可以收集和分析来自其他系统的 OpenTelemetry 日志跨度。

**解释查询**。与其他数据库一样，SELECT 查询可以由 EXPLAIN 前置，用于深入了解查询的 AST、逻辑和物理运算符计划及执行时间行为。
### 6.2 基准测试 {#6-2-benchmarks}

尽管基准测试因未准确反映真实情况而受到批评 [\[10,](#page-12-27) [52,](#page-13-22) [66,](#page-13-23) [74\]](#page-13-24)，但它仍然便于识别数据库的优缺点。接下来，我们讨论基准测试如何用来评估 ClickHouse 的性能。
#### 6.2.1 非规范化表 {#6-2-1-denormalized-tables}

针对非规范化事实表的过滤和聚合查询历史上是 ClickHouse 的主要用例。我们报告了 ClickBench 的运行时间，这是一个典型的工作负载，模拟了用于点击流和流量分析的临时和定期报告查询。基准测试包括对一个具有 1 亿个匿名页面点击的表进行的 43 个查询，这些数据来源于网络上最大的分析平台之一。一个在线仪表板 [\[17\]](#page-12-28) 显示了截至 2024 年 6 月的 45 个商业和研究数据库的测量（冷/热运行时间、数据导入时间、磁盘大小）。结果由独立贡献者根据公开可用的数据集和查询提交 [\[16\]](#page-12-29)。这些查询测试了顺序和索引扫描访问路径，并定期暴露 CPU、IO 或内存约束的关系操作符。

[图 10](#page-10-0) 显示了在用于分析的数据库中，顺序执行所有 ClickBench 查询的总相对冷和热运行时间。这些测量是在具有 16 个 vCPU、32 GB RAM 和 5000 IOPS / 1000 MiB/s 磁盘的单节点 AWS EC2 c6a.4xlarge 实例上进行的。类似的系统用于 Redshift ([ra3.4xlarge](https://clickhou.se/redshift-sizes)，12 vCPUs，96 GB RAM) 和 Snowfake ([仓库大小 S](https://clickhou.se/snowflake-sizes)：2x8 vCPUs，2x16 GB RAM)。物理数据库设计只有轻微调整，例如，我们指定了主键，但不更改单独列的压缩，不创建投影或跳过索引。在每次冷查询运行之前，我们还清空 Linux 页缓存，但不调整数据库或操作系统的设置。对于每个查询，所有数据库中最快的运行时间作为基准。其他数据库的相对查询运行时间计算为 ( + 10)/(_ + 10)。数据库的总相对运行时间是每个查询比率的几何均值。虽然研究数据库 Umbra [\[54\]](#page-13-25) 在热运行时间上表现最佳，但 ClickHouse 在热和冷运行时间上优于所有其他生产级数据库。

<Anchor id="page-10-0"/><Image img={image_11} size="lg" alt="Image 11"/>

图 10: ClickBench 的相对冷和热运行时间。

为了跟踪 SELECT 在更广泛工作负载中的性能，我们 [使用](https://clickhou.se/performance-over-years) 四个基准组合称为 VersionsBench [\[19\]](#page-12-30)。该基准每月执行一次，当发布新版本时以评估其性能 [\[20\]](#page-12-31) 并识别可能导致性能下降的代码更改：各个基准包括：1. ClickBench（如上所述），2. 15 个 MgBench [\[21\]](#page-12-32) 查询，3. 13 个针对一个具有 6 亿行的非规范化星型架构基准 [\[57\]](#page-13-26) 的查询。4. 针对 [NYC Taxi Rides](https://clickhou.se/nyc-taxi-rides-benchmark) 进行的 4 个查询，数据量为 34 亿行 [\[70\]](#page-13-27)。

[图 11](#page-10-5) 显示了 2018 年 3 月至 2024 年 3 月间 77 个 ClickHouse 版本的 VersionsBench 运行时间的发展。为了弥补各个查询相对运行时间的差异，我们使用几何均值将运行时间进行归一化，以最低查询运行时间作为权重。VersionBench 在过去六年中的性能提高了 1.72 ×。长期支持（LTS）版本的发布日期在 x 轴上标记。尽管在某些时期性能短暂恶化，但 LTS 版本通常具有与之前的 LTS 版本相当或更好的性能。 2022 年 8 月显著的改进是由第 [4.4](#page-7-0) 节中描述的逐列过滤评估技术引起的。

<Anchor id="page-10-5"/><Image img={image_12} size="lg" alt="Image 12"/>

图 11: VersionsBench 2018-2024 的相对热运行时间。

#### 6.2.2 规范化表 {#6-2-2-normalized-tables}

在经典的数据仓库中，数据通常使用星型或雪花模式建模。我们展示了 TPC-H 查询（规模因子 100）的运行时间，但指出规范化表是 ClickHouse 的新兴用例。 [图 12](#page-10-6) 显示了基于第 [4.4](#page-7-0) 节中描述的并行哈希连接算法的 TPC-H 查询的热运行时间。这些测量是在具有 64 个 vCPUs、128 GB RAM 和 5000 IOPS / 1000 MiB/s 磁盘的单节点 AWS EC2 c6i.16xlarge 实例上进行的。记录了五次运行中最快的一次。作为参考，我们在一个大小相当的 Snowfake 系统中进行了相同的测量（仓库大小 L，8x8 vCPUs，8x16 GB RAM）。表中排除了 11 个查询的结果：查询 Q2、Q4、Q13、Q17 和 Q20-22 包含自相关子查询，而截至 ClickHouse v24.6 版本并不支持这些查询。查询 Q7-Q9 和 Q19 依赖于针对连接的扩展计划级优化，如连接重排序和连接谓词下推（截至 ClickHouse v24.6 均缺失），以实现可行的运行时间。计划在 2024 年实现的自动子查询去相关和更好的连接优化器支持 [\[18\]](#page-12-33)。在剩下的 11 个查询中，5 个（6 个）查询在 ClickHouse 中执行得更快（比 Snowfake 更快）。如上所述的优化对于性能至关重要 [\[27\]](#page-12-34)，我们期望在实现后进一步改善这些查询的运行时间。

<Anchor id="page-10-6"/><Image img={image_13} size="lg" alt="Image 13"/>

图 12: TPC-H 查询的热运行时间（以秒为单位）。

## 7 相关工作 {#7-related-work}

在最近几十年，分析数据库引起了极大的学术和商业兴趣 [\[1\]](#page-12-35)。早期的系统如 Sybase IQ [\[48\]](#page-13-28)、Teradata [\[72\]](#page-13-29)、Vertica [\[42\]](#page-12-36) 和 Greenplum [\[47\]](#page-13-30) 的特点是昂贵的批量 ETL 作业和由于其本地部署性质而有限的弹性。在 2010 年代初，云原生数据仓库和数据库即服务（DBaaS）产品的出现，例如 Snowfake [\[22\]](#page-12-37)、BigQuery [\[49\]](#page-13-31) 和 Redshift [\[4\]](#page-12-38) 大大降低了组织进行分析的成本和复杂性，同时受益于高可用性和自动资源扩展。最近，分析执行内核（例如 Photon [\[5\]](#page-12-39) 和 Velox [\[62\]](#page-13-32)）提供了共修改的数据处理功能，可用于不同的分析、流处理和机器学习应用。

在目标和设计原则方面，最与 ClickHouse 相似的数据库是 Druid [\[78\]](#page-13-33) 和 Pinot [\[34\]](#page-12-40)。这两个系统以高数据摄取率实现实时分析。与 ClickHouse 类似，表被分割成称为段的水平部分。虽然 ClickHouse 会不断合并较小的部分，并可选地利用第 [3.3](#page-4-3) 节中的技术来减少数据量，但在 Druid 和 Pinot 中部分永远是不可变的。此外，Druid 和 Pinot 需要专业节点来创建、修改和搜索表，而 ClickHouse 使用单一的二进制文件来完成这些任务。

Snowfake [\[22\]](#page-12-37) 是一个基于共享磁盘架构的流行专有云数据仓库。它将表分割成微分区的方法与 ClickHouse 中的部分概念相似。Snowfake 使用混合 PAX 页 [\[3\]](#page-12-41) 进行持久化，而 ClickHouse 的存储格式严格为列式。Snowfake 还强调使用自动创建的轻量级索引 [\[31,](#page-12-13) [51\]](#page-13-14) 进行本地缓存和数据修剪，以获得良好的性能。与 ClickHouse 中的主键类似，用户可以选择创建聚集索引以共定位具有相同值的数据。

Photon [\[5\]](#page-12-39) 和 Velox [\[62\]](#page-13-32) 是设计用作复杂数据管理系统中组件的查询执行引擎。这两个系统接受查询计划作为输入，然后在其本地节点上对 Parquet（Photon）或 Arrow（Velox）文件进行执行 [\[46\]](#page-13-34)。ClickHouse 能够使用这些通用格式消费和生成数据，但更喜欢其本地文件格式进行存储。虽然 Velox 和 Photon 不优化查询计划（Velox 执行基本的表达式优化），但它们利用运行时适应技术，例如根据数据特征动态切换计算内核。类似地，ClickHouse 中的计划操作符可以在运行时创建其他操作符，主要是基于查询内存消耗，切换到外部聚合或连接操作符。Photon 论文指出，代码生成设计 [\[38,](#page-12-22) [41,](#page-12-42) [53\]](#page-13-0) 比解释性的向量化设计 [\[11\]](#page-12-0) 更难开发和调试。Velox 中对代码生成的（实验性）支持构建并链接从运行时生成的 C++ 代码创建的共享库，而 ClickHouse 直接与 LLVM 的请求编译 API 进行交互。

DuckDB [\[67\]](#page-13-6) 也旨在由宿主进程嵌入，但还提供查询优化和事务。它旨在处理 OLAP 查询并混合偶尔的 OLTP 语句。DuckDB 因而选择了 DataBlocks [\[43\]](#page-12-43) 存储格式，该格式采用轻量级压缩方法，如顺序保留字典或参照框架 [\[2\]](#page-12-10)，以在混合工作负载中获得良好性能。相反，ClickHouse 针对追加只使用情况进行了优化，即没有或很少的更新和删除。块使用重型技术（如 LZ4）进行压缩，假定用户广泛使用数据修剪以加快频繁查询，并且 I/O 成本远大于剩余查询的解压缩成本。DuckDB 还基于 Hyper 的 MVCC 方案 [\[55\]](#page-13-35) 提供可序列化事务，而 ClickHouse 仅提供快照隔离。

## 8 结论与展望 {#8-conclusion-and-outlook}

我们展示了 ClickHouse 的架构，一个开源的高性能 OLAP 数据库。凭借优化写入的存储层和最先进的向量化查询引擎作为基础，ClickHouse 能够对 PB 级数据集进行实时分析，具有高摄取速度。通过在后台异步合并和转换数据，ClickHouse 有效地解耦了数据维护和并行插入。其存储层通过稀疏主索引、跳过索引和投影表实现了积极的数据修剪。我们描述了 ClickHouse 的更新和删除的实现、幂等插入以及跨节点的数据复制，以确保高可用性。查询处理层使用大量技术优化查询，并在所有服务器和集群资源之间并行执行。集成表引擎和函数提供了便捷的方式与其他数据管理系统和数据格式无缝互动。通过基准测试，我们证明了 ClickHouse 是市场上最快的分析数据库之一，并且展示了 ClickHouse 在实际部署中的典型查询性能显著提高。

所有计划在 2024 年实施的功能和增强可以在公共路线图上找到 [\[18\]](#page-12-33)。计划改进包括对用户事务的支持、PromQL [\[69\]](#page-13-36) 作为替代查询语言、新的半结构化数据（例如 JSON）的数据类型、对连接的更好的计划级优化，以及实现轻量级更新以补充轻量级删除。

## 致谢 {#acknowledgements}

根据版本 24.6，SELECT * FROM system.contributors 返回 1994 位对 ClickHouse 作出贡献的个人。我们感谢 ClickHouse Inc. 的整个工程团队以及 ClickHouse 令人惊叹的开源社区在共同构建这个数据库方面的辛勤工作和奉献精神。
## REFERENCES {#references}

- <Anchor id="page-12-35"/>[1] Daniel Abadi, Peter Boncz, Stavros Harizopoulos, Stratos Idreaos, and Samuel Madden. 2013. 现代列式数据库系统的设计与实现. https://doi.org/10.1561/9781601987556
- <Anchor id="page-12-10"/>[2] Daniel Abadi, Samuel Madden, and Miguel Ferreira. 2006. 在列式数据库系统中集成压缩和执行. 在 2006 年 ACM SIGMOD 数据管理国际会议 (SIGMOD '06) 的论文集. 671–682. https://doi.org/10.1145/1142473.1142548
- <Anchor id="page-12-41"/>[3] Anastassia Ailamaki, David J. DeWitt, Mark D. Hill, and Marios Skounakis. 2001. 为缓存性能编织关系. 在第 27 届国际大型数据库会议 (VLDB '01) 的论文集中. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 169–180.
- <Anchor id="page-12-38"/>[4] Nikos Armenatzoglou, Sanuj Basu, Naga Bhanoori, Mengchu Cai, Naresh Chainani, Kiran Chinta, Venkatraman Govindaraju, Todd J. Green, Monish Gupta, Sebastian Hillig, Eric Hotinger, Yan Leshinksy, Jintian Liang, Michael McCreedy, Fabian Nagel, Ippokratis Pandis, Panos Parchas, Rahul Pathak, Orestis Polychroniou, Foyzur Rahman, Gaurav Saxena, Gokul Soundararajan, Sriram Subramanian, and Doug Terry. 2022. 亚马逊 Redshift 重新构想. 在 2022 年国际数据管理会议 (费城, PA, USA) (SIGMOD '22) 的论文集中。计算机协会，纽约，NY，USA, 2205–2217. https://doi.org/10.1145/3514221.3526045
- <Anchor id="page-12-39"/>[5] Alexander Behm, Shoumik Palkar, Utkarsh Agarwal, Timothy Armstrong, David Cashman, Ankur Dave, Todd Greenstein, Shant Hovsepian, Ryan Johnson, Arvind Sai Krishnan, Paul Leventis, Ala Luszczak, Prashanth Menon, Mostafa Mokhtar, Gene Pang, Sameer Paranjpye, Greg Rahn, Bart Samwel, Tom van Bussel, Herman van Hovell, Maryann Xue, Reynold Xin, and Matei Zaharia. 2022. Photon: 一个快速的查询引擎，对 Lakehouse 系统 (SIGMOD '22). 计算机协会，纽约，NY，USA, 2326–2339. https://doi.org/10.1145/3514221.3526054
- <Anchor id="page-12-18"/>[6] Philip A. Bernstein and Nathan Goodman. 1981. 分布式数据库系统中的并发控制. ACM 计算机调查 13, 2 (1981), 185–221. https://doi.org/10.1145/356842.356846
- <Anchor id="page-12-23"/>[7] Spyros Blanas, Yinan Li, and Jignesh M. Patel. 2011. 为多核CPU设计和评估主内存哈希连接算法. 在 2011 年 ACM SIGMOD 数据管理国际会议 (希腊雅典) (SIGMOD '11) 的论文集中。计算机协会，纽约，NY，USA, 37–48. https://doi.org/10.1145/1989323.1989328
- <Anchor id="page-12-26"/><Anchor id="page-12-14"/>[8] Daniel Gomez Blanco. 2023. 实用 OpenTelemetry. Springer Nature.
- [9] Burton H. Bloom. 1970. 允许错误的哈希编码中的空间/时间权衡. Commun. ACM 13, 7 (1970), 422–426. https://doi.org/10.1145/362686.362692
- <Anchor id="page-12-27"/>[10] Peter Boncz, Thomas Neumann, and Orri Erling. 2014. TPC-H 分析: 从一个影响力基准中获得的隐藏信息和经验教训. 在性能特征与基准测试中. 61–76. https://doi.org/10.1007/978-3-319-04936-6_5
- <Anchor id="page-12-0"/>[11] Peter Boncz, Marcin Zukowski, and Niels Nes. 2005. MonetDB/X100: 超管道查询执行. 在 CIDR.
- <Anchor id="page-12-8"/>[12] Martin Burtscher and Paruj Ratanaworabhan. 2007. 高吞吐量双精度浮点数据压缩. 在数据压缩会议 (DCC). 293–302. https://doi.org/10.1109/DCC.2007.44
- <Anchor id="page-12-6"/>[13] Jef Carpenter and Eben Hewitt. 2016. Cassandra: 权威指南（第 2 版）。O'Reilly 媒体公司.
- <Anchor id="page-12-17"/>[14] Bernadette Charron-Bost, Fernando Pedone, and André Schiper (编辑). 2010. 复制: 理论与实践. Springer-Verlag.
- <Anchor id="page-12-3"/>[15] chDB. 2024. chDB - 嵌入式 OLAP SQL 引擎. 取自 https://github.com/chdb-io/chdb 于 2024-06-20
- <Anchor id="page-12-29"/>[16] ClickHouse. 2024. ClickBench: 分析数据库的基准测试. 取自 https://github.com/ClickHouse/ClickBench 于 2024-06-20
- <Anchor id="page-12-28"/>[17] ClickHouse. 2024. ClickBench: 比较测量. 取自 https://benchmark.clickhouse.com 于 2024-06-20
- <Anchor id="page-12-33"/>[18] ClickHouse. 2024. ClickHouse 路线图 2024 (GitHub). 取自 https://github.com/ClickHouse/ClickHouse/issues/58392 于 2024-06-20
- <Anchor id="page-12-30"/>[19] ClickHouse. 2024. ClickHouse 版本基准测试. 取自 https://github.com/ClickHouse/ClickBench/tree/main/versions 于 2024-06-20
- <Anchor id="page-12-31"/>[20] ClickHouse. 2024. ClickHouse 版本基准测试结果. 取自 https://benchmark.clickhouse.com/versions/ 于 2024-06-20
- <Anchor id="page-12-32"/>[21] Andrew Crotty. 2022. MgBench. 取自 https://github.com/andrewcrotty/mgbench 于 2024-06-20
- <Anchor id="page-12-37"/>[22] Benoit Dageville, Thierry Cruanes, Marcin Zukowski, Vadim Antonov, Artin Avanes, Jon Bock, Jonathan Claybaugh, Daniel Engovatov, Martin Hentschel, Jiansheng Huang, Allison W. Lee, Ashish Motivala, Abdul Q. Munir, Steven Pelley, Peter Povinec, Greg Rahn, Spyridon Triantafyllis, and Philipp Unterbrunner. 2016. Snowflake 弹性数据仓库. 在 2016 年国际数据管理会议 (旧金山，加利福尼亚，USA) (SIGMOD '16) 的论文集中. 计算机协会，纽约，NY，USA, 215–226. https://doi.org/10.1145/2882903.2903741
- <Anchor id="page-12-9"/>[23] Patrick Damme, Annett Ungethüm, Juliana Hildebrandt, Dirk Habich, and Wolfgang Lehner. 2019. 从全面的实验调查到基于成本的轻量级整数压缩算法选择策略. ACM Trans. Database Syst. 44, 3, Article 9 (2019), 46 页. https://doi.org/10.1145/3323991
- <Anchor id="page-12-1"/>[24] Philippe Dobbelaere and Kyumars Sheykh Esmaili. 2017. Kafka 与 RabbitMQ: 两个行业参考发布/订阅实现的比较研究: 行业论文 (DEBS '17). 计算机协会，纽约，NY，USA, 227–238. https://doi.org/10.1145/3093742.3093908
- <Anchor id="page-12-19"/>[25] LLVM documentation. 2024. LLVM中的自动向量化. 取自 https://llvm.org/docs/Vectorizers.html 于 2024-06-20
- <Anchor id="page-12-7"/>[26] Siying Dong, Andrew Kryczka, Yanqin Jin, and Michael Stumm. 2021. RocksDB: 为大规模应用提供服务的键值存储中的开发优先级演变. ACM Transactions on Storage 17, 4, Article 26 (2021), 32 页. https://doi.org/10.1145/3483840
- <Anchor id="page-12-34"/>[27] Markus Dreseler, Martin Boissier, Tilmann Rabl, and Matthias Ufacker. 2020. 定量分析 TPC-H 瓶颈及其优化. Proc. VLDB Endow. 13, 8 (2020), 1206–1220. https://doi.org/10.14778/3389133.3389138
- <Anchor id="page-12-12"/>[28] Ted Dunning. 2021. t-digest: 高效的分布估计. 软件影响 7 (2021). https://doi.org/10.1016/j.simpa.2020.100049
- <Anchor id="page-12-16"/>[29] Martin Faust, Martin Boissier, Marvin Keller, David Schwalb, Holger Bischof, Katrin Eisenreich, Franz Färber, and Hasso Plattner. 2016. 利用哈希索引在 SAP HANA 中降低足迹和执行唯一性. 在数据库与专家系统应用中. 137–151. https://doi.org/10.1007/978-3-319-44406-2_11
- <Anchor id="page-12-11"/>[30] Philippe Flajolet, Eric Fusy, Olivier Gandouet, and Frederic Meunier. 2007. HyperLogLog: 近似最优基数估计算法的分析. 在 AofA: 算法分析, Vol. DMTCS Proceedings vol. AH, 2007 年算法分析会议 (AofA 07). 离散数学与理论计算机科学, 137–156. https://doi.org/10.46298/dmtcs.3545
- <Anchor id="page-12-13"/>[31] Hector Garcia-Molina, Jefrey D. Ullman, and Jennifer Widom. 2009. 数据库系统 - 完整书籍 (第 2 版)。
- <Anchor id="page-12-24"/>[32] Pawan Goyal, Harrick M. Vin, and Haichen Chen. 1996. 启动时间公平排队：用于综合服务数据包交换网络的调度算法. 26, 4 (1996), 157–168. https://doi.org/10.1145/248157.248171
- <Anchor id="page-12-21"/>[33] Goetz Graefe. 1993. 大型数据库的查询评估技术. ACM Comput. Surv. 25, 2 (1993), 73–169. https://doi.org/10.1145/152610.152611
- <Anchor id="page-12-40"/>[34] Jean-François Im, Kishore Gopalakrishna, Subbu Subramaniam, Mayank Shrivastava, Adwait Tumbde, Xiaotian Jiang, Jennifer Dai, Seunghyun Lee, Neha Pawar, Jialiang Li, and Ravi Aringunram. 2018. Pinot: 为 5.3 亿用户提供实时 OLAP. 在 2018 年国际数据管理会议 (休斯顿, TX, USA) (SIGMOD '18) 的论文集中. 计算机协会，纽约，NY, USA, 583–594. https://doi.org/10.1145/3183713.3190661
- <Anchor id="page-12-25"/>[35] ISO/IEC 9075-9:2001 2001. 信息技术 — 数据库语言 — SQL — 第 9 部分: 外部数据管理 (SQL/MED). 标准. 国际标准化组织.
- <Anchor id="page-12-2"/>[36] Paras Jain, Peter Kraft, Conor Power, Tathagata Das, Ion Stoica, and Matei Zaharia. 2023. 分析与比较 Lakehouse 存储系统. CIDR.
- <Anchor id="page-12-4"/>[37] Project Jupyter. 2024. Jupyter 笔记本. 取自 https://jupyter.org/ 于 2024-06-20
- <Anchor id="page-12-22"/>[38] Timo Kersten, Viktor Leis, Alfons Kemper, Thomas Neumann, Andrew Pavlo, and Peter Boncz. 2018. 一切你想了解的关于编译和向量化查询的知识，但你又不敢问. Proc. VLDB Endow. 11, 13 (2018 年 9 月), 2209–2222. https://doi.org/10.14778/3275366.3284966
- <Anchor id="page-12-15"/>[39] Changkyu Kim, Jatin Chhugani, Nadathur Satish, Eric Sedlar, Anthony D. Nguyen, Tim Kaldewey, Victor W. Lee, Scott A. Brandt, and Pradeep Dubey. 2010. FAST: 现代 CPU 和 GPU 上快速架构敏感的树搜索. 在 2010 年 ACM SIGMOD 数据管理国际会议 (印度纳波利斯, 印第安纳州, USA) (SIGMOD '10) 的论文集. 计算机协会，纽约，NY，USA, 339–350. https://doi.org/10.1145/1807167.1807206
- <Anchor id="page-12-5"/>[40] Donald E. Knuth. 1973. 计算机程序设计的艺术, 第三卷: 排序与搜索. Addison-Wesley.
- <Anchor id="page-12-42"/>[41] André Kohn, Viktor Leis, and Thomas Neumann. 2018. 编译查询的自适应执行. 在 2018 IEEE 第 34 屆数据工程国际会议 (ICDE). 197–208. https://doi.org/10.1109/ICDE.2018.00027
- <Anchor id="page-12-36"/>[42] Andrew Lamb, Matt Fuller, Ramakrishna Varadarajan, Nga Tran, Ben Vandiver, Lyric Doshi, and Chuck Bear. 2012. Vertica 分析数据库：C-Store 七年后. Proc. VLDB Endow. 5, 12 (2012 年 8 月), 1790–1801. https://doi.org/10.14778/2367502.2367518
- <Anchor id="page-12-43"/>[43] Harald Lang, Tobias Mühlbauer, Florian Funke, Peter A. Boncz, Thomas Neumann, and Alfons Kemper. 2016. 数据块：使用向量化和编译的压缩存储实现混合 OLTP 和 OLAP. 在 2016 年国际数据管理会议 (旧金山，加利福尼亚，USA) (SIGMOD '16) 的论文中。计算机协会，纽约，NY，USA, 311–326. https://doi.org/10.1145/2882903.2882925
- <Anchor id="page-12-20"/>[44] Viktor Leis, Peter Boncz, Alfons Kemper, and Thomas Neumann. 2014. 以碎片驱动的并行性: 对于许多核心时代的 NUMA 感知查询评估框架. 在 2014 年 ACM SIGMOD 数据管理国际会议 (雪鸟, 犹他州, USA) (SIGMOD '14) 的论文集中. 计算机协会，纽约，NY，USA, 743–754. https://doi.org/10.1145/2588555.2610507
- <Anchor id="page-13-17"/>[45] Viktor Leis, Alfons Kemper, and Thomas Neumann. 2013. 自适应基数树：用于主内存数据库的 ARTful 索引. 在 2013 IEEE 第 29 屆数据工程国际会议 (ICDE). 38–49. https://doi.org/10.1109/ICDE.2013.6544812
- <Anchor id="page-13-34"/>[46] Chunwei Liu, Anna Pavlenko, Matteo Interlandi, and Brandon Haynes. 2023. 深入探讨分析数据库管理系统的常见开放格式. 16, 11 (2023 年 7 月), 3044–3056. https://doi.org/10.14778/3611479.3611507
- <Anchor id="page-13-30"/>[47] Zhenghua Lyu, Huan Hubert Zhang, Gang Xiong, Gang Guo, Haozhou Wang, Jinbao Chen, Asim Praveen, Yu Yang, Xiaoming Gao, Alexandra Wang, Wen Lin, Ashwin Agrawal, Junfeng Yang, Hao Wu, Xiaoliang Li, Feng Guo, Jiang Wu, Jesse Zhang, and Venkatesh Raghavan. 2021. Greenplum: 一个处理事务和分析工作负载的混合数据库 (SIGMOD '21). 计算机协会，纽约，NY，USA, 2530–2542. https://doi.org/10.1145/3448016.3457562
- <Anchor id="page-13-28"/>[48] Roger MacNicol and Blaine French. 2004. Sybase IQ Multiplex - 针对分析设计. 在第 30 届国际大型数据库会议 (多伦多, 加拿大) (VLDB '04) 的论文集中. VLDB 基金会, 1227–1230.
- <Anchor id="page-13-31"/>[49] Sergey Melnik, Andrey Gubarev, Jing Jing Long, Geofrey Romer, Shiva Shivakumar, Matt Tolton, Theo Vassilakis, Hossein Ahmadi, Dan Delorey, Slava Min, Mosha Pasumansky, and Jef Shute. 2020. Dremel: 十年的互动 SQL 分析在网络规模. Proc. VLDB Endow. 13, 12 (2020 年 8 月), 3461–3472. https://doi.org/10.14778/3415478.3415568
- <Anchor id="page-13-2"/>[50] Microsoft. 2024. Kusto 查询语言. 取自 https://github.com/microsoft/Kusto-Query-Language 于 2024-06-20
- <Anchor id="page-13-14"/>[51] Guido Moerkotte. 1998. 小物化聚合：数据仓库的轻量级索引结构. 在第 24 届国际大型数据库会议 (VLDB '98) 的论文集中. 476–487.
- <Anchor id="page-13-22"/>[52] Jalal Mostafa, Sara Wehbi, Suren Chilingaryan, and Andreas Kopmann. 2022. SciTS: 一个用于科学实验和工业物联网的时间序列数据库基准. 在第 34 届科学与统计数据库管理国际会议 (SSDBM '22) 的论文集中. 第 12 篇文章. https://doi.org/10.1145/3538712.3538723
- <Anchor id="page-13-0"/>[53] Thomas Neumann. 2011. 为现代硬件高效编译有效查询计划. Proc. VLDB Endow. 4, 9 (2011 年 6 月), 539–550. https://doi.org/10.14778/2002938.2002940
- <Anchor id="page-13-25"/>[54] Thomas Neumann和 Michael J. Freitag. 2020. Umbra: 一个基于磁盘的系统，具有内存性能. 在第 10 届创新数据系统研究会议，CIDR 2020，阿姆斯特丹, 荷兰，2020 年 1 月 12-15，在线论文集. www.cidrdb.org. http://cidrdb.org/cidr2020/papers/p29-neumann-cidr20.pdf
- <Anchor id="page-13-35"/>[55] Thomas Neumann, Tobias Mühlbauer, and Alfons Kemper. 2015. 对主内存数据库系统的快速可序列化多版本并发控制. 在 2015 年 ACM SIGMOD 数据管理国际会议 (墨尔本，维多利亚，澳大利亚) (SIGMOD '15) 的论文集中. 计算机协会，纽约，NY，USA, 677–689. https://doi.org/10.1145/2723372.2749436
- <Anchor id="page-13-8"/>[56] LevelDB on GitHub. 2024. LevelDB. 取自 https://github.com/google/leveldb 于 2024-06-20
- <Anchor id="page-13-26"/>[57] Patrick O'Neil, Elizabeth O'Neil, Xuedong Chen, and Stephen Revilak. 2009. 星型模式基准和扩展事实表索引. 在性能评估和基准测试中. Springer Berlin Heidelberg, 237–252. https://doi.org/10.1007/978-3-642-10424-4_17
- <Anchor id="page-13-7"/>[58] Patrick E. O'Neil, Edward Y. C. Cheng, Dieter Gawlick, and Elizabeth J. O'Neil. 1996. 日志结构合并树 (LSM-tree). Acta Informatica 33 (1996), 351–385. https://doi.org/10.1007/s002360050048
- <Anchor id="page-13-4"/>[59] Diego Ongaro and John Ousterhout. 2014. 寻找可理解的共识算法. 在 2014 年 USENIX 年度技术会议 (USENIX ATC'14) 的论文集中. 305–320. https://doi.org/doi/10.5555/2643634.2643666
- <Anchor id="page-13-3"/>[60] Patrick O'Neil, Edward Cheng, Dieter Gawlick, and Elizabeth O'Neil. 1996. 日志结构合并树 (LSM-Tree). Acta Inf. 33, 4 (1996), 351–385. https://doi.org/10.1007/s002360050048
- <Anchor id="page-13-5"/>[61] Pandas. 2024. Pandas 数据框. 取自 https://pandas.pydata.org/ 于 2024-06-20
- <Anchor id="page-13-32"/>[62] Pedro Pedreira, Orri Erling, Masha Basmanova, Kevin Wilfong, Laith Sakka, Krishna Pai, Wei He, and Biswapesh Chattopadhyay. 2022. Velox: Meta 的统一执行引擎. Proc. VLDB Endow. 15, 12 (2022 年 8 月), 3372–3384. https://doi.org/10.14778/3554821.3554829
- <Anchor id="page-13-10"/>[63] Tuomas Pelkonen, Scott Franklin, Justin Teller, Paul Cavallaro, Qi Huang, Justin Meza, and Kaushik Veeraraghavan. 2015. Gorilla: 一个快速、可扩展的内存时间序列数据库. VLDB 纪要 8, 12 (2015), 1816–1827. https://doi.org/10.14778/2824032.2824078
- <Anchor id="page-13-18"/>[64] Orestis Polychroniou, Arun Raghavan, and Kenneth A. Ross. 2015. 重新思考针对内存数据库的 SIMD 向量化. 在 2015 年 ACM SIGMOD 数据管理国际会议 (SIGMOD '15) 的论文集中. 1493–1508. https://doi.org/10.1145/2723372.2747645
- <Anchor id="page-13-21"/>[65] PostgreSQL. 2024. PostgreSQL - 外部数据封装器. 取自 https://wiki.postgresql.org/wiki/Foreign_data_wrappers 于 2024-06-20
- <Anchor id="page-13-23"/>[66] Mark Raasveldt, Pedro Holanda, Tim Gubner, and Hannes Mühleisen. 2018. 公平基准测试被认为困难: 数据库性能测试中的常见陷阱. 在数据库系统测试研讨会的论文集 (休斯顿, TX, USA) (DBTest'18). 第 2 篇文章, 6 页. https://doi.org/10.1145/3209950.3209955
- <Anchor id="page-13-6"/>[67] Mark Raasveldt and Hannes Mühleisen. 2019. DuckDB: 一个可嵌入的分析数据库 (SIGMOD '19). 计算机协会，纽约，NY，USA, 1981–1984. https://doi.org/10.1145/3299869.3320212
- <Anchor id="page-13-16"/>[68] Jun Rao and Kenneth A. Ross. 1999. 面向决策支持的缓存意识索引. 在第 25 届国际大型数据库会议 (VLDB '99) 的论文集中. 旧金山，加州, USA, 78–89.
- <Anchor id="page-13-36"/>[69] Navin C. Sabharwal and Piyush Kant Pandey. 2020. 使用 Prometheus 查询语言 (PromQL). 在监控微服务和容器化应用程序中. https://doi.org/10.1007/978-1-4842-6216-0_5
- <Anchor id="page-13-27"/>[70] Todd W. Schneider. 2022. 纽约市出租车和租赁车辆数据. 取自 https://github.com/toddwschneider/nyc-taxi-data 于 2024-06-20
- <Anchor id="page-13-13"/>[71] Mike Stonebraker, Daniel J. Abadi, Adam Batkin, Xuedong Chen, Mitch Cherniack, Miguel Ferreira, Edmond Lau, Amerson Lin, Sam Madden, Elizabeth O'Neil, Pat O'Neil, Alex Rasin, Nga Tran, and Stan Zdonik. 2005. C-Store: 一个面向列的数据库管理系统. 在第 31 届国际大型数据库会议 (VLDB '05) 的论文集中. 553–564.
- <Anchor id="page-13-29"/>[72] Teradata. 2024. Teradata 数据库. 取自 https://www.teradata.com/resources/datasheets/teradata-database 于 2024-06-20
- <Anchor id="page-13-15"/>[73] Frederik Transier. 2010. 内存文本搜索引擎的算法和数据结构. 博士论文. https://doi.org/10.5445/IR/1000015824
- <Anchor id="page-13-24"/>[74] Adrian Vogelsgesang, Michael Haubenschild, Jan Finis, Alfons Kemper, Viktor Leis, Tobias Muehlbauer, Thomas Neumann, and Manuel Then. 2018. 现实的基准测试: 基准如何无法代表现实世界. 在数据库系统测试研讨会的论文集中 (休斯顿, TX, USA) (DBTest'18). 第 1 篇文章, 6 页. https://doi.org/10.1145/3209950.3209952
- <Anchor id="page-13-9"/>[75] LZ4 网站. 2024. LZ4. 取自 https://lz4.org/ 于 2024-06-20
- <Anchor id="page-13-11"/><Anchor id="page-13-1"/>[76] PRQL 网站. 2024. PRQL. 取自 https://prql-lang.org 于 2024-06-20 
- [77] Till Westmann, Donald Kossmann, Sven Helmer, and Guido Moerkotte. 2000. 压缩数据库的实现与性能. SIGMOD 记录.
- <Anchor id="page-13-33"/>29, 3 (2000 年 9 月), 55–67. https://doi.org/10.1145/362084.362137 
- [78] Fangjin Yang, Eric Tschetter, Xavier Léauté, Nelson Ray, Gian Merlino, and Deep Ganguli. 2014. Druid: 实时分析数据存储. 在 2014 年 ACM SIGMOD 数据管理国际会议 (雪鸟, 犹他州, USA) (SIGMOD '14) 的论文集中. 计算机协会，纽约，NY, USA, 157–168. https://doi.org/10.1145/2588555.2595631
- <Anchor id="page-13-20"/>[79] Tianqi Zheng, Zhibin Zhang, and Xueqi Cheng. 2020. SAHA: 一个针对分析数据库的字符串自适应哈希表. 应用科学 10, 6 (2020). https://doi.org/10.3390/app10061915
- <Anchor id="page-13-19"/>[80] Jingren Zhou and Kenneth A. Ross. 2002. 使用 SIMD 指令实现数据库操作. 在 2002 年 ACM SIGMOD 数据管理国际会议 (SIGMOD '02) 的论文集中. 145–156. https://doi.org/10.1145/564691.564709
- <Anchor id="page-13-12"/>[81] Marcin Zukowski, Sandor Heman, Niels Nes, and Peter Boncz. 2006. 超标量 RAM-CPU 缓存压缩. 在第 22 届国际数据工程会议 (ICDE '06) 的论文集中. 59. https://doi.org/10.1109/ICDE.2006.150
