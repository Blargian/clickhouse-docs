---
slug: /academic_overview
title: 'アーキテクチャの概要'
description: '2024 VLDB論文のドキュメント版'
keywords: ['architecture']
---

import useBrokenLinks from "@docusaurus/useBrokenLinks";
import image_01 from '@site/static/images/managing-data/core-concepts/_vldb2024_1_Figure_0.png'
import image_02 from '@site/static/images/managing-data/core-concepts/_vldb2024_2_Figure_0.png'
import image_03 from '@site/static/images/managing-data/core-concepts/_vldb2024_2_Figure_5.png'
import image_04 from '@site/static/images/managing-data/core-concepts/_vldb2024_3_Figure_7.png'
import image_05 from '@site/static/images/managing-data/core-concepts/_vldb2024_4_Figure_6.png'
import image_06 from '@site/static/images/managing-data/core-concepts/_vldb2024_5_Figure_8.png'
import image_07 from '@site/static/images/managing-data/core-concepts/_vldb2024_6_Figure_0.png'
import image_08 from '@site/static/images/managing-data/core-concepts/_vldb2024_7_Figure_1.png'
import image_09 from '@site/static/images/managing-data/core-concepts/_vldb2024_8_Figure_7.png'
import image_10 from '@site/static/images/managing-data/core-concepts/_vldb2024_10_Figure_14.png'
import image_11 from '@site/static/images/managing-data/core-concepts/_vldb2024_10_Figure_0.png'
import image_12 from '@site/static/images/managing-data/core-concepts/_vldb2024_10_Figure_12.png'
import image_13 from '@site/static/images/managing-data/core-concepts/_vldb2024_10_Figure_13.png'
import Image from '@theme/IdealImage';

<!-- needed as docusaurus cannot resolve links to span ids, we need a custom span -->
export function Anchor(props) {
    useBrokenLinks().collectAnchor(props.id);
    return <span style={{scrollMarginTop: "var(--ifm-navbar-height)"}} {...props}/>;
}

This is the web version of our [VLDB 2024 scientific paper](https://www.vldb.org/pvldb/vol17/p3731-schulze.pdf). We also [blogged](https://clickhouse.com/blog/first-clickhouse-research-paper-vldb-lightning-fast-analytics-for-everyone) about its background and journey, and recommend watching the VLDB 2024 presentation by ClickHouse CTO and creator, Alexey Milovidov:

<iframe width="1024" height="576" src="https://www.youtube.com/embed/7QXKBKDOkJE?si=5uFerjqPSXQWqDkF" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
## ABSTRACT {#abstract}

Over the past several decades, the amount of data being stored and analyzed has increased exponentially. Businesses across industries and sectors have begun relying on this data to improve products, evaluate performance, and make business-critical decisions. However, as data volumes have increasingly become internetscale, businesses have needed to manage historical and new data in a cost-effective and scalable manner, while analyzing it using a high number of concurrent クエリ and an expectation of real-time latencies (e.g. less than one second, depending on the use case).

This paper presents an overview of ClickHouse, a popular opensource OLAP database designed for high-performance analytics over petabyte-scale data sets with high ingestion rates. Its storage layer combines a data format based on traditional log-structured merge (LSM) trees with novel techniques for continuous transformation (e.g. aggregation, archiving) of historical data in the background. クエリs are written in a convenient SQL dialect and processed by a state-of-the-art vectorized クエリ execution engine with optional code compilation. ClickHouse makes aggressive use of pruning techniques to avoid evaluating irrelevant data in クエリs. Other data management systems can be integrated at the table function, table engine, or database engine level. Real-world benchmarks demonstrate that ClickHouse is amongst the fastest analytical databases on the market.
## 1 INTRODUCTION  {#1-introduction}

This paper describes ClickHouse, a columnar OLAP database designed for high-performance analytical クエリs on tables with trillions of 行s and hundreds of カラムs. ClickHouse was [started](https://clickhou.se/evolution) in 2009 as a filter and aggregation operator for web-scale log file data and was open sourced in 2016. [Figure 1](#page-1-0) illustrates when major features described in this paper were introduced to ClickHouse.

ClickHouse is designed to address five key challenges of modern analytical data management:

1. **Huge data sets with high ingestion rates**. Many datadriven applications in industries like web analytics, finance, and e-commerce are characterized by huge and continuously growing amounts of data. To handle huge data sets, analytical databases must not only provide efficient インデックスing and compression strategies, but also allow data distribution across multiple nodes (scale-out) as single servers are limited to several dozen terabytes of storage. Moreover, recent data is often more relevant for real-time insights than historical data. As a result, analytical databases must be able to ingest new data at consistently high rates or in bursts, as well as continuously "deprioritize" (e.g. aggregate, archive) historical data without slowing down parallel reporting クエリs.

2. **Many simultaneous クエリs with an expectation of low latencies**. クエリs can generally be categorized as ad-hoc (e.g. exploratory data analysis) or recurring (e.g. periodic dashboard クエリs). The more interactive a use case is, the lower クエリ latencies are expected, leading to challenges in クエリ optimization and execution. Recurring クエリs additionally provide an opportunity to adapt the physical database layout to the workload. As a result, databases should offer pruning techniques that allow optimizing frequent クエリs. Depending on the クエリ priority, databases must further grant equal or prioritized access to shared system resources such as CPU, memory, disk and network I/O, even if a large number of クエリs run simultaneously.

3. **Diverse landscapes of data stores, storage locations, and formats**. To integrate with existing data architectures, modern analytical databases should exhibit a high degree of openness to read and write external data in any system, location, or format.

4. **A convenient クエリ language with support for performance introspection**. Real-world usage of OLAP databases poses additional "soft" requirements. For example, instead of a niche programming language, users often prefer to interface with databases in an expressive SQL dialect with nested data types and a broad range of regular, aggregation, and window functions. Analytical databases should also provide sophisticated tooling to introspect the performance of the system or individual クエリs.

5. **Industry-grade robustness and versatile deployment**. As commodity hardware is unreliable, databases must provide data レプリケーション for robustness against node failures. Also, databases should run on any hardware, from old laptops to powerful servers. Finally, to avoid the overhead of garbage collection in JVM-based programs and enable bare-metal performance (e.g. SIMD), databases are ideally deployed as native binaries for the target platform.

<Anchor id="page-1-0"/><Image img={image_01} size="lg" alt="Image 01"/>

Figure 1: ClickHouse timeline.
## 2 ARCHITECTURE {#2-architecture}

<Anchor id="page-2-0"/><Image img={image_02} size="lg" alt="Image 02"/>

Figure 2: The high-level architecture of the ClickHouse database engine.

As shown by [Figure 2,](#page-2-0) the ClickHouse engine is split into three main layers: the クエリ processing layer (described in Section [4)](#page-6-0), the storage layer (Section [3)](#page-1-1), and the integration layer (Section [5)](#page-9-0). Besides these, an access layer manages user sessions and communication with applications via different protocols. There are orthogonal components for threading, caching, role-based access control, backups, and continuous monitoring. ClickHouse is built in C++ as a single, statically-linked binary without dependencies.

クエリ processing follows the traditional paradigm of parsing incoming クエリs, building and optimizing logical and physical クエリ plans, and execution. ClickHouse uses a vectorized execution model similar to MonetDB/X100 [\[11\]](#page-12-0), in combination with opportunistic code compilation [\[53\]](#page-13-0). クエリs can be written in a feature-rich SQL dialect, PRQL [\[76\]](#page-13-1), or Kusto's KQL [\[50\]](#page-13-2).

The storage layer consists of different table engines that encapsulate the format and location of table data. Table engines fall into three categories: The first category is the MergeTree* family of table engines which represent the primary persistence format in ClickHouse. Based on the idea of LSM trees [\[60\]](#page-13-3), tables are split into horizontal, sorted パーツ, which are continuously merged by a background process. Individual MergeTree* table engines differ in the way the merge combines the 行s from its input パーツ. For example, 行s can be aggregated or replaced, if outdated.

The second category are special-purpose table engines, which are used to speed up or distribute クエリ execution. This category includes in-memory key-value table engines called dictionaries. A [dictionary](https://clickhou.se/dictionaries) caches the result of a クエリ periodically executed against an internal or external data source. This significantly reduces access latencies in scenarios, where a degree of data staleness can be tolerated. Other examples of special-purpose table engines include a pure in-memory engine used for temporary tables and the Distributed table engine for transparent data シャーディング (see below).

The third category of table engines are virtual table engines for bidirectional data exchange with external systems such as relational databases (e.g. PostgreSQL, MySQL), publish/subscribe systems (e.g. Kafka, RabbitMQ [\[24\]](#page-12-1)), or key/value stores (e.g. Redis). Virtual engines can also interact with data lakes (e.g. Iceberg, DeltaLake, Hudi [\[36\]](#page-12-2)) or files in object storage (e.g. AWS S3, Google GCP).

ClickHouse supports シャーディング and レプリケーション of tables across multiple cluster nodes for scalability and availability. シャーディング partitions a table into a set of table シャードs according to a シャーディング expression. The individual シャードs are mutually independent tables and typically located on different nodes. Clients can read and write シャードs directly, i.e. treat them as separate tables, or use the Distributed special table engine, which provides a global view of all table シャードs. The main purpose of シャーディング is to process data sets which exceed the capacity of individual nodes (typically, a few dozens terabytes of data). Another use of シャーディング is to distribute the read-write load for a table over multiple nodes, i.e., load balancing. Orthogonal to that, a シャード can be replicated across multiple nodes for tolerance against node failures. To that end, each Merge-Tree* table engine has a corresponding ReplicatedMergeTree* engine which uses a multi-master coordination scheme based on Raft consensus [\[59\]](#page-13-4) (implemented by [Keeper](https://clickhou.se/keeper), a drop-in replacement for Apache Zookeeper written in C++) to guarantee that every シャード has, at all times, a configurable number of レプリカs. Section [3.6](#page-5-0) discusses the レプリケーション mechanism in detail. As an example, [Figure 2](#page-2-0) shows a table with two シャードs, each replicated to two nodes.

Finally, the ClickHouse database engine can be operated in on-premise, cloud, standalone, or in-process modes. In the on-premise mode, users set up ClickHouse locally as a single server or multinode cluster with シャーディング and/or レプリケーション. Clients communicate with the database over the native, MySQL's, PostgreSQL's binary wire protocols, or an HTTP REST API. The cloud mode is represented by ClickHouse Cloud, a fully managed and autoscaling DBaaS offering. While this paper focuses on the on-premise mode, we plan to describe the architecture of ClickHouse Cloud in a follow-up publication. The [standalone mode](https://clickhou.se/local-fastest-tool) turns ClickHouse into a command line utility for analyzing and transforming files, making it a SQL-based alternative to Unix tools like cat and grep. While this requires no prior configuration, the standalone mode is restricted to a single server. Recently, an in-process mode called chDB [\[15\]](#page-12-3) has been developed for interactive data analysis use cases like Jupyter notebooks [\[37\]](#page-12-4) with Pandas dataframes [\[61\]](#page-13-5). Inspired by DuckDB [\[67\]](#page-13-6), [chDB](https://clickhou.se/chdb-rocket-engine) embeds ClickHouse as a high-performance OLAP engine into a host process. Compared to the other modes, this allows to pass source and result data between the database engine and the application efficiently without copying as they run in the same address space.
## <Anchor id="page-1-1"/>3 STORAGE LAYER {#3-storage-layer}

This section discusses MergeTree* table engines as ClickHouse's native storage format. We describe their on-disk representation and discuss three data pruning techniques in ClickHouse. Afterwards, we present merge strategies which continuously transform data without impacting simultaneous inserts. Finally, we explain how updates and deletes are implemented, as well as data deduplication, data レプリケーション, and ACID compliance.
### <Anchor id="page-2-2"/>3.1 On-Disk Format {#3-1-on-disk-format}

Each table in the MergeTree* table engine is organized as a collection of immutable table parts. A part is created whenever a set of 行s is inserted into the table. Parts are self-contained in the sense that they include all metadata required to interpret their content without additional lookups to a central catalog. To keep the number of parts per table low, a background merge job periodically combines multiple smaller parts into a larger part until a configurable part size is reached (150 GB by default). Since parts are sorted by the table's 主キー columns (see Section [3.2)](#page-3-0), efficient k-way merge sort [\[40\]](#page-12-5) is used for merging. The source parts are marked as inactive and eventually deleted as soon as their reference count drops to zero, i.e. no further クエリs read from them.

行s can be inserted in two modes: In synchronous insert mode, each INSERT statement creates a new part and appends it to the table. To minimize the overhead of merges, database clients are encouraged to insert tuples in bulk, e.g. 20,000 行s at once. However, delays caused by client-side batching are often unacceptable if the data should be analyzed in real-time. For example, observability use cases frequently involve thousands of monitoring agents continuously sending small amounts of event and metrics data. Such scenarios can utilize the asynchronous insert mode, in which ClickHouse buffers 行s from multiple incoming INSERTs into the same table and creates a new part only after the buffer size exceeds a configurable threshold or a timeout expires.

<Anchor id="page-2-1"/><Image img={image_03} size="lg" alt="Image 03"/>

Figure 3: Inserts and merges for MergeTree*-engine tables.

[Figure 3](#page-2-1) illustrates four synchronous and two asynchronous inserts into a MergeTree*-engine table. Two merges reduced the number of active parts from initially five to two.

Compared to LSM trees [\[58\]](#page-13-7) and their implementation in various databases [\[13,](#page-12-6) [26,](#page-12-7) [56\]](#page-13-8), ClickHouse treats all parts as equal instead of arranging them in a hierarchy. As a result, merges are no longer limited to parts in the same level. Since this also forgoes the implicit chronological ordering of parts, alternative mechanisms for updates and deletes not based on tombstones are required (see Section [3.4)](#page-4-0). ClickHouse writes inserts directly to disk while other LSM-treebased stores typically use write-ahead logging (see Section [3.7)](#page-5-1).

A part corresponds to a directory on disk, containing one file for each カラム. As an optimization, the カラムs of a small part (smaller than 10 MB by default) are stored consecutively in a single file to increase the spatial locality for reads and writes. The 行s of a part are further logically divided into groups of 8192 records, called granules. A granule represents the smallest indivisible data unit processed by the scan and インデックス lookup operators in ClickHouse. Reads and writes of on-disk data are, however, not performed at the granule level but at the granularity of blocks, which combine multiple neighboring granules within a column. New blocks are formed based on a configurable byte size per block (by default 1 MB), i.e., the number of granules in a block is variable and depends on the カラム's data type and distribution. Blocks are furthermore compressed to reduce their size and I/O costs. By default, ClickHouse employs LZ4 [\[75\]](#page-13-9) as a general-purpose compression algorithm, but users can also specify specialized codecs like Gorilla [\[63\]](#page-13-10) or FPC [\[12\]](#page-12-8) for floating-point data. Compression algorithms can also be chained. For example, it is possible to first reduce logical redundancy in numeric values using delta coding [\[23\]](#page-12-9), then perform heavy-weight compression, and finally encrypt the data using an AES codec. Blocks are decompressed on-the-fly when they are loaded from disk into memory. To enable fast random access to individual granules despite compression, ClickHouse additionally stores for each カラム a mapping that associates every granule id with the offset of its containing compressed block in the カラム file and the offset of the granule in the uncompressed block.

カラムs can further be dictionary-encoded [\[2,](#page-12-10) [77,](#page-13-11) [81\]](#page-13-12) or made nullable using two special wrapper data types: LowCardinality(T) replaces the original カラム values by integer ids and thus significantly reduces the storage overhead for data with few unique values. Nullable(T) adds an internal bitmap to カラム T, representing whether カラム values are NULL or not.

Finally, tables can be range, hash, or round-robin partitioned using arbitrary partitioning expressions. To enable partition pruning, ClickHouse additionally stores the partitioning expression's minimum and maximum values for each partition. Users can optionally create more advanced カラム statistics (e.g., HyperLogLog [\[30\]](#page-12-11) or t-digest [\[28\]](#page-12-12) statistics) that also provide cardinality estimates.
### <Anchor id="page-3-0"/>3.2 Data Pruning {#3-2-data-pruning}

In most use cases, scanning petabytes of data just to answer a single クエリ is too slow and expensive. ClickHouse supports three data pruning techniques that allow skipping the majority of 行s during searches and therefore speed up クエリs significantly.

First, users can define a **主キー インデックス** for a table. The 主キー columns determine the sort order of the 行s within each part, i.e. the インデックス is locally clustered. ClickHouse additionally stores, for every part, a mapping from the 主キー column values of each granule's first 行 to the granule's id, i.e. the インデックス is sparse [\[31\]](#page-12-13). The resulting data structure is typically small enough to remain fully in-memory, e.g., only 1000 entries are required to index 8.1 million 行s. The main purpose of a 主キー is to evaluate equality and range predicates for frequently filtered columns using binary search instead of sequential scans (Section [4.4)](#page-7-0). The local sorting can furthermore be exploited for part merges and クエリ optimization, e.g. sort-based aggregation or to remove sorting operators from the physical execution plan when the 主キー columns form a prefix of the sorting columns.

[Figure 4](#page-3-1) shows a 主キー インデックス on column EventTime for a table with page impression statistics. Granules that match the range predicate in the クエリ can be found by binary searching the 主キー インデックス instead of scanning EventTime sequentially.

<Anchor id="page-3-1"/><Image img={image_04} size="lg" alt="Image 04"/>

Figure 4: Evaluating filters with a 主キー インデックス.

Second, users can create **table projections**, i.e., alternative versions of a table that contain the same 行s sorted by a different 主キー [\[71\]](#page-13-13). Projections allow to speed up クエリs that filter on columns different than the main table's 主キー at the cost of an increased overhead for inserts, merges, and space consumption. By default, projections are populated lazily only from parts newly inserted into the main table but not from existing parts unless the user materializes the projection in full. The クエリ optimizer chooses between reading from the main table or a projection based on estimated I/O costs. If no projection exists for a part, クエリ execution falls back to the corresponding main table part.

Third, **skipping インデックス** provide a lightweight alternative to projections. The idea of skipping インデックス is to store small amounts of metadata at the level of multiple consecutive granules which allows to avoid scanning irrelevant 行s. Skipping インデックス can be created for arbitrary index expressions and using a configurable granularity, i.e. number of granules in a skipping インデックス block. Available skipping インデックス types include: 1. Min-max インデックス [\[51\]](#page-13-14), storing the minimum and maximum values of the インデックス expression for each インデックス block. This インデックス type works well for locally clustered data with small absolute ranges, e.g. loosely sorted data. 2. Set インデックス, storing a configurable number of unique インデックス block values. These インデックス are best used with data with a small local cardinality, i.e. "clumped together" values. 3. Bloom filter インデックス [\[9\]](#page-12-14) build for row, token, or n-gram values with a configurable false positive rate. These インデックス support text search [\[73\]](#page-13-15), but unlike min-max and set インデックス, they cannot be used for range or negative predicates.
### <Anchor id="page-4-3"/>3.3 Merge-time Data Transformation {#3-3-merge-time-data-transformation}

Business intelligence and observability use cases often need to handle data generated at constantly high rates or in bursts. Also, recently generated data is typically more relevant for meaningful real-time insights than historical data. Such use cases require databases to sustain high data ingestion rates while continuously reducing the volume of historical data through techniques like aggregation or data aging. ClickHouse allows a continuous incremental transformation of existing data using different merge strategies. Merge-time data transformation does not compromise the performance of INSERT statements, but it cannot guarantee that tables never contain unwanted (e.g. outdated or non-aggregated) values. If necessary, all merge-time transformations can be applied at クエリ time by specifying the keyword FINAL in SELECT statements.

**Replacing merges** retain only the most recently inserted version of a tuple based on the creation timestamp of its containing part, older versions are deleted. Tuples are considered equivalent if they have the same 主キー column values. For explicit control over which tuple is preserved, it is also possible to specify a special version column for comparison. Replacing merges are commonly used as a merge-time update mechanism (normally in use cases where updates are frequent), or as an alternative to insert-time data deduplication (Section [3.5)](#page-5-2).

**Aggregating merges** collapse 行s with equal 主キー column values into an aggregated 行. Non-primary key columns must be of a partial aggregation state that holds the summary values. Two partial aggregation states, e.g. a sum and a count for avg(), are combined into a new partial aggregation state. Aggregating merges are typically used in materialized views instead of normal tables. Materialized views are populated based on a transformation クエリ against a source table. Unlike other databases, ClickHouse does not refresh materialized views periodically with the entire content of the source table. Materialized views are rather updated incrementally with the result of the transformation クエリ when a new part is inserted into the source table.

[Figure 5](#page-4-1) shows a materialized view defined on a table with page impression statistics. For new parts inserted into the source table, the transformation クエリ computes the maximum and average latencies, grouped by region, and inserts the result into a materialized view. Aggregation functions avg() and max() with extension -State return partial aggregation states instead of actual results. An aggregating merge defined for the materialized view continuously combines partial aggregation states in different parts. To obtain the final result, users consolidate the partial aggregation states in the materialized view using avg() and max() with -Merge extension.

<Anchor id="page-4-1"/><Image img={image_05} size="lg" alt="Image 05"/>

Figure 5: Aggregating merges in materialized views.

**TTL (有効期限 (TTL)) merges** provide aging for historical data. Unlike deleting and aggregating merges, TTL merges process only one part at a time. TTL merges are defined in terms of rules with triggers and actions. A trigger is an expression computing a timestamp for every row, which is compared against the time at which the TTL merge runs. While this allows users to control actions at row granularity, we found it sufficient to check whether all rows satisfy a given condition and run the action on the entire part. Possible actions include 1. move the part to another volume (e.g. cheaper and slower storage), 2. re-compress the part (e.g. with a more heavy-weight codec), 3. delete the part, and 4. roll-up, i.e. aggregate the 行s using a grouping key and aggregate functions.

As an example, consider the logging table definition in [Listing 1.](#page-4-2) ClickHouse will move parts with timestamp column values older than one week to slow but inexpensive S3 object storage.
<Anchor id="page-4-2"/>
```
1 CREATE TABLE tab ( ts DateTime , msg String )
2 ENGINE MergeTree PRIMARY KEY ts
3 TTL ( ts + INTERVAL 1 WEEK ) TO VOLUME 's3 '
```
Listing 1: Move part to object storage after one week.
### <Anchor id="page-4-0"/>3.4 Updates and Deletes {#3-4-updates-and-deletes}

The design of the MergeTree* table engines favors append-only workloads, yet some use cases require to modify existing data occasionally, e.g. for regulatory compliance. Two approaches for updating or deleting data exist, neither of which block parallel inserts.

**Mutations** rewrite all parts of a table in-place. To prevent a table (delete) or column (update) from doubling temporarily in size, this operation is non-atomic, i.e. parallel SELECT statements may read mutated and non-mutated parts. Mutations guarantee that the data is physically changed at the end of the operation. Delete mutations are still expensive as they rewrite all columns in all parts.

As an alternative, **軽量削除** only update an internal bitmap column, indicating if a row is deleted or not. ClickHouse amends SELECT クエリs with an additional filter on the bitmap column to exclude deleted 行s from the result. Deleted 行s are physically removed only by regular merges at an unspecified time in the future. Depending on the column count, 軽量削除 can be much faster than mutations, at the cost of slower SELECTs.

Update and delete operations performed on the same table are expected to be rare and serialized to avoid logical conflicts.
### <Anchor id="page-5-2"/>3.5 Idempotent Inserts {#3-5-idempotent-inserts}

A problem that frequently occurs in practice is how clients should handle connection timeouts after sending data to the server for insertion into a table. In this situation, it is difficult for clients to distinguish between whether the data was successfully inserted or not. The problem is traditionally solved by re-sending the data from the client to the server and relying on 主キー or unique constraints to reject duplicate inserts. Databases perform the required point lookups quickly using インデックス structures based on binary trees [\[39,](#page-12-15) [68\]](#page-13-16), radix trees [\[45\]](#page-13-17), or hash tables [\[29\]](#page-12-16). Since these data structures index every tuple, their space and update overhead becomes prohibitive for large data sets and high ingest rates.

ClickHouse provides a more light-weight alternative based on the fact that each insert eventually creates a part. More specifically, the server maintains hashes of the N last inserted parts (e.g. N=100) and ignores re-inserts of parts with a known hash. Hashes for non-replicated and replicated tables are stored locally, respectively, in Keeper. As a result, inserts become idempotent, i.e. clients can simply re-send the same batch of 行s after a timeout and assume that the server takes care of deduplication. For more control over the deduplication process, clients can optionally provide an insert token that acts as a part hash. While hash-based deduplication incurs an overhead associated with hashing the new 行s, the cost of storing and comparing hashes is negligible.
```

### <Anchor id="page-5-0"/>3.6 データレプリケーション {#3-6-data-replication}

レプリケーションは高可用性（ノード障害に対する耐性）の前提条件ですが、負荷分散やゼロダウンタイムアップグレードにも使用されます [\[14\]](#page-12-17)。ClickHouseでは、レプリケーションは、カラム名やタイプなどのテーブルメタデータと、一連のテーブルパーツ（セクション [3.1)](#page-2-2) で構成されるテーブルステートの概念に基づいています。ノードは、次の3つの操作を使用してテーブルのステートを進めます：1. INSERTは新しいパーツをステートに追加します、2. マージは新しいパーツを追加し、既存のパーツをステートから削除します、3. 変更およびDDLステートメントはパーツを追加および/または削除し、テーブルメタデータを変更しますが、具体的な操作に応じて異なります。操作はローカルで単一のノードで実行され、グローバルレプリケーションログにおけるステート遷移のシーケンスとして記録されます。

レプリケーションログは、通常、3つのClickHouse Keeperプロセスからなるアンサンブルによって維持され、Raft合意アルゴリズム [\[59\]](#page-13-4) を使用して、ClickHouseノードのクラスタに対する分散型でフォールトトレラントなコーディネーションレイヤーを提供します。すべてのクラスタノードは、初めにレプリケーションログの同じ位置を指します。ノードがローカルなINSERT、マージ、ミューテーション、DDLステートメントを実行している間、レプリケーションログはすべての他のノードで非同期に再生されます。その結果、レプリケートされたテーブルは最終的には一貫性があり、つまり、ノードは最新のステートに収束する際に一時的に古いテーブルステートを読み取ることがあります。前述のほとんどの操作は、ノードのクォーラム（例：多数のノードまたはすべてのノード）が新しいステートを採用するまで、代わりに同期して実行できます。

例として、[図 6](#page-5-3) は、3つのClickHouseノードのクラスタ内で最初は空のレプリケーテッドテーブルを示しています。ノード1はまず2つのINSERTステートメントを受け取り、それらをレプリケーションログ（1 2）に記録します。次に、ノード2は最初のログエントリを再生するために、（3）それを取得し、ノード1から新しいパーツをダウンロードします（4）、一方でノード3は両方のログエントリ（3 4 5 6）を再生します。最後に、ノード3は両方のパーツを新しいパーツにマージし、入力パーツを削除し、レプリケーションログにマージエントリ（7）を記録します。

<Anchor id="page-5-3"/><Image img={image_06} size="lg" alt="Image 06"/>

図 6: 三ノードクラスタにおけるレプリケーション。

3つの同期化を迅速化するための最適化が存在します：まず、クラスタに追加された新しいノードは、ゼロからレプリケーションログを再生するのではなく、最後のレプリケーションログエントリを書いたノードのステートを単にコピーします。次に、マージはローカルで繰り返すか、別のノードから結果パーツを取得することによって再生されます。正確な動作は設定可能であり、CPU消費とネットワークI/Oのバランスを取ることができます。たとえば、データセンター間のレプリケーションでは、運用コストを最小限に抑えるために通常ローカルのマージを優先します。第三に、ノードは互いに独立したレプリケーションログエントリを並行して再生します。これには、同じテーブルに連続して挿入された新しいパーツの取得や、異なるテーブルへの操作が含まれます。
### <Anchor id="page-5-1"/>3.7 ACID準拠 {#3-7-acid-compliance}

同時実行の読み取りおよび書き込み操作のパフォーマンスを最大化するために、ClickHouseはできるだけロックを避けます。クエリは、クエリの開始時に作成されたすべての関与テーブルのパーツのスナップショットに対して実行されます。これにより、並行するINSERTまたはマージによって挿入された新しいパーツ（セクション [3.1)](#page-2-2）が実行に参加しないことが保証されます。同時にパーツが修正または削除されるのを防ぐために（セクション [3.4)](#page-4-0)、処理されたパーツの参照カウントはクエリの実行中に増加します。形式的には、これはバージョン付きパーツに基づくMVCCのバリアントによって実現されるスナップショット隔離に対応します [\[6\]](#page-12-18)。その結果、ステートメントは通常ACID準拠ではなく、スナップショットが取得された時点で同時書き込みが単一のパーツにのみ影響する稀なケースを除きます。

実際には、ClickHouseの書き込みの多い意思決定を行うユースケースのほとんどは、停電の場合に新しいデータが失われる小さなリスクを許容します。データベースはこれを利用して、新しく挿入されたパーツのコミット（fsync）をデフォルトでディスクに強制せず、カーネルが原子性を犠牲にして書き込みをバッチ処理できるようにします。
## <Anchor id="page-6-0"/>4 クエリ処理レイヤー {#4-query-processing-layer}


<Anchor id="page-6-1"/><Image img={image_07} size="lg" alt="Image 07"/>

図 7: SIMDユニット、コア、およびノード間の並列化。

[図 7](#page-6-1) に示すように、ClickHouseはデータ要素、データチャンク、テーブルシャードのレベルでクエリを並列化します。複数のデータ要素は、SIMD命令を使用してオペレーター内で同時に処理できます。単一のノード上で、クエリエンジンは複数のスレッドでオペレーターを同時に実行します。ClickHouseはMonetDB/X100 [\[11\]](#page-12-0) と同じベクトル化モデルを使用し、オペレーターは単一の行ではなく、複数の行（データチャンク）を生成、渡し、消費して、仮想関数呼び出しのオーバーヘッドを最小限に抑えます。もしソーステーブルが相互に排他的なテーブルシャードに分割されている場合、複数のノードが同時にシャードをスキャンできます。その結果、すべてのハードウェアリソースが十分に活用され、クエリ処理はノードを追加することによって水平方向に、コアを追加することで垂直にスケールできます。

このセクションの残りでは、データ要素、データチャンク、およびシャードの粒度での並列処理を詳細に説明します。次に、クエリパフォーマンスを最大化するためのいくつかの重要な最適化を提示します。最後に、ClickHouseが同時クエリの存在下で共有システムリソースをどのように管理するかについて説明します。
### 4.1 SIMD並列化 {#4-1-simd-parallelization}

オペレーター間で複数の行を渡すことは、ベクトル化の機会を生み出します。ベクトル化は手動で記述されたインライン関数 [\[64,](#page-13-18) [80\]](#page-13-19) またはコンパイラーの自動ベクトル化 [\[25\]](#page-12-19) に基づきます。ベクトル化の恩恵を受けるコードは、異なる計算カーネルにコンパイルされます。たとえば、クエリオペレーターの内部ホットループは、非ベクトル化カーネル、自動ベクトル化されたAVX2カーネル、および手動ベクトル化されたAVX-512カーネルの観点から実装できます。最も速いカーネルは、cpuid命令に基づいて[ランタイム](https://clickhou.se/cpu-dispatch)で選択されます。このアプローチにより、ClickHouseは古いシステム（最低限SSE 4.2を必要とする）で動作できる一方で、最近のハードウェアで著しいスピードアップを提供します。
### 4.2 マルチコア並列化 {#4-2-multi-core-parallelization}

<Anchor id="page-7-1"/><Image img={image_08} size="lg" alt="Image 08"/>

図 8: 三つのレーンを持つ物理オペレーター計画。

ClickHouseは、SQLクエリを物理計画オペレーターの有向グラフに変換する従来のアプローチ [\[31\]](#page-12-13) に従います。オペレーター計画の入力は、ネイティブ形式またはサポートされているサードパーティ形式（セクション [5)](#page-9-0) を使用してデータを読み取る特別なソースオペレーターによって表されます。同様に、特別なシンクオペレーターは結果を希望の出力形式に変換します。物理オペレーター計画は、クエリコンパイル時に、設定可能な最大スレッド数（デフォルトではコア数）とソーステーブルのサイズに基づいて独立した実行レーンに展開されます。レーンは、並列オペレーターによって処理されるデータを重複しない範囲に分解します。並列処理の機会を最大化するために、レーンはできるだけ遅くマージされます。

例として、[図 8](#page-7-1) のノード1のボックスは、ページインプレッション統計を持つテーブルに対する典型的なOLAPクエリのオペレーターグラフを示しています。最初の段階では、ソーステーブルの3つの相互に排他的な範囲が同時にフィルタリングされます。再分配交換オペレーターは、最初と次の段階の間で結果チャンクを動的にルーティングして、処理スレッドを均等に利用します。スキャンされた範囲が大きく異なる選択性を持っている場合、最初の段階の後にレーンが不均衡になることがあります。第二段階では、フィルターを通過した行がRegionIDでグループ化されます。集約オペレーターはRegionIDをグループ化カラムとして持ち、avg()の部分集約状態として各グループの合計およびカウントを維持します。ローカル集約結果は最終的にGroupStateMergeオペレーターによってグローバル集約結果にマージされます。このオペレーターはパイプラインブレイカーでもあり、集約結果が完全に計算されるまで第三段階は開始できません。第三段階では、結果グループは最初に再分配交換オペレーターによって等しいサイズの3つの排他的なパーティションに分割され、その後AvgLatencyでソートされます。ソートは3つのステップで実行されます。最初に、ChunkSortオペレーターが各パーティションの個々のチャンクをソートします。次に、StreamSortオペレーターがローカルなソート済み結果を維持し、2ウェイマージソートを使用して新しく受信したソート済みチャンクと組み合わせます。最後に、MergeSortオペレーターがローカルな結果をk-wayソートを使用して結合し、最終的な結果を得ます。

オペレーターは状態マシンであり、入力ポートと出力ポートを介して互いに接続されています。オペレーターの3つの可能な状態はneed-chunk、ready、およびdoneです。need-chunkからreadyに移行するためには、チャンクがオペレーターの入力ポートに置かれます。readyからdoneに移行するためには、オペレーターが入力チャンクを処理し、出力チャンクを生成します。doneからneed-chunkに移行するためには、出力チャンクがオペレーターの出力ポートから削除されます。2つの接続されたオペレーターの最初と3番目の状態遷移は、結合ステップでのみ実行できます。ソースオペレーター（シンクオペレーター）は、readyおよびdoneの状態のみを持ち（need-chunkおよびdone）。

ワーカースレッドは、物理オペレーター計画を継続的に走査し、状態遷移を実行します。CPUキャッシュを温かく保つために、計画には同じスレッドが同じレーン内で連続するオペレーターを処理すべきとのヒントが含まれています。並列処理は、ステージ内の排他的入力を通じて水平方向に（例えば、[図 8](#page-7-1) では集約オペレーターが同時に実行されます）およびパイプラインブレイカーで分離されていないステージ間で垂直に発生します（例：同じレーン内のフィルターオペレーターと集約オペレーターは同時に実行できます）。新しいクエリが開始されるときや同時クエリが完了する際に、オーバーおよびアンダーサブスクリプションを避けるために、並列度はクエリスタート時に指定されたワーカーの最大スレッド数の間で中間的に変更できます（セクション [4.5)](#page-9-1)。

また、オペレーターはランタイムでクエリ実行に影響を与える2つの方法があります。第一に、オペレーターは動的に新しいオペレーターを作成して接続できます。これは、メモリ消費が設定可能な閾値を超えた場合にクエリをキャンセルする代わりに、外部集約、ソート、または結合アルゴリズムに切り替えるために主に使用されます。第二に、オペレーターはワーカースレッドに非同期キューに移動するよう要求できます。これは、リモートデータを待っているときにワーカースレッドをより効果的に利用できます。

ClickHouseのクエリ実行エンジンとモーサルドリブン並列性 [\[44\]](#page-12-20) の類似点は、レーンが通常異なるコア/NUMAソケットで実行され、ワーカースレッドが他のレーンからタスクを盗むことができるという点です。また、中央スケジューリングコンポーネントは存在せず、ワーカースレッドはオペレーター計画を継続的に走査することによって各自のタスクを選択します。モーサルドリブン並列性とは異なり、ClickHouseは最大並列度を計画に組み込み、デフォルトのモーサルサイズ（約10万行）と比較してソーステーブルをパーティショニングするためにはるかに大きな範囲を使用します。これは、一部の場合において停止を引き起こす可能性があります（例：異なるレーンのフィルターオペレーターのランタイムが大きく異なる場合）が、再分配オペレーターの自由な使用は、少なくともステージ間での不均衡が蓄積されるのを避けます。
### 4.3 マルチノード並列化 {#4-3-multi-node-parallelization}

クエリのソーステーブルがシャード化されている場合、クエリを受けたノード（イニシエーターノード）は、他のノードでできるだけ多くの作業を行おうとします。他のノードからの結果は、クエリ計画のさまざまなポイントに統合できます。クエリに応じて、リモートノードは以下の操作を実行できます：1. 生のソーステーブルカラムをイニシエーターノードにストリームします、2. ソースカラムをフィルターし、残った行を送信します、3. フィルタリングおよび集計ステップを実行し、部分集計状態を持つローカル結果グループを送信します、または4. フィルター、集計、およびソートを含む完全なクエリを実行します。

[図 8](#page-7-1) のノード2 ... Nは、ヒットテーブルのシャードを保持する他のノードで実行される計画フラグメントを示しています。これらのノードはローカルデータをフィルタリングおよびグループ化し、結果をイニシエーターノードに送信します。ノード1のGroupStateMergeオペレーターは、ローカル結果とリモート結果をマージし、最終的に結果グループがソートされる前に処理されます。
### <Anchor id="page-7-0"/>4.4 全体的なパフォーマンス最適化 {#4-4-holistic-performance-optimization}

このセクションでは、クエリ実行のさまざまなステージに適用される主要なパフォーマンス最適化を紹介します。

**クエリ最適化**。最初の最適化セットは、クエリのASTから得られた意味論的クエリ表現の上に適用されます。そのような最適化の例としては、定数畳み込み（例：concat(lower('a'),upper('b')) は 'aB' になる）、特定の集約関数からスカラを抽出する（例：sum(a*2) は 2 * sum(a) になる）、共通の部分式の排除、等しいフィルターの論理和をINリストに変換する（例：x=c OR x=d は x IN (c,d) になる）などがあります。最適化された意味論的クエリ表現は、その後、論理オペレーター計画に変換されます。論理プランの上に適用される最適化には、フィルタープッシュダウン、関数評価およびソートステップの順序付けが含まれます。どちらがより高価であると見積もられるかに応じて、最終的に論理クエリプランは物理オペレーター計画に変換されます。この変換は、関与するテーブルエンジンの特性を利用することができます。たとえば、MergeTree*-テーブルエンジンの場合、ORDER BYカラムが主キーのプレフィックスを形成する場合、データはディスク順に読み取られ、ソートオペレーターは計画から削除できます。また、集約のグループ化カラムが主キーのプレフィックスを形成する場合、ClickHouseはソート集約 [\[33\]](#page-12-21) を使用でき、事前にソートされた入力から同じ値の集約ランを直接集約できます。ハッシュ集約と比較して、ソート集約はメモリ集約度が大幅に低く、集約値はランが処理された後すぐに次のオペレーターに渡すことができます。

**クエリコンパイル**。ClickHouseは、[LLVMに基づくクエリコンパイル](https://clickhou.se/jit)を使用して、隣接するプランオペレーターを動的に融合します [\[38,](#page-12-22) [53\]](#page-13-0)。たとえば、式 a * b + c + 1 は3つのオペレーターではなく、単一のオペレーターとして結合できます。式の他に、ClickHouseはGROUP BY用の複数の集約関数を評価するためや、複数のソートキーによるソートのためにコンパイルを使用します。クエリコンパイルは、仮想呼び出しの数を減少させ、データをレジスタまたはCPUキャッシュに保持し、実行されるコードの量を減らすことで分岐予測器を助けます。さらに、ランタイムコンパイルは、論理最適化やコンパイラーに実装されたピーホール最適化などの豊富な最適化セットを有効にし、利用できる最速のローカルCPU命令にアクセスを提供します。コンパイルは、同じ通常の集約またはソート式が異なるクエリによって設定可能な回数以上に実行される場合にのみ開始されます。コンパイルされたクエリオペレーターはキャッシュされ、将来のクエリで再利用できます。[7]

**主キーインデックス評価**。ClickHouseは、条件の論理積形式のフィルタ句のサブセットが主キーのカラムのプレフィックスを構成する場合に、主キーインデックスを使用してWHERE条件を評価します。主キーインデックスは、辞書順にソートされたキー値の範囲に対して左から右に分析されます。主キーのカラムに対応するフィルタ句は三値論理を使用して評価されます - すべて真、すべて偽、または範囲内の値に対して混合の真/偽です。後者のケースでは、範囲は再帰的に分析されるサブ範囲に分割されます。フィルター条件の関数には追加の最適化が存在します。第一に、関数にはその単調性を記述する特性があります。たとえば、toDayOfMonth(date) は月内で区分的に単調です。単調性特性により、関数がソートされた入力キー値範囲のソートされた結果を生成するかどうかを推測することができます。第二に、いくつかの関数は指定された関数結果の前向きイメージを計算することができます。これは、定数の比較をキー列に対して関数呼び出しに置き換えるために使用され、キー列の値を前向きイメージと比較します。たとえば、toYear(k) = 2024 は k >= 2024-01-01 && k < 2025-01-01 に置き換えることができます。

**データスキッピング**。ClickHouseは、セクション [3.2.](#page-3-0) で示されたデータ構造を使用して、クエリ実行時のデータ読み取りを回避しようとします。さらに、異なる列に対するフィルターは、推定選択性の降順および（オプションの）カラム統計に基づいて逐次評価されます。少なくとも1つの一致する行を含むデータチャンクのみが次の述語に渡されます。これにより、読み取るデータの量と述語間で実行される計算の数が徐々に減少します。最適化は、少なくとも1つの選択性が高い述語が存在する場合にのみ適用されます。そうでなければ、すべての述語を並行して評価する際のクエリレイテンシが悪化することになります。

**ハッシュテーブル**。ハッシュテーブルは、集約やハッシュ結合のための基本的なデータ構造です。適切な種類のハッシュテーブルを選択することは、パフォーマンスにとって重要です。ClickHouseは、ハッシュ関数、アロケーター、セルタイプ、リサイズポリシーを変化点として、汎用ハッシュテーブルテンプレートからさまざまなハッシュテーブルを[インスタンス化](https://clickhou.se/hashtables)します（2024年3月時点で30種類以上）。グループ化カラムのデータ型、推定ハッシュテーブルのカーディナリティなどの要因に基づいて、各クエリオペレーターに対して最速のハッシュテーブルが個別に選択されます。ハッシュテーブルに対して実装されているさらなる最適化には次のものがあります：

- 256のサブテーブルを持つ2層レイアウト（ハッシュの最初のバイトに基づく）で、大規模なキーセットをサポート。
- 文字列ハッシュテーブル [\[79\]](#page-13-20) は、4つのサブテーブルと異なる文字列長に対する異なるハッシュ関数を使用。
- キーの数が少ない場合に、キーを直接バケットインデックスとして（ハッシュなし）使用するルックアップテーブル。
- 比較が高価な場合に衝突解決を迅速化するために埋め込まれたハッシュを持つ値（例：文字列、AST）。
- 実行時統計に基づいて予測されるサイズに基づいてハッシュテーブルを作成し、不必要なリサイズを避ける。
- 単一のメモリスラブで同じ作成/破棄ライフサイクルを持つ複数の小さなハッシュテーブルを割り当てる。
- ハッシュマップごととセルごとのバージョンカウンターを使用して、再利用のためにハッシュテーブルを瞬時にクリア。
- ハッシュキーの後に値を取得する速度を上げるためにCPUのプリフェッチ (__builtin_prefetch) を使用。

**結合**。ClickHouseはもともと結合をわずかにサポートしていただけでしたが、歴史的に多くのユースケースは非正規化テーブルに頼ることがありました。今日では、データベースはSQLで利用可能なすべての結合タイプ（内部、左/右/完全外部、クロス、非同期）と、ハッシュ結合（ナイーブ、グレース）、ソートマージ結合、迅速なキー-値ルックアップを持つテーブルエンジンのためのインデックス結合などの異なる結合アルゴリズムを[提供](https://clickhou.se/joins)しています。

結合はデータベースオペレーションの中で最もコストが高いため、理想的には設定可能な空間/時間のトレードオフを持った古典的な結合アルゴリズムの並列バリアントを提供することが重要です。ClickHouseは、[\[7\]](#page-12-23) からのノンブロッキングの共有パーティションアルゴリズムを実装しています。たとえば、[図 9](#page-8-3) のクエリは、ページヒット統計テーブルに対する自己結合を介して、ユーザーがURL間を移動する方法を計算します。結合のビルドフェーズは、ソーステーブルの3つの排他的な範囲をカバーする3つのレーンに分割されます。グローバルハッシュテーブルの代わりに、パーティション化されたハッシュテーブルが使用されます。（通常は3つの）ワーカースレッドは、ビルドサイドの各入力行のターゲットパーティションをハッシュ関数のモジュロを計算することによって決定します。ハッシュテーブルパーティションへのアクセスは、Gather交換オペレーターを使用して同期されます。プローブフェーズは、入力タプルのターゲットパーティションを同様に見つけます。このアルゴリズムは、各タプルごとに2つの追加のハッシュ計算を導入しますが、ビルドフェーズでは、大きくハッシュテーブルパーティション数によってロックコンテンションを大幅に削減します。

<Anchor id="page-8-3"/><Image img={image_09} size="lg" alt="Image 09"/>

図 9: 三つのハッシュテーブルパーティションによる並列ハッシュ結合。
### <Anchor id="page-9-1"/>4.5 ワークロードの分離 {#4-5-workload-isolation}

ClickHouseは同時実行制御、メモリ使用制限、およびI/Oスケジューリングを提供し、ユーザーがクエリをワークロードクラスに分離できるようにします。特定のワークロードクラスに対して共有リソース（CPUコア、DRAM、ディスクおよびネットワークI/O）に制限を設定することで、これらのクエリが他の重要なビジネスクエリに影響しないようにします。

同時実行制御は、高い数の同時クエリがあるシナリオでスレッドのオーバサブスクリプションを防止します。より具体的には、クエリごとのワーカースレッドの数は、使用可能なCPUコアの数に対する指定された比率に基づいて動的に調整されます。

ClickHouseは、サーバー、ユーザー、およびクエリレベルでのメモリ割り当てのバイトサイズを追跡し、柔軟なメモリ使用制限を設定できるようにします。メモリオーバーコミットを利用すると、クエリは保証されたメモリを超える追加の空きメモリを使用でき、他のクエリに対するメモリ制限が保証されます。さらに、集約、ソート、および結合句のためのメモリ使用は制限でき、メモリ制限を超えた場合に外部アルゴリズムへのフォールバックを引き起こします。

最後に、I/Oスケジューリングは、特定のワークロードクラスに対して、最大帯域幅、進行中のリクエストおよびポリシー（例：FIFO、SFC [\[32\]](#page-12-24)）に基づいてローカルおよびリモートディスクアクセスを制限できるようにします。
### <Anchor id="page-9-0"/>5 インテグレーションレイヤー {#5-integration-layer}

リアルタイムの意思決定アプリケーションは、しばしば複数の場所のデータに対する効率的で低遅延のアクセスに依存しています。OLAPデータベースに外部データを利用可能にするための2つのアプローチがあります。プッシュベースのデータアクセスでは、サードパーティコンポーネントがデータベースと外部データストレージの橋渡しをします。これに該当するものの一例は、リモートデータを宛先システムにプッシュする専門の抽出-変換-読み込み（ETL）ツールです。プルベースのモデルでは、データベース自体がリモートデータソースに接続し、ローカルテーブルのためにデータをプルします。プッシュベースのアプローチは、より柔軟で一般的ですが、より大きなアーキテクチャのフットプリントやスケーラビリティボトルネックが伴います。それに対して、データベース内でのリモート接続は、ローカルデータとリモートデータ間の結合のような面白い機能を提供し、全体のアーキテクチャをシンプルに保ちながら洞察までの時間を短縮します。

このセクションの残りでは、ClickHouseにおけるプルベースのデータ統合方法を探り、リモート場所のデータにアクセスすることを目指します。SQLデータベースにおけるリモート接続のアイデアは新しいものではありません。たとえば、SQL/MED標準 [\[35\]](#page-12-25) は2001年に導入され、PostgreSQLにより2011年から実装されている [\[65\]](#page-13-21) 外部データラッパーを、外部データを管理するための統一インターフェースとして提案しています。他のデータストアやストレージ形式との相互運用性を最大化することは、ClickHouseの設計目標の一つです。2024年3月の時点で、ClickHouseは、すべての分析データベースにわたって、知る限り最も多くの組み込みデータ統合オプションを提供しています。

外部接続。ClickHouseは、ODBC、MySQL、PostgreSQL、SQLite、Kafka、Hive、MongoDB、Redis、S3/GCP/Azureオブジェクトストレージ、さまざまなデータレイクなど、外部システムやストレージロケーションとの接続のために [50+](https://clickhou.se/query-integrations) 統合テーブル関数およびエンジンを提供します。これらは次のボーナス図に分別され（元のvldb論文の一部ではありません）、カテゴリに分けて示します。

<Anchor id="bonus-figure"/><Image img={image_10} size="lg" alt="Image 10"/>

ボーナス図: ClickBenchの相互運用性オプション。

統合 **テーブル関数** による一時的アクセス。テーブル関数は、SELECTクエリのFROM句で呼び出して、探索的なアドホッククエリのためにリモートデータを読み取ることができます。また、INSERT INTO TABLE FUNCTIONステートメントを使用してデータをリモートストレージに書き込むためにも使用できます。

永続的アクセス。リモートデータストアおよび処理システムとの永久的な接続を作成するための3つのメソッドが存在します。

まず、統合 **テーブルエンジン** は、MySQLテーブルなどのリモートデータソースを永続的なローカルテーブルとして表現します。ユーザーはCREATE TABLE AS構文を使用してテーブル定義を保存し、SELECTクエリとテーブル関数を組み合わせます。リモートカラムのサブセットのみを参照するためにカスタムスキーマを指定したり、スキーマ推論を使用してカラム名と同等のClickHouse型を自動的に決定したりすることが可能です。また、パッシブおよびアクティブな実行時の動作を区別します：パッシブテーブルエンジンはクエリをリモートシステムに転送し、結果でローカルプロキシテーブルを埋めます。対照的に、アクティブテーブルエンジンは、リモートシステムから定期的にデータをプルしたり、PostgreSQLの論理レプリケーションプロトコルなどを介してリモートの変更にサブスクライブしたりします。その結果、ローカルテーブルはリモートテーブルのフルコピーを含みます。

第二に、統合 **データベースエンジン** は、リモートデータストア内のテーブルスキーマのすべてのテーブルをClickHouseにマッピングします。前者とは異なり、通常はリモートデータストアがリレーショナルデータベースであることが求められ、DDLステートメントのサポートも制限的です。

第三に、**辞書** は、対応する統合テーブル関数またはエンジンを用いて、ほぼすべての可能なデータソースに対して任意のクエリを用いてポピュレートできます。ランタイム動作はアクティブであり、リモートストレージから定期的にデータがプルされます。

データ形式。サードパーティシステムと相互作用するために、現代の分析データベースは任意の形式のデータを処理することもできる必要があります。ClickHouseは、ネイティブ形式の他に、CSV、JSON、Parquet、Avro、ORC、Arrow、プロトバフなど [90+](https://clickhou.se/query-formats) の形式をサポートしています。各形式は入力形式（ClickHouseが読み取れる）、出力形式（ClickHouseがエクスポートできる）、またはその両方である可能性があります。Parquetのような分析指向の形式は、クエリ処理とも統合されており、すなわちオプティマイザーは埋め込まれた統計値を活用でき、フィルターは圧縮データ上で直接評価されます。

互換性インターフェース。ネイティブなバイナリワイヤプロトコルとHTTPの他に、クライアントはMySQLまたはPostgreSQLのワイヤプロトコル互換インターフェースを介してClickHouseと対話できます。この互換性機能は、ベンダーがまだネイティブなClickHouse接続を実装していないプロプライエタリアプリケーション（例：特定のビジネスインテリジェンスツール）からのアクセスを可能にするのに役立ちます。
## 6 パフォーマンスを機能として {#6-performance-as-a-feature}

このセクションでは、パフォーマンス分析のための組み込みツールを紹介し、実世界のクエリおよびベンチマーククエリを使用してパフォーマンスを評価します。
### 6.1 組み込みパフォーマンス分析ツール {#6-1-built-in-performance-analysis-tools}

個々のクエリやバックグラウンド操作におけるパフォーマンスボトルネックを調査するための幅広いツールが利用可能です。ユーザーはすべてのツールに、システムテーブルに基づいた統一インターフェースを通じて対話します。

**サーバーおよびクエリメトリクス**。アクティブパート数、ネットワークスループット、キャッシュヒット率などのサーバーレベルの統計は、読み取ったブロック数やインデックス使用統計のようなクエリごとの統計で補完されます。メトリクスは、要求に応じて同期的に（要求時）、あるいは設定可能な間隔で非同期的に計算されます。

**サンプリングプロファイラー**。サーバースレッドのコールスタックをサンプリングプロファイラーを使用して収集できます。結果は、オプションでflamegraphビジュアライザーなどの外部ツールにエクスポートできます。

**OpenTelemetry統合**。OpenTelemetryは、複数のデータ処理システムにおけるデータ行のトレーシングのためのオープンスタンダードです [\[8\]](#page-12-26)。ClickHouseは、すべてのクエリ処理ステップに対して設定可能な粒度でOpenTelemetryログスパンを生成し、他のシステムからのOpenTelemetryログスパンを収集・分析できます。

**クエリ説明**。他のデータベースと同様に、SELECTクエリはEXPLAINの前に置くことで、クエリのAST、論理及び物理オペレーター計画、実行時の振る舞いについての詳細情報が得られます。
### 6.2 ベンチマーク {#6-2-benchmarks}


ベンチマークは現実的ではないと批判されることもありますが [\[10,](#page-12-27) [52,](#page-13-22) [66,](#page-13-23) [74\]](#page-13-24)、データベースの強みと弱みを特定するために役立ちます。以下では、ClickHouseのパフォーマンスを評価するためにベンチマークがどのように使用されるかを説明します。
```
```yaml
title: '6.2.1 非正規化テーブル'
sidebar_label: '非正規化テーブル'
keywords: ['ClickHouse', '非正規化', 'テーブル', 'データベース']
description: '非正規化ファクトテーブルにおけるフィルタおよび集約クエリのパフォーマンスを示します。'
```

#### 6.2.1 非正規化テーブル {#6-2-1-denormalized-tables}

フィルタおよび集約クエリは、非正規化されたファクトテーブルに対して ClickHouse の歴史的な主要利用ケースを表しています。私たちは、クリックストリームおよびトラフィック分析で使用されるアドホックおよび定期レポートのクエリをシミュレートするこの種の典型的なワークロードである ClickBench の実行時間を報告します。このベンチマークは、1億件の匿名化されたページヒットが記録されたテーブルに対して43のクエリから構成されており、ウェブ上で最大規模の分析プラットフォームの1つから取得されています。2024年6月の時点で、オンラインダッシュボード [\[17\]](#page-12-28) は、商業用および研究用データベース45件以上の測定値（コールド/ホット実行時間、データインポート時間、ディスクサイズ）を示しています。結果は、公開されているデータセットとクエリに基づいて独立した貢献者によって提出されます [\[16\]](#page-12-29)。クエリは、逐次およびインデックススキャンアクセス経路をテストし、CPU、IO、またはメモリに制約のあるリレーショナルオペレーターを定期的に露出します。

[Figure 10](#page-10-0) は、分析に頻繁に使用されるデータベースにおけるすべての ClickBench クエリを逐次実行した場合の総相対コールドおよびホット実行時間を示します。測定は、16 vCPU、32 GB RAM、5000 IOPS / 1000 MiB/s のディスクを備えた単一ノードの AWS EC2 c6a.4xlarge インスタンスで行われました。Redshift の比較可能なシステムは ([ra3.4xlarge](https://clickhou.se/redshift-sizes)、12 vCPUs、96 GB RAM)、および Snowfake の ([warehouse size S](https://clickhou.se/snowflake-sizes): 2x8 vCPUs、2x16 GB RAM) を使用しています。物理データベース設計は軽く調整されており、例えば主キーは指定しますが、個々のカラムの圧縮を変更したり、プロジェクションを作成したり、スキッピングインデックスを作成したりはしません。また、各コールドクエリの実行前に Linux ページキャッシュをフラッシュしますが、データベースやオペレーティングシステムの設定は調整しません。すべてのクエリに対して、データベース間での最速の実行時間がベースラインとして使用されます。他のデータベースの相対クエリ実行時間は ( + 10)/(_ + 10) として計算されます。データベースの総相対実行時間は、各クエリの比率の幾何平均です。研究データベース Umbra [\[54\]](#page-13-25) が全体のホット実行時間で最高の結果を達成する一方で、ClickHouse はホットおよびコールド実行時間において他のすべての商用データベースを上回っています。

<Anchor id="page-10-0"/><Image img={image_11} size="lg" alt="Image 11"/>

Figure 10: ClickBench の相対コールドおよびホット実行時間。

さまざまなワークロードでの SELECT のパフォーマンスを時間の経過とともに追跡するために、私たちは [use](https://clickhou.se/performance-over-years) と呼ばれる 4 つのベンチマークの組み合わせである VersionsBench [\[19\]](#page-12-30) を使用します。このベンチマークは、新しいリリースが公開されるたびに月に1回実行され、そのパフォーマンスを評価し [\[20\]](#page-12-31) 、パフォーマンスを低下させる可能性のあるコード変更を特定します。個々のベンチマークには、1. 上述の ClickBench、2. 15 の MgBench [\[21\]](#page-12-32) クエリ、3. 6 億行の非正規化されたスタースキーマベンチマーク [\[57\]](#page-13-26) ファクトテーブルに対する 13 のクエリ、4. 34 億行の [NYC タクシーライド](https://clickhou.se/nyc-taxi-rides-benchmark) に対する 4 つのクエリが含まれます [\[70\]](#page-13-27)。

[Figure 11](#page-10-5) は、2018年3月から2024年3月の間に 77 の ClickHouse バージョンの VersionsBench 実行時間の推移を示します。個々のクエリの相対実行時間の違いを補うために、すべてのバージョンでの最小クエリ実行時間に対する比率を重みとして幾何平均を用いて実行時間を正規化します。VersionBench のパフォーマンスは、過去6年間で 1.72 × 向上しました。長期サポート (LTS) リリースの日付は x 軸にマークされています。いくつかの期間においてパフォーマンスが一時的に低下したものの、LTS リリースは一般的に前の LTS バージョンと比較して同等またはそれ以上のパフォーマンスを持っています。2022年8月の大幅な改善は、セクション [4.4.](#page-7-0) で説明されているカラムごとのフィルター評価技術によるものでした。

<Anchor id="page-10-5"/><Image img={image_12} size="lg" alt="Image 12"/>

Figure 11: VersionsBench 2018-2024 の相対ホット実行時間。

```yaml
title: '6.2.2 正規化テーブル'
sidebar_label: '正規化テーブル'
keywords: ['ClickHouse', '正規化', 'テーブル']
description: '正規化テーブルにおけるTPC-Hクエリの実行時間を示します。'
```

#### 6.2.2 正規化テーブル {#6-2-2-normalized-tables}

古典的なデータウェアハウジングでは、データはしばしばスタースキーマまたはスノーフレークスキーマを使用してモデル化されます。私たちは TPC-H クエリ (スケールファクター 100) の実行時間を示しますが、正規化テーブルは ClickHouse の新たな利用ケースであることに注意します。 [Figure 12](#page-10-6) は、セクション [4.4.](#page-7-0) で説明されている並列ハッシュ結合アルゴリズムに基づく TPC-H クエリのホット実行時間を示します。測定は、64 vCPU、128 GB RAM、5000 IOPS / 1000 MiB/s のディスクを備えた単一ノードの AWS EC2 c6i.16xlarge インスタンスで行われ、最も早い5回の実行が記録されました。参考のため、同程度のサイズの Snowfake システム (warehouse size L、8x8 vCPUs、8x16 GB RAM) でも同様の測定を行いました。11のクエリの結果は、このテーブルから除外されています：Q2、Q4、Q13、Q17、およびQ20-22のクエリには、ClickHouse v24.6 ではサポートされていない関連サブクエリが含まれています。Q7-Q9およびQ19のクエリは、結合のための計画レベルの拡張最適化（ClickHouse v24.6 では未実装）に依存しており、実行可能な実行時間を達成しています。自動サブクエリデコリレーションと、結合のためのより良い最適化サポートは、2024年に実装が計画されています [\[18\]](#page-12-33)。残りの11のクエリのうち、ClickHouse で実行された 5 （6）クエリが Snowfake よりも速かったです。前述の最適化はパフォーマンスにとって重要であることが知られているため [\[27\]](#page-12-34)、実装後にこれらのクエリの実行時間がさらに改善されることを期待しています。

<Anchor id="page-10-6"/><Image img={image_13} size="lg" alt="Image 13"/>

Figure 12: TPC-H クエリのホット実行時間（秒単位）。

## 7 関連研究 {#7-related-work}

分析データベースは、近年の学術および商業的関心の対象となっています [\[1\]](#page-12-35)。Sybase IQ [\[48\]](#page-13-28)、Teradata [\[72\]](#page-13-29)、Vertica [\[42\]](#page-12-36)、Greenplum [\[47\]](#page-13-30) のような初期のシステムは、高価なバッチETLジョブとオンプレミスの性質に起因する限られた弾力性を特徴としていました。2010年代初頭、クラウドネイティブなデータウェアハウスやデータベース-as-a-service（DBaaS）オファリングの登場は、Snowfake [\[22\]](#page-12-37)、BigQuery [\[49\]](#page-13-31)、Redshift [\[4\]](#page-12-38) のようなツールで、分析のコストと複雑さを劇的に削減し、同時に高い可用性と自動リソーススケーリングの恩恵を受けました。最近では、分析実行カーネル（例：Photon [\[5\]](#page-12-39) および Velox [\[62\]](#page-13-32)）が、さまざまな分析、ストリーミング、機械学習アプリケーションで使用されるための共同処理データを提供します。

ClickHouse と目標と設計原則が最も似ているデータベースは、Druid [\[78\]](#page-13-33) と Pinot [\[34\]](#page-12-40) です。両システムは、高いデータ取り込み率でリアルタイム分析をターゲットとしています。ClickHouse のように、テーブルはセグメントと呼ばれる水平パーツに分割されています。ClickHouse は、小さなパーツを継続的にマージし、セクション [3.3,](#page-4-3) のテクニックを使ってオプションでデータボリュームを削減しますが、Druid と Pinot のパーツは永続的に不変です。また、Druid と Pinot は、テーブルを作成、変更、検索するために専門のノードを必要としますが、ClickHouse はこれらのタスクに対してモノリシックバイナリを使用します。

Snowfake [\[22\]](#page-12-37) は、共有ディスクアーキテクチャに基づく人気のあるプロプライエタリなクラウドデータウェアハウスです。テーブルをマイクロパーティションに分けるアプローチは、ClickHouse におけるパーツの概念に似ています。Snowfake は、永続性のためにハイブリッド PAX ページ [\[3\]](#page-12-41) を使用しますが、ClickHouse のストレージフォーマットは厳密に列指向です。Snowfake はまた、良好なパフォーマンスを引き出すために自動的に作成された軽量インデックスを使用してローカルキャッシュとデータプルーニングを強調します [\[31,](#page-12-13) [51\]](#page-13-14)。ClickHouse における主キーのように、ユーザーはオプションでクラスタ化インデックスを作成して同じ値を持つデータを共同配置することができます。

Photon [\[5\]](#page-12-39) と Velox [\[62\]](#page-13-32) は、複雑なデータ管理システムでコンポーネントとして使用するために設計されたクエリ実行エンジンです。両システムは、クエリプランを入力として受け取り、その後パラケット（Photon）または Arrow（Velox）ファイル [\[46\]](#page-13-34) の上でローカルノードで実行されます。ClickHouse は、これらの一般的なフォーマットでデータを消費および生成できますが、ストレージにはネイティブファイルフォーマットを好みます。Velox と Photon はクエリプランを最適化しません（Velox は基本的な式の最適化を行います）が、データ特性に応じて計算カーネルを動的に切り替えるなどの実行時適応技術を利用します。同様に、ClickHouse の計画オペレーターは、主にクエリメモリ消費に基づいて外部集約または結合オペレーターに切り替えるために、他のオペレーターを実行時に作成することができます。Photon 論文では、コード生成の設計 [\[38,](#page-12-22) [41,](#page-12-42) [53\]](#page-13-0) は、解釈されたベクタライズデザイン [\[11\]](#page-12-0) よりも開発およびデバッグが難しいことを指摘しています。Velox のコード生成に対する（実験的）サポートは、実行時に生成された C++ コードから生成された共有ライブラリをビルドおよびリンクしますが、ClickHouse は LLVM のオンデマンドコンパイル API と直接対話します。

DuckDB [\[67\]](#page-13-6) もホストプロセスによって組み込まれることを目的としていますが、さらにクエリ最適化とトランザクションを提供します。OLAP クエリと時折の OLTP ステートメントを混合するために設計されました。DuckDB は、ハイブリッドワークロードで良好なパフォーマンスを達成するために、順序を保持する辞書やフレームオブリファレンス [\[2\]](#page-12-10) のような軽量の圧縮方法を採用した DataBlocks [\[43\]](#page-12-43) ストレージフォーマットを選択しました。それに対して、ClickHouse は追加専用のユースケース（すなわち、更新や削除がないか、稀であること）に最適化されています。ブロックは、LZ4 などの重い圧縮技術を使用して圧縮され、ユーザーが頻繁なクエリを高速化するためにデータプルーニングを寛大に使用することを前提とし、残りのクエリに対する非圧縮コストが、デコンプレッションコストを上回ると想定されています。DuckDB はまた、Hyper の MVCC スキームに基づく可視性トランザクションを提供します [\[55\]](#page-13-35)、一方 ClickHouse はスナップショット隔離のみを提供します。

## 8 結論と展望 {#8-conclusion-and-outlook}

私たちは、オープンソースの高性能 OLAP データベースである ClickHouse のアーキテクチャを紹介しました。書き込み最適化ストレージレイヤーと最先端のベクタライズドクエリエンジンを基盤に、ClickHouse はペタバイト規模のデータセットに対するリアルタイム分析を高い取り込み率で可能にします。データをバックグラウンドで非同期にマージおよび変換することによって、ClickHouse はデータメンテナンスと並列挿入を効率的に切り離します。ストレージレイヤーは、スパース主インデックス、スキッピングインデックス、プロジェクションテーブルを使用して積極的なデータプルーニングを可能にします。私たちは、更新と削除、冪等性のある挿入、そして高可用性のためのノード間データレプリケーションに対する ClickHouse の実装を説明しました。クエリ処理レイヤーは豊富な技術を使用してクエリを最適化し、すべてのサーバーおよびクラスターリソースにわたって実行を並列化します。統合テーブルエンジンおよび関数は、他のデータ管理システムおよびデータフォーマットとシームレスに対話する便利な方法を提供します。ベンチマークを通じて、ClickHouse が市場で最も高速な分析データベースの1つであることを示し、ClickHouse の実世界の展開における典型的なクエリのパフォーマンスの大きな改善を示しました。

2024年に計画されているすべての機能および強化については、公開されたロードマップ [\[18\]](#page-12-33) を参照してください。計画されている改善には、ユーザートランザクションのサポート、代替クエリ言語としての PromQL [\[69\]](#page-13-36)、半構造化データ（例：JSON）用の新しいデータ型、結合の計画レベルの最適化の改善、軽量削除を補完する軽量更新の実装が含まれています。

## 9 謝辞 {#acknowledgements}

バージョン 24.6 に従い、SELECT * FROM system.contributors は、ClickHouse に貢献した1994人の個人を返します。私たちは、ClickHouse Inc. の全エンジニアリングチームと、共にこのデータベースを構築するために彼らの努力と献身に感謝します。
## 参考文献 {#references}

- <Anchor id="page-12-35"/>[1] Daniel Abadi, Peter Boncz, Stavros Harizopoulos, Stratos Idreaos, and Samuel Madden. 2013. 現代の列指向データベースシステムの設計と実装. https://doi.org/10.1561/9781601987556
- <Anchor id="page-12-10"/>[2] Daniel Abadi, Samuel Madden, and Miguel Ferreira. 2006. 列指向データベースシステムにおける圧縮と実行の統合. ACM SIGMOD 国際データ管理会議 (SIGMOD '06) 論文集. 671–682.https://doi.org/10.1145/1142473.1142548
- <Anchor id="page-12-41"/>[3] Anastassia Ailamaki, David J. DeWitt, Mark D. Hill, and Marios Skounakis. 2001. キャッシュ性能のための関係の織り交ぜ. 第27回非常に大きなデータベース国際会議 (VLDB '01) 論文集. Morgan Kaufmann Publishers Inc., サンフランシスコ, CA, USA, 169–180.
- <Anchor id="page-12-38"/>[4] Nikos Armenatzoglou, Sanuj Basu, Naga Bhanoori, Mengchu Cai, Naresh Chainani, Kiran Chinta, Venkatraman Govindaraju, Todd J. Green, Monish Gupta, Sebastian Hillig, Eric Hotinger, Yan Leshinksy, Jintian Liang, Michael McCreedy, Fabian Nagel, Ippokratis Pandis, Panos Parchas, Rahul Pathak, Orestis Polychroniou, Foyzur Rahman, Gaurav Saxena, Gokul Soundararajan, Sriram Subramanian, and Doug Terry. 2022. Amazon Redshiftの再発明. 2022年国際データ管理会議 (フィラデルフィア, PA, USA) 論文集 (SIGMOD '22). コンピュータ機械協会, ニューヨーク, NY, USA, 2205–2217. https://doi.org/10.1145/3514221.3526045
- <Anchor id="page-12-39"/>[5] Alexander Behm, Shoumik Palkar, Utkarsh Agarwal, Timothy Armstrong, David Cashman, Ankur Dave, Todd Greenstein, Shant Hovsepian, Ryan Johnson, Arvind Sai Krishnan, Paul Leventis, Ala Luszczak, Prashanth Menon, Mostafa Mokhtar, Gene Pang, Sameer Paranjpye, Greg Rahn, Bart Samwel, Tom van Bussel, Herman van Hovell, Maryann Xue, Reynold Xin, and Matei Zaharia. 2022. Photon: レイクハウスシステムのための高速クエリエンジン (SIGMOD '22). コンピュータ機械協会, ニューヨーク, NY, USA, 2326–2339. [https://doi.org/10.1145/3514221.](https://doi.org/10.1145/3514221.3526054) [3526054](https://doi.org/10.1145/3514221.3526054)
- <Anchor id="page-12-18"/>[6] Philip A. Bernstein and Nathan Goodman. 1981. 分散データベースシステムにおける同時制御. ACM Computing Survey 13, 2 (1981), 185–221. https://doi.org/10.1145/356842.356846
- <Anchor id="page-12-23"/>[7] Spyros Blanas, Yinan Li, and Jignesh M. Patel. 2011. マルチコアCPUのためのメインメモリハッシュ結合アルゴリズムの設計と評価. 2011年ACM SIGMOD国際データ管理会議 (アテネ, ギリシャ) 論文集 (SIGMOD '11). コンピュータ機械協会, ニューヨーク, NY, USA, 37–48. https://doi.org/10.1145/1989323.1989328
- <Anchor id="page-12-26"/><Anchor id="page-12-14"/>[8] Daniel Gomez Blanco. 2023. 実践的OpenTelemetry. Springer Nature.
- [9] Burton H. Bloom. 1970. 許容誤差を伴うハッシュコーディングにおける時間/空間トレードオフ. Commun. ACM 13, 7 (1970), 422–426. [https://doi.org/10.1145/362686.](https://doi.org/10.1145/362686.362692) [362692](https://doi.org/10.1145/362686.362692)
- <Anchor id="page-12-27"/>[10] Peter Boncz, Thomas Neumann, and Orri Erling. 2014. TPC-H分析: 影響力のあるベンチマークからの隠されたメッセージと教訓. パフォーマンスキャラクタリゼーションとベンチマーキング. 61–76. [https://doi.org/10.1007/978-3-319-](https://doi.org/10.1007/978-3-319-04936-6_5) [04936-6_5](https://doi.org/10.1007/978-3-319-04936-6_5)
- <Anchor id="page-12-0"/>[11] Peter Boncz, Marcin Zukowski, and Niels Nes. 2005. MonetDB/X100: ハイパーパイプライン化クエリ実行. CIDR.
- <Anchor id="page-12-8"/>[12] Martin Burtscher and Paruj Ratanaworabhan. 2007. 倍精度浮動小数点データの高スループット圧縮. データ圧縮会議 (DCC). 293–302. https://doi.org/10.1109/DCC.2007.44
- <Anchor id="page-12-6"/>[13] Jef Carpenter and Eben Hewitt. 2016. Cassandra: 完全ガイド (第2版). O'Reilly Media, Inc.
- <Anchor id="page-12-17"/>[14] Bernadette Charron-Bost, Fernando Pedone, and André Schiper (Eds.). 2010. レプリケーション: 理論と実践. Springer-Verlag.
- <Anchor id="page-12-3"/>[15] chDB. 2024. chDB - 組み込みOLAP SQLエンジン. https://github.com/chdb-io/chdb
- <Anchor id="page-12-29"/>[16] ClickHouse. 2024. ClickBench: 分析データベースのためのベンチマーク. https://github.com/ClickHouse/ClickBench
- <Anchor id="page-12-28"/>[17] ClickHouse. 2024. ClickBench: 比較測定. https://benchmark.clickhouse.com
- <Anchor id="page-12-33"/>[18] ClickHouse. 2024. ClickHouse ロードマップ 2024 (GitHub). https://github.com/ClickHouse/ClickHouse/issues/58392
- <Anchor id="page-12-30"/>[19] ClickHouse. 2024. ClickHouse バージョンベンチマーク. https://github.com/ClickHouse/ClickBench/tree/main/versions
- <Anchor id="page-12-31"/>[20] ClickHouse. 2024. ClickHouse バージョンベンチマーク結果. https://benchmark.clickhouse.com/versions/
- <Anchor id="page-12-32"/>[21] Andrew Crotty. 2022. MgBench. https://github.com/andrewcrotty/mgbench
- <Anchor id="page-12-37"/>[22] Benoit Dageville, Thierry Cruanes, Marcin Zukowski, Vadim Antonov, Artin Avanes, Jon Bock, Jonathan Claybaugh, Daniel Engovatov, Martin Hentschel, Jiansheng Huang, Allison W. Lee, Ashish Motivala, Abdul Q. Munir, Steven Pelley, Peter Povinec, Greg Rahn, Spyridon Triantafyllis, and Philipp Unterbrunner. 2016. Snowflake Elastic Data Warehouse. 2016年国際データ管理会議 (サンフランシスコ, カリフォルニア, USA) 論文集 (SIGMOD '16). コンピュータ機械協会, ニューヨーク, NY, USA, 215–226. [https:](https://doi.org/10.1145/2882903.2903741) [//doi.org/10.1145/2882903.2903741](https://doi.org/10.1145/2882903.2903741)
- <Anchor id="page-12-9"/>[23] Patrick Damme, Annett Ungethüm, Juliana Hildebrandt, Dirk Habich, and Wolfgang Lehner. 2019. 包括的実験調査から軽量整数圧縮アルゴリズムのコストベース選択戦略への移行. ACM Trans. Database Syst. 44, 3, Article 9 (2019), 46ページ. https://doi.org/10.1145/3323991
- <Anchor id="page-12-1"/>[24] Philippe Dobbelaere and Kyumars Sheykh Esmaili. 2017. Kafka対RabbitMQ: 2つの業界リファレンスのパブリッシュ/サブスクライブ実装の比較研究: 業界論文 (DEBS '17). コンピュータ機械協会, ニューヨーク, NY, USA, 227–238. https://doi.org/10.1145/3093742.3093908
- <Anchor id="page-12-19"/>[25] LLVMドキュメンテーション. 2024. LLVMにおける自動ベクトル化. https://llvm.org/docs/Vectorizers.html
- <Anchor id="page-12-7"/>[26] Siying Dong, Andrew Kryczka, Yanqin Jin, and Michael Stumm. 2021. RocksDB: 大規模アプリケーションにサービスを提供するキー・バリュー・ストアにおける開発優先順位の進化. ACM Transactions on Storage 17, 4, Article 26 (2021), 32ページ. https://doi.org/10.1145/3483840
- <Anchor id="page-12-34"/>[27] Markus Dreseler, Martin Boissier, Tilmann Rabl, and Matthias Ufacker. 2020. TPC-Hのボトルネックを定量化し、それらの最適化. Proc. VLDB Endow. 13, 8 (2020), 1206–1220. https://doi.org/10.14778/3389133.3389138
- <Anchor id="page-12-12"/>[28] Ted Dunning. 2021. t-digest: 分布の効率的推定. Software Impacts 7 (2021). https://doi.org/10.1016/j.simpa.2020.100049
- <Anchor id="page-12-16"/>[29] Martin Faust, Martin Boissier, Marvin Keller, David Schwalb, Holger Bischof, Katrin Eisenreich, Franz Färber, and Hasso Plattner. 2016. SAP HANAにおけるハッシュインデックスを用いたフットプリント削減と一意性保証. データベースと専門システムアプリケーションの会議. 137–151. [https://doi.org/10.1007/978-3-319-44406-](https://doi.org/10.1007/978-3-319-44406-2_11) [2_11](https://doi.org/10.1007/978-3-319-44406-2_11)
- <Anchor id="page-12-11"/>[30] Philippe Flajolet, Eric Fusy, Olivier Gandouet, and Frederic Meunier. 2007. HyperLogLog: ほぼ最適なカーディナリティ推定アルゴリズムの分析. AofA: アルゴリズムの分析, Vol. DMTCS Proceedings vol. AH, 2007年アルゴリズム分析会議 (AofA 07). 離散数学と理論計算機科学, 137–156. https://doi.org/10.46298/dmtcs.3545
- <Anchor id="page-12-13"/>[31] Hector Garcia-Molina, Jefrey D. Ullman, and Jennifer Widom. 2009. データベースシステム - 完全な書籍 (第2版).
- <Anchor id="page-12-24"/>[32] Pawan Goyal, Harrick M. Vin, and Haichen Chen. 1996. スタートタイムフェアキューイング: 統合サービスパケットスイッチングネットワークのためのスケジューリングアルゴリズム. 26, 4 (1996), 157–168. https://doi.org/10.1145/248157.248171
- <Anchor id="page-12-21"/>[33] Goetz Graefe. 1993. 大規模データベースのためのクエリ評価技術. ACM Comput. Surv. 25, 2 (1993), 73–169. https://doi.org/10.1145/152610.152611
- <Anchor id="page-12-40"/>[34] Jean-François Im, Kishore Gopalakrishna, Subbu Subramaniam, Mayank Shrivastava, Adwait Tumbde, Xiaotian Jiang, Jennifer Dai, Seunghyun Lee, Neha Pawar, Jialiang Li, and Ravi Aringunram. 2018. Pinot: リアルタイムOLAPを530百万ユーザーに提供. 2018年国際データ管理会議 (ヒューストン, TX, USA) 論文集 (SIGMOD '18). コンピュータ機械協会, ニューヨーク, NY, USA, 583–594. https://doi.org/10.1145/3183713.3190661
- <Anchor id="page-12-25"/>[35] ISO/IEC 9075-9:2001 2001. 情報技術 — データベース言語 — SQL — 第9部: 外部データの管理 (SQL/MED). 標準. 国際標準化機構.
- <Anchor id="page-12-2"/>[36] Paras Jain, Peter Kraft, Conor Power, Tathagata Das, Ion Stoica, and Matei Zaharia. 2023. レイクハウスストレージシステムの分析と比較. CIDR.
- <Anchor id="page-12-4"/>[37] Project Jupyter. 2024. Jupyter Notebooks. https://jupyter.org/
- <Anchor id="page-12-22"/>[38] Timo Kersten, Viktor Leis, Alfons Kemper, Thomas Neumann, Andrew Pavlo, and Peter Boncz. 2018. コンパイル済みおよびベクトル化されたクエリについて知っておくべきすべてのこと. Proc. VLDB Endow. 11, 13 (2018年9月), 2209–2222. https://doi.org/10.14778/3275366.3284966
- <Anchor id="page-12-15"/>[39] Changkyu Kim, Jatin Chhugani, Nadathur Satish, Eric Sedlar, Anthony D. Nguyen, Tim Kaldewey, Victor W. Lee, Scott A. Brandt, and Pradeep Dubey. 2010. FAST: 現代のCPUおよびGPU上でのアーキテクチャに敏感な木検索. 2010年ACM SIGMOD国際データ管理会議 (インディアナポリス, インディアナ, USA) 論文集 (SIGMOD '10). コンピュータ機械協会, ニューヨーク, NY, USA, 339–350. https://doi.org/10.1145/1807167.1807206
- <Anchor id="page-12-5"/>[40] Donald E. Knuth. 1973. コンピュータプログラミングの技法, 第III巻: ソートと検索. Addison-Wesley.
- <Anchor id="page-12-42"/>[41] André Kohn, Viktor Leis, and Thomas Neumann. 2018. コンパイル済みクエリの適応実行. 2018年IEEE第34回国際データ工学会議 (ICDE). 197–208. https://doi.org/10.1109/ICDE.2018.00027
- <Anchor id="page-12-36"/>[42] Andrew Lamb, Matt Fuller, Ramakrishna Varadarajan, Nga Tran, Ben Vandiver, Lyric Doshi, and Chuck Bear. 2012. Vertica分析データベース: C-Store 7年後. Proc. VLDB Endow. 5, 12 (2012年8月), 1790–1801. [https://doi.org/10.](https://doi.org/10.14778/2367502.2367518) [14778/2367502.2367518](https://doi.org/10.14778/2367502.2367518)
- <Anchor id="page-12-43"/>[43] Harald Lang, Tobias Mühlbauer, Florian Funke, Peter A. Boncz, Thomas Neumann, and Alfons Kemper. 2016. データブロック: ベクトル化とコンパイルの両方を使用した圧縮ストレージでのハイブリッドOLTPとOLAP. 2016年国際データ管理会議 (サンフランシスコ, カリフォルニア, USA) (SIGMOD '16). コンピュータ機械協会, ニューヨーク, NY, USA, 311–326. https://doi.org/10.1145/2882903.2882925
- <Anchor id="page-12-20"/>[44] Viktor Leis, Peter Boncz, Alfons Kemper, and Thomas Neumann. 2014. モルセルトドライブ並列性: 多コア時代のNUMAを意識したクエリ評価フレームワーク. 2014年ACM SIGMOD国際データ管理会議 (スノーバード, ユタ, USA) 論文集 (SIGMOD '14). コンピュータ機械協会, ニューヨーク, NY, USA, 743–754. [https://doi.org/10.1145/2588555.](https://doi.org/10.1145/2588555.2610507) [2610507](https://doi.org/10.1145/2588555.2610507)
- <Anchor id="page-13-17"/>[45] Viktor Leis, Alfons Kemper, and Thomas Neumann. 2013. 適応型ラジックス木: メインメモリデータベースのためのARTfulインデクシング. 2013年IEEE第29回国際データ工学会議 (ICDE). 38–49. [https://doi.org/10.1109/ICDE.](https://doi.org/10.1109/ICDE.2013.6544812) [2013.6544812](https://doi.org/10.1109/ICDE.2013.6544812)
- <Anchor id="page-13-34"/>[46] Chunwei Liu, Anna Pavlenko, Matteo Interlandi, and Brandon Haynes. 2023. 分析DBMSのための一般的なオープンフォーマットの深堀り. 16, 11 (2023年7月), 3044–3056. https://doi.org/10.14778/3611479.3611507
- <Anchor id="page-13-30"/>[47] Zhenghua Lyu, Huan Hubert Zhang, Gang Xiong, Gang Guo, Haozhou Wang, Jinbao Chen, Asim Praveen, Yu Yang, Xiaoming Gao, Alexandra Wang, Wen Lin, Ashwin Agrawal, Junfeng Yang, Hao Wu, Xiaoliang Li, Feng Guo, Jiang Wu, Jesse Zhang, and Venkatesh Raghavan. 2021. Greenplum: トランザクションおよび分析ワークロードのためのハイブリッドデータベース (SIGMOD '21). コンピュータ機械協会, ニューヨーク, NY, USA, 2530–2542. [https:](https://doi.org/10.1145/3448016.3457562) [//doi.org/10.1145/3448016.3457562](https://doi.org/10.1145/3448016.3457562)
- <Anchor id="page-13-28"/>[48] Roger MacNicol and Blaine French. 2004. Sybase IQ Multiplex - 分析のために設計されました. 第三十回データベース会議 (トロント, カナダ) (VLDB '04). VLDB Endowment, 1227–1230.
- <Anchor id="page-13-31"/>[49] Sergey Melnik, Andrey Gubarev, Jing Jing Long, Geofrey Romer, Shiva Shivakumar, Matt Tolton, Theo Vassilakis, Hossein Ahmadi, Dan Delorey, Slava Min, Mosha Pasumansky, and Jef Shute. 2020. Dremel: ウェブスケールのインタラクティブSQL分析の10年. Proc. VLDB Endow. 13, 12 (2020年8月), 3461–3472. https://doi.org/10.14778/3415478.3415568
- <Anchor id="page-13-2"/>[50] Microsoft. 2024. Kusto Query Language. https://github.com/microsoft/Kusto-Query-Language
- <Anchor id="page-13-14"/>[51] Guido Moerkotte. 1998. 小さなマテリアライズド集計: データウェアハウジングのための軽量インデックス構造. 第24回国際データベース会議 (VLDB '98) 論文集. 476–487.
- <Anchor id="page-13-22"/>[52] Jalal Mostafa, Sara Wehbi, Suren Chilingaryan, and Andreas Kopmann. 2022. SciTS: 科学的実験と産業用IoTにおける時系列データベースのベンチマーク. 第34回科学的および統計的データベース管理に関する国際会議 (SSDBM '22) 論文集. Article 12. [https:](https://doi.org/10.1145/3538712.3538723) [//doi.org/10.1145/3538712.3538723](https://doi.org/10.1145/3538712.3538723)
- <Anchor id="page-13-0"/>[53] Thomas Neumann. 2011. 現代のハードウェア向けに効率的なクエリプランをコンパイルする. Proc. VLDB Endow. 4, 9 (2011年6月), 539–550. [https://doi.org/10.14778/](https://doi.org/10.14778/2002938.2002940) [2002938.2002940](https://doi.org/10.14778/2002938.2002940)
- <Anchor id="page-13-25"/>[54] Thomas Neumann and Michael J. Freitag. 2020. Umbra: メモリ内性能を持つディスクベースシステム. 第10回革新的データシステム研究会議, CIDR 2020, アムステルダム, オランダ, 2020年1月12-15, オンライン議事録. www.cidrdb.org. [http://cidrdb.org/cidr2020/papers/p29-neumann](http://cidrdb.org/cidr2020/papers/p29-neumann-cidr20.pdf)[cidr20.pdf](http://cidrdb.org/cidr2020/papers/p29-neumann-cidr20.pdf)
- <Anchor id="page-13-35"/>[55] Thomas Neumann, Tobias Mühlbauer, and Alfons Kemper. 2015. メインメモリデータベースシステムのための高速な可視化マルチバージョン同時制御. 2015年ACM SIGMOD国際データ管理会議 (メルボルン, ビクトリア, オーストラリア) 論文集 (SIGMOD '15). コンピュータ機械協会, ニューヨーク, NY, USA, 677–689. [https://doi.org/10.1145/2723372.](https://doi.org/10.1145/2723372.2749436) [2749436](https://doi.org/10.1145/2723372.2749436)
- <Anchor id="page-13-8"/>[56] LevelDB on GitHub. 2024. LevelDB. https://github.com/google/leveldb
- <Anchor id="page-13-26"/>[57] Patrick O'Neil, Elizabeth O'Neil, Xuedong Chen, and Stephen Revilak. 2009. スター スキーマベンチマークと拡張されたファクトテーブルインデクシング. パフォーマンス評価とベンチマーキング. Springer Berlin Heidelberg, 237–252. [https:](https://doi.org/10.1007/978-3-642-10424-4_17) [//doi.org/10.1007/978-3-642-10424-4_17](https://doi.org/10.1007/978-3-642-10424-4_17)
- <Anchor id="page-13-7"/>[58] Patrick E. O'Neil, Edward Y. C. Cheng, Dieter Gawlick, and Elizabeth O'Neil. 1996. ログ構造マージツリー (LSMツリー). Acta Informatica 33 (1996), 351–385. https://doi.org/10.1007/s002360050048
- <Anchor id="page-13-4"/>[59] Diego Ongaro and John Ousterhout. 2014. 理解可能なコンセンサスアルゴリズムを求めて. 2014年USENIXカンファレンスのUSENIX年次技術会議論文集 (USENIX ATC'14). 305–320. [https://doi.org/doi/10.](https://doi.org/doi/10.5555/2643634.2643666) [5555/2643634.2643666](https://doi.org/doi/10.5555/2643634.2643666)
- <Anchor id="page-13-3"/>[60] Patrick O'Neil, Edward Cheng, Dieter Gawlick, and Elizabeth O'Neil. 1996. ログ構造マージツリー (LSM-ツリー). Acta Inf. 33, 4 (1996), 351–385. [https:](https://doi.org/10.1007/s002360050048) [//doi.org/10.1007/s002360050048](https://doi.org/10.1007/s002360050048)
- <Anchor id="page-13-5"/>[61] Pandas. 2024. Pandas Dataframes. https://pandas.pydata.org/
- <Anchor id="page-13-32"/>[62] Pedro Pedreira, Orri Erling, Masha Basmanova, Kevin Wilfong, Laith Sakka, Krishna Pai, Wei He, and Biswapesh Chattopadhyay. 2022. Velox: Metaの統一実行エンジン. Proc. VLDB Endow. 15, 12 (2022年8月), 3372–3384. [https:](https://doi.org/10.14778/3554821.3554829) [//doi.org/10.14778/3554821.3554829](https://doi.org/10.14778/3554821.3554829)
- <Anchor id="page-13-10"/>[63] Tuomas Pelkonen, Scott Franklin, Justin Teller, Paul Cavallaro, Qi Huang, Justin Meza, and Kaushik Veeraraghavan. 2015. Gorilla: 高速でスケーラブルなメモリ内時系列データベース. Proceedings of the VLDB Endowment 8, 12 (2015), 1816–1827. https://doi.org/10.14778/2824032.2824078
- <Anchor id="page-13-18"/>[64] Orestis Polychroniou, Arun Raghavan, and Kenneth A. Ross. 2015. メモリ内データベースのためのSIMDベクトル化の再考. 2015年ACM SIGMOD国際データ管理会議 (SIGMOD '15) 論文集. 1493–1508. https://doi.org/10.1145/2723372.2747645
- <Anchor id="page-13-21"/>[65] PostgreSQL. 2024. PostgreSQL - 外部データラッパー. https://wiki.postgresql.org/wiki/Foreign_data_wrappers
- <Anchor id="page-13-23"/>[66] Mark Raasveldt, Pedro Holanda, Tim Gubner, and Hannes Mühleisen. 2018. 公正なベンチマーキングの考慮すべき難しさ: データベースパフォーマンステストにおける一般的な落とし穴. データベースシステムのテストに関するワークショップ (ヒューストン, TX, USA) (DBTest'18). Article 2, 6ページ. https://doi.org/10.1145/3209950.3209955
- <Anchor id="page-13-6"/>[67] Mark Raasveldt and Hannes Mühleisen. 2019. DuckDB: 組み込み可能な分析データベース (SIGMOD '19). コンピュータ機械協会, ニューヨーク, NY, USA, 1981–1984. https://doi.org/10.1145/3299869.3320212
- <Anchor id="page-13-16"/>[68] Jun Rao and Kenneth A. Ross. 1999. メインメモリにおける意思決定支援のためのキャッシュ意識型インデクシング. 第25回非常に大きなデータベース国際会議 (VLDB '99). サンフランシスコ, CA, USA, 78–89.
- <Anchor id="page-13-36"/>[69] Navin C. Sabharwal and Piyush Kant Pandey. 2020. Prometheus クエリ言語 (PromQL) の作業. マイクロサービスおよびコンテナ化アプリケーションの監視に関する書籍. https://doi.org/10.1007/978-1-4842-6216-0_5
- <Anchor id="page-13-27"/>[70] Todd W. Schneider. 2022. ニューヨーク市タクシーおよび依頼車両データ. https://github.com/toddwschneider/nyc-taxi-data
- <Anchor id="page-13-13"/>[71] Mike Stonebraker, Daniel J. Abadi, Adam Batkin, Xuedong Chen, Mitch Cherniack, Miguel Ferreira, Edmond Lau, Amerson Lin, Sam Madden, Elizabeth O'Neil, Pat O'Neil, Alex Rasin, Nga Tran, and Stan Zdonik. 2005. C-Store: 列指向DBMS. 第31回非常に大きなデータベース国際会議 (VLDB '05) 論文集. 553–564.
- <Anchor id="page-13-29"/>[72] Teradata. 2024. Teradata データベース. https://www.teradata.com/resources/datasheets/teradata-database
- <Anchor id="page-13-15"/>[73] Frederik Transier. 2010. メモリ内テキスト検索エンジンのためのアルゴリズムとデータ構造. 博士論文. https://doi.org/10.5445/IR/1000015824
- <Anchor id="page-13-24"/>[74] Adrian Vogelsgesang, Michael Haubenschild, Jan Finis, Alfons Kemper, Viktor Leis, Tobias Muehlbauer, Thomas Neumann, and Manuel Then. 2018. 現実を把握する: ベンチマークが現実世界をいかに表現できないか. データベースシステムのテストに関するワークショップ (ヒューストン, TX, USA) (DBTest'18). Article 1, 6ページ. https://doi.org/10.1145/3209950.3209952
- <Anchor id="page-13-9"/>[75] LZ4ウェブサイト. 2024. LZ4. https://lz4.org/
- <Anchor id="page-13-11"/><Anchor id="page-13-1"/>[76] PRQLウェブサイト. 2024. PRQL. https://prql-lang.org [77] Till Westmann, Donald Kossmann, Sven Helmer, and Guido Moerkotte. 2000. 圧縮データベースの実装とパフォーマンス. SIGMOD Rec.
- <Anchor id="page-13-33"/>29, 3 (2000年9月), 55–67. https://doi.org/10.1145/362084.362137 [78] Fangjin Yang, Eric Tschetter, Xavier Léauté, Nelson Ray, Gian Merlino, and Deep Ganguli. 2014. Druid: リアルタイム分析データストア. 2014年ACM SIGMOD国際データ管理会議 (スノーバード, ユタ, USA) 論文集 (SIGMOD '14). コンピュータ機械協会, ニューヨーク, NY, USA, 157–168. https://doi.org/10.1145/2588555.2595631
- <Anchor id="page-13-20"/>[79] Tianqi Zheng, Zhibin Zhang, and Xueqi Cheng. 2020. SAHA: 分析データベースのための文字列適応ハッシュテーブル. Applied Sciences 10, 6 (2020). [https:](https://doi.org/10.3390/app10061915) [//doi.org/10.3390/app10061915](https://doi.org/10.3390/app10061915)
- <Anchor id="page-13-19"/>[80] Jingren Zhou and Kenneth A. Ross. 2002. SIMD命令を利用したデータベース操作の実装. 2002年ACM SIGMOD国際データ管理会議 (SIGMOD '02) 論文集. 145–156. [https://doi.org/10.](https://doi.org/10.1145/564691.564709) [1145/564691.564709](https://doi.org/10.1145/564691.564709)
- <Anchor id="page-13-12"/>[81] Marcin Zukowski, Sandor Heman, Niels Nes, and Peter Boncz. 2006. スーパー スカラ RAM-CPU キャッシュ圧縮. 第22回データ工学国際会議 (ICDE '06) 論文集. 59. [https://doi.org/10.1109/ICDE.](https://doi.org/10.1109/ICDE.2006.150) [2006.150](https://doi.org/10.1109/ICDE.2006.150)
