---
slug: /academic_overview
title: アーキテクチャ概要
description: 2024年のVLDB論文のドキュメント版
keywords: [アーキテクチャ]
---

import image_01 from '@site/static/images/managing-data/core-concepts/_vldb2024_1_Figure_0.png'
import image_02 from '@site/static/images/managing-data/core-concepts/_vldb2024_2_Figure_0.png'
import image_03 from '@site/static/images/managing-data/core-concepts/_vldb2024_2_Figure_5.png'
import image_04 from '@site/static/images/managing-data/core-concepts/_vldb2024_3_Figure_7.png'
import image_05 from '@site/static/images/managing-data/core-concepts/_vldb2024_4_Figure_6.png'
import image_06 from '@site/static/images/managing-data/core-concepts/_vldb2024_5_Figure_8.png'
import image_07 from '@site/static/images/managing-data/core-concepts/_vldb2024_6_Figure_0.png'
import image_08 from '@site/static/images/managing-data/core-concepts/_vldb2024_7_Figure_1.png'
import image_09 from '@site/static/images/managing-data/core-concepts/_vldb2024_8_Figure_7.png'
import image_10 from '@site/static/images/managing-data/core-concepts/_vldb2024_10_Figure_14.png'
import image_11 from '@site/static/images/managing-data/core-concepts/_vldb2024_10_Figure_0.png'
import image_12 from '@site/static/images/managing-data/core-concepts/_vldb2024_10_Figure_12.png'
import image_13 from '@site/static/images/managing-data/core-concepts/_vldb2024_10_Figure_13.png'

これは私たちの[VLDB 2024に関する科学論文](https://www.vldb.org/pvldb/vol17/p3731-schulze.pdf)のウェブ版です。また、その背景と経緯についても[ブログで紹介](https://clickhouse.com/blog/first-clickhouse-research-paper-vldb-lightning-fast-analytics-for-everyone)しており、ClickHouseのCTOであり創設者のアレクセイ・ミロビドフによるVLDB 2024のプレゼンテーションを視聴することをお勧めします:

<iframe width="768" height="432" src="https://www.youtube.com/embed/7QXKBKDOkJE?si=5uFerjqPSXQWqDkF" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
## 概要 {#abstract}

過去数十年で、蓄積され分析されるデータの量は指数関数的に増加しています。企業は、製品を改善し、パフォーマンスを評価し、ビジネスに不可欠な意思決定を行うために、このデータに依存し始めています。しかし、データボリュームがインターネット規模に、ますます成長するにつれ、企業は歴史的データと新しいデータを、コスト効果が高くスケーラブルな方法で管理する必要があり、同時に高負荷のクエリを利用し、リアルタイムの待機時間（ケースによっては1秒未満）で分析することが期待されています。

本論文では、ペタバイト規模のデータセットに対して高パフォーマンスの分析を提供することを目的とした人気のオープンソースOLAPデータベースであるClickHouseの概要を紹介します。そのストレージ層は、従来のログ構造マージ（LSM）ツリーに基づくデータフォーマットと、バックグラウンドでの歴史データの継続的変換（例: 集約、アーカイブ）に関する新しい技術を組み合わせています。クエリは便利なSQLダイアレクトで書かれ、最先端のベクトル化されたクエリ実行エンジンによって処理され、オプションでコードコンパイルを行います。ClickHouseは、クエリ内で無関係なデータを評価しないように積極的にプルーニング技術を活用します。他のデータ管理システムは、テーブル関数、テーブルエンジン、またはデータベースエンジンレベルで統合することができます。実世界のベンチマークにより、ClickHouseは市場で最も高速な分析データベースの一つであることが証明されています。

## 1 はじめに {#1-introduction}

本論文は、高パフォーマンスな分析クエリのために設計された、数兆行と数百カラムを持つ列指向OLAPデータベースであるClickHouseについて説明します。ClickHouseは、2009年にウェブスケールのログファイルデータのためのフィルタおよび集約オペレータとして[スタート](https://clickhou.se/evolution)し、2016年にオープンソース化されました。[図1](#page-1-0)は、この論文で説明されている主な機能がClickHouseに導入された時期を示しています。

ClickHouseは、現代の分析データ管理の5つの主要な課題に対処するために設計されています：

1. **高い取り込み速度を持つ巨大データセット**。ウェブ分析、金融、およびeコマースのような多くのデータ駆動型アプリケーションは、巨大で継続的に増加するデータ量で特徴付けられます。巨大データセットを扱うために、分析データベースは効率的なインデックスと圧縮戦略を提供するだけでなく、複数のノードへのデータ配布を許可する必要があります（スケールアウト）。単一のサーバーは数十テラバイトのストレージに制限されています。さらに、最近のデータは歴史的データよりもリアルタイムの洞察にとってより関連性が高いことが多いです。その結果、分析データベースは新しいデータを常に高い速度で取り込むか、バースト的に取り込むことができ、並行レポートクエリの処理を止めることなく、歴史的データを「優先度を下げる」ことができる必要があります（例: 集約、アーカイブ）。

2. **多くの同時クエリと低い待機時間の期待**。クエリは一般的にアドホック（例: 探索的データ分析）または再発（例: 定期的なダッシュボードクエリ）に分類できます。ユースケースがインタラクティブであればあるほど、クエリの待機時間は短くなることが期待され、クエリの最適化と実行に課題が生じます。再発クエリは、ワークロードに応じて物理データベースのレイアウトを適応させる機会を提供します。その結果、データベースは頻繁にクエリを最適化するためのプルーニング技術を提供するべきです。クエリの優先度に応じて、データベースはCPU、メモリ、ディスク、およびネットワークI/Oなどの共有システムリソースへの均等または優先されたアクセスを保証しなければなりません、たとえ多くのクエリが同時に実行されている場合でも。

3. **データストア、ストレージの場所、形式の多様な風景**。既存のデータアーキテクチャと統合するために、現代の分析データベースは、任意のシステム、ロケーション、形式で外部データを読み書きするために高い自由度を示すべきです。

4. **性能インストロスペクションをサポートする便利なクエリ言語**。OLAPデータベースの実用途は、追加の「ソフト」な要求を提示します。たとえば、ニッチなプログラミング言語の代わりに、ユーザーはしばしばネストされたデータ型や広範囲にわたる通常の集約およびウィンドウ関数を持つ表現力豊かなSQLダイアレクトでデータベースとインターフェースを取ることを好みます。分析データベースは、システムや個々のクエリの性能をインストロスペクトするための高度なツールを提供すべきです。

5. **業界標準の堅牢性および多目的な展開**。コモディティハードウェアは信頼性が低いため、データベースはノード障害に対する堅牢性を確保するためにデータのレプリケーションを提供する必要があります。また、データベースは旧式のラップトップから強力なサーバーまで、あらゆるハードウェア上で動作するべきです。最後に、JVMベースのプログラムのガーベジコレクションのオーバーヘッドを回避し、ベアメタルパフォーマンス（例: SIMD）を可能にするために、データベースはターゲットプラットフォーム用のネイティブバイナリとして展開されるのが理想的です。

<span id="page-1-0"></span><img src={image_01}/>

図1: ClickHouseのタイムライン

## 2 アーキテクチャ {#2-architecture}

<span id="page-2-0"></span><img src={image_02}/>

図2: ClickHouseデータベースエンジンの高レベルのアーキテクチャ。

[図2](#page-2-0)に示されたように、ClickHouseエンジンは主に3つの層に分かれています：クエリ処理層（[第4節](#page-6-0)で説明）、ストレージ層（[第3節](#page-1-1)）、および統合層（[第5節](#page-9-0)）。これに加えて、アクセス層はユーザーセッションの管理と、異なるプロトコルを介してアプリケーションとの通信を管理します。スレッド処理、キャッシュ、ロールベースのアクセス制御、バックアップ、継続的監視のための直交コンポーネントがあります。ClickHouseは、依存関係のないC++で単一の静的リンクバイナリとして構築されています。

クエリ処理は、受信クエリの解析、論理および物理クエリ計画の構築と最適化、実行という従来の枠組みに従います。ClickHouseでは、MonetDB/X100 [\[11\]](#page-12-0)に類似したベクトル化実行モデルを使用し、機会的なコードコンパイル [\[53\]](#page-13-0)と組み合わせています。クエリは、機能豊富なSQLダイアレクト、PRQL [\[76\]](#page-13-1)、またはKustoのKQL [\[50\]](#page-13-2)で記述できます。

ストレージ層は、テーブルデータのフォーマットと配置をカプセル化する異なるテーブルエンジンで構成されています。テーブルエンジンは3つのカテゴリに分かれます：最初のカテゴリは、ClickHouseにおける主要な永続性フォーマットを表すMergeTree*ファミリーのテーブルエンジンです。LSMツリー [\[60\]](#page-13-3)のアイデアに基づき、テーブルは水平にソートされたパーツに分割され、バックグラウンドプロセスによって継続的にマージされます。個々のMergeTree*テーブルエンジンは、行を入力パーツからどのようにマージするかの方法で異なります。たとえば、古くなった場合、行を集約または置き換えることができます。

第二のカテゴリは、クエリの実行を加速または分散させるために使用される特別目的のテーブルエンジンです。このカテゴリは、辞書と呼ばれるインメモリのキー・バリューテーブルエンジンを含みます。[辞書](https://clickhou.se/dictionaries)は、内部または外部データソースに対して定期的に実行されるクエリの結果をキャッシュします。これにより、データが古くなる程度を許容できるシナリオでアクセスの待機時間が大幅に短縮されます。特別目的のテーブルエンジンの他の例には、仮想テーブルのために使用される純粋なインメモリエンジンと、透明なデータシャーディングのための分散テーブルエンジンが含まれます（以下を参照）。

第三のカテゴリのテーブルエンジンは、リレーショナルデータベース（例: PostgreSQL、MySQL）、パブリッシュ/サブスクライブシステム（例: Kafka、RabbitMQ [\[24\]](#page-12-1)）、またはキー・バリューストア（例: Redis）との双方向データ交換用の仮想テーブルエンジンです。仮想エンジンは、データレイク（例: Iceberg、DeltaLake、Hudi [\[36\]](#page-12-2)）やオブジェクトストレージ内のファイル（例: AWS S3、Google GCP）とも連携できます。

ClickHouseは、スケーラビリティと可用性のために、複数のクラスタノードにわたるテーブルのシャーディングとレプリケーションをサポートします。シャーディングは、シャーディング式に従ってテーブルを一連のテーブルシャードにパーティション化します。個々のシャードは、互いに独立したテーブルであり、通常は異なるノードに配置されます。クライアントは、シャードを直接読み書きすることができ、すなわちそれらを別のテーブルとして扱うことができるか、すべてのテーブルシャードのグローバルビューを提供するDistributed特別テーブルエンジンを使用できます。シャーディングの主な目的は、個々のノードの容量を超えるデータセットを処理することです（通常は数十テラバイトのデータ）。シャーディングのもう一つの用途は、テーブルの読み書き負荷を複数のノードに分散させること、すなわち負荷の均等化です。それとは別に、シャードは、ノードの障害に対する耐障害性のために、複数のノードにレプリケートできます。そのために、各Merge-Tree*テーブルエンジンには、Raftコンセンサス [\[59\]](#page-13-4)に基づくマルチマスター調整スキームを使用する対応するReplicatedMergeTree*エンジンがあります（[Keeper](https://clickhou.se/keeper)によって実装されており、Apache Zookeeperのドロップイン置換品でC++で書かれています））。これにより、すべてのシャードは、常に構成可能な数のレプリカを持つことが保証されます。レプリケーションメカニズムの詳細は[第3.6節](#page-5-0)で説明します。例として、[図2](#page-2-0)は、2つのシャードを持つテーブルを示しており、それぞれは2つのノードにレプリケートされています。

最後に、ClickHouseデータベースエンジンは、オンプレミス、クラウド、スタンドアロン、またはプロセス内モードで動作させることができます。オンプレミスモードでは、ユーザーはClickHouseをローカルで単一サーバーまたは複数ノードクラスタとしてシャーディングおよび/またはレプリケーションを用いて設定します。クライアントは、ネイティブ、MySQL、PostgreSQLのバイナリワイヤプロトコル、またはHTTP REST APIを介してデータベースと通信します。クラウドモードは、完全に管理された自動スケーリングDBaaS提供であるClickHouse Cloudによって表されます。この論文ではオンプレミスモードに焦点を当てていますが、ClickHouse Cloudのアーキテクチャについては今後の出版物で説明する予定です。[スタンドアロンモード](https://clickhou.se/local-fastest-tool)は、ClickHouseをファイルを分析・変換するためのコマンドラインユーティリティに変え、catやgrepといったUnixツールのSQLベースの代替品にします。これは事前の設定が不要ですが、スタンドアロンモードは単一サーバーに制限されています。最近、Jupyter notebooks [\[37\]](#page-12-4)のようなインタラクティブなデータ分析ユースケースのためにchDB [\[15\]](#page-12-3)というプロセス内モードが開発されました。DuckDB [\[67\]](#page-13-6)に触発されて、[chDB](https://clickhou.se/chdb-rocket-engine)はClickHouseをホストプロセスに組み込んだ高性能OLAPエンジンです。他のモードと比較して、これにより、データベースエンジンとアプリケーション間でのソースおよび結果データの効率的なパスが可能となり、同じアドレス空間内で動作します。

## <span id="page-1-1"></span>3 ストレージ層 {#3-storage-layer}

このセクションでは、ClickHouseのネイティブストレージ形式としてのMergeTree*テーブルエンジンについて説明します。ディスク上の表現を説明し、ClickHouseにおける3つのデータプルーニング技術について論じます。その後、同時挿入に影響を与えないようにデータを継続的に変換するマージ戦略を提示します。最後に、更新および削除の実装、データの重複排除、データのレプリケーション、およびACID適合性について説明します。

### <span id="page-2-2"></span>3.1 ディスク上のフォーマット {#3-1-on-disk-format}

MergeTree*テーブルエンジン内の各テーブルは、変更不可能なテーブルパーツのコレクションとして整理されています。部分は、行のセットがテーブルに挿入されるたびに作成されます。パーツは、その内容を追加の中央カタログへのルックアップなしに解釈するために必要なすべてのメタデータを含む自己完結型のものであります。テーブルごとのパーツの数を低く保つために、バックグラウンドのマージジョブは、構成可能なパートサイズ（デフォルトは150 GB）に達するまで、定期的に小さな複数のパーツを結合します。パーツはテーブルの主キー列でソートされているため（[第3.2節](#page-3-0)を参照）、効率的なk-wayマージソート [\[40\]](#page-12-5)がマージに使用されます。ソースパーツは非アクティブとしてマークされ、最終的にその参照カウントがゼロに落ちると削除されます。つまり、さらにクエリがそれらから読み取られない場合です。

行は2つのモードで挿入できます：同期挿入モードでは、各INSERT文が新しいパートを作成し、それをテーブルに追加します。マージのオーバーヘッドを最小限に抑えるため、データベースクライアントには、例えば20,000行を一度にバルクで挿入することが奨励されています。ただし、クライアント側でのバッチ処理によって生じる遅延は、データがリアルタイムで分析される必要がある場合にはしばしば許容できません。たとえば、可観測ユースケースは、通常数千の監視エージェントが小さなイベントおよびメトリクスデータを継続的に送信することを含みます。このようなシナリオでは、ClickHouseは複数の受信INSERTからの行を同じテーブルにバッファリングし、バッファサイズが構成可能な閾値を超えるか、タイムアウトが切れた後にのみ新しいパーツを作成する非同期挿入モードを活用できます。

<span id="page-2-1"></span><img src={image_03}/>

図3: MergeTree*-エンジンテーブルの挿入とマージ。

[図3](#page-2-1)は、MergeTree*-エンジンテーブルへの4つの同期挿入と2つの非同期挿入を示しています。2つのマージにより、アクティブなパーツの数が最初は5から2に削減されました。

LSMツリー [\[58\]](#page-13-7) とそれが異なるデータベースで実装されているもの [\[13,](#page-12-6) [26,](#page-12-7) [56\]](#page-13-8)と比較して、ClickHouseはすべてのパーツを平等に扱い、階層に配置するのではなく、同じレベルのパーツのみに制限されません。これにより、マージはもはや同じレベル内に制限されません。また、これはパーツの暗黙の時間的順序も放棄しているため、トンボストンに基づかない更新や削除のための代替メカニズムが必要となります（[第3.4節](#page-4-0)を参照）。ClickHouseは、挿入を直接ディスクに書き込みますが、他のLSMツリーに基づいたストアは通常、書き込み前ログを使用します（[第3.7節](#page-5-1)を参照）。

パートはディスク上のディレクトリに対応し、各カラムごとに1つのファイルを含みます。最適化として、小さなパート（デフォルトでは10MB未満）のカラムは、読み込みと書き込みの空間的局所性を増やすために単一のファイルに連続して保存されます。パートの行はさらに8192レコードのグループに論理的に分割され、これをグラニュールと呼びます。グラニュールは、ClickHouseのスキャンおよびインデックスルックアップオペレータによって処理される最小の不可分なデータ単位を表します。ただし、ディスク上のデータの読み書きは、グラニュールレベルではなくブロックの粒度レベルで行われ、これはカラム内の隣接グラニュールを組み合わせたものです。新しいブロックは、デフォルトで1MBの構成可能なバイトサイズごとに形成されます。すなわち、ブロック内のグラニュールの数は可変であり、カラムのデータ型および分布に依存します。ブロックはさらに圧縮されて、そのサイズとI/Oコストを削減します。デフォルトで、ClickHouseは一般的な圧縮アルゴリズムとしてLZ4 [\[75\]](#page-13-9)を採用していますが、ユーザーは浮動小数点データのためにGorilla [\[63\]](#page-13-10)やFPC [\[12\]](#page-12-8)のような特殊なコーデックを指定することもできます。圧縮アルゴリズムはチェーン可能でもあります。たとえば、数字の値で論理的重複を最初にデルタコーディング [\[23\]](#page-12-9)を使用して削減し、その後、重い圧縮を行い、最終的にAESコーデックを使用してデータを暗号化することが可能です。ブロックは、ディスクからメモリに読み込まれる際に即座に非圧縮されます。圧縮にもかかわらず、個々のグラニュールへの高速ランダムアクセスを可能にするために、ClickHouseはまた、各カラムのグラニュールIDとそのカラムファイル内の圧縮ブロックのオフセット、および非圧縮ブロック内のグラニュールのオフセットを関連付けるマッピングを保管します。

カラムはさらに、辞書コーディング [\[2,](#page-12-10) [77,](#page-13-11) [81\]](#page-13-12)されるか、特別なラッパーデータ型を使用してNullableにすることができます。LowCardinality(T)は元のカラム値を整数IDに置き換え、その結果、ユニークな値の数が少ないデータに対するストレージオーバーヘッドを大幅に削減します。Nullable(T)はカラムTに、カラム値がNULLかどうかを表す内部ビットマップを追加します。

最後に、テーブルは任意のパーティショニング式を用いて範囲、ハッシュ、またはラウンドロビン形式でパーティション化できます。パーティションプルーニングを可能にするために、ClickHouseは各パーティションのパーティショニング式の最小値と最大値をさらに保存します。ユーザーは、高度なカラム統計（例: HyperLogLog [\[30\]](#page-12-11)やt-digest [\[28\]](#page-12-12)の統計）を任意に作成し、基数推定を提供できます。

### <span id="page-3-0"></span>3.2 データプルーニング {#3-2-data-pruning}

ほとんどのユースケースにおいて、ペタバイトのデータをスキャンして単一のクエリに答えることは非常に遅く、コストがかかります。ClickHouseは、検索中に大多数の行をスキップすることを可能にし、したがってクエリを大幅に加速する3つのデータプルーニング技術をサポートします。

最初に、ユーザーはテーブルに**主キーインデックス**を定義できます。主キー列は、各パーツ内の行のソート順を決定します。つまり、インデックスはローカルにクラスター化されています。ClickHouseは、各グラニュールの最初の行の主キー列値からグラニュールのIDへのマッピングを、すべてのパートについて追加的に保存します。つまり、インデックスはスパースです [\[31\]](#page-12-13)。生成されたデータ構造は通常、完全にメモリ内に保持できるほど小さく、例えば、810万行をインデックスするのに1000エントリのみが必要です。主キーの主な目的は、通常フィルタリングされる列に対して、逐次走査の代わりに二分探索を使用して等号および範囲述語を評価することです（[第4.4節](#page-7-0)を参照）。ローカルソートは、パートのマージやクエリ最適化にも利用できます。たとえば、ソートベースの集約を行ったり、主キー列がソーティング列のプレフィックスを形成する場合、物理的実行計画からソートオペレータを削除することができます。

[図4](#page-3-1)は、ページインプレッション統計を持つテーブルのカラムEventTimeの主キーインデックスを示しています。クエリ内の範囲述語に一致するグラニュールは、EventTimeを逐次走査するのではなく、主キーインデックスを二分探索することによって見つけられます。

<span id="page-3-1"></span><img src={image_04}/>

図4: 主キーインデックスを使用したフィルタの評価。

次に、ユーザーは**テーブルプロジェクション**を作成できます。すなわち、異なる主キーでソートされた同じ行を含むテーブルの別バージョンです [\[71\]](#page-13-13)。プロジェクションは、主テーブルの主キーとは異なるカラムをフィルタリングするクエリを加速することを可能にしますが、挿入、マージ、およびスペース消費のオーバーヘッドが増加するコストがあります。デフォルトでは、プロジェクションは、主テーブルに新しく挿入されたパーツからのみ遅延的に設定され、既存のパーツからは全体をマテリアライズしない限り設定されません。クエリオプティマイザは、主テーブルまたはプロジェクションからの読み取りの間で、推定I/Oコストに基づいて選択します。特定のパートにプロジェクションが存在しない場合、クエリの実行は対応する主テーブルパートにフォールバックします。

第三に、**スキッピングインデックス**はプロジェクションに対する軽量な代替手段を提供します。スキッピングインデックスのアイデアは、無関係な行をスキャンせずに済むように、複数の連続するグラニュールのレベルで少量のメタデータを保存することです。スキッピングインデックスは、任意のインデックス式に対して、構成可能な粒度で作成できます。すなわち、スキッピングインデックスブロック内のグラニュールの数です。利用可能なスキッピングインデックスタイプには次のものが含まれます：
1. 最小-最大インデックス [\[51\]](#page-13-14)は、各インデックスブロックのインデックス式の最小値と最大値を保存します。このインデックスタイプは、小さな絶対範囲を持つローカルにクラスター化されたデータに適しており、例えば緩やかにソートされたデータとなります。
2. セットインデックスは、構成可能な数のユニークなインデックスブロック値を保存します。これらのインデックスは、小さなローカル基数を持つデータに最適です。すなわち、「まとめられた」値です。
3. ブルームフィルターインデックス [\[9\]](#page-12-14)は、構成可能な誤検知率を持つ行、トークン、またはn-グラム値に対して構築されます。これらのインデックスはテキスト検索 [\[73\]](#page-13-15)をサポートしますが、最小-最大およびセットインデックスとは異なり、範囲や否定的述語には使用できません。

### <span id="page-4-3"></span>3.3 マージ時データ変換 {#3-3-merge-time-data-transformation}

ビジネスインテリジェンスと可観測性のユースケースでは、通常高レートまたはバーストで生成されるデータを処理する必要があります。また、最近生成されたデータは、歴史的データよりも意味のあるリアルタイムの洞察にとって通常より関連性が高いです。このようなユースケースでは、データベースが高データ取り込みレートを維持しながら、集計やデータの老朽化のような技術を通じて歴史的データのボリュームを継続的に削減させる必要があります。ClickHouseは、異なるマージ戦略を使用して既存のデータの継続的な漸進的変換を許可します。マージ時のデータ変換はINSERT文のパフォーマンスを損なうことはありませんが、テーブルが常に不要な（例: 古くなったまたは非集約）値を含まないことを保証することはできません。必要に応じて、すべてのマージ時変換は、SELECT文内でキーワードFINALを指定することによってクエリ時に適用されることがあります。

**置き換えマージ**は、含まれている部分の作成タイムスタンプに基づき、タプルの最も最近挿入されたバージョンのみを保持します。古いバージョンは削除されます。タプルは、同じ主キー列値を持つ場合に等価であると見なされます。どのタプルを保持するかを明示的に制御するために、比較用に特別なバージョンカラムを指定することも可能です。置き換えマージは、通常、マージ時の更新メカニズムとして（更新が頻繁なユースケースで通常）、または挿入時のデータ重複排除の代替として一般的に使用されます（[第3.5節](#page-5-2)を参照）。

**集約マージ**は、同じ主キー列値を持つ行を集約された行に圧縮します。非主キー列は、要約値を保持する部分集約状態である必要があります。たとえば、平均（avg()）のための合計とカウントという2つの部分集約状態が新しい部分集約状態に結合されます。集約マージは、通常のテーブルではなく、マテリアライズドビューで使用されます。マテリアライズドビューは、ソーステーブルに対する変換クエリに基づいてpopulateされます。他のデータベースとは異なり、ClickHouseはソーステーブルの内容全体でマテリアライズドビューを定期的に更新しません。むしろ、マテリアライズドビューは、新しいパーツがソーステーブルに挿入されるときに変換クエリの結果とともに漸進的に更新されます。

[図5](#page-4-1)は、ページインプレッション統計を持つテーブルに対して定義されたマテリアライズドビューを示しています。ソーステーブルに挿入された新しいパーツに対して、変換クエリは、地域別に最大および平均の待機時間を計算し、結果をマテリアライズドビューに挿入します。集約関数avg()およびmax()は、実際の結果ではなく部分集約状態を返します。マテリアライズドビューについて定義された集約マージは、異なる部分にある部分集約状態を継続的に統合します。最終的な結果を得るために、ユーザーはマテリアライズドビューの部分集約状態をavg()およびmax()を使用して統合します。

<span id="page-4-1"></span><img src={image_05}/>

図5: マテリアライズドビューにおける集約マージ。

**TTL（有効期限）マージ**は、歴史的データの老朽化を提供します。削除マージや集約マージとは異なり、TTLマージは1つのパートのみを処理します。TTLマージは、トリガーとアクションを持つルールの観点で定義されます。トリガーは、TTLマージが実行される時刻と比較される各行のタイムスタンプを計算する式です。これにより、ユーザーは行の粒度でアクションを制御できるようになりますが、すべての行が与えられた条件を満たしているかどうかを確認し、全体のパートにアクションを実行することが十分であることがわかりました。考えられるアクションには、1.パートを別のボリューム（例：より安価で遅いストレージ）に移動する、2.パートを再圧縮する（例：より重いコーデックで）、3.パートを削除する、4.ロールアップするすなわち、グルーピングキーと集約関数を使用して行を集約する、などがあります。

例として、[リスト1](#page-4-2)のロギングテーブル定義を考えてみます。ClickHouseは、タイムスタンプのカラム値が1週間を超える部分を遅いが低コストのS3オブジェクトストレージに移動します。
<span id="page-4-2"></span>
```
1 CREATE TABLE tab ( ts DateTime , msg String )
2 ENGINE MergeTree PRIMARY KEY ts
3 TTL ( ts + INTERVAL 1 WEEK ) TO VOLUME 's3 '
```
リスト1: 1週間後にパートをオブジェクトストレージに移動。

### <span id="page-4-0"></span>3.4 更新および削除 {#3-4-updates-and-deletes}

MergeTree*テーブルエンジンの設計は付加オンリーのワークロードを好むものの、いくつかのユースケースでは、規制準拠のために既存のデータを時折変更する必要があります。データを更新または削除するための2つのアプローチが存在し、どちらも並列挿入をブロックすることはありません。

**ミューテーション**は、テーブルのすべてのパーツをインプレースで再書き込みます。テーブル（削除）またはカラム（更新）が一時的にサイズを二重にするのを防ぐために、この操作は非原子的です。つまり、並列SELECT文が変更された部分と変更されていない部分を読み取る可能性があります。ミューテーションは、操作の終了時にデータが物理的に変更されることを保証します。削除ミューテーションは、すべてのカラムをすべてのパーツに再書き込むため、依然として高コストです。

代わりに、**軽量削除**は、行が削除されているかどうかを示す内部ビットマップカラムを更新するだけです。ClickHouseは、削除された行を結果から除外するためにビットマップカラムに追加フィルタを持つSELECTクエリを修正します。削除された行は、将来の不明な時点での定期的なマージによってのみ物理的に削除されます。カラムの数に依存して、軽量削除はミューテーションよりもはるかに高速であり、SELECTには遅延が発生します。

同じテーブルに対して行われる更新および削除操作は、論理的衝突を避けるためにまれで直列化されることが期待されます。

### <span id="page-5-2"></span>3.5 冪等挿入 {#3-5-idempotent-inserts}

実務で頻繁に発生する問題は、クライアントがテーブルへの挿入のためにデータをサーバに送信した後に接続タイムアウトを扱う方法です。この状況では、クライアントがデータが正常に挿入されたかどうかを区別するのは困難です。この問題は、クライアントからサーバにデータを再送信し、主キーまたは一意の制約を利用して重複挿入を拒否することによって伝統的に解決されます。データベースは、二分木 [\[39,](#page-12-15) [68\]](#page-13-16)、ラジクステリー [\[45\]](#page-13-17)、またはハッシュテーブル [\[29\]](#page-12-16)に基づくインデックス構造を使用して、必要なポイントルックアップを迅速に実行します。これらのデータ構造はすべてのタプルをインデックスするため、大規模データセットや高速インジェスト率に対するスペースと更新のオーバーヘッドが厳しくなります。

ClickHouseは、各挿入が最終的にパートを作成するという事実に基づいた、より軽量な代替手段を提供します。具体的には、サーバはN個の最近挿入されたパートのハッシュ（例：N=100）を保持し、既知のハッシュを持つパートの再挿入を無視します。レプリケートされたテーブルの場合、非レプリケートの場合にはそれぞれKeeperにローカルに保存されます。その結果、挿入は冪等に行われます。すなわち、クライアントはタイムアウトの後に同じバッチの行を再送信でき、サーバが重複排除を行うことを想定できます。重複排除のプロセスをより制御するために、クライアントはオプションで挿入トークンを提供し、それがパートのハッシュとして機能します。ハッシュベースの重複排除には、新しい行のハッシュを生成するためのオーバーヘッドが伴いますが、ハッシュの保存および比較のコストはほとんど無視できます。
### <span id="page-5-0"></span>3.6 データレプリケーション {#3-6-data-replication}

レプリケーションは高可用性（ノード障害に対する耐性）の前提条件であるだけでなく、ロードバランシングやゼロダウンタイムのアップグレードにも使用されます [\[14\]](#page-12-17)。ClickHouse では、レプリケーションは、カラム名や型などのテーブルメタデータと、テーブルパーツのセットから構成されるテーブル状態の概念に基づいています（セクション [3.1](#page-2-2)）。ノードは3つの操作を使用してテーブルの状態を進めます：1. 挿入は新しいパーツを状態に追加し、2. マージは新しいパーツを状態に追加し、既存のパーツを削除します、3. 変異とDDL文はパーツを追加したり削除したり、またはテーブルメタデータを変更します。これらの操作はローカルで1つのノード上で実行され、グローバルなレプリケーションログに状態遷移のシーケンスとして記録されます。

レプリケーションログは、通常、3つのClickHouse Keeperプロセスのアンサンブルによって維持され、このプロセスはRaft合意アルゴリズムを使用して [\[59\]](#page-13-4) ClickHouseノードのクラスターに対する分散かつフォールトトレラントなコーディネーション層を提供します。すべてのクラスターのノードは、最初にレプリケーションログの同じ位置を指します。ノードがローカルの挿入、マージ、変異、DDL文を実行している間、レプリケーションログは他のすべてのノードで非同期に再生されます。その結果、レプリケートされたテーブルは最終的に一貫性があるだけであり、すなわち、ノードは最新の状態に収束する間、一時的に古いテーブル状態を読み取ることができます。前述のほとんどの操作は、過半数のノード（例：過半数のノードまたはすべてのノード）が新しい状態を受け入れたまで、代わりに同期的に実行できます。

たとえば、[図6](#page-5-3)は、3つのClickHouseノードのクラスターで最初は空のレプリケートされたテーブルを示しています。ノード1は最初に2つのINSERT文を受け取り、それらをレプリケーションログ（1、2）に記録します。次に、ノード2は最初のログエントリを再生し（3）、ノード1から新しいパーツをダウンロードします（4）、一方ノード3は両方のログエントリ（3、4、5、6）を再生します。最後に、ノード3は両方のパーツを新しいパーツにマージし、入力パーツを削除し、レプリケーションログにマージエントリを記録します（7）。

<span id="page-5-3"></span><img src={image_06}/>

図6: 3ノードのクラスター内でのレプリケーション。

同期化を迅速化するための3つの最適化が存在します。まず、クラスターに追加された新しいノードは、レプリケーションログを最初から再生するのではなく、最後のレプリケーションログエントリを書き込んだノードの状態を単純にコピーします。第二に、マージはローカルで再生するか、別のノードから結果パーツを取得することによって再生されます。正確な動作は設定可能で、CPU消費とネットワークI/Oのバランスを取ることができます。たとえば、データセンター間のレプリケーションは、運用コストを最小化するためにローカルマージを優先することが一般的です。第三に、ノードは相互に独立したレプリケーションログエントリを並行して再生します。これには、例えば、同じテーブルに連続して挿入された新しいパーツの取得や、異なるテーブルに対する操作が含まれます。

### <span id="page-5-1"></span>3.7 ACIDコンプライアンス {#3-7-acid-compliance}

同時読取りと書込み操作のパフォーマンスを最大化するために、ClickHouseは可能な限りロックを避けます。クエリは、クエリの開始時に作成された、すべての関与するテーブルのすべてのパーツのスナップショットに対して実行されます。これにより、並行INSERTやマージによって挿入された新しいパーツ（セクション [3.1](#page-2-2)）が実行に参加しないことが保証されます。同時にパーツが変更または削除されないように（セクション [3.4](#page-4-0)）、処理されるパーツの参照カウントがクエリの間、増加します。形式的には、これはバージョン化されたパーツに基づくMVCC変種によって実現されたスナップショット分離に該当します [\[6\]](#page-12-18)。その結果、ステートメントは、スナップショットが取得された時に同時に書き込まれる場合にのみ、単一のパーツに影響を与える稀なケースを除いて、一般的にはACID準拠ではありません。

実際には、ClickHouseの書き込みが多い意思決定のユースケースのほとんどは、停電の場合に新しいデータが失われるリスクの小さなものを許容します。このデータベースは、デフォルトで新しく挿入されたパーツのコミット（fsync）をディスクに強制せず、カーネルが原子性を放棄するコストで書き込みをバッチ処理できるように活用します。
## <span id="page-6-0"></span>4 クエリ処理レイヤー {#4-query-processing-layer}

<span id="page-6-1"></span><img src={image_07}/>

図7: SIMDユニット、コア、ノード間の並列化。

[図7](#page-6-1)に示すように、ClickHouseはデータ要素、データチャンク、およびテーブルシャードのレベルでクエリを並列化します。複数のデータ要素は、SIMD命令を使用してオペレーター内で一度に処理されることができます。単一のノードで、クエリエンジンは、複数のスレッドで同時にオペレーターを実行します。ClickHouseは、MonetDB/X100 [\[11\]](#page-12-0) と同じベクタリゼーションモデルを使用しており、つまり、オペレーターは単一の行の代わりに複数の行（データチャンク）を生成、渡し、消費します。これにより、仮想関数呼び出しのオーバーヘッドを最小限に抑えます。ソーステーブルが無関係なテーブルシャードに分割されると、複数のノードがシャードを同時にスキャンできます。その結果、すべてのハードウェアリソースが完全に活用され、クエリ処理はノードを追加することで水平にスケールさせることができ、コアを追加することで垂直にスケールさせることができます。

このセクションの残りでは、データ要素、データチャンク、およびシャードの粒度での並列処理について、より詳細に説明します。その後、クエリパフォーマンスを最大化するための重要な最適化をいくつか紹介します。最後に、ClickHouseが同時クエリの存在下で共有システムリソースを管理する方法について説明します。

### 4.1 SIMD並列化 {#4-1-simd-parallelization}

オペレーター間で複数の行を渡すことにより、ベクタリゼーションの機会が生まれます。ベクタリゼーションは、手動で作成されたイントリンシック [\[64,](#page-13-18) [80\]](#page-13-19) またはコンパイラによる自動ベクタリゼーション [\[25\]](#page-12-19) に基づいています。ベクタリゼーションの恩恵を受けるコードは、異なる計算カーネルにコンパイルされます。たとえば、クエリオペレーターの内部ホットループは、非ベクタイズカーネル、自動ベクタイズAVX2カーネル、および手動ベクタイズAVX-512カーネルとして実装できます。最も速いカーネルは、cpuid命令に基づいて[実行時に選択されます](https://clickhou.se/cpu-dispatch)。このアプローチにより、ClickHouseは15年前のシステム（最低でもSSE4.2を必要とする）で動作しながら、最近のハードウェアでの顕著な高速化を提供できます。

### 4.2 マルチコア並列化 {#4-2-multi-core-parallelization}

<span id="page-7-1"></span><img src={image_08}/>

図8: 3レーンの物理オペレーター計画。

ClickHouseは、SQLクエリを物理計画オペレーターの有向グラフに変換する従来のアプローチ [\[31\]](#page-12-13) に従います。オペレーター計画の入力は、ネイティブまたはサポートされているサードパーティフォーマットのいずれかでデータを読み取る特別なソースオペレーターによって表現されます（セクション [5](#page-9-0) を参照）。同様に、特別なシンクオペレーターは、結果を所望の出力フォーマットに変換します。物理オペレーター計画は、クエリコンパイル時に、設定可能な最大ワーカースレッド数（デフォルトではコア数）およびソーステーブルサイズに基づいて独立した実行レーンに展開されます。レーンは、並列オペレーターによって処理されるデータを非重複の範囲に分解します。並列処理の機会を最大化するために、レーンは可能な限り遅くマージされます。

たとえば、[図8](#page-7-1)のノード1のボックスは、ページインプレッション統計のテーブルに対する典型的なOLAPクエリのオペレーターグラフを示しています。最初のステージでは、ソーステーブルの3つの無関係な範囲が同時にフィルタリングされます。再分配交換オペレーターは、第一ステージと第二ステージの間で結果チャンクを動的にルーティングし、処理スレッドが均等に利用されるようにします。スキャンされた範囲が著しく異なる選択性を持っている場合、第一ステージの後にレーンが不均衡になることがあります。第二ステージでは、フィルタを通過した行がRegionIDによってグループ化されます。集計オペレーターは、グループ化カラムとしてRegionIDを持つローカル結果グループを維持し、avg()のための部分集計状態としてグループごとの合計とカウントを持っています。ローカルな集計結果は、最終的にGroupStateMergeオペレーターによってグローバルな集計結果にマージされます。このオペレーターはパイプラインブレイカーでもあり、集計結果が完全に計算された後にのみ第三ステージを開始できます。第三ステージでは、最初に結果グループを再分配交換オペレーターによって3つの同じ大きさの無関係なパーティションに分け、その後AvgLatencyでソートします。ソートは3つのステップで行われます：最初に、ChunkSortオペレーターが各パーティションの個々のチャンクをソートします。次に、StreamSortオペレーターがローカルにソートされた結果を維持し、実行中のソートチャンクと2ウェイマージソートを使用して結合します。最後に、MergeSortオペレーターがk-wayソートを使用してローカルな結果を組み合わせ、最終的な結果を得ます。

オペレーターは状態機械であり、入力ポートと出力ポートを介して互いに接続されています。オペレーターの3つの可能な状態は、need-chunk、ready、およびdoneです。need-chunkからreadyに移動するためには、チャンクがオペレーターの入力ポートに配置されます。readyからdoneに移動するには、オペレーターが入力チャンクを処理し、出力チャンクを生成します。doneからneed-chunkへの移動には、出力チャンクがオペレーターの出力ポートから削除されます。接続された2つのオペレーター間の最初と第三の状態遷移は、結合ステップでのみ実行できます。ソースオペレーター（シンクオペレーター）は、readyとdoneの状態のみを持っています（need-chunkとdone）。

ワーカースレッドは物理オペレーター計画を継続的にトラバースし、状態遷移を実行します。CPUキャッシュを熱く保つために、計画には同じスレッドが同じレーン内の連続オペレーターを処理するべきであるというヒントが含まれています。並列処理は、ステージ内の無関係な入力間を横に（例：[図8](#page-7-1)でAggregateオペレーターが同時に実行される）行うだけでなく、パイプラインブレイカーによって分離されていないステージ間を縦に行われます（例：[図8](#page-7-1)の同じレーン内のFilterおよびAggregateオペレーターは同時に実行できます）。新しいクエリが開始されたり、同時クエリが終了したりした際に過剰または過小なサブスクリプションを避けるために、並列度はクエリ開始時に指定されたワーカースレッドの数の間で変更可能です（セクション [4.5](#page-9-1) を参照）。

オペレーターはまた、2つの方法で実行時にクエリの実行に影響を与えることができます。第一に、オペレーターは動的に新しいオペレーターを作成し接続できます。これは、メモリ使用量が設定可能なしきい値を超えた場合にクエリをキャンセルする代わりに、外部の集約、ソート、または結合アルゴリズムに切り替えるために主に使用されます。第二に、オペレーターはワーカースレッドに非同期キューに移動するよう要求できます。これにより、リモートデータを待つ間にワーカースレッドをより効果的に使用できます。

ClickHouseのクエリ実行エンジンとモーサル駆動並列性 [\[44\]](#page-12-20) は、レーンが通常異なるコア / NUMAソケットで実行され、ワーカースレッドが他のレーンからタスクを盗むことができるという点で類似しています。また、中央スケジューリングコンポーネントはありません; 代わりに、ワーカースレッドは物理オペレーター計画を継続的にトラバースすることによって個々にタスクを選択します。モーサル駆動の並列性とは異なり、ClickHouseは最大の並列度を計画に組み込み、デフォルトのモーサルサイズである約100,000行と比較して、ソーステーブルを分割するためにはるかに大きな範囲を使用します。これにより、一部のケースで停止が発生する可能性があります（例：異なるレーンでのフィルタオペレーターの実行時間が大きく異なる場合）が、再分配のような交換オペレーターを自由に使用することで、少なくともそのような不均衡がステージ間で蓄積されないようにします。

### 4.3 マルチノード並列化 {#4-3-multi-node-parallelization}

クエリのソーステーブルがシャード化されている場合、クエリを受け取ったノード（イニシエーターノード）のクエリオプティマイザは、他のノードで可能な限り多くの作業を試みます。他のノードからの結果は、クエリプランの異なるポイントに統合できます。クエリによっては、リモートノードは次のいずれかを行うことがあります。1. ソーステーブルの生のカラムをイニシエーターノードにストリーミングする、2. ソースカラムをフィルタリングし、生き残った行を送信する、3. フィルタリングと集計ステップを実行し、部分集計状態を持つローカル結果グループを送信する、または4. フィルタ、集計、およびソートを含む完全なクエリを実行します。

[図8](#page-7-1)のノード2 ... Nは、ヒットテーブルのシャードを保持する他のノードで実行されるプランフラグメントを示しています。これらのノードはローカルデータをフィルタリングおよびグループ化し、結果をイニシエーターノードに送信します。ノード1のGroupStateMergeオペレーターは、ローカルおよびリモートの結果をマージした後、結果グループを最終的にソートします。

### <span id="page-7-0"></span>4.4 全体的なパフォーマンス最適化 {#4-4-holistic-performance-optimization}

このセクションでは、クエリ実行のさまざまなステージに適用される主要なパフォーマンス最適化をいくつか紹介します。

**クエリ最適化**。最初の最適化セットは、クエリのASTから得られる意味論的クエリ表現の上に適用されます。そのような最適化の例には、定数折りたたみ（例：concat(lower('a'), upper('b'))は'aB'になります）、特定の集約関数からスカラーを抽出すること（例：sum(a*2)は2 * sum(a)になります）、共通部分式の排除、および平等フィルタの論理和をINリストに変換すること（例：x=c OR x=dはx IN (c,d)になります）があります。最適化された意味論的クエリ表現は、その後論理オペレーター計画に変換されます。論理計画の最適化にはフィルタプッシュダウン、関数評価とソートステップの順序変更が含まれ、どちらが高コストであると推定されるかに依存します。最後に、論理クエリ計画は物理オペレーター計画に変換されます。この変換は、関与するテーブルエンジンの特性を利用できます。たとえば、MergeTree*-テーブルエンジンの場合、ORDER BYカラムが主キーの接頭辞を形成している場合、ディスク順にデータを読み取ることができ、ソートオペレーターを計画から削除できます。また、集約でのグループ化カラムが主キーの接頭辞を形成している場合、ClickHouseはソート集約 [\[33\]](#page-12-21) を使用できます。これは、事前にソートされた入力内の同じ値の集約ランを直接集約します。ハッシュ集約に比べ、ソート集約ははるかにメモリを消費せず、集約値はランが処理された後すぐに次のオペレーターに渡すことができます。

**クエリコンパイル**。ClickHouseは [LLVMに基づくクエリコンパイル](https://clickhou.se/jit) を採用しており、隣接する計画オペレーターを動的に融合します [\[38,](#page-12-22) [53\]](#page-13-0)。たとえば、式a * b + c + 1は、3つのオペレーターの代わりに単一のオペレーターに結合できます。式の他に、ClickHouseは同時に複数の集約関数を評価するため（つまりGROUP BY用）や、1つ以上のソートキーでのソートのためのコンパイルも利用しています。クエリコンパイルは、仮想呼び出しの数を減らし、データをレジスタまたはCPUキャッシュに保持し、実行する必要のあるコードを減らすため、分岐予測機能を助けます。さらに、実行時コンパイルは、論理最適化やコンパイラで実装されたぺぺホール最適化などの豊富な最適化のセットを可能にし、最速のローカルで利用可能なCPU命令にアクセスすることを可能にします。同じ通常の集約またはソート表現が異なるクエリによって設定可能な回数を超えて実行されるときのみ、コンパイルが開始されます。コンパイルされたクエリオペレーターはキャッシュされ、将来のクエリで再利用できます。

**主キーインデックス評価**。ClickHouseは、WHERE条件のフィルタ句のサブセットが主キーのカラムの接頭辞を構成する場合に主キーインデックスを使用して条件を評価します。主キーインデックスは、キー値の辞書式にソートされた範囲に対して左から右に分析されます。主キーのカラムに対応するフィルタ句は、三項論理を使用して評価されます - 範囲内の値に対してすべて真、すべて偽、または混合真/偽です。後者の場合、範囲は再帰的に分析されるサブ範囲に分割されます。フィルタ条件内の関数には追加の最適化があります。第一に、関数にはその単調性を記述する特性があり、たとえば、toDayOfMonth(date)は1ヶ月内で区分的単調です。単調性の特性により、関数がソートされた入力のキー値範囲でソートされた結果を生産するかどうかを推測できます。第二に、一部の関数は特定の関数結果の前像を計算できます。これは、キーカラムの定数の比較をキーカラムの値と前像を比較することで置き換えるために使用されます。たとえば、toYear(k) = 2024はk >= 2024-01-01 && k < 2025-01-01に置き換えることができます。

**データスキッピング**。ClickHouseは、セクション [3.2](#page-3-0) で紹介したデータ構造を使用して、クエリ実行時のデータ読み取りを回避することを試みます。さらに、異なるカラムのフィルタは、ヒューリスティックと（オプションの）カラム統計に基づいて降順の推定選択性の順序で順に評価されます。少なくとも1つの一致する行を含むデータチャンクのみが次の述語に渡されます。これにより、述語間で読み取りデータの量と実行すべき計算の数が徐々に減少します。最適化は、少なくとも1つの高選択的述語が存在する場合にのみ適用されます。そうでない場合、クエリの遅延はすべての述語を並行して評価するのと比べて悪化します。

**ハッシュテーブル**。ハッシュテーブルは、集約およびハッシュジョイン用の基本的なデータ構造です。適切なタイプのハッシュテーブルを選択することはパフォーマンスにとって重要です。ClickHouseは [さまざまなハッシュテーブル](https://clickhou.se/hashtables) （2024年3月の時点で30以上）を、ハッシュ関数、アロケータ、セルタイプ、およびリサイズポリシーが変化点の一般的なハッシュテーブルテンプレートからインスタンス化します。グループ化カラムのデータ型、推定されるハッシュテーブルの基数、およびその他の要因に基づいて、それぞれのクエリオペレーターには最も速いハッシュテーブルが選択されます。ハッシュテーブルに実装されるその他の最適化には含まれます：

- 256のサブテーブルを持つ2レベルのレイアウト（ハッシュの最初のバイトに基づく）を大規模なキーセットをサポートするために使用
- さまざまな文字列長に対する異なるハッシュ関数を持つ4つのサブテーブルを持つ文字列ハッシュテーブル [\[79\]](#page-13-20)
- キーが少ない場合、バケットインデックスとしてキーを直接使用するルックアップテーブル（つまり、ハッシュなし）
- 比較が高コストな場合に衝突解決を早めるための埋め込みハッシュを持つ値（例：文字列、AST）
- 実行時の統計から予測されたサイズに基づいてハッシュテーブルを作成して不必要なリサイズを回避
- 単一のメモリスラブ上に同じ生成/削除ライフサイクルを持つ複数の小さなハッシュテーブルを割り当て
- ハッシュマップおよびセルごとのバージョンカウンターを使用してハッシュテーブルを即座にクリアして再利用
- ハッシュキーをハッシュ化した後の値の取得を加速するためにCPUプリフェッチ（__builtin_prefetch）を使用

**結合**。ClickHouseは当初、結合をわずかにしかサポートしていなかったため、多くのユースケースは歴史的に非正規化されたテーブルに依存していました。現在、データベースは [すべてのSQLで利用可能な結合タイプ](https://clickhou.se/joins)（内部、左/右/完全外部、クロス、as-of）や、ハッシュ結合（ナイーブ、グレース）、ソートマージ結合、キー値ルックアップが高速なテーブルエンジン向けのインデックス結合などのさまざまな結合アルゴリズムを提供しています。

結合はデータベースの操作の中で最も高コストなものであるため、古典的な結合アルゴリズムの並列バージョンを提供することが重要です。理想的には、設定可能なスペース/時間のトレードオフを持たせるべきです。ハッシュ結合の場合、ClickHouseは[非ブロッキングで共有パーティションアルゴリズム](\[7\])を実装しています。たとえば、[図9](#page-8-3)のクエリは、ページヒット統計テーブル上での自己結合を介してユーザーがURL間を移動する方法を計算します。結合のビルドフェーズは、ソーステーブルの3つの無関係な範囲をカバーする3つのレーンに分解されます。一元的なハッシュテーブルの代わりに、パーティションされたハッシュテーブルが使用されます。複数のワーカースレッドは、ハッシュ関数のモジュロを計算することによってビルド側の各入力行のターゲットパーティションを決定します。ハッシュテーブルのパーティションへのアクセスは、Gather交換オペレーターを使用して同期されます。プローブフェーズは、入力タプルのターゲットパーティションを同様に見つけます。このアルゴリズムは、各タプルごとに2回の追加ハッシュ計算を導入しますが、ビルドフェーズでは、ハッシュテーブルのパーティションの数に応じてロック競合を大幅に削減します。

<span id="page-8-3"></span><img src={image_09}/>

図9: 3つのハッシュテーブルパーティションを使用した並列ハッシュ結合。

### <span id="page-9-1"></span>4.5 ワークロードアイソレーション {#4-5-workload-isolation}

ClickHouseは同時制御、メモリ使用制限、およびI/Oスケジューリングを提供しており、ユーザーがクエリをワークロードクラスに分離します。特定のワークロードクラスのための共有リソース（CPUコア、DRAM、ディスクおよびネットワークI/O）に制限を設定することで、これらのクエリが他の重要なビジネスクエリに影響しないようにします。

同時制御は、高い同時クエリ数のシナリオでスレッドの過剰サブスクリプションを防ぎます。より具体的には、クエリごとのワーカースレッド数は、利用可能なCPUコアの数に対する指定された比率に基づいて動的に調整されます。

ClickHouseは、サーバー、ユーザー、およびクエリレベルでのメモリ割り当てのバイトサイズを追跡し、それによって柔軟なメモリ使用制限を設定できます。メモリオーバーコミットにより、クエリは保証されたメモリを超えて追加の空きメモリを使用できるようになり、他のクエリのメモリ制限を確保します。さらに、集計、ソート、および結合句のメモリ使用も制限でき、メモリ制限が超過した際には外部アルゴリズムにフォールバックします。

最後に、I/Oスケジューリングにより、ユーザーは最大帯域幅、リクエストの同時処理、ポリシー（例：FIFO、SFC [\[32\]](#page-12-24)）に基づいてワークロードクラスのローカルおよびリモートディスクアクセスを制限できます。

### <span id="page-9-0"></span>5 統合レイヤー {#5-integration-layer}

リアルタイムの意思決定アプリケーションはしばしば、複数の場所にあるデータへの効率的かつ低遅延のアクセスに依存しています。OLAPデータベースで外部データを利用可能にするためには、2つのアプローチがあります。プッシュベースのデータアクセスでは、サードパーティコンポーネントがデータベースと外部データストアとの橋渡しを行います。この例の1つは、リモートデータを目的のシステムにプッシュするための専門のETLツールです。プルベースのモデルでは、データベース自体がリモートデータソースに接続し、ローカルテーブルへのクエリのためにデータを引き出すか、リモートシステムにデータをエクスポートします。プッシュベースのアプローチは、より多様性があり一般的ですが、より大きなアーキテクチャの足跡とスケーラビリティのボトルネックを伴います。それに対して、データベース内のリモート接続は、ローカルデータとの結合などの興味深い機能を提供しながら、全体的なアーキテクチャをシンプルに保ち、洞察までの時間を短縮します。

このセクションの残りでは、ClickHouseにおけるプルベースのデータ統合方法を探り、リモートロケーションのデータへのアクセスを目指します。SQLデータベースにおけるリモート接続のアイデアは新しいものではないことに注意してください。たとえば、SQL/MED標準 [\[35\]](#page-12-25) は、2001年に導入され、PostgreSQLで2011年から実装されており、外部データ管理のための統一インターフェースとして外国データラッパーを提案しています。ClickHouseの設計目標の1つは、他のデータストアやストレージフォーマットとの最大の相互運用性です。2024年3月現在、ClickHouseは、すべての分析データベースの中で最も組み込みのデータ統合オプションを提供していると思われます。

外部接続。ClickHouseは、外部システムやストレージロケーションとの接続のために [50+](https://clickhou.se/query-integrations) の統合テーブル関数およびエンジンを提供しており、ODBC、MySQL、PostgreSQL、SQLite、Kafka、Hive、MongoDB、Redis、S3/GCP/Azureオブジェクトストア、およびさまざまなデータレイクを含みます。これらは、次のボーナス図で示されるように、カテゴリにさらに分けられます（元のVLDB論文の一部ではありません）。

<span id="bonus-figure"></span><img src={image_10}/>

ボーナス図：ClickBenchの相互運用性オプション。

一時的なアクセスを持つ統合 **テーブル関数**。テーブル関数は、SELECTクエリのFROM句内で呼び出され、探索的なアドホッククエリのためにリモートデータを読み取ります。あるいは、INSERT INTO TABLE FUNCTIONステートメントを使用してリモートストアにデータを書き込むためにも使用されます。

永続的なアクセス。リモートデータストアや処理システムとの永続的な接続を作成するための3つの方法があります。

最初に、統合 **テーブルエンジン** は、MySQLテーブルなどのリモートデータソースを永続的なローカルテーブルとして表現します。ユーザーは、CREATE TABLE AS構文を使用してテーブル定義を保存し、SELECTクエリとテーブル関数を組み合わせます。リモートカラムのサブセットのみに言及する、またはスキーマ推論を使用してカラム名と同等のClickHouse型を自動的に決定するカスタムスキーマを指定できます。また受動的および能動的なランタイム動作を区別します：受動的テーブルエンジンは、クエリをリモートシステムに転送し、結果でローカルプロキシテーブルを埋めます。それに対して能動的テーブルエンジンは、リモートシステムから定期的にデータを引き出したり、リモートの変更にサブスクライブします。たとえば、PostgreSQLの論理レプリケーションプロトコルを通じて、ローカルテーブルはリモートテーブルの完全なコピーを含むことになります。

第二に、統合 **データベースエンジン** は、リモートデータストア内のテーブルスキーマのすべてのテーブルをClickHouseにマッピングします。前者とは異なり、一般にリモートデータストアがリレーショナルデータベースであることを要求し、DDL文への制限されたサポートを提供します。

第三に、**辞書**は、対応する統合テーブル関数またはエンジンに対してほぼすべての可能なデータソースに対して任意のクエリを使用してポピュレートできます。ランタイムの動作はアクティブであり、データはリモートストレージから定期的に引き込まれます。

データ形式。3rdパーティシステムと相互作用するために、現代の分析データベースは、任意の形式のデータを処理できる必要があります。ClickHouseは、そのネイティブフォーマットの他に、[90+](https://clickhou.se/query-formats)フォーマットをサポートしており、CSV、JSON、Parquet、Avro、ORC、Arrow、Protobufが含まれます。各フォーマットは、ClickHouseが読み込むことができる入力フォーマット（ClickHouseが読み取ることができる）、ClickHouseがエクスポートすることができる出力フォーマット、または両方である可能性があります。一部の分析特化型フォーマット（Parquetなど）は、クエリ処理と統合されており、すなわち、オプティマイザは埋め込まれた統計を利用でき、フィルタは圧縮されたデータ上で直接評価されます。

互換性インターフェース。そのネイティブバイナリワイヤプロトコルおよびHTTPの他に、クライアントはMySQLまたはPostgreSQLワイヤプロトコル互換インターフェースを通じてClickHouseと対話することができます。この互換性機能は、ベンダーが未だにネイティブなClickHouse接続を実装していない独自のアプリケーション（例：特定のビジネスインテリジェンスツール）からのアクセスを可能にするために便利です。

## 6 パフォーマンスを特徴として {#6-performance-as-a-feature}

このセクションでは、パフォーマンス分析のための組み込みツールを提示し、実際のワールドおよびベンチマーククエリを用いてパフォーマンスを評価します。

### 6.1 組み込みパフォーマンス分析ツール {#6-1-built-in-performance-analysis-tools}

個々のクエリやバックグラウンド操作のパフォーマンスボトルネックを調査するための幅広いツールが利用可能です。ユーザーは、システムテーブルに基づく統一インターフェースを介してすべてのツールと対話します。

**サーバーおよびクエリメトリクス**。アクティブパート数、ネットワークスループット、キャッシュヒット率などのサーバーレベルの統計は、読み込まれたブロック数やインデックス使用統計などのクエリごとの統計で補完されます。メトリクスは、要求に応じて同期的に（または非同期的に）設定可能な間隔で計算されます。

**サンプリングプロファイラ**。サーバースレッドのコールスタックをサンプリングプロファイラを使用して収集できます。結果はオプションで、フレームグラフビジュアライザなどの外部ツールにエクスポートすることができます。

**OpenTelemetry統合**。OpenTelemetryは、複数のデータ処理システム間でデータ行を追跡するためのオープンスタンダードです [\[8\]](#page-12-26)。ClickHouseは、クエリ処理ステップのすべてに対して、設定可能な粒度でOpenTelemetryログスパンを生成でき、他のシステムからOpenTelemetryログスパンを収集および分析できます。

**クエリの説明**。他のデータベースと同様に、SELECTクエリの前にEXPLAINを置くことで、クエリのAST、論理および物理オペレーター計画、実行時の動作の詳細な洞察を得ることができます。

### 6.2 ベンチマーク {#6-2-benchmarks}

ベンチマークは現実的ではないと批判されてきましたが [\[10,\]](#page-12-27) [52,](#page-13-22) [66,\] (#page-13-23) [74\]](#page-13-24) それでも、データベースの長所と短所を特定するために役立ちます。以下では、ClickHouseのパフォーマンスを評価するためにベンチマークがどのように使用されるかについて説明します。
```html
#### 6.2.1 非正規化テーブル {#6-2-1-denormalized-tables}

非正規化ファクトテーブルに対するフィルタおよび集約クエリは、歴史的にClickHouseの主要なユースケースを表しています。我々は、Clickstreamやトラフィック分析で使用されるアドホックおよび定期レポートクエリをシミュレートする典型的なワークロードであるClickBenchの実行時間を報告します。このベンチマークは、1億の匿名ページヒットを持つテーブルに対して43のクエリで構成されており、これはウェブの最大の分析プラットフォームの一つから取得されたものです。オンラインダッシュボード [\[17\]](#page-12-28) は、2024年6月時点で45以上の商用および研究データベースに対する測定値（コールド/ホット実行時間、データインポート時間、ディスク上のサイズ）を示しています。結果は、公開されているデータセットとクエリに基づき、独立した寄稿者によって提出されます [\[16\]](#page-12-29)。これらのクエリは、連続アクセスパスとインデックススキャンのテストを行い、CPU、IO、またはメモリに依存したリレーショナルオペレーターを定期的に露出させます。

[Figure 10](#page-10-0) は、分析に頻繁に利用されるデータベースにおいて、すべてのClickBenchクエリを順次実行した際の総相対コールドおよびホット実行時間を示しています。測定は、16 vCPU、32 GB RAM、5000 IOPS / 1000 MiB/sディスクを持つ単一ノードのAWS EC2 c6a.4xlargeインスタンスで行われました。Redshift（[ra3.4xlarge](https://clickhou.se/redshift-sizes), 12 vCPU、96 GB RAM）およびSnowfake（[warehouse size S](https://clickhou.se/snowflake-sizes): 2x8 vCPU、2x16 GB RAM）でも同様のシステムが使用されました。物理データベース設計は軽く調整されており、主キーを指定しますが、個々のカラムの圧縮を変更したり、プロジェクションやスキッピングインデックスを作成したりはしていません。また、各コールドクエリ実行の前にLinuxページキャッシュをフラッシュしますが、データベースやオペレーティングシステムの設定を調整することはありません。各クエリに対して、データベース間での最速の実行時間をベースラインとして使用します。他のデータベースの相対クエリ実行時間は、( + 10)/(_ + 10)として計算されます。データベースの総相対実行時間は、各クエリの比率の幾何平均です。研究データベースUmbra [\[54\]](#page-13-25)は全体のホット実行時間で最良の結果を達成しましたが、ClickHouseはホットおよびコールド実行時間において他のすべての商用データベースを上回っています。

<span id="page-10-0"></span><img src={image_11}/>

Figure 10: ClickBenchの相対コールドおよびホット実行時間。

SELECTのパフォーマンスをより多様なワークロードで時間を追って追跡するために、我々は「VersionsBench」と呼ばれる4つのベンチマークの組み合わせを[使用します](https://clickhou.se/performance-over-years) [\[19\]](#page-12-30)。このベンチマークは、新しいリリースが公開されるたびに1回実行され、そのパフォーマンスを評価し [\[20\]](#page-12-31)、パフォーマンスを劣化させる可能性のあるコード変更を特定します：個別のベンチマークには以下が含まれます：1. ClickBench（上記で説明）、2. 15のMgBench [\[21\]](#page-12-32) クエリ、3. 6億行の非正規化スタースキーマベンチマーク [\[57\]](#page-13-26)ファクトテーブルに対する13のクエリ。4. 34億行 [\[70\]](#page-13-27) の[NYC Taxi Rides](https://clickhou.se/nyc-taxi-rides-benchmark)に対する4つのクエリ。

[Figure 11](#page-10-5) は、2018年3月から2024年3月までの77のClickHouseバージョンに対するVersionsBench実行時間の推移を示しています。個別クエリの相対実行時間の違いを補償するために、我々は、すべてのバージョンにおける最小クエリ実行時間に対する比率を重みとして、幾何平均を使用して実行時間を正規化します。VersionBenchのパフォーマンスは過去6年間で1.72倍向上しました。長期サポート（LTS）付きのリリース日のマークはx軸に表示されています。一部の期間では一時的にパフォーマンスが悪化しましたが、LTSリリースは一般に前のLTSバージョンと比較して同等または優れたパフォーマンスを持っています。2022年8月の大幅な改善は、セクション[4.4.](#page-7-0)で説明されるカラムごとのフィルタ評価技術によるものでした。

<span id="page-10-5"></span><img src={image_12}/>

Figure 11: VersionsBench 2018-2024の相対ホット実行時間。
#### 6.2.2 正規化テーブル {#6-2-2-normalized-tables}

従来のデータウェアハウジングでは、データはしばしばスターやスノーフレークスキーマを使用してモデリングされます。TPC-Hクエリ（スケールファクター100）の実行時間を示しますが、正規化テーブルはClickHouseにとって新たなユースケースであることにも言及します。[Figure 12](#page-10-6) は、セクション[4.4.](#page-7-0)で説明される並列ハッシュ結合アルゴリズムに基づくTPC-Hクエリのホット実行時間を示しています。測定は、64 vCPU、128 GB RAM、5000 IOPS / 1000 MiB/sディスクを持つ単一ノードのAWS EC2 c6i.16xlargeインスタンスで行われました。5回の実行の中で最速のものが記録されました。参考のために、同じサイズのSnowfakeシステムで同じ測定を行いました（ウェアハウスサイズL、8x8 vCPU、8x16 GB RAM）。11のクエリに関する結果は表から除外されています：クエリQ2、Q4、Q13、Q17、Q20-22は、ClickHouse v24.6ではサポートされていない相関サブクエリを含んでいます。クエリQ7-Q9およびQ19は、結合に関する拡張プランレベル最適化に依存しており（ClickHouse v24.6では両方とも不足）、実行可能な実行時間を達成します。自動サブクエリデコリレーションおよび結合に対するより良いオプティマイザサポートは2024年に実装される予定です [\[18\]](#page-12-33)。残りの11のクエリのうち、5（6）クエリがClickHouseでSnowfakeよりも速く実行されました。前述の最適化がパフォーマンスにとって重要であることが知られています [\[27\]](#page-12-34)、実装されるとこれらのクエリの実行時間がさらに改善されると期待しています。

<span id="page-10-6"></span><img src={image_13}/>

Figure 12: TPC-Hクエリのホット実行時間（秒）。
## 7 関連研究 {#7-related-work}

分析データベースは、近年大きな学術的および商業的関心を集めています [\[1\]](#page-12-35)。Sybase IQ [\[48\]](#page-13-28)、Teradata [\[72\]](#page-13-29)、Vertica [\[42\]](#page-12-36)、およびGreenplum [\[47\]](#page-13-30)のような初期のシステムは、高価なバッチETLジョブおよびオンプレミスの性質からの限られた弾力性が特徴でした。2010年代初頭には、Snowfake [\[22\]](#page-12-37)、BigQuery [\[49\]](#page-13-31)、そしてRedshift [\[4\]](#page-12-38)のようなクラウドネイティブデータウェアハウスおよびデータベース-as-a-service（DBaaS）オファリングの登場により、組織の分析にかかるコストと複雑さが劇的に減少し、高可用性と自動リソーススケーリングの恩恵を受けました。最近では、分析実行カーネル（例：Photon [\[5\]](#page-12-39)およびVelox [\[62\]](#page-13-32)）が、異なる分析、ストリーミング、および機械学習アプリケーションで使用するために共同修正されたデータ処理を提供しています。

ClickHouseに最も似たデータベースは、目標や設計原則の観点からDruid [\[78\]](#page-13-33)とPinot [\[34\]](#page-12-40)です。両システムは、高いデータ取り込み速度でリアルタイム分析を目指しています。ClickHouseと同様に、テーブルはセグメントと呼ばれる水平部分に分割されています。ClickHouseは小さな部分を継続的にマージし、必要に応じてデータボリュームを減らしますが、DruidとPinotでは部分は永遠に不変です。また、DruidとPinotはテーブルの作成、変更、検索のために特別なノードを必要としますが、ClickHouseはこれらのタスクに単一のバイナリを使用します。

Snowfake [\[22\]](#page-12-37)は、共有ディスクアーキテクチャに基づいた人気のあるプロプライエタリクラウドデータウェアハウスです。テーブルをマイクロパーティションに分割するアプローチは、ClickHouseのパーツの概念に似ています。Snowfakeは永続化のためにハイブリッドPAXページ [\[3\]](#page-12-41)を使用しますが、ClickHouseのストレージ形式は厳密に列指向です。Snowfakeは、良好なパフォーマンスの資源として自動的に作成された軽量インデックス [\[31,](#page-12-13) [51\]](#page-13-14)を使用したローカルキャッシュとデータプルーニングを強調しています。ClickHouseの主キーと同様に、ユーザーはオプションでデータを同じ値で共同配置するためのクラスターインデックスを作成できます。

Photon [\[5\]](#page-12-39)およびVelox [\[62\]](#page-13-32)は、複雑なデータ管理システムで使用されるコンポーネントとして設計されたクエリ実行エンジンです。両システムは、入力としてクエリプランを受け取り、その後、ローカルノードでParquet（Photon）またはArrow（Velox）ファイル上で実行されます [\[46\]](#page-13-34)。ClickHouseはこれらの一般的なフォーマットでデータを消費および生成できますが、ストレージにはネイティブファイル形式を好みます。VeloxとPhotonはクエリプランの最適化を行いません（Veloxは基本的な式最適化を行います）が、データ特性に応じて計算カーネルを動的に切り替えるなどのランタイム適応技術を利用します。ClickHouseのプランオペレーターも、主にクエリのメモリ消費に基づいて外部集約や結合オペレーターに切り替えるために他のオペレーターをランタイムで作成できます。Photonの論文では、コード生成デザイン [\[38,](#page-12-22) [41,](#page-12-42) [53\]](#page-13-0)は、解釈されたベクトル化デザイン [\[11\]](#page-12-0)よりも開発およびデバッグが難しいことが記されています。Veloxにおけるコード生成の（実験的な）サポートは、ランタイム生成されたC++コードから生成された共有ライブラリをビルドしリンクしますが、ClickHouseはLLVMのリクエストコンパイルAPIと直接対話します。

DuckDB [\[67\]](#page-13-6)もホストプロセスに組み込まれることを意図されていますが、さらにクエリ最適化やトランザクションを提供します。OLAPクエリを時折OLTPステートメントと混在させるために設計されました。DuckDBは、良好なパフォーマンスを達成するために、順序保持辞書や参照フレームのような軽量圧縮方法を使用するDataBlocks [\[43\]](#page-12-43)ストレージ形式を選択しました。それに対して、ClickHouseは追加のみのユースケース、つまり、更新と削除がないまたはまれな場合に最適化されています。ブロックは、ユーザーが頻繁なクエリを高速化するためにデータプルーニングを広範に利用し、残りのクエリに対するデコンプレッションコストが費用的に負けることを前提に、重い圧縮技術（例：LZ4）を使用して圧縮されます。DuckDBは、HyperのMVCCスキームに基づいた直列化可能なトランザクションを提供します [\[55\]](#page-13-35)が、ClickHouseはスナップショット分離のみを提供します。
## 8 結論と展望 {#8-conclusion-and-outlook}

我々は、オープンソースの高性能OLAPデータベースClickHouseのアーキテクチャを紹介しました。書き込み最適化されたストレージレイヤーと最先端のベクトル化クエリエンジンを基盤に持つClickHouseは、ペタバイトスケールのデータセットにリアルタイム解析を可能にし、高い取り込み速度を実現しています。データを非同期でバックグラウンドでマージおよび変換することで、ClickHouseはデータメンテナンスと並行して挿入を効率的に分離します。そのストレージレイヤーは、スパース主インデックス、スキッピングインデックス、およびプロジェクションテーブルを使用して攻撃的なデータプルーニングを可能にします。我々は、更新と削除のClickHouseにおける実装、冪等挿入、およびノード間のデータレプリケーションによる高可用性について説明しました。クエリ処理レイヤーは、豊富なテクニックを使用してクエリを最適化し、すべてのサーバーおよびクラスタリソースに実行を並列化します。統合テーブルエンジンと関数は、他のデータ管理システムおよびデータフォーマットとシームレスにやり取りする便利な方法を提供します。ベンチマークを通じて、ClickHouseが市場で最も速い分析データベースの一つであることを示し、実際のClickHouseのデプロイメントにおける典型的なクエリのパフォーマンスにおける重要な改善を示しました。

2024年に予定されているすべての機能と改善点は、公開ロードマップ [\[18\]](#page-12-33) で確認できます。予定されている改善には、ユーザートランザクションのサポート、PromQL [\[69\]](#page-13-36)の代替クエリ言語としての導入、半構造化データ（例：JSON）用の新しいデータ型、結合のためのプランレベルの最適化の改善、軽量削除を補完するための軽量更新の実装が含まれます。
## 謝辞 {#acknowledgements}

バージョン24.6に従って、SELECT * FROM system.contributorsはClickHouseに貢献した1994人の個人を返します。ClickHouse Inc.のエンジニアリングチーム全体およびClickHouseの素晴らしいオープンソースコミュニティに、このデータベースを共に構築するための努力と献身に感謝いたします。
```
## REFERENCES {#references}

- <span id="page-12-35"></span>[1] Daniel Abadi, Peter Boncz, Stavros Harizopoulos, Stratos Idreaos, and Samuel Madden. 2013. 現代の列指向データベースシステムの設計と実装. https://doi.org/10.1561/9781601987556
- <span id="page-12-10"></span>[2] Daniel Abadi, Samuel Madden, and Miguel Ferreira. 2006. 列指向データベースシステムにおける圧縮と実行の統合. In Proceedings of the 2006 ACM SIGMOD International Conference on Management of Data (SIGMOD '06). 671–682. https://doi.org/10.1145/1142473.1142548
- <span id="page-12-41"></span>[3] Anastassia Ailamaki, David J. DeWitt, Mark D. Hill, and Marios Skounakis. 2001. キャッシュ性能のための関係の織り交ぜ. In Proceedings of the 27th International Conference on Very Large Data Bases (VLDB '01). Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 169–180.
- <span id="page-12-38"></span>[4] Nikos Armenatzoglou, Sanuj Basu, Naga Bhanoori, Mengchu Cai, Naresh Chainani, Kiran Chinta, Venkatraman Govindaraju, Todd J. Green, Monish Gupta, Sebastian Hillig, Eric Hotinger, Yan Leshinksy, Jintian Liang, Michael McCreedy, Fabian Nagel, Ippokratis Pandis, Panos Parchas, Rahul Pathak, Orestis Polychroniou, Foyzur Rahman, Gaurav Saxena, Gokul Soundararajan, Sriram Subramanian, and Doug Terry. 2022. Amazon Redshiftの再発明. In Proceedings of the 2022 International Conference on Management of Data (Philadelphia, PA, USA) (SIGMOD '22). Association for Computing Machinery, New York, NY, USA, 2205–2217. https://doi.org/10.1145/3514221.3526045
- <span id="page-12-39"></span>[5] Alexander Behm, Shoumik Palkar, Utkarsh Agarwal, Timothy Armstrong, David Cashman, Ankur Dave, Todd Greenstein, Shant Hovsepian, Ryan Johnson, Arvind Sai Krishnan, Paul Leventis, Ala Luszczak, Prashanth Menon, Mostafa Mokhtar, Gene Pang, Sameer Paranjpye, Greg Rahn, Bart Samwel, Tom van Bussel, Herman van Hovell, Maryann Xue, Reynold Xin, and Matei Zaharia. 2022. Photon: レイクハウスシステム向けの高速クエリエンジン (SIGMOD '22). Association for Computing Machinery, New York, NY, USA, 2326–2339. [https://doi.org/10.1145/3514221.](https://doi.org/10.1145/3514221.3526054) [3526054](https://doi.org/10.1145/3514221.3526054)
- <span id="page-12-18"></span>[6] Philip A. Bernstein and Nathan Goodman. 1981. 分散データベースシステムにおける同時制御. ACM Computing Survey 13, 2 (1981), 185–221. https://doi.org/10.1145/356842.356846
- <span id="page-12-23"></span>[7] Spyros Blanas, Yinan Li, and Jignesh M. Patel. 2011. マルチコアCPU向けのメインメモリハッシュ結合アルゴリズムの設計と評価. In Proceedings of the 2011 ACM SIGMOD International Conference on Management of Data (Athens, Greece) (SIGMOD '11). Association for Computing Machinery, New York, NY, USA, 37–48. https://doi.org/10.1145/1989323.1989328
- <span id="page-12-26"></span><span id="page-12-14"></span>[8] Daniel Gomez Blanco. 2023. 実践的なOpenTelemetry. Springer Nature.
- [9] Burton H. Bloom. 1970. 許容誤差を持つハッシュコーディングにおける空間/時間トレードオフ. Commun. ACM 13, 7 (1970), 422–426. [https://doi.org/10.1145/362686.](https://doi.org/10.1145/362686.362692) [362692](https://doi.org/10.1145/362686.362692)
- <span id="page-12-27"></span>[10] Peter Boncz, Thomas Neumann, and Orri Erling. 2014. TPC-Hの分析: 影響力のあるベンチマークからの隠れたメッセージと教訓. In Performance Characterization and Benchmarking. 61–76. [https://doi.org/10.1007/978-3-319-](https://doi.org/10.1007/978-3-319-04936-6_5) [04936-6_5](https://doi.org/10.1007/978-3-319-04936-6_5)
- <span id="page-12-0"></span>[11] Peter Boncz, Marcin Zukowski, and Niels Nes. 2005. MonetDB/X100: ハイパーパイプラインによるクエリ実行. In CIDR.
- <span id="page-12-8"></span>[12] Martin Burtscher and Paruj Ratanaworabhan. 2007. 二重精度浮動小数点データの高スループット圧縮. In Data Compression Conference (DCC). 293–302. https://doi.org/10.1109/DCC.2007.44
- <span id="page-12-6"></span>[13] Jef Carpenter and Eben Hewitt. 2016. Cassandra: Definitive Guide (第2版). O'Reilly Media, Inc.
- <span id="page-12-17"></span>[14] Bernadette Charron-Bost, Fernando Pedone, and André Schiper (Eds.). 2010. レプリケーション: 理論と実践. Springer-Verlag.
- <span id="page-12-3"></span>[15] chDB. 2024. chDB - 埋め込みOLAP SQLエンジン. 2024-06-20に取得した https://github.com/chdb-io/chdb
- <span id="page-12-29"></span>[16] ClickHouse. 2024. ClickBench: 分析データベースのベンチマーク. 2024-06-20に取得した https://github.com/ClickHouse/ClickBench
- <span id="page-12-28"></span>[17] ClickHouse. 2024. ClickBench: 比較測定. 2024-06-20に取得した https://benchmark.clickhouse.com
- <span id="page-12-33"></span>[18] ClickHouse. 2024. ClickHouseロードマップ2024 (GitHub). 2024-06-20に取得した https://github.com/ClickHouse/ClickHouse/issues/58392
- <span id="page-12-30"></span>[19] ClickHouse. 2024. ClickHouseバージョンベンチマーク. 2024-06-20に取得した https://github.com/ClickHouse/ClickBench/tree/main/versions
- <span id="page-12-31"></span>[20] ClickHouse. 2024. ClickHouseバージョンベンチマーク結果. 2024-06-20に取得した https://benchmark.clickhouse.com/versions/
- <span id="page-12-32"></span>[21] Andrew Crotty. 2022. MgBench. 2024-06-20に取得した [https://github.com/](https://github.com/andrewcrotty/mgbench) [andrewcrotty/mgbench](https://github.com/andrewcrotty/mgbench)
- <span id="page-12-37"></span>[22] Benoit Dageville, Thierry Cruanes, Marcin Zukowski, Vadim Antonov, Artin Avanes, Jon Bock, Jonathan Claybaugh, Daniel Engovatov, Martin Hentschel, Jiansheng Huang, Allison W. Lee, Ashish Motivala, Abdul Q. Munir, Steven Pelley, Peter Povinec, Greg Rahn, Spyridon Triantafyllis, and Philipp Unterbrunner. 2016. Snowflake Elastic Data Warehouse. In Proceedings of the 2016 International Conference on Management of Data (San Francisco, California, USA) (SIGMOD '16). Association for Computing Machinery, New York, NY, USA, 215–226. [https:](https://doi.org/10.1145/2882903.2903741) [//doi.org/10.1145/2882903.2903741](https://doi.org/10.1145/2882903.2903741)
- <span id="page-12-9"></span>[23] Patrick Damme, Annett Ungethüm, Juliana Hildebrandt, Dirk Habich, and Wolfgang Lehner. 2019. 包括高効率整数圧縮アルゴリズムのコストベースの選択戦略への包括的実験調査. ACM Trans. Database Syst. 44, 3, Article 9 (2019), 46ページ. https://doi.org/10.1145/3323991
- <span id="page-12-1"></span>[24] Philippe Dobbelaere and Kyumars Sheykh Esmaili. 2017. Kafka対RabbitMQ: 2つの業界参照Publish/Subscribe実装の比較研究: 業界論文 (DEBS '17). Association for Computing Machinery, New York, NY, USA, 227–238. https://doi.org/10.1145/3093742.3093908
- <span id="page-12-19"></span>[25] LLVMドキュメント. 2024. LLVMにおける自動ベクトル化. 2024-06-20に取得した https://llvm.org/docs/Vectorizers.html
- <span id="page-12-7"></span>[26] Siying Dong, Andrew Kryczka, Yanqin Jin, and Michael Stumm. 2021. RocksDB: 大規模アプリケーション用のキーバリューストアにおける開発優先順位の進化. ACM Transactions on Storage 17, 4, Article 26 (2021), 32ページ. https://doi.org/10.1145/3483840
- <span id="page-12-34"></span>[27] Markus Dreseler, Martin Boissier, Tilmann Rabl, and Matthias Ufacker. 2020. TPC-Hのボトルネックの定量化とその最適化. Proc. VLDB Endow. 13, 8 (2020), 1206–1220. https://doi.org/10.14778/3389133.3389138
- <span id="page-12-12"></span>[28] Ted Dunning. 2021. t-digest: 分布の効率的な推定. Software Impacts 7 (2021). https://doi.org/10.1016/j.simpa.2020.100049
- <span id="page-12-16"></span>[29] Martin Faust, Martin Boissier, Marvin Keller, David Schwalb, Holger Bischof, Katrin Eisenreich, Franz Färber, and Hasso Plattner. 2016. SAP HANAにおけるハッシュインデックスによるフットプリント削減と一意性の強制. In Database and Expert Systems Applications. 137–151. [https://doi.org/10.1007/978-3-319-44406-](https://doi.org/10.1007/978-3-319-44406-2_11) [2_11](https://doi.org/10.1007/978-3-319-44406-2_11)
- <span id="page-12-11"></span>[30] Philippe Flajolet, Eric Fusy, Olivier Gandouet, and Frederic Meunier. 2007. HyperLogLog: ほぼ最適な基数推定アルゴリズムの分析. In AofA: Algorithmsの分析, Vol. DMTCS Proceedings vol. AH, 2007 Conference on Analysis of Algorithms (AofA 07). Discrete Mathematics and Theoretical Computer Science, 137–156. https://doi.org/10.46298/dmtcs.3545
- <span id="page-12-13"></span>[31] Hector Garcia-Molina, Jefrey D. Ullman, and Jennifer Widom. 2009. データベースシステム - 完全な本 (第2版).
- <span id="page-12-24"></span>[32] Pawan Goyal, Harrick M. Vin, and Haichen Chen. 1996. スタートタイムフェアキューイング: 統合サービスパケットスイッチングネットワークのスケジューリングアルゴリズム. 26, 4 (1996), 157–168. https://doi.org/10.1145/248157.248171
- <span id="page-12-21"></span>[33] Goetz Graefe. 1993. 大規模データベースのクエリ評価手法. ACM Comput. Surv. 25, 2 (1993), 73–169. https://doi.org/10.1145/152610.152611
- <span id="page-12-40"></span>[34] Jean-François Im, Kishore Gopalakrishna, Subbu Subramaniam, Mayank Shrivastava, Adwait Tumbde, Xiaotian Jiang, Jennifer Dai, Seunghyun Lee, Neha Pawar, Jialiang Li, and Ravi Aringunram. 2018. Pinot: 5億ユーザー向けのリアルタイムOLAP. In Proceedings of the 2018 International Conference on Management of Data (Houston, TX, USA) (SIGMOD '18). Association for Computing Machinery, New York, NY, USA, 583–594. https://doi.org/10.1145/3183713.3190661
- <span id="page-12-25"></span>[35] ISO/IEC 9075-9:2001 2001. 情報技術 — データベース言語 — SQL — パート9: 外部データの管理 (SQL/MED). 標準. International Organization for Standardization.
- <span id="page-12-2"></span>[36] Paras Jain, Peter Kraft, Conor Power, Tathagata Das, Ion Stoica, and Matei Zaharia. 2023. レイクハウスストレージシステムの分析と比較. CIDR.
- <span id="page-12-4"></span>[37] Project Jupyter. 2024. Jupyter Notebooks. 2024-06-20に取得した [https:](https://jupyter.org/) [//jupyter.org/](https://jupyter.org/)
- <span id="page-12-22"></span>[38] Timo Kersten, Viktor Leis, Alfons Kemper, Thomas Neumann, Andrew Pavlo, and Peter Boncz. 2018. コンパイルされたベクトル化クエリに関するあなたの知りたかったすべてのこと. Proc. VLDB Endow. 11, 13 (2018年9月), 2209–2222. https://doi.org/10.14778/3275366.3284966
- <span id="page-12-15"></span>[39] Changkyu Kim, Jatin Chhugani, Nadathur Satish, Eric Sedlar, Anthony D. Nguyen, Tim Kaldewey, Victor W. Lee, Scott A. Brandt, and Pradeep Dubey. 2010. FAST: 現代のCPUとGPUでの高速アーキテクチャ感度ツリー検索. In Proceedings of the 2010 ACM SIGMOD International Conference on Management of Data (Indianapolis, Indiana, USA) (SIGMOD '10). Association for Computing Machinery, New York, NY, USA, 339–350. https://doi.org/10.1145/1807167.1807206
- <span id="page-12-5"></span>[40] Donald E. Knuth. 1973. コンピュータプログラミングの技法, 第3巻: ソートと検索. Addison-Wesley.
- <span id="page-12-42"></span>[41] André Kohn, Viktor Leis, and Thomas Neumann. 2018. コンパイルされたクエリの適応実行. In 2018 IEEE第34回国際データ工学会議 (ICDE). 197–208. https://doi.org/10.1109/ICDE.2018.00027
- <span id="page-12-36"></span>[42] Andrew Lamb, Matt Fuller, Ramakrishna Varadarajan, Nga Tran, Ben Vandiver, Lyric Doshi, and Chuck Bear. 2012. Vertica分析データベース: C-Store 7年後. Proc. VLDB Endow. 5, 12 (2012年8月), 1790–1801. [https://doi.org/10.](https://doi.org/10.14778/2367502.2367518) [14778/2367502.2367518](https://doi.org/10.14778/2367502.2367518)
- <span id="page-12-43"></span>[43] Harald Lang, Tobias Mühlbauer, Florian Funke, Peter A. Boncz, Thomas Neumann, and Alfons Kemper. 2016. データブロック: ベクトル化とコンパイルの両方を使用した圧縮ストレージ上のハイブリッドOLTPとOLAP. In Proceedings of the 2016 International Conference on Management of Data (San Francisco, California, USA) (SIGMOD '16). Association for Computing Machinery, New York, NY, USA, 311–326. https://doi.org/10.1145/2882903.2882925
- <span id="page-12-20"></span>[44] Viktor Leis, Peter Boncz, Alfons Kemper, and Thomas Neumann. 2014. モーサル駆動並列性: マルチコア時代のNUMA対応クエリ評価フレームワーク. In Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data (Snowbird, Utah, USA) (SIGMOD '14). Association for Computing Machinery, New York, NY, USA, 743–754. [https://doi.org/10.1145/2588555.](https://doi.org/10.1145/2588555.2610507) [2610507](https://doi.org/10.1145/2588555.2610507)
- <span id="page-13-17"></span>[45] Viktor Leis, Alfons Kemper, and Thomas Neumann. 2013. 適応式ラジックスレート: メインメモリデータベース用のARTfulインデクシング. In 2013 IEEE第29回国際データ工学会議 (ICDE). 38–49. [https://doi.org/10.1109/ICDE.](https://doi.org/10.1109/ICDE.2013.6544812) [2013.6544812](https://doi.org/10.1109/ICDE.2013.6544812)
- <span id="page-13-34"></span>[46] Chunwei Liu, Anna Pavlenko, Matteo Interlandi, and Brandon Haynes. 2023. 分析DBMS向けの一般的なオープンフォーマットの深い分析. 16, 11 (2023年7月), 3044–3056. https://doi.org/10.14778/3611479.3611507
- <span id="page-13-30"></span>[47] Zhenghua Lyu, Huan Hubert Zhang, Gang Xiong, Gang Guo, Haozhou Wang, Jinbao Chen, Asim Praveen, Yu Yang, Xiaoming Gao, Alexandra Wang, Wen Lin, Ashwin Agrawal, Junfeng Yang, Hao Wu, Xiaoliang Li, Feng Guo, Jiang Wu, Jesse Zhang, and Venkatesh Raghavan. 2021. Greenplum: 取引および分析ワークロード向けのハイブリッドデータベース (SIGMOD '21). Association for Computing Machinery, New York, NY, USA, 2530–2542. [https:](https://doi.org/10.1145/3448016.3457562) [//doi.org/10.1145/3448016.3457562](https://doi.org/10.1145/3448016.3457562)
- <span id="page-13-28"></span>[48] Roger MacNicol and Blaine French. 2004. Sybase IQ Multiplex - 分析用に設計されています. In Proceedings of the Thirtieth International Conference on Very Large Data Bases - Volume 30 (Toronto, Canada) (VLDB '04). VLDB Endowment, 1227–1230.
- <span id="page-13-31"></span>[49] Sergey Melnik, Andrey Gubarev, Jing Jing Long, Geofrey Romer, Shiva Shivakumar, Matt Tolton, Theo Vassilakis, Hossein Ahmadi, Dan Delorey, Slava Min, Mosha Pasumansky, and Jef Shute. 2020. Dremel: ウェブスケールのインタラクティブなSQL分析の10年. Proc. VLDB Endow. 13, 12 (2020年8月), 3461–3472. https://doi.org/10.14778/3415478.3415568
- <span id="page-13-2"></span>[50] Microsoft. 2024. Kusto Query Language. 2024-06-20に取得した [https:](https://github.com/microsoft/Kusto-Query-Language) [//github.com/microsoft/Kusto-Query-Language](https://github.com/microsoft/Kusto-Query-Language)
- <span id="page-13-14"></span>[51] Guido Moerkotte. 1998. 小さなマテリアライズド集約: データウェアハウジング用の軽量インデックス構造. In Proceedings of the 24rd International Conference on Very Large Data Bases (VLDB '98). 476–487.
- <span id="page-13-22"></span>[52] Jalal Mostafa, Sara Wehbi, Suren Chilingaryan, and Andreas Kopmann. 2022. SciTS: 科学実験および産業のIoTにおける時系列データベース用のベンチマーク. In Proceedings of the 34th International Conference on Scientific and Statistical Database Management (SSDBM '22). Article 12. [https:](https://doi.org/10.1145/3538712.3538723) [//doi.org/10.1145/3538712.3538723](https://doi.org/10.1145/3538712.3538723)
- <span id="page-13-0"></span>[53] Thomas Neumann. 2011. 現代ハードウェア向けの効率的なクエリプランのコンパイル. Proc. VLDB Endow. 4, 9 (2011年6月), 539–550. [https://doi.org/](https://doi.org/10.14778/2002938.2002940) [2002938.2002940](https://doi.org/10.14778/2002938.2002940)
- <span id="page-13-25"></span>[54] Thomas Neumann and Michael J. Freitag. 2020. Umbra: メモリ内性能を持つディスクベースのシステム. In 10th Conference on Innovative Data Systems Research, CIDR 2020, Amsterdam, The Netherlands, January 12-15, 2020, Online Proceedings. www.cidrdb.org. [http://cidrdb.org/cidr2020/papers/p29-neumann](http://cidrdb.org/cidr2020/papers/p29-neumann-cidr20.pdf)[cidr20.pdf](http://cidrdb.org/cidr2020/papers/p29-neumann-cidr20.pdf)
- <span id="page-13-35"></span>[55] Thomas Neumann, Tobias Mühlbauer, and Alfons Kemper. 2015. メインメモリデータベースシステムのための迅速な可直列化マルチバージョン同時制御. In Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data (Melbourne, Victoria, Australia) (SIGMOD '15). Association for Computing Machinery, New York, NY, USA, 677–689. [https://doi.org/10.1145/2723372.](https://doi.org/10.1145/2723372.2749436) [2749436](https://doi.org/10.1145/2723372.2749436)
- <span id="page-13-8"></span>[56] LevelDB on GitHub. 2024. LevelDB. 2024-06-20に取得した [https://github.](https://github.com/google/leveldb) [com/google/leveldb](https://github.com/google/leveldb)
- <span id="page-13-26"></span>[57] Patrick O'Neil, Elizabeth O'Neil, Xuedong Chen, and Stephen Revilak. 2009. スタースキーマベンチマークと拡張ファクトテーブルインデクシング. In Performance Evaluation and Benchmarking. Springer Berlin Heidelberg, 237–252. [https:](https://doi.org/10.1007/978-3-642-10424-4_17) [//doi.org/10.1007/978-3-642-10424-4_17](https://doi.org/10.1007/978-3-642-10424-4_17)
- <span id="page-13-7"></span>[58] Patrick E. O'Neil, Edward Y. C. Cheng, Dieter Gawlick, and Elizabeth J. O'Neil. 1996. ログ構造マージツリー (LSMツリー). Acta Informatica 33 (1996), 351–385. https://doi.org/10.1007/s002360050048
- <span id="page-13-4"></span>[59] Diego Ongaro and John Ousterhout. 2014. 理解可能な合意アルゴリズムを探して. In Proceedings of the 2014 USENIX Conference on USENIX Annual Technical Conference (USENIX ATC'14). 305–320. [https://doi.org/doi/10.](https://doi.org/doi/10.5555/2643634.2643666) [5555/2643634.2643666](https://doi.org/doi/10.5555/2643634.2643666)
- <span id="page-13-3"></span>[60] Patrick O'Neil, Edward Cheng, Dieter Gawlick, and Elizabeth O'Neil. 1996. ログ構造マージツリー (LSM-ツリー). Acta Inf. 33, 4 (1996), 351–385. [https:](https://doi.org/10.1007/s002360050048) [//doi.org/10.1007/s002360050048](https://doi.org/10.1007/s002360050048)
- <span id="page-13-5"></span>[61] Pandas. 2024. Pandas Dataframes. 2024-06-20に取得した [https://pandas.](https://pandas.pydata.org/) [pydata.org/](https://pandas.pydata.org/)
- <span id="page-13-32"></span>[62] Pedro Pedreira, Orri Erling, Masha Basmanova, Kevin Wilfong, Laith Sakka, Krishna Pai, Wei He, and Biswapesh Chattopadhyay. 2022. Velox: Metaの統一実行エンジン. Proc. VLDB Endow. 15, 12 (2022年8月), 3372–3384. [https:](https://doi.org/10.14778/3554821.3554829) [//doi.org/10.14778/3554821.3554829](https://doi.org/10.14778/3554821.3554829)
- <span id="page-13-10"></span>[63] Tuomas Pelkonen, Scott Franklin, Justin Teller, Paul Cavallaro, Qi Huang, Justin Meza, and Kaushik Veeraraghavan. 2015. Gorilla: 高速でスケーラブルなメモリ内時系列データベース. Proceedings of the VLDB Endowment 8, 12 (2015), 1816–1827. https://doi.org/10.14778/2824032.2824078
- <span id="page-13-18"></span>[64] Orestis Polychroniou, Arun Raghavan, and Kenneth A. Ross. 2015. メモリ内データベースのためのSIMDベクトル化の再考. In Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data (SIGMOD '15). 1493–1508. https://doi.org/10.1145/2723372.2747645
- <span id="page-13-21"></span>[65] PostgreSQL. 2024. PostgreSQL - 外部データラッパー. 2024-06-20に取得した https://wiki.postgresql.org/wiki/Foreign_data_wrappers
- <span id="page-13-23"></span>[66] Mark Raasveldt, Pedro Holanda, Tim Gubner, and Hannes Mühleisen. 2018. 公平なベンチマーキングは困難である: データベース性能テストの一般的な落とし穴. In Proceedings of the Workshop on Testing Database Systems (Houston, TX, USA) (DBTest'18). Article 2, 6 pages. https://doi.org/10.1145/3209950.3209955
- <span id="page-13-6"></span>[67] Mark Raasveldt and Hannes Mühleisen. 2019. DuckDB: 埋め込み可能な分析データベース (SIGMOD '19). Association for Computing Machinery, New York, NY, USA, 1981–1984. https://doi.org/10.1145/3299869.3320212
- <span id="page-13-16"></span>[68] Jun Rao and Kenneth A. Ross. 1999. キャッシュ意識のあるインデクシングによるメインメモリの意思決定支援. In Proceedings of the 25th International Conference on Very Large Data Bases (VLDB '99). San Francisco, CA, USA, 78–89.
- <span id="page-13-36"></span>[69] Navin C. Sabharwal and Piyush Kant Pandey. 2020. Prometheus Query Language (PromQL)を使った作業. In Monitoring Microservices and Containerized Applications. https://doi.org/10.1007/978-1-4842-6216-0_5
- <span id="page-13-27"></span>[70] Todd W. Schneider. 2022. ニューヨーク市のタクシーおよびハイヤーデータ. 2024-06-20に取得した https://github.com/toddwschneider/nyc-taxi-data
- <span id="page-13-13"></span>[71] Mike Stonebraker, Daniel J. Abadi, Adam Batkin, Xuedong Chen, Mitch Cherniack, Miguel Ferreira, Edmond Lau, Amerson Lin, Sam Madden, Elizabeth O'Neil, Pat O'Neil, Alex Rasin, Nga Tran, and Stan Zdonik. 2005. C-Store: 列指向DBMS. In Proceedings of the 31st International Conference on Very Large Data Bases (VLDB '05). 553–564.
- <span id="page-13-29"></span>[72] Teradata. 2024. Teradata Database. 2024-06-20に取得した [https://www.](https://www.teradata.com/resources/datasheets/teradata-database) [teradata.com/resources/datasheets/teradata-database](https://www.teradata.com/resources/datasheets/teradata-database)
- <span id="page-13-15"></span>[73] Frederik Transier. 2010. メモリ内テキスト検索エンジンのためのアルゴリズムとデータ構造. Ph.D.論文. https://doi.org/10.5445/IR/1000015824
- <span id="page-13-24"></span>[74] Adrian Vogelsgesang, Michael Haubenschild, Jan Finis, Alfons Kemper, Viktor Leis, Tobias Muehlbauer, Thomas Neumann, and Manuel Then. 2018. 現実を直視する: ベンチマークが現実世界を表現するのに失敗する方法. In Proceedings of the Workshop on Testing Database Systems (Houston, TX, USA) (DBTest'18). Article 1, 6ページ. https://doi.org/10.1145/3209950.3209952
- <span id="page-13-9"></span>[75] LZ4 website. 2024. LZ4. 2024-06-20に取得した https://lz4.org/
- <span id="page-13-11"></span><span id="page-13-1"></span>[76] PRQL website. 2024. PRQL. 2024-06-20に取得した https://prql-lang.org [77] Till Westmann, Donald Kossmann, Sven Helmer, and Guido Moerkotte. 2000. 圧縮データベースの実装と性能. SIGMOD Rec.
- <span id="page-13-33"></span>29, 3 (2000年9月), 55–67. https://doi.org/10.1145/362084.362137 [78] Fangjin Yang, Eric Tschetter, Xavier Léauté, Nelson Ray, Gian Merlino, and Deep Ganguli. 2014. Druid: リアルタイム分析データストア. In Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data (Snowbird, Utah, USA) (SIGMOD '14). Association for Computing Machinery, New York, NY, USA, 157–168. https://doi.org/10.1145/2588555.2595631
- <span id="page-13-20"></span>[79] Tianqi Zheng, Zhibin Zhang, and Xueqi Cheng. 2020. SAHA: 分析データベースのための文字列適応型ハッシュテーブル. Applied Sciences 10, 6 (2020). [https:](https://doi.org/10.3390/app10061915) [//doi.org/10.3390/app10061915](https://doi.org/10.3390/app10061915)
- <span id="page-13-19"></span>[80] Jingren Zhou and Kenneth A. Ross. 2002. SIMD命令を用いたデータベース操作の実装. In Proceedings of the 2002 ACM SIGMOD International Conference on Management of Data (SIGMOD '02). 145–156. [https://doi.org/10.](https://doi.org/10.1145/564691.564709) [1145/564691.564709](https://doi.org/10.1145/564691.564709)
- <span id="page-13-12"></span>[81] Marcin Zukowski, Sandor Heman, Niels Nes, and Peter Boncz. 2006. スーパー・スカラーRAM-CPUキャッシュ圧縮. In Proceedings of the 22nd International Conference on Data Engineering (ICDE '06). 59. [https://doi.org/10.1109/ICDE.](https://doi.org/10.1109/ICDE.2006.150) [2006.150](https://doi.org/10.1109/ICDE.2006.150)
