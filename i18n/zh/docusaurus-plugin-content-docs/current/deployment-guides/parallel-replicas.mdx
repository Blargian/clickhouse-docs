---
'slug': '/deployment-guides/parallel-replicas'
'title': '并行副本'
'keywords':
- 'parallel replica'
'description': '在本指南中，我们将首先讨论ClickHouse如何通过分布式表将查询分布在多个分片上，然后讨论查询如何利用多个副本进行执行。'
---

import Image from '@theme/IdealImage';
import BetaBadge from '@theme/badges/BetaBadge';
import image_1 from '@site/static/images/deployment-guides/parallel-replicas-1.png'
import image_2 from '@site/static/images/deployment-guides/parallel-replicas-2.png'
import image_3 from '@site/static/images/deployment-guides/parallel-replicas-3.png'
import image_4 from '@site/static/images/deployment-guides/parallel-replicas-4.png'
import image_5 from '@site/static/images/deployment-guides/parallel-replicas-5.png'
import image_6 from '@site/static/images/deployment-guides/parallel-replicas-6.png'
import image_7 from '@site/static/images/deployment-guides/parallel-replicas-7.png'
import image_8 from '@site/static/images/deployment-guides/parallel-replicas-8.png'
import image_9 from '@site/static/images/deployment-guides/parallel-replicas-9.png'

<BetaBadge/>

## 介绍 {#introduction}

ClickHouse 处理查询的速度非常快，但这些查询是如何在多个服务器上分配和并行执行的?

> 在本指南中，我们将先讨论 ClickHouse 如何通过分布式表将查询分发到多个分片，然后讨论查询如何利用多个副本进行执行。

## 分片架构 {#sharded-architecture}

在无共享架构中，集群通常被划分为多个分片，每个分片包含整体数据的一个子集。一个分布式表位于这些分片之上，提供完整数据的统一视图。

读取可以发送到本地表。查询执行仅会在指定的分片上进行，或者可以发送到分布式表，在这种情况下，每个分片将执行给定的查询。发出查询的分布式表所在的服务器将聚合数据并回应客户端：

<Image img={image_1} size="md" alt="sharded archtiecture" />

上图可视化了当客户端查询分布式表时发生的情况：

<ol className="docs-ordered-list">
    <li>
        select 查询通过负载均衡器发送到任意一个节点的分布式表上（通过轮询策略或被路由到特定服务器）。这个节点现在将作为协调者。
    </li>
    <li>
        节点将根据分布式表指定的信息找到需要执行查询的每个分片，并将查询发送到每个分片。
    </li>
    <li>
        每个分片在本地读取、过滤和聚合数据，然后将可合并的状态发送回协调者。
    </li>
    <li>
        协调节点合并数据，然后将响应发送回客户端。
    </li>
</ol>

当我们将副本添加到流程中时，过程相似，唯一的区别是每个分片只会执行单个副本的查询。这意味着可以并行处理更多查询。

## 非分片架构 {#non-sharded-architecture}

ClickHouse Cloud 具有与上面所述架构非常不同的结构。（有关详情，请参见 [“ClickHouse Cloud 架构”](https://clickhouse.com/docs/cloud/reference/architecture)）。在计算和存储分离的情况下，几乎可以提供无限的存储，分片的需求变得不那么重要。

下图展示了 ClickHouse Cloud 架构：

<Image img={image_2} size="md" alt="non sharded architecture" />

这种架构允许我们几乎瞬间添加和删除副本，从而确保集群的高度可扩展性。ClickHouse Keeper 集群（右侧所示）确保我们拥有元数据的单一真相来源。副本可以从 ClickHouse Keeper 集群获取元数据，并保持相同的数据。数据存储在对象存储中，SSD 缓存使查询加速成为可能。

但我们现在如何在多个服务器之间分配查询执行呢？在分片架构中，这是相当明显的，因为每个分片实际上可以在数据的子集上执行查询。那么在没有分片的情况下，它是如何工作的呢？

## 引入并行副本 {#introducing-parallel-replicas}

为了通过多个服务器并行化查询执行，我们首先需要能够将我们的其中一个服务器指定为协调者。协调者创建需要执行的任务列表，确保所有任务都被执行、聚合，并且结果返回给客户端。与大多数分布式系统一样，这将是接收初始查询的节点的角色。我们还需要定义工作单元。在分片架构中，工作单元是分片，是数据的子集。通过并行副本，我们将使用表的一小部分，称为 [granules](/docs/guides/best-practices/sparse-primary-indexes#data-is-organized-into-granules-for-parallel-data-processing)，作为工作单元。

现在，让我们看看实际效果，借助下图：

<Image img={image_3} size="md" alt="Parallel replicas" />

使用并行副本时：

<ol className="docs-ordered-list">
    <li>
        客户端的查询在经过负载均衡器后发送到一个节点。该节点成为此查询的协调者。
    </li>
    <li>
        节点分析每个部分的索引，并选择正确的部分和 granules 进行处理。
    </li>
    <li>
        协调者将工作负载拆分为一组可以分配给不同副本的 granules。
    </li>
    <li>
        每组 granules 由相应的副本处理，当完成时将可合并的状态发送给协调者。
    </li>
    <li>
        最终，协调者合并来自副本的所有结果，然后将响应返回给客户端。
    </li>
</ol>

上述步骤概述了并行副本理论上的工作原理。
然而，在实践中，有很多因素可能会影响这种逻辑的完美执行：

<ol className="docs-ordered-list">
    <li>
        一些副本可能不可用。
    </li>
    <li>
        ClickHouse 的复制是异步的，某些副本在某些时间点可能没有相同的部分。
    </li>
    <li>
        副本之间的尾部延迟需要以某种方式处理。
    </li>
    <li>
        文件系统缓存因每个副本的活动而异，意味着随机的任务分配可能会导致不太理想的性能，考虑到缓存的局部性。
    </li>
</ol>

我们将在接下来的部分中探讨如何克服这些因素。

### 公告 {#announcements}

为了解决上面列表中的 (1) 和 (2)，我们引入了公告的概念。让我们用下图可视化它是如何工作的：

<Image img={image_4} size="md" alt="Announcements" />

<ol className="docs-ordered-list">
    <li>
        客户端的查询在经过负载均衡器后发送到一个节点。该节点成为此查询的协调者。
    </li>
    <li>
        协调节点发送请求以从集群中的所有副本获取公告。副本可能对表的当前部分集有略微不同的视图。因此，我们需要收集此信息以避免不正确的调度决策。
    </li>
    <li>
        然后协调节点利用公告定义一组可以分配给不同副本的 granules。例如，我们可以看到第 3 部分没有 granules 被分配给副本 2，因为该副本在其公告中没有提供这部分。同时注意，没有任务被分配给副本 3，因为该副本没有提供公告。
    </li>
    <li>
        每个副本在其 granules 子集上处理查询并将可合并的状态发送回协调者后，协调者将结果合并，并将响应发送给客户端。
    </li>
</ol>

### 动态协调 {#dynamic-coordination}

为了解决尾部延迟问题，我们添加了动态协调。这意味着所有 granules 不会在一个请求中发送到副本，而是每个副本可以向协调者请求一个新任务（需要处理的一组 granules）。协调者将根据收到的公告给副本分配 granules 的集合。

我们设想在处理过程中，所有的副本都已发送公告，并包含所有部分。

下图展示了动态协调如何工作：

<Image img={image_5} size="md" alt="Dynamic Coordination - part 1" />

<ol className="docs-ordered-list">
    <li>
        副本通知协调节点它们能够处理任务，并且可以指定它们可以处理的工作量。
    </li>
    <li>
        协调者将任务分配给副本。
    </li>
</ol>

<Image img={image_6} size="md" alt="Dynamic Coordination - part 2" />

<ol className="docs-ordered-list">
    <li>
        副本 1 和 2 能快速完成它们的任务。它们会向协调节点请求另一个任务。
    </li>
    <li>
        协调者将新的任务分配给副本 1 和 2。
    </li>
</ol>

<Image img={image_7} size="md" alt="Dynamic Coordination - part 3" />

<ol className="docs-ordered-list">
    <li>
        所有副本现在已完成任务处理。它们请求更多任务。
    </li>
    <li>
        协调者根据公告检查哪些任务仍需处理，但没有剩余任务。
    </li>
    <li>
        协调者告知副本所有内容已处理。它现在将合并所有可合并的状态，并响应查询。
    </li>
</ol>

### 管理缓存局部性 {#managing-cache-locality}

最后剩下的潜在问题是我们如何处理缓存局部性。如果查询被执行多次，我们如何确保相同的任务被路由到相同的副本？在上一个示例中，我们有以下任务分配：

<table>
    <thead>
        <tr>
            <th></th>
            <th>副本 1</th>
            <th>副本 2</th>
            <th>副本 3</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>部分 1</td>
            <td>g1, g6, g7</td>
            <td>g2, g4, g5</td>
            <td>g3</td>
        </tr>
        <tr>
            <td>部分 2</td>
            <td>g1</td>
            <td>g2, g4, g5</td>
            <td>g3</td>
        </tr>
        <tr>
            <td>部分 3</td>
            <td>g1, g6</td>
            <td>g2, g4, g5</td>
            <td>g3</td>
        </tr>
    </tbody>
</table>

为了确保相同的任务被分配给相同的副本并能受益于缓存，会进行两个操作。计算部分 + granules 集合（任务）的哈希。并对任务分配应用副本数量的模运算。

在纸面上，这听起来很好，但实际上，某个副本的负载突然增加，或网络衰退，可能会导致尾部延迟，因为在执行某些任务时始终使用相同的副本。如果 `max_parallel_replicas` 小于副本数量，则会随机选择副本进行查询执行。

### 任务窃取 {#task-stealing}

如果某些副本处理任务的速度慢于其他副本，其他副本将尝试通过哈希“窃取”本应属于该副本的任务，以减少尾部延迟。

### 限制 {#limitations}

此功能有已知的限制，主要限制在本节中记录。

:::note
如果您发现的问题不是下面列出的限制，并怀疑并行副本是原因，请在 GitHub 上创建一个问题并使用标签 `comp-parallel-replicas`。
:::

| 限制                                           | 描述                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
|------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 复杂查询                                       | 目前并行副本在处理简单查询时效果相当好。CTE、子查询、JOIN、非扁平查询等复杂性层可能对查询性能产生负面影响。                                                                                                                                                                                                                                                                                                                                                       |
| 小查询                                         | 如果您执行的查询处理的行数不多，在多个副本上执行可能不会提高性能，因为副本之间的协调的网络时间可能导致查询执行中的额外循环。您可以通过使用设置 [`parallel_replicas_min_number_of_rows_per_replica`](/docs/operations/settings/settings#parallel_replicas_min_number_of_rows_per_replica) 来限制这些问题。  |
| 使用 FINAL 时禁用并行副本                    |                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| 高基数数据和复杂聚合                          | 需要传输大量数据的高基数聚合可能会显著减慢查询速度。                                                                                                                                                                                                                                                                                                                                                                                                |
| 与新分析器的兼容性                            | 新分析器可能会在特定情况下显著减慢或加速查询执行。                                                                                                                                                                                                                                                                                                                                                                                                      |

## 与并行副本相关的设置 {#settings-related-to-parallel-replicas}

| 设置                                                | 描述                                                                                                                                                                                                                                                         |
|----------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `enable_parallel_replicas`                         | `0`: 禁用<br/> `1`: 启用 <br/>`2`: 强制使用并行副本，如果未使用将抛出异常。                                                                                                                                                          |
| `cluster_for_parallel_replicas`                    | 用于并行复制的集群名称；如果您使用 ClickHouse Cloud，请使用 `default`。                                                                                                                                                                 |
| `max_parallel_replicas`                            | 用于查询执行的最大副本数，如果指定的数量小于集群中的副本数量，将随机选择节点。此值也可以超额承诺以适应横向扩展。 |
| `parallel_replicas_min_number_of_rows_per_replica` | 帮助根据需要处理的行数限制所使用的副本数，使用的副本数定义为：<br/> `estimated rows to read` / `min_number_of_rows_per_replica`。                                                               |
| `allow_experimental_analyzer`                      | `0`: 使用旧分析器<br/> `1`: 使用新分析器。 <br/><br/>并行副本的行为可能会因使用的分析器而有所变化。                                                                                                                                    |

## 调查并行副本中的问题 {#investigating-issues-with-parallel-replicas}

您可以检查每个查询使用的设置在 [`system.query_log`](/docs/operations/system-tables/query_log) 表中。您还可以查看 [`system.events`](/docs/operations/system-tables/events) 表以查看服务器上发生的所有事件，并且可以使用 [`clusterAllReplicas`](/docs/sql-reference/table-functions/cluster) 表函数查看所有副本上的表（如果您是云用户，请使用 `default`）。

```sql title="Query"
SELECT
   hostname(),
   *
FROM clusterAllReplicas('default', system.events)
WHERE event ILIKE '%ParallelReplicas%'
```
<details>
<summary>响应</summary>
```response title="Response"
┌─hostname()───────────────────────┬─event──────────────────────────────────────────┬─value─┬─description──────────────────────────────────────────────────────────────────────────────────────────┐
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasHandleRequestMicroseconds      │   438 │ Time spent processing requests for marks from replicas                                               │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasHandleAnnouncementMicroseconds │   558 │ Time spent processing replicas announcements                                                         │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasReadUnassignedMarks            │   240 │ Sum across all replicas of how many unassigned marks were scheduled                                  │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasReadAssignedForStealingMarks   │     4 │ Sum across all replicas of how many of scheduled marks were assigned for stealing by consistent hash │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasStealingByHashMicroseconds     │     5 │ Time spent collecting segments meant for stealing by hash                                            │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasProcessingPartsMicroseconds    │     5 │ Time spent processing data parts                                                                     │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasStealingLeftoversMicroseconds  │     3 │ Time spent collecting orphaned segments                                                              │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasUsedCount                      │     2 │ Number of replicas used to execute a query with task-based parallel replicas                         │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasAvailableCount                 │     6 │ Number of replicas available to execute a query with task-based parallel replicas                    │
└──────────────────────────────────┴────────────────────────────────────────────────┴───────┴──────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─hostname()───────────────────────┬─event──────────────────────────────────────────┬─value─┬─description──────────────────────────────────────────────────────────────────────────────────────────┐
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasHandleRequestMicroseconds      │   698 │ Time spent processing requests for marks from replicas                                               │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasHandleAnnouncementMicroseconds │   644 │ Time spent processing replicas announcements                                                         │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasReadUnassignedMarks            │   190 │ Sum across all replicas of how many unassigned marks were scheduled                                  │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasReadAssignedForStealingMarks   │    54 │ Sum across all replicas of how many of scheduled marks were assigned for stealing by consistent hash │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasStealingByHashMicroseconds     │     8 │ Time spent collecting segments meant for stealing by hash                                            │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasProcessingPartsMicroseconds    │     4 │ Time spent processing data parts                                                                     │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasStealingLeftoversMicroseconds  │     2 │ Time spent collecting orphaned segments                                                              │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasUsedCount                      │     2 │ Number of replicas used to execute a query with task-based parallel replicas                         │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasAvailableCount                 │     6 │ Number of replicas available to execute a query with task-based parallel replicas                    │
└──────────────────────────────────┴────────────────────────────────────────────────┴───────┴──────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─hostname()───────────────────────┬─event──────────────────────────────────────────┬─value─┬─description──────────────────────────────────────────────────────────────────────────────────────────┐
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasHandleRequestMicroseconds      │   620 │ Time spent processing requests for marks from replicas                                               │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasHandleAnnouncementMicroseconds │   656 │ Time spent processing replicas announcements                                                         │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasReadUnassignedMarks            │     1 │ Sum across all replicas of how many unassigned marks were scheduled                                  │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasReadAssignedForStealingMarks   │     1 │ Sum across all replicas of how many of scheduled marks were assigned for stealing by consistent hash │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasStealingByHashMicroseconds     │     4 │ Time spent collecting segments meant for stealing by hash                                            │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasProcessingPartsMicroseconds    │     3 │ Time spent processing data parts                                                                     │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasStealingLeftoversMicroseconds  │     1 │ Time spent collecting orphaned segments                                                              │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasUsedCount                      │     2 │ Number of replicas used to execute a query with task-based parallel replicas                         │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasAvailableCount                 │    12 │ Number of replicas available to execute a query with task-based parallel replicas                    │
└──────────────────────────────────┴────────────────────────────────────────────────┴───────┴──────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─hostname()───────────────────────┬─event──────────────────────────────────────────┬─value─┬─description──────────────────────────────────────────────────────────────────────────────────────────┐
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasHandleRequestMicroseconds      │   696 │ Time spent processing requests for marks from replicas                                               │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasHandleAnnouncementMicroseconds │   717 │ Time spent processing replicas announcements                                                         │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasReadUnassignedMarks            │     2 │ Sum across all replicas of how many unassigned marks were scheduled                                  │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasReadAssignedForStealingMarks   │     2 │ Sum across all replicas of how many of scheduled marks were assigned for stealing by consistent hash │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasStealingByHashMicroseconds     │    10 │ Time spent collecting segments meant for stealing by hash                                            │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasProcessingPartsMicroseconds    │     6 │ Time spent processing data parts                                                                     │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasStealingLeftoversMicroseconds  │     2 │ Time spent collecting orphaned segments                                                              │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasUsedCount                      │     2 │ Number of replicas used to execute a query with task-based parallel replicas                         │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasAvailableCount                 │    12 │ Number of replicas available to execute a query with task-based parallel replicas                    │
└──────────────────────────────────┴────────────────────────────────────────────────┴───────┴──────────────────────────────────────────────────────────────────────────────────────────────────────┘
```
</details>

[`system.text_log`](/docs/operations/system-tables/text_log) 表还包含有关使用并行副本执行查询的信息：

```sql title="Query"
SELECT message
FROM clusterAllReplicas('default', system.text_log)
WHERE query_id = 'ad40c712-d25d-45c4-b1a1-a28ba8d4019c'
ORDER BY event_time_microseconds ASC
```

<details>
<summary>响应</summary>
```response title="Response"
┌─message────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ (from 54.218.178.249:59198) SELECT * FROM session_events WHERE type='type2' LIMIT 10 SETTINGS allow_experimental_parallel_reading_from_replicas=2; (stage: Complete)                                                                                       │
│ Query SELECT __table1.clientId AS clientId, __table1.sessionId AS sessionId, __table1.pageId AS pageId, __table1.timestamp AS timestamp, __table1.type AS type FROM default.session_events AS __table1 WHERE __table1.type = 'type2' LIMIT _CAST(10, 'UInt64') SETTINGS allow_experimental_parallel_reading_from_replicas = 2 to stage Complete │
│ Access granted: SELECT(clientId, sessionId, pageId, timestamp, type) ON default.session_events                                                                                                                                                             │
│ Query SELECT __table1.clientId AS clientId, __table1.sessionId AS sessionId, __table1.pageId AS pageId, __table1.timestamp AS timestamp, __table1.type AS type FROM default.session_events AS __table1 WHERE __table1.type = 'type2' LIMIT _CAST(10, 'UInt64') to stage WithMergeableState only analyze │
│ Access granted: SELECT(clientId, sessionId, pageId, timestamp, type) ON default.session_events                                                                                                                                                             │
│ Query SELECT __table1.clientId AS clientId, __table1.sessionId AS sessionId, __table1.pageId AS pageId, __table1.timestamp AS timestamp, __table1.type AS type FROM default.session_events AS __table1 WHERE __table1.type = 'type2' LIMIT _CAST(10, 'UInt64') from stage FetchColumns to stage WithMergeableState only analyze │
│ Query SELECT __table1.clientId AS clientId, __table1.sessionId AS sessionId, __table1.pageId AS pageId, __table1.timestamp AS timestamp, __table1.type AS type FROM default.session_events AS __table1 WHERE __table1.type = 'type2' LIMIT _CAST(10, 'UInt64') SETTINGS allow_experimental_parallel_reading_from_replicas = 2 to stage WithMergeableState only analyze │
│ Access granted: SELECT(clientId, sessionId, pageId, timestamp, type) ON default.session_events                                                                                                                                                             │
│ Query SELECT __table1.clientId AS clientId, __table1.sessionId AS sessionId, __table1.pageId AS pageId, __table1.timestamp AS timestamp, __table1.type AS type FROM default.session_events AS __table1 WHERE __table1.type = 'type2' LIMIT _CAST(10, 'UInt64') SETTINGS allow_experimental_parallel_reading_from_replicas = 2 from stage FetchColumns to stage WithMergeableState only analyze │
│ Query SELECT __table1.clientId AS clientId, __table1.sessionId AS sessionId, __table1.pageId AS pageId, __table1.timestamp AS timestamp, __table1.type AS type FROM default.session_events AS __table1 WHERE __table1.type = 'type2' LIMIT _CAST(10, 'UInt64') SETTINGS allow_experimental_parallel_reading_from_replicas = 2 from stage WithMergeableState to stage Complete │
│ The number of replicas requested (100) is bigger than the real number available in the cluster (6). Will use the latter number to execute the query.                                                                                                       │
│ Initial request from replica 4: 2 parts: [part all_0_2_1 with ranges [(0, 182)], part all_3_3_0 with ranges [(0, 62)]]----------
Received from 4 replica
                                                                                                   │
│ Reading state is fully initialized: part all_0_2_1 with ranges [(0, 182)] in replicas [4]; part all_3_3_0 with ranges [(0, 62)] in replicas [4]                                                                                                            │
│ Sent initial requests: 1 Replicas count: 6                                                                                                                                                                                                                 │
│ Initial request from replica 2: 2 parts: [part all_0_2_1 with ranges [(0, 182)], part all_3_3_0 with ranges [(0, 62)]]----------
Received from 2 replica
                                                                                                   │
│ Sent initial requests: 2 Replicas count: 6                                                                                                                                                                                                                 │
│ Handling request from replica 4, minimal marks size is 240                                                                                                                                                                                                 │
│ Going to respond to replica 4 with 1 parts: [part all_0_2_1 with ranges [(128, 182)]]. Finish: false; mine_marks=0, stolen_by_hash=54, stolen_rest=0                                                                                                       │
│ Initial request from replica 1: 2 parts: [part all_0_2_1 with ranges [(0, 182)], part all_3_3_0 with ranges [(0, 62)]]----------
Received from 1 replica
                                                                                                   │
│ Sent initial requests: 3 Replicas count: 6                                                                                                                                                                                                                 │
│ Handling request from replica 4, minimal marks size is 240                                                                                                                                                                                                 │
│ Going to respond to replica 4 with 2 parts: [part all_0_2_1 with ranges [(0, 128)], part all_3_3_0 with ranges [(0, 62)]]. Finish: false; mine_marks=0, stolen_by_hash=0, stolen_rest=190                                                                  │
│ Initial request from replica 0: 2 parts: [part all_0_2_1 with ranges [(0, 182)], part all_3_3_0 with ranges [(0, 62)]]----------
Received from 0 replica
                                                                                                   │
│ Sent initial requests: 4 Replicas count: 6                                                                                                                                                                                                                 │
│ Initial request from replica 5: 2 parts: [part all_0_2_1 with ranges [(0, 182)], part all_3_3_0 with ranges [(0, 62)]]----------
Received from 5 replica
                                                                                                   │
│ Sent initial requests: 5 Replicas count: 6                                                                                                                                                                                                                 │
│ Handling request from replica 2, minimal marks size is 240                                                                                                                                                                                                 │
│ Going to respond to replica 2 with 0 parts: []. Finish: true; mine_marks=0, stolen_by_hash=0, stolen_rest=0                                                                                                                                                │
│ Initial request from replica 3: 2 parts: [part all_0_2_1 with ranges [(0, 182)], part all_3_3_0 with ranges [(0, 62)]]----------
Received from 3 replica
                                                                                                   │
│ Sent initial requests: 6 Replicas count: 6                                                                                                                                                                                                                 │
│ Total rows to read: 2000000                                                                                                                                                                                                                                │
│ Handling request from replica 5, minimal marks size is 240                                                                                                                                                                                                 │
│ Going to respond to replica 5 with 0 parts: []. Finish: true; mine_marks=0, stolen_by_hash=0, stolen_rest=0                                                                                                                                                │
│ Handling request from replica 0, minimal marks size is 240                                                                                                                                                                                                 │
│ Going to respond to replica 0 with 0 parts: []. Finish: true; mine_marks=0, stolen_by_hash=0, stolen_rest=0                                                                                                                                                │
│ Handling request from replica 1, minimal marks size is 240                                                                                                                                                                                                 │
│ Going to respond to replica 1 with 0 parts: []. Finish: true; mine_marks=0, stolen_by_hash=0, stolen_rest=0                                                                                                                                                │
│ Handling request from replica 3, minimal marks size is 240                                                                                                                                                                                                 │
│ Going to respond to replica 3 with 0 parts: []. Finish: true; mine_marks=0, stolen_by_hash=0, stolen_rest=0                                                                                                                                                │
│ (c-crimson-vd-86-server-rdhnsx3-0.c-crimson-vd-86-server-headless.ns-crimson-vd-86.svc.cluster.local:9000) Cancelling query because enough data has been read                                                                                              │
│ Read 81920 rows, 5.16 MiB in 0.013166 sec., 6222087.194288318 rows/sec., 391.63 MiB/sec.                                                                                                                                                                   │
│ Coordination done: Statistics: replica 0 - {requests: 2 marks: 0 assigned_to_me: 0 stolen_by_hash: 0 stolen_unassigned: 0}; replica 1 - {requests: 2 marks: 0 assigned_to_me: 0 stolen_by_hash: 0 stolen_unassigned: 0}; replica 2 - {requests: 2 marks: 0 assigned_to_me: 0 stolen_by_hash: 0 stolen_unassigned: 0}; replica 3 - {requests: 2 marks: 0 assigned_to_me: 0 stolen_by_hash: 0 stolen_unassigned: 0}; replica 4 - {requests: 3 marks: 244 assigned_to_me: 0 stolen_by_hash: 54 stolen_unassigned: 190}; replica 5 - {requests: 2 marks: 0 assigned_to_me: 0 stolen_by_hash: 0 stolen_unassigned: 0} │
│ Peak memory usage (for query): 1.81 MiB.                                                                                                                                                                                                                   │
│ Processed in 0.024095586 sec.                                                                                                                                                                                                                              │
└────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
```
</details>

最后，您还可以使用 `EXPLAIN PIPELINE`。它突显 ClickHouse 将如何执行查询以及执行查询所使用的资源。让我们以以下查询为例：

```sql
SELECT count(), uniq(pageId) , min(timestamp), max(timestamp) 
FROM session_events 
WHERE type='type3' 
GROUP BY toYear(timestamp) LIMIT 10
```

让我们看看没有并行副本的查询管道：

```sql title="EXPLAIN PIPELINE (without parallel replica)"
EXPLAIN PIPELINE graph = 1, compact = 0 
SELECT count(), uniq(pageId) , min(timestamp), max(timestamp) 
FROM session_events 
WHERE type='type3' 
GROUP BY toYear(timestamp) 
LIMIT 10 
SETTINGS allow_experimental_parallel_reading_from_replicas=0 
FORMAT TSV;
```

<Image img={image_8} size="lg" alt="EXPLAIN without parallel_replica" />

现在来看使用并行副本的情况：

```sql title="EXPLAIN PIPELINE (with parallel replica)"
EXPLAIN PIPELINE graph = 1, compact = 0 
SELECT count(), uniq(pageId) , min(timestamp), max(timestamp) 
FROM session_events 
WHERE type='type3' 
GROUP BY toYear(timestamp) 
LIMIT 10 
SETTINGS allow_experimental_parallel_reading_from_replicas=2 
FORMAT TSV;
```

<Image img={image_9} size="lg" alt="EXPLAIN with parallel_replica"/>
