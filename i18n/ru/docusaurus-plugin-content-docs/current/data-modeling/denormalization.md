---
slug: /data-modeling/denormalization
title: 'Денормализация данных'
description: 'Как использовать денормализацию для улучшения производительности запросов'
keywords: ['денормализация данных', 'денормализовать', 'оптимизация запросов']
---

import denormalizationDiagram from '@site/static/images/data-modeling/denormalization-diagram.png';
import denormalizationSchema from '@site/static/images/data-modeling/denormalization-schema.png';
import Image from '@theme/IdealImage';


# Денормализация данных

Денормализация данных — это метод в ClickHouse, который используется для создания плоских таблиц, чтобы минимизировать задержку запросов, избегая соединений.

## Сравнение нормализованных и денормализованных схем {#comparing-normalized-vs-denormalized-schemas}

Денормализация данных включает в себя намеренное обращение нормализации, чтобы оптимизировать производительность базы данных для определенных паттернов запросов. В нормализованных базах данных данные разделяются на несколько связанных таблиц для минимизации избыточности и обеспечения целостности данных. Денормализация восстанавливает избыточность путем объединения таблиц, дублирования данных и включения вычисляемых полей в одну таблицу или меньшее количество таблиц, что эффективно перемещает любые соединения с времени запроса на время вставки.

Этот процесс уменьшает необходимость в сложных соединениях во время выполнения запросов и может значительно ускорить операции чтения, что делает его идеальным для приложений с высокими требованиями к чтению и сложными запросами. Однако это может усложнить операции записи и обслуживание, поскольку любые изменения в дублированных данных должны быть распространены на все экземпляры для поддержания согласованности.

<Image img={denormalizationDiagram} size="lg" alt="Денормализация в ClickHouse"/>

<br />

Общая техника, популяризированная решениями NoSQL, заключается в том, чтобы денормализовать данные в отсутствие поддержки `JOIN`, эффективно храня все статистики или связанные строки на родительской строке в виде колонок и вложенных объектов. Например, в схемах блога мы можем хранить все `Комментарии` в виде `Array` объектов на соответствующих публикациях.

## Когда использовать денормализацию {#when-to-use-denormalization}

В общем случае, мы рекомендуем денормализовать в следующих случаях:

- Денормализовать таблицы, которые меняются нечасто или для которых допустима задержка перед тем, как данные станут доступны для аналитических запросов, т.е. данные могут быть полностью перезагружены пакетно.
- Избежать денормализации отношений "многие ко многим". Это может привести к необходимости обновлять множество строк, если изменится одна исходная строка.
- Избежать денормализации отношений с высокой кардинальностью. Если каждая строка в таблице имеет тысячи связанных записей в другой таблице, они должны быть представлены в виде `Array` - либо простого типа, либо кортежей. Обычно не рекомендуется использовать массивы с более чем 1000 кортежами.
- Вместо того чтобы денормализовать все колонки как вложенные объекты, рассмотрите возможность денормализации только одной статистики с помощью материализованных представлений (см. ниже).

Не вся информация должна быть денормализована - только ключевая информация, которая должна часто использоваться.

Работы по денормализации могут проводиться как в ClickHouse, так и на стороне сервера, например, с использованием Apache Flink.

## Избегайте денормализации часто обновляемых данных {#avoid-denormalization-on-frequently-updated-data}

Для ClickHouse денормализация является одним из нескольких вариантов, которые пользователи могут использовать для оптимизации производительности запросов, но следует использовать с осторожностью. Если данные обновляются часто и должны обновляться в почти реальном времени, этот подход следует избегать. Используйте это, если основная таблица в основном представляет собой только добавления или может периодически перезагружаться пакетно, например, ежедневно.

Как подход, он страдает от одной главной проблемы - производительность записи и обновление данных. Более конкретно, денормализация фактически смещает ответственность за соединение данных с времени запроса на время загрузки. Хотя это может значительно улучшить производительность запроса, это усложняет загрузку и означает, что потоки данных должны повторно вставлять строку в ClickHouse, если меняются какие-либо из строк, которые использовались для ее составления. Это может означать, что изменение одной исходной строки потенциально приводит к необходимости обновления множества строк в ClickHouse. В сложных схемах, где строки составлены из сложных соединений, изменение одной строки в вложенной части соединения потенциально может означать необходимость обновления миллионов строк.

Достигнуть этого в реальном времени часто нереалистично и требует значительного инженерного усилия из-за двух проблем:

1. Запуск правильных операторов соединения при изменении строки таблицы. Это не должно переработать все объекты для соединения - а только те, которые были затронуты. Изменение соединений для эффективной фильтрации к правильным строкам и достижение этого при высокой пропускной способности требует внешних инструментов или инженерных решений.
2. Обновления строк в ClickHouse должны быть аккуратно управляемыми, что вводит дополнительную сложность.

<br />

Поэтому более распространен процесс пакетного обновления, при котором все денормализованные объекты периодически перезагружаются.

## Практические случаи для денормализации {#practical-cases-for-denormalization}

Рассмотрим несколько практических примеров, где денормализация может иметь смысл, и других, где более желательны альтернативные подходы.

Рассмотрим таблицу `Публикации`, которая уже была денормализована со статистикой такой как `КоличествоОтветов` и `КоличествоКомментариев` - исходные данные предоставлены в этой форме. На самом деле, мы, возможно, захотим фактически нормализовать эту информацию, так как она, вероятно, будет часто изменяться. Многие из этих колонок также доступны через другие таблицы, например, комментарии для публикации можно получить через колонку `PostId` и таблицу `Комментарии`. Для целей примера мы предположим, что публикации перезагружаются в пакетном процессе.

Также мы рассматриваем только денормализацию других таблиц в `Публикации`, так как мы считаем это нашей основной таблицей для аналитики. Денормализация в другом направлении также была бы уместна для некоторых запросов, с теми же вышеуказанными соображениями.

*Для каждого из следующих примеров предположим, что существует запрос, который требует использования обеих таблиц в соединении.*

### Публикации и Голоса {#posts-and-votes}

Голоса за публикации представлены в виде отдельных таблиц. Оптимизированная схема для этого показана ниже, а также команда вставки для загрузки данных:

```sql
CREATE TABLE votes
(
        `Id` UInt32,
        `PostId` Int32,
        `VoteTypeId` UInt8,
        `CreationDate` DateTime64(3, 'UTC'),
        `UserId` Int32,
        `BountyAmount` UInt8
)
ENGINE = MergeTree
ORDER BY (VoteTypeId, CreationDate, PostId)

INSERT INTO votes SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/stackoverflow/parquet/votes/*.parquet')

0 rows in set. Elapsed: 26.272 sec. Processed 238.98 million rows, 2.13 GB (9.10 million rows/s., 80.97 MB/s.)
```

На первый взгляд, это могут быть кандидаты для денормализации в таблице публикаций. С этим подходом есть несколько проблем.

Голоса часто добавляются к публикациям. Хотя это может уменьшиться с течением времени, следующий запрос показывает, что у нас около 40k голосов в час на 30k публикаций.

```sql
SELECT round(avg(c)) AS avg_votes_per_hr, round(avg(posts)) AS avg_posts_per_hr
FROM
(
        SELECT
        toStartOfHour(CreationDate) AS hr,
        count() AS c,
        uniq(PostId) AS posts
        FROM votes
        GROUP BY hr
)

┌─avg_votes_per_hr─┬─avg_posts_per_hr─┐
│               41759 │         33322 │
└──────────────────┴──────────────────┘
```

Это можно решить пакетной обработкой, если задержка допустима, но это все равно требует от нас обработки обновлений, если мы не будем периодически перезагружать все публикации (что маловероятно нежелательно).

Более проблематично, что некоторые публикации имеют исключительно большое количество голосов:

```sql
SELECT PostId, concat('https://stackoverflow.com/questions/', PostId) AS url, count() AS c
FROM votes
GROUP BY PostId
ORDER BY c DESC
LIMIT 5

┌───PostId─┬─url──────────────────────────────────────────┬─────c─┐
│ 11227902 │ https://stackoverflow.com/questions/11227902 │ 35123 │
│   927386 │ https://stackoverflow.com/questions/927386   │ 29090 │
│ 11227809 │ https://stackoverflow.com/questions/11227809 │ 27475 │
│   927358 │ https://stackoverflow.com/questions/927358   │ 26409 │
│  2003515 │ https://stackoverflow.com/questions/2003515  │ 25899 │
└──────────┴──────────────────────────────────────────────┴───────┘
```

Основное замечание здесь заключается в том, что агрегированные статистики голосов для каждой публикации будут достаточны для большинства анализов - нам не нужно денормализовать всю информацию о голосовании. Например, текущая колонка `Score` представляет собой такую статистику, т.е. общее количество положительных голосов минус отрицательные голоса. В идеале, мы хотели бы просто иметь возможность получать эти статистики во время выполнения запроса с помощью простого запроса (см. [словарь](/dictionary)).

### Пользователи и Награды {#users-and-badges}

Теперь давайте рассмотрим наших `Пользователей` и `Награды`:

<Image img={denormalizationSchema} size="lg" alt="Схема Пользователи и Награды"/>

<p></p>
Сначала мы вставляем данные с помощью следующей команды:
<p></p>

```sql
CREATE TABLE users
(
    `Id` Int32,
    `Reputation` LowCardinality(String),
    `CreationDate` DateTime64(3, 'UTC') CODEC(Delta(8), ZSTD(1)),
    `DisplayName` String,
    `LastAccessDate` DateTime64(3, 'UTC'),
    `AboutMe` String,
    `Views` UInt32,
    `UpVotes` UInt32,
    `DownVotes` UInt32,
    `WebsiteUrl` String,
    `Location` LowCardinality(String),
    `AccountId` Int32
)
ENGINE = MergeTree
ORDER BY (Id, CreationDate)
```

```sql
CREATE TABLE badges
(
    `Id` UInt32,
    `UserId` Int32,
    `Name` LowCardinality(String),
    `Date` DateTime64(3, 'UTC'),
    `Class` Enum8('Gold' = 1, 'Silver' = 2, 'Bronze' = 3),
    `TagBased` Bool
)
ENGINE = MergeTree
ORDER BY UserId

INSERT INTO users SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/stackoverflow/parquet/users.parquet')

0 rows in set. Elapsed: 26.229 sec. Processed 22.48 million rows, 1.36 GB (857.21 thousand rows/s., 51.99 MB/s.)

INSERT INTO badges SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/stackoverflow/parquet/badges.parquet')

0 rows in set. Elapsed: 18.126 sec. Processed 51.29 million rows, 797.05 MB (2.83 million rows/s., 43.97 MB/s.)
```

Хотя пользователи могут часто получать награды, это маловероятно, что это набор данных, который необходимо обновлять чаще чем раз в день. Отношение между наградами и пользователями является отношением "один ко многим". Возможно, мы можем просто денормализовать награды на пользователей в виде списка кортежей? Хотя это возможно, быстрая проверка на подтверждение наибольшего количества наград на пользователя показывает, что это не идеально:

```sql
SELECT UserId, count() AS c FROM badges GROUP BY UserId ORDER BY c DESC LIMIT 5

┌─UserId─┬─────c─┐
│  22656 │ 19334 │
│   6309 │ 10516 │
│ 100297 │  7848 │
│ 157882 │  7574 │
│  29407 │  6512 │
└────────┴───────┘
```

Вероятно, нереалистично денормализовать 19k объектов на одну строку. Это отношение, возможно, лучше оставить как отдельные таблицы или добавить статистику.

> Мы можем пожелать денормализовать статистику наград на пользователей, например, количество наград. Мы рассматриваем такой пример, когда используем словари для этого набора данных во время вставки.

### Публикации и СвязиПубликаций {#posts-and-postlinks}

`СвязиПубликаций` соединяют `Публикации`, которые пользователи считают связанными или дублирующимися. Следующий запрос показывает схему и команду загрузки:

```sql
CREATE TABLE postlinks
(
  `Id` UInt64,
  `CreationDate` DateTime64(3, 'UTC'),
  `PostId` Int32,
  `RelatedPostId` Int32,
  `LinkTypeId` Enum('Linked' = 1, 'Duplicate' = 3)
)
ENGINE = MergeTree
ORDER BY (PostId, RelatedPostId)

INSERT INTO postlinks SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/stackoverflow/parquet/postlinks.parquet')

0 rows in set. Elapsed: 4.726 sec. Processed 6.55 million rows, 129.70 MB (1.39 миллион строк/с, 27.44 МБ/с.)
```

Мы можем подтвердить, что у никаких публикаций нет чрезмерного количества ссылок, что предотвращает денормализацию:

```sql
SELECT PostId, count() AS c
FROM postlinks
GROUP BY PostId
ORDER BY c DESC LIMIT 5

┌───PostId─┬───c─┐
│ 22937618 │ 125 │
│  9549780 │ 120 │
│  3737139 │ 109 │
│ 18050071 │ 103 │
│ 25889234 │  82 │
└──────────┴─────┘
```

Аналогично, эти ссылки не являются событиями, которые происходят слишком часто:

```sql
SELECT
  round(avg(c)) AS avg_votes_per_hr,
  round(avg(posts)) AS avg_posts_per_hr
FROM
(
  SELECT
  toStartOfHour(CreationDate) AS hr,
  count() AS c,
  uniq(PostId) AS posts
  FROM postlinks
  GROUP BY hr
)

┌─avg_votes_per_hr─┬─avg_posts_per_hr─┐
│                54 │                    44     │
└──────────────────┴──────────────────┘
```

Мы используем это как наш пример денормализации ниже.

### Простой пример статистики {#simple-statistic-example}

В большинстве случаев, денормализация требует добавления одной колонки или статистики к родительской строке. Например, мы можем просто пожелать обогатить наши публикации количеством дублирующихся публикаций и просто добавить колонку.

```sql
CREATE TABLE posts_with_duplicate_count
(
  `Id` Int32 CODEC(Delta(4), ZSTD(1)),
   ... -другие колонки
   `DuplicatePosts` UInt16
) ENGINE = MergeTree
ORDER BY (PostTypeId, toDate(CreationDate), CommentCount)
```

Чтобы заполнить эту таблицу, мы используем `INSERT INTO SELECT`, объединяя нашу статистику дубликатов с нашими публикациями.

```sql
INSERT INTO posts_with_duplicate_count SELECT
    posts.*,
    DuplicatePosts
FROM posts AS posts
LEFT JOIN
(
    SELECT PostId, countIf(LinkTypeId = 'Duplicate') AS DuplicatePosts
    FROM postlinks
    GROUP BY PostId
) AS postlinks ON posts.Id = postlinks.PostId
```

### Использование сложных типов для отношений "один-ко-многим" {#exploiting-complex-types-for-one-to-many-relationships}

Чтобы провести денормализацию, нам часто необходимо использовать сложные типы. Если денормализуется отношение "один-ко-одному" с малым количеством колонок, пользователи могут просто добавлять их как строки с их оригинальными типами, как показано выше. Однако это часто нежелательно для больших объектов и невозможно для отношений "один-ко-многим".

В случаях сложных объектов или отношений "один-ко-многим" пользователи могут использовать:

- Именованные кортежи - они позволяют связанной структуре представляться в виде набора колонок.
- Array(Tuple) или Nested - массив именованных кортежей, также известных как вложенные, где каждая запись представляет собой объект. Применимо к отношениям "один-ко-многим".

В качестве примера, ниже мы демонстрируем денормализацию `СвязиПубликаций` на `Публикации`.

Каждая публикация может содержать множество ссылок на другие публикации, как показано в схеме `СвязиПубликаций` ранее. В качестве вложенного типа мы можем представить эти связанные и дублирующиеся публикации следующим образом:

```sql
SET flatten_nested=0
CREATE TABLE posts_with_links
(
  `Id` Int32 CODEC(Delta(4), ZSTD(1)),
   ... -другие колонки
   `LinkedPosts` Nested(CreationDate DateTime64(3, 'UTC'), PostId Int32),
   `DuplicatePosts` Nested(CreationDate DateTime64(3, 'UTC'), PostId Int32),
) ENGINE = MergeTree
ORDER BY (PostTypeId, toDate(CreationDate), CommentCount)
```

> Обратите внимание на использование настройки `flatten_nested=0`. Мы рекомендуем отключить уплотнение вложенных данных.

Мы можем провести эту денормализацию, используя `INSERT INTO SELECT` с запросом `OUTER JOIN`:

```sql
INSERT INTO posts_with_links
SELECT
    posts.*,
    arrayMap(p -> (p.1, p.2), arrayFilter(p -> p.3 = 'Linked' AND p.2 != 0, Related)) AS LinkedPosts,
    arrayMap(p -> (p.1, p.2), arrayFilter(p -> p.3 = 'Duplicate' AND p.2 != 0, Related)) AS DuplicatePosts
FROM posts
LEFT JOIN (
    SELECT
         PostId,
         groupArray((CreationDate, RelatedPostId, LinkTypeId)) AS Related
    FROM postlinks
    GROUP BY PostId
) AS postlinks ON posts.Id = postlinks.PostId

0 rows in set. Elapsed: 155.372 sec. Processed 66.37 million rows, 76.33 GB (427.18 thousand rows/s., 491.25 MB/s.)
Peak memory usage: 6.98 GiB.
```

> Обратите внимание на время. Мы смогли денормализовать 66м строк примерно за 2 минуты. Как мы увидим позже, это операция, которую мы можем запланировать.

Обратите внимание на использование функции `groupArray` для уменьшения `СвязиПубликаций` в массив для каждого `PostId`, прежде чем выполнить объединение. Этот массив затем фильтруется на два подсписка: `LinkedPosts` и `DuplicatePosts`, которые также исключают любые пустые результаты из внешнего соединения.

Мы можем выбрать некоторые строки, чтобы увидеть нашу новую денормализованную структуру:

```sql
SELECT LinkedPosts, DuplicatePosts
FROM posts_with_links
WHERE (length(LinkedPosts) > 2) AND (length(DuplicatePosts) > 0)
LIMIT 1
FORMAT Vertical

Row 1:
──────
LinkedPosts:    [('2017-04-11 11:53:09.583',3404508),('2017-04-11 11:49:07.680',3922739),('2017-04-11 11:48:33.353',33058004)]
DuplicatePosts: [('2017-04-11 12:18:37.260',3922739),('2017-04-11 12:18:37.260',33058004)]
```

## Организация и планирование денормализации {#orchestrating-and-scheduling-denormalization}

### Пакет {#batch}

Использование денормализации требует процесса преобразования, в котором это может быть выполнено и организовано.

Мы показали выше, как ClickHouse может быть использован для выполнения этого преобразования после загрузки данных через `INSERT INTO SELECT`. Это подходит для периодических пакетных преобразований.

Пользователи имеют несколько опций для организации этого в ClickHouse, при условии, что периодический пакетный процесс загрузки приемлем:

- **[Обновляемые материализованные представления](/materialized-view/refreshable-materialized-view)** - Обновляемые материализованные представления могут использоваться для периодического планирования запроса с результатами, отправляемыми в целевую таблицу. При выполнении запроса представление обеспечивает атомарное обновление целевой таблицы. Это предоставляет нативный способ ClickHouse для планирования этой работы.
- **Внешние инструменты** - Использование инструментов, таких как [dbt](https://www.getdbt.com/) и [Airflow](https://airflow.apache.org/), для периодического планирования преобразования. [Интеграция ClickHouse для dbt](/integrations/dbt) гарантирует, что это выполняется атомарно с созданием новой версии целевой таблицы, которая затем атомарно заменяется версией, получающей запросы (с помощью команды [EXCHANGE](/sql-reference/statements/exchange)).

### Потоковая обработка {#streaming}

Пользователи могут также желать выполнять это за пределами ClickHouse, перед вставкой, используя потоковые технологии, такие как [Apache Flink](https://flink.apache.org/). В качестве альтернативы могут быть использованы инкрементные [материализованные представления](/guides/developer/cascading-materialized-views) для выполнения этого процесса по мере вставки данных.
