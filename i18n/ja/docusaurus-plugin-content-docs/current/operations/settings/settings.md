---
title: セッション設定
sidebar_label: セッション設定
slug: /operations/settings/settings
toc_max_heading_level: 2
---

import ExperimentalBadge from '@theme/badges/ExperimentalBadge';
import BetaBadge from '@theme/badges/BetaBadge';
import CloudAvailableBadge from '@theme/badges/CloudAvailableBadge';

<!-- Autogenerated -->
以下のすべての設定は、テーブル [system.settings](/operations/system-tables/settings) でも利用可能です。これらの設定は、[source](https://github.com/ClickHouse/ClickHouse/blob/master/src/Core/Settings.cpp) から自動生成されています。

## add_http_cors_header {#add_http_cors_header}

Type: Bool

Default value: 0

HTTP CORS ヘッダーを追加します。

## additional_result_filter {#additional_result_filter}

Type: String

Default value: 

`SELECT` クエリの結果に適用する追加のフィルター式です。
この設定は、サブクエリには適用されません。

**例**

``` sql
INSERT INTO table_1 VALUES (1, 'a'), (2, 'bb'), (3, 'ccc'), (4, 'dddd');
SElECT * FROM table_1;
```
```response
┌─x─┬─y────┐
│ 1 │ a    │
│ 2 │ bb   │
│ 3 │ ccc  │
│ 4 │ dddd │
└───┴──────┘
```
```sql
SELECT *
FROM table_1
SETTINGS additional_result_filter = 'x != 2'
```
```response
┌─x─┬─y────┐
│ 1 │ a    │
│ 3 │ ccc  │
│ 4 │ dddd │
└───┴──────┘
```

## additional_table_filters {#additional_table_filters}

Type: Map

Default value: {}

指定されたテーブルから読み込んだ後に適用される追加のフィルター式です。

**例**

``` sql
INSERT INTO table_1 VALUES (1, 'a'), (2, 'bb'), (3, 'ccc'), (4, 'dddd');
SELECT * FROM table_1;
```
```response
┌─x─┬─y────┐
│ 1 │ a    │
│ 2 │ bb   │
│ 3 │ ccc  │
│ 4 │ dddd │
└───┴──────┘
```
```sql
SELECT *
FROM table_1
SETTINGS additional_table_filters = {'table_1': 'x != 2'}
```
```response
┌─x─┬─y────┐
│ 1 │ a    │
│ 3 │ ccc  │
│ 4 │ dddd │
└───┴──────┘
```

## aggregate_functions_null_for_empty {#aggregate_functions_null_for_empty}

Type: Bool

Default value: 0

クエリ内のすべての集約関数を書き換え、[-OrNull](../../sql-reference/aggregate-functions/combinators.md/#agg-functions-combinator-ornull) サフィックスを追加します。SQL 標準の互換性のために有効にします。
これはクエリの書き換えによって実装されており、分散クエリに対して一貫した結果を得るために、[count_distinct_implementation](#count_distinct_implementation) 設定に似ています。

可能な値:

- 0 — 無効。
- 1 — 有効。

**例**

集約関数を含む次のクエリを考えてみましょう：
```sql
SELECT SUM(-1), MAX(0) FROM system.one WHERE 0;
```

`aggregate_functions_null_for_empty = 0` の場合、次の結果が得られます：
```text
┌─SUM(-1)─┬─MAX(0)─┐
│       0 │      0 │
└─────────┴────────┘
```

`aggregate_functions_null_for_empty = 1` の場合、結果は次のようになります：
```text
┌─SUMOrNull(-1)─┬─MAXOrNull(0)─┐
│          NULL │         NULL │
└───────────────┴──────────────┘
```

## aggregation_in_order_max_block_bytes {#aggregation_in_order_max_block_bytes}

Type: UInt64

Default value: 50000000

主キーの順序で集約中に蓄積されるブロックの最大サイズ（バイト）。

## aggregation_memory_efficient_merge_threads {#aggregation_memory_efficient_merge_threads}

Type: UInt64

Default value: 0

メモリ効率の良いモードで中間集約結果をマージするために使用するスレッドの数。より大きければより多くのメモリが消費されます。0 は 'max_threads' と同じです。

## allow_aggregate_partitions_independently {#allow_aggregate_partitions_independently}

Type: Bool

Default value: 0

パーティションキーがグループ化キーに適合する場合、別のスレッドでのパーティションの独立した集約を有効にします。パーティション数がコア数に近く、パーティションのサイズがほぼ同じ場合に有効です。

## allow_archive_path_syntax {#allow_archive_path_syntax}
<ExperimentalBadge/>

Type: Bool

Default value: 1

File\/S3 エンジン/テーブル関数は、アーカイブに正しい拡張子がある場合、'::' を `<archive>::<file>` としてパースします。

## allow_asynchronous_read_from_io_pool_for_merge_tree {#allow_asynchronous_read_from_io_pool_for_merge_tree}

Type: Bool

Default value: 0

MergeTree テーブルから読み込むためにバックグラウンド I/O プールを使用します。この設定は、I/O 制約のあるクエリのパフォーマンスを向上させる可能性があります。

## allow_changing_replica_until_first_data_packet {#allow_changing_replica_until_first_data_packet}

Type: Bool

Default value: 0

有効にすると、ヘッジされたリクエストでは、最初のデータパケットを受信するまでに新しい接続を開始できます。ただし、既に進捗があれば、初めて進捗を表示した後はレプリカの変更を無効にします。

## allow_create_index_without_type {#allow_create_index_without_type}

Type: Bool

Default value: 0

タイプなしで CREATE INDEX クエリを許可します。このクエリは無視されます。SQL 互換性テスト用に作成されました。

## allow_custom_error_code_in_throwif {#allow_custom_error_code_in_throwif}

Type: Bool

Default value: 0

throwIf() 関数内のカスタムエラーコードを有効にします。true の場合、スローされた例外には予期しないエラーコードが付与される可能性があります。

## allow_ddl {#allow_ddl}

Type: Bool

Default value: 1

true に設定されている場合、ユーザーは DDL クエリを実行できます。

## allow_deprecated_database_ordinary {#allow_deprecated_database_ordinary}

Type: Bool

Default value: 0

非推奨の Ordinary エンジンでデータベースを作成することを許可します。

## allow_deprecated_error_prone_window_functions {#allow_deprecated_error_prone_window_functions}

Type: Bool

Default value: 0

（neighbor、runningAccumulate、runningDifferenceStartingWithFirstValue、runningDifference）などのエラーが発生しやすいウィンドウ関数の使用を許可します。

## allow_deprecated_snowflake_conversion_functions {#allow_deprecated_snowflake_conversion_functions}

Type: Bool

Default value: 0

`snowflakeToDateTime`、`snowflakeToDateTime64`、`dateTimeToSnowflake`、`dateTime64ToSnowflake` 関数は非推奨でデフォルトで無効になっています。
代わりに `snowflakeIDToDateTime`、`snowflakeIDToDateTime64`、`dateTimeToSnowflakeID`、`dateTime64ToSnowflakeID` 関数を使用してください。

移行期間中に非推奨の関数を再度有効にするには、この設定を true に設定してください。

## allow_deprecated_syntax_for_merge_tree {#allow_deprecated_syntax_for_merge_tree}

Type: Bool

Default value: 0

非推奨エンジン定義構文で *MergeTree テーブルを作成することを許可します。

## allow_distributed_ddl {#allow_distributed_ddl}

Type: Bool

Default value: 1

true に設定されている場合、ユーザーは分散 DDL クエリを実行できます。

## allow_drop_detached {#allow_drop_detached}

Type: Bool

Default value: 0

ALTER TABLE ... DROP DETACHED PART[ITION] ... クエリを許可します。

## allow_execute_multiif_columnar {#allow_execute_multiif_columnar}

Type: Bool

Default value: 1

multiIf 関数を列指向で実行することを許可します。

## allow_experimental_analyzer {#allow_experimental_analyzer}

Type: Bool

Default value: 1

新しいクエリアナライザーを許可します。

## allow_experimental_codecs {#allow_experimental_codecs}
<ExperimentalBadge/>

Type: Bool

Default value: 0

true に設定すると、実験的な圧縮コーデックを指定できるようになります（現在は利用できないため、このオプションは何もしません）。

## allow_experimental_database_iceberg {#allow_experimental_database_iceberg}
<ExperimentalBadge/>

Type: Bool

Default value: 0

実験的なデータベースエンジン Iceberg を許可します。

## allow_experimental_database_materialized_postgresql {#allow_experimental_database_materialized_postgresql}
<ExperimentalBadge/>

Type: Bool

Default value: 0

Engine=MaterializedPostgreSQL(...) でデータベースを作成することを許可します。

## allow_experimental_dynamic_type {#allow_experimental_dynamic_type}
<BetaBadge/>

Type: Bool

Default value: 0

[Dynamic](../../sql-reference/data-types/dynamic.md) データタイプの作成を許可します。

## allow_experimental_full_text_index {#allow_experimental_full_text_index}
<ExperimentalBadge/>

Type: Bool

Default value: 0

true に設定すると、実験的な全文インデックスを使用できるようになります。

## allow_experimental_funnel_functions {#allow_experimental_funnel_functions}
<ExperimentalBadge/>

Type: Bool

Default value: 0

ファネル分析用の実験的な関数を有効にします。

## allow_experimental_hash_functions {#allow_experimental_hash_functions}
<ExperimentalBadge/>

Type: Bool

Default value: 0

実験的なハッシュ関数を有効にします。

## allow_experimental_inverted_index {#allow_experimental_inverted_index}
<ExperimentalBadge/>

Type: Bool

Default value: 0

true に設定すると、実験的な逆インデックスを使用できるようになります。

## allow_experimental_join_condition {#allow_experimental_join_condition}
<ExperimentalBadge/>

Type: Bool

Default value: 0

左テーブルと右テーブルの両方のカラムを含む不等条件での結合をサポートします。例： `t1.y < t2.y`。

## allow_experimental_join_right_table_sorting {#allow_experimental_join_right_table_sorting}
<ExperimentalBadge/>

Type: Bool

Default value: 0

true に設定され、`join_to_sort_minimum_perkey_rows` と `join_to_sort_maximum_table_rows` の条件が満たされる場合、左または内部ハッシュ結合のパフォーマンスを向上させるために、キーで右テーブルの並べ替えを行います。

## allow_experimental_json_type {#allow_experimental_json_type}
<BetaBadge/>

Type: Bool

Default value: 0

[JSON](../../sql-reference/data-types/newjson.md) データタイプの作成を許可します。

## allow_experimental_kafka_offsets_storage_in_keeper {#allow_experimental_kafka_offsets_storage_in_keeper}
<ExperimentalBadge/>

Type: Bool

Default value: 0

Kafka に関連するオフセットを ClickHouse Keeper に保存するための実験的な機能を許可します。有効にすると、ClickHouse Keeper のパスとレプリカ名を Kafka テーブルエンジンに指定できます。その結果、通常の Kafka エンジンの代わりに、コミットされたオフセットを主に ClickHouse Keeper に保存する新しいタイプのストレージエンジンが使用されます。

## allow_experimental_kusto_dialect {#allow_experimental_kusto_dialect}
<ExperimentalBadge/>

Type: Bool

Default value: 0

Kusto Query Language (KQL) - SQL の代替を有効にします。

## allow_experimental_live_view {#allow_experimental_live_view}
<ExperimentalBadge/>

Type: Bool

Default value: 0

非推奨の LIVE VIEW の作成を許可します。

可能な値：

- 0 — ライブビューの操作が無効になります。
- 1 — ライブビューの操作が有効になります。

## allow_experimental_materialized_postgresql_table {#allow_experimental_materialized_postgresql_table}
<ExperimentalBadge/>

Type: Bool

Default value: 0

MaterializedPostgreSQL テーブルエンジンを使用することを許可します。デフォルトでは無効です。これは実験的な機能です。

## allow_experimental_nlp_functions {#allow_experimental_nlp_functions}
<ExperimentalBadge/>

Type: Bool

Default value: 0

自然言語処理のための実験的な関数を有効にします。

## allow_experimental_object_type {#allow_experimental_object_type}
<ExperimentalBadge/>

Type: Bool

Default value: 0

廃止されたオブジェクトデータタイプを許可します。

## allow_experimental_parallel_reading_from_replicas {#allow_experimental_parallel_reading_from_replicas}
<BetaBadge/>

Type: UInt64

Default value: 0

SELECT クエリの実行のために、各シャードから最大 `max_parallel_replicas` 数のレプリカを使用します。読み取りは並行して行われ、動的に調整されます。0 - 無効、1 - 有効（失敗の場合は静かに無効）、2 - 有効（失敗の場合は例外をスロー）。

## allow_experimental_prql_dialect {#allow_experimental_prql_dialect}
<ExperimentalBadge/>

Type: Bool

Default value: 0

PRQL - SQL の代替を有効にします。

## allow_experimental_query_deduplication {#allow_experimental_query_deduplication}
<ExperimentalBadge/>

Type: Bool

Default value: 0

部分 UUID に基づく SELECT クエリのための実験的なデータ重複排除機能。

## allow_experimental_shared_set_join {#allow_experimental_shared_set_join}
<ExperimentalBadge/>

Type: Bool

Default value: 0

ClickHouse Cloud のみ。ShareSet および SharedJoin の作成を許可します。

## allow_experimental_statistics {#allow_experimental_statistics}
<ExperimentalBadge/>

Type: Bool

Default value: 0

[統計](../../engines/table-engines/mergetree-family/mergetree.md/#table_engine-mergetree-creating-a-table) を定義し、[統計を操作する](../../engines/table-engines/mergetree-family/mergetree.md/#column-statistics)ことを許可します。

## allow_experimental_time_series_table {#allow_experimental_time_series_table}
<ExperimentalBadge/>

Type: Bool

Default value: 0

[TimeSeries](../../engines/table-engines/integrations/time-series.md) テーブルエンジンを使用してテーブルを作成することを許可します。

可能な値：

- 0 — [TimeSeries](../../engines/table-engines/integrations/time-series.md) テーブルエンジンが無効になります。
- 1 — [TimeSeries](../../engines/table-engines/integrations/time-series.md) テーブルエンジンが有効になります。

## allow_experimental_ts_to_grid_aggregate_function {#allow_experimental_ts_to_grid_aggregate_function}
<ExperimentalBadge/>

Type: Bool

Default value: 0

Prometheus のような時系列のリサンプリングのための実験的な tsToGrid 集約関数。Cloud のみ。

## allow_experimental_variant_type {#allow_experimental_variant_type}
<BetaBadge/>

Type: Bool

Default value: 0

[Variant](../../sql-reference/data-types/variant.md) データタイプの作成を許可します。

## allow_experimental_vector_similarity_index {#allow_experimental_vector_similarity_index}
<ExperimentalBadge/>

Type: Bool

Default value: 0

実験的なベクトル類似性インデックスを許可します。

## allow_experimental_window_view {#allow_experimental_window_view}
<ExperimentalBadge/>

Type: Bool

Default value: 0

WINDOW VIEW を有効にします。成熟していません。

## allow_general_join_planning {#allow_general_join_planning}

Type: Bool

Default value: 1

より複雑な条件を処理できる一般的な結合計画アルゴリズムを許可しますが、ハッシュ結合でのみ動作します。ハッシュ結合が有効でない場合、この設定の値に関係なく、通常の結合計画アルゴリズムが使用されます。

## allow_get_client_http_header {#allow_get_client_http_header}

Type: Bool

Default value: 0

現在の HTTP リクエストのヘッダーの値を取得できる `getClientHTTPHeader` 関数の使用を許可します。セキュリティ上の理由からデフォルトでは無効にされており、`Cookie` などの一部のヘッダーは敏感な情報を含む可能性があります。なお、`X-ClickHouse-*` および `Authentication` ヘッダーは常に制限されており、この関数では取得できません。

## allow_hyperscan {#allow_hyperscan}

Type: Bool

Default value: 1

Hyperscan ライブラリを使用する関数を許可します。長いコンパイル時間や過剰なリソース使用を避けるために無効にします。

## allow_introspection_functions {#allow_introspection_functions}

Type: Bool

Default value: 0

クエリプロファイリングのための [内部調査機能](../../sql-reference/functions/introspection.md) を有効または無効にします。

可能な値：

- 1 — 内部調査機能が有効。
- 0 — 内部調査機能が無効。

**参照**

- [Sampling Query Profiler](../../operations/optimizing-performance/sampling-query-profiler.md)
- システムテーブル [trace_log](../../operations/system-tables/trace_log.md/#system_tables-trace_log)

## allow_materialized_view_with_bad_select {#allow_materialized_view_with_bad_select}

Type: Bool

Default value: 1

存在しないテーブルやカラムを参照する SELECT クエリを使用して CREATE MATERIALIZED VIEW を許可します。それでも文法的に有効でなければなりません。リフレッシュ可能な MV には適用されません。SELECT クエリから MV スキーマが推測される必要がある場合（つまり、CREATE にカラムリストや TO テーブルがない場合）にも適用されません。ソーステーブルの作成前に MV を作成するために使用できます。

## allow_named_collection_override_by_default {#allow_named_collection_override_by_default}

Type: Bool

Default value: 1

デフォルトで、名前付きコレクションのフィールドを上書きすることを許可します。

## allow_non_metadata_alters {#allow_non_metadata_alters}

Type: Bool

Default value: 1

テーブルのメタデータだけでなく、ディスク上のデータにも影響を与えるALTER 操作を実行することを許可します。

## allow_nonconst_timezone_arguments {#allow_nonconst_timezone_arguments}

Type: Bool

Default value: 0

toTimeZone()、fromUnixTimestamp*()、snowflakeToDateTime*() などの特定の時間関連関数で非固定のタイムゾーン引数を許可します。

## allow_nondeterministic_mutations {#allow_nondeterministic_mutations}

Type: Bool

Default value: 0

ユーザーレベルの設定で、レプリケートされたテーブルの更新が非決定的な関数（例： `dictGet`）を使用できるようにします。

例えば、辞書がノード間で同期されていない場合、それから値を取得するような変更はデフォルトではレプリケートされたテーブルで禁止されています。この設定を有効にすると、このような動作が許可され、全ノード間で使用されるデータが同期されていることをユーザーが確認する責任を負います。

**例**

``` xml
<profiles>
    <default>
        <allow_nondeterministic_mutations>1</allow_nondeterministic_mutations>

        <!-- ... -->
    </default>

    <!-- ... -->

</profiles>
```

## allow_nondeterministic_optimize_skip_unused_shards {#allow_nondeterministic_optimize_skip_unused_shards}

Type: Bool

Default value: 0

シャーディングキーに非決定的（`rand` や `dictGet` など、後者は更新時に問題があるため）な関数を許可します。

可能な値：

- 0 — 禁止。
- 1 — 許可。

## allow_not_comparable_types_in_comparison_functions {#allow_not_comparable_types_in_comparison_functions}

Type: Bool

Default value: 0

比較関数 `equal/less/greater/etc` において、比較できない型（例えば JSON/オブジェクト/集約関数）の使用を許可または制限します。

## allow_not_comparable_types_in_order_by {#allow_not_comparable_types_in_order_by}

Type: Bool

Default value: 0

ORDER BY キーにおいて、比較できない型（例えば JSON/オブジェクト/集約関数）の使用を許可または制限します。

## allow_prefetched_read_pool_for_local_filesystem {#allow_prefetched_read_pool_for_local_filesystem}

Type: Bool

Default value: 0

すべてのパーツがローカルファイルシステムにある場合、プリーフェッチしたスレッドプールを優先します。

## allow_prefetched_read_pool_for_remote_filesystem {#allow_prefetched_read_pool_for_remote_filesystem}

Type: Bool

Default value: 1

すべてのパーツがリモートファイルシステムにある場合、プリーフェッチしたスレッドプールを優先します。

## allow_push_predicate_ast_for_distributed_subqueries {#allow_push_predicate_ast_for_distributed_subqueries}

Type: Bool

Default value: 1

有効化されたアナライザーに対して、分散サブクエリのASTレベルでのプッシュ条件を許可します。

## allow_push_predicate_when_subquery_contains_with {#allow_push_predicate_when_subquery_contains_with}

Type: Bool

Default value: 1

WITH 句を含むサブクエリの条件をプッシュすることを許可します。

## allow_reorder_prewhere_conditions {#allow_reorder_prewhere_conditions}

Type: Bool

Default value: 1

WHERE から PREWHERE に条件を移動するとき、フィルタリングの最適化のためにそれらを再配置することを許可します。

## allow_settings_after_format_in_insert {#allow_settings_after_format_in_insert}

Type: Bool

Default value: 0

`INSERT` クエリの `FORMAT` 後に `SETTINGS` が許可されるかどうかを制御します。これは推奨されていません。なぜなら、`SETTINGS` の一部を値として解釈する可能性があるからです。

例：

```sql
INSERT INTO FUNCTION null('foo String') SETTINGS max_threads=1 VALUES ('bar');
```

しかし、以下のクエリは `allow_settings_after_format_in_insert` の場合のみ機能します：

```sql
SET allow_settings_after_format_in_insert=1;
INSERT INTO FUNCTION null('foo String') VALUES ('bar') SETTINGS max_threads=1;
```

可能な値：

- 0 — 禁止。
- 1 — 許可。

:::note
この設定は、以前の構文に依存するユースケースがある場合にのみ、後方互換性のために使用してください。
:::

## allow_simdjson {#allow_simdjson}

Type: Bool

Default value: 1

AVX2 命令が使用可能な場合、'JSON*' 関数で simdjson ライブラリを使用することを許可します。無効にすると rapidjson が使用されます。

## allow_statistics_optimize {#allow_statistics_optimize}
<ExperimentalBadge/>

Type: Bool

Default value: 0

クエリを最適化するために統計を使用することを許可します。

## allow_suspicious_codecs {#allow_suspicious_codecs}

Type: Bool

Default value: 0

true に設定されている場合、意味のない圧縮コーデックを指定できるようになります。

## allow_suspicious_fixed_string_types {#allow_suspicious_fixed_string_types}

Type: Bool

Default value: 0

CREATE TABLE 文において、n > 256 の FixedString(n) 型のカラムを作成することを許可します。長さ >= 256 の FixedString は疑わしく、誤用を示している可能性があります。

## allow_suspicious_indices {#allow_suspicious_indices}

Type: Bool

Default value: 0

同一の式を持つ主キー/補助キーおよびソートキーを拒否します。

## allow_suspicious_low_cardinality_types {#allow_suspicious_low_cardinality_types}

Type: Bool

Default value: 0

8 バイト以下の固定サイズのデータ型（数値型および `FixedString(8_bytes_or_less)`）と一緒に [LowCardinality](../../sql-reference/data-types/lowcardinality.md) を使用することを許可または制限します。

小さな固定値では `LowCardinality` の使用が通常は非効率的です。ClickHouse は各行に対して数値インデックスを保存するためです。結果として：

- ディスク領域の使用が増加する可能性があります。
- 辞書のサイズに応じてRAM 含む消費が高くなる可能性があります。
- 余分なコーディング/エンコーディング操作のために一部の関数が遅くなる可能性があります。

MergeTree エンジンテーブルのマージ時間は、上記に記載されたすべての理由により増加する可能性があります。

可能な値：

- 1 — `LowCardinality` の使用は制限されない。
- 0 — `LowCardinality` の使用は制限される。

## allow_suspicious_primary_key {#allow_suspicious_primary_key}

Type: Bool

Default value: 0

MergeTree に対する疑わしい `PRIMARY KEY`/`ORDER BY` を許可します（例：SimpleAggregateFunction）。

## allow_suspicious_ttl_expressions {#allow_suspicious_ttl_expressions}

Type: Bool

Default value: 0

テーブルのカラムに依存しない TTL 式を拒否します。これはほとんどの場合、ユーザーのエラーを示しています。

## allow_suspicious_types_in_group_by {#allow_suspicious_types_in_group_by}

Type: Bool

Default value: 0

GROUP BY キーにおいて、[Variant](../../sql-reference/data-types/variant.md) および [Dynamic](../../sql-reference/data-types/dynamic.md) 型の使用を許可または制限します。

## allow_suspicious_types_in_order_by {#allow_suspicious_types_in_order_by}

Type: Bool

Default value: 0

ORDER BY キーにおいて、[Variant](../../sql-reference/data-types/variant.md) および [Dynamic](../../sql-reference/data-types/dynamic.md) 型の使用を許可または制限します。

## allow_suspicious_variant_types {#allow_suspicious_variant_types}

Type: Bool

Default value: 0

CREATE TABLE 文において、類似のバリアント型（例えば、異なる数値型または日付型）で Variant 型を指定することを許可します。この設定を有効にすると、類似の型の値を扱う際にはいくつかのあいまいさが生じる可能性があります。

## allow_unrestricted_reads_from_keeper {#allow_unrestricted_reads_from_keeper}

Type: Bool

Default value: 0

システム.zookeeper テーブルからの無条件（パスに条件がない）読み取りを許可します。便利ですが、安全ではありません。

## alter_move_to_space_execute_async {#alter_move_to_space_execute_async}

Type: Bool

Default value: 0

ALTER TABLE MOVE ... TO [DISK|VOLUME] を非同期的に実行します。

## alter_partition_verbose_result {#alter_partition_verbose_result}

Type: Bool

Default value: 0

パーティションおよびパーツに対して操作が成功裏に適用された情報の表示を有効または無効にします。
[ATTACH PARTITION|PART](../../sql-reference/statements/alter/partition.md/#alter_attach-partition) や [FREEZE PARTITION](../../sql-reference/statements/alter/partition.md/#alter_freeze-partition) に適用されます。

可能な値：

- 0 — 冗長性を無効にする。
- 1 — 冗長性を有効にする。

**例**

```sql
CREATE TABLE test(a Int64, d Date, s String) ENGINE = MergeTree PARTITION BY toYYYYMDECLARE(d) ORDER BY a;
INSERT INTO test VALUES(1, '2021-01-01', '');
INSERT INTO test VALUES(1, '2021-01-01', '');
ALTER TABLE test DETACH PARTITION ID '202101';

ALTER TABLE test ATTACH PARTITION ID '202101' SETTINGS alter_partition_verbose_result = 1;

┌─command_type─────┬─partition_id─┬─part_name────┬─old_part_name─┐
│ ATTACH PARTITION │ 202101       │ 202101_7_7_0 │ 202101_5_5_0  │
│ ATTACH PARTITION │ 202101       │ 202101_8_8_0 │ 202101_6_6_0  │
└──────────────────┴──────────────┴──────────────┴───────────────┘

ALTER TABLE test FREEZE SETTINGS alter_partition_verbose_result = 1;

┌─command_type─┬─partition_id─┬─part_name────┬─backup_name─┬─backup_path───────────────────┬─part_backup_path────────────────────────────────────────────┐
│ FREEZE ALL   │ 202101       │ 202101_7_7_0 │ 8           │ /var/lib/clickhouse/shadow/8/ │ /var/lib/clickhouse/shadow/8/data/default/test/202101_7_7_0 │
│ FREEZE ALL   │ 202101       │ 202101_8_8_0 │ 8           │ /var/lib/clickhouse/shadow/8/ │ /var/lib/clickhouse/shadow/8/data/default/test/202101_8_8_0 │
└──────────────┴──────────────┴──────────────┴─────────────┴───────────────────────────────┴─────────────────────────────────────────────────────────────┘
```

## alter_sync {#alter_sync}

Type: UInt64

Default value: 1

[ALTER](../../sql-reference/statements/alter/index.md)、[OPTIMIZE](../../sql-reference/statements/optimize.md)、または [TRUNCATE](../../sql-reference/statements/truncate.md) クエリによって実行される操作がレプリカで実行されるのを待つように設定できます。

可能な値：

- 0 — 待たない。
- 1 — 自分の実行を待つ。
- 2 — すべてを待つ。

Cloud のデフォルト値: `0`。

:::note
`alter_sync` は `Replicated` テーブルのみに適用され、`Replicated` テーブル以外の ALTER には何の効果もありません。
:::

## analyze_index_with_space_filling_curves {#analyze_index_with_space_filling_curves}

Type: Bool

Default value: 1

テーブルにインデックス内に空間充填曲線がある場合（例：`ORDER BY mortonEncode(x, y)` または `ORDER BY hilbertEncode(x, y)`）、かつクエリにその引数に関する条件がある場合（例：`x >= 10 AND x <= 20 AND y >= 20 AND y <= 30`）、インデックス分析に空間充填曲線を使用します。

## analyzer_compatibility_join_using_top_level_identifier {#analyzer_compatibility_join_using_top_level_identifier}

Type: Bool

Default value: 0

プロジェクションから JOIN USING の識別子を解決するように強制します（例えば、`SELECT a + 1 AS b FROM t1 JOIN t2 USING (b)` では、`t1.a + 1 = t2.b` によって結合され、`t1.b = t2.b` ではありません）。

## any_join_distinct_right_table_keys {#any_join_distinct_right_table_keys}

Type: Bool

Default value: 0

`ANY INNER|LEFT JOIN` 操作での ClickHouse サーバーのレガシー動作を有効にします。

:::note
この設定は、古い `JOIN` 挙動に依存するユースケースがある場合にのみ、後方互換性のために使用してください。
:::

レガシー動作が有効な場合：

- `t1 ANY LEFT JOIN t2` と `t2 ANY RIGHT JOIN t1` の結果は等しくありません。なぜなら、ClickHouse は多対1の左から右へのテーブルキーのマッピングロジックを使うためです。
- `ANY INNER JOIN` の結果は、左テーブルからのすべての行が含まれ、`SEMI LEFT JOIN` 操作のようになります。

レガシー動作が無効な場合：

- `t1 ANY LEFT JOIN t2` と `t2 ANY RIGHT JOIN t1` の結果は等しくなります。なぜなら、ClickHouse は `ANY RIGHT JOIN` 操作において1対多のキーのマッピングロジックを使用するからです。
- `ANY INNER JOIN` の結果は、左テーブルと右テーブルの両方からキーごとに1行を含みます。

可能な値：

- 0 — レガシー動作が無効。
- 1 — レガシー動作が有効。

参照：

- [JOIN の厳密性](../../sql-reference/statements/select/join.md/#join-settings)

## apply_deleted_mask {#apply_deleted_mask}

Type: Bool

Default value: 1

軽量 DELETE で削除された行をフィルタリングすることを有効にします。無効にすると、クエリはこれらの行を読み取ることができます。これはデバッグや「アンデリート」シナリオに役立ちます。

## apply_mutations_on_fly {#apply_mutations_on_fly}

Type: Bool

Default value: 0

true の場合、データパートに反映されていない変更（UPDATE と DELETE）が SELECT に適用されます。

## apply_settings_from_server {#apply_settings_from_server}

Type: Bool

Default value: 1

クライアントがサーバーからの設定を受け入れるべきかどうか。

これはクライアント側で行われる操作にのみ影響します。特に、INSERT 入力データの解析とクエリ結果のフォーマットに影響します。クエリの実行の大部分はサーバー上で行われ、この設定には影響されません。

通常、この設定はユーザープロファイル（users.xml または `ALTER USER` のようなクエリ）で設定されるべきです。クライアントを通じては false に変更できますが、true に変更することはできません（ユーザープロファイルに `apply_settings_from_server = false` がある場合、サーバーは設定を送信しません）。

なお、当初（24.12）にはサーバー設定（`send_settings_to_client`）がありましたが、後にこのクライアント設定に置き換えられ、使いやすさが向上しました。

## asterisk_include_alias_columns {#asterisk_include_alias_columns}

Type: Bool

Default value: 0

ワイルドカードクエリ（`SELECT *`）のために [ALIAS](../../sql-reference/statements/create/table.md/#alias) カラムを含めます。

可能な値：

- 0 - 無効
- 1 - 有効

## asterisk_include_materialized_columns {#asterisk_include_materialized_columns}

Type: Bool

Default value: 0

ワイルドカードクエリ（`SELECT *`）のために [MATERIALIZED](../../sql-reference/statements/create/table.md/#materialized) カラムを含めます。

可能な値：

- 0 - 無効
- 1 - 有効

## async_insert {#async_insert}

Type: Bool

Default value: 0

true の場合、INSERT クエリからのデータはキューに保存され、バックグラウンドでテーブルにフラッシュ処理されます。wait_for_async_insert が false の場合、INSERT クエリはほぼ瞬時に処理されます。それ以外の場合、クライアントはデータがテーブルにフラッシュされるまで待機します。

## async_insert_busy_timeout_decrease_rate {#async_insert_busy_timeout_decrease_rate}

Type: Double

Default value: 0.2

適応的非同期挿入タイムアウトが減少する指数的成長率。

## async_insert_busy_timeout_increase_rate {#async_insert_busy_timeout_increase_rate}

Type: Double

Default value: 0.2

適応的非同期挿入タイムアウトが増加する指数的成長率。

## async_insert_busy_timeout_max_ms {#async_insert_busy_timeout_max_ms}

Type: Milliseconds

（内容は未提供のため、この行はそのままとします。）
デフォルト値: 200

最初のデータが出現してからのクエリごとに収集されたデータをダンプするまでの最大待機時間。

## async_insert_busy_timeout_min_ms {#async_insert_busy_timeout_min_ms}



型: ミリ秒

デフォルト値: 50

async_insert_use_adaptive_busy_timeoutによって自動調整が有効な場合、最初のデータが出現してからのクエリごとの収集データをダンプするまでの最小待機時間。この値は、適応アルゴリズムの初期値としても機能します。

## async_insert_deduplicate {#async_insert_deduplicate}



型: ブール値

デフォルト値: 0

レプリケートテーブルに対する非同期INSERTクエリの場合、挿入ブロックの重複排除を実施することを指定します。

## async_insert_max_data_size {#async_insert_max_data_size}



型: UInt64

デフォルト値: 10485760

挿入される前にクエリごとに収集された未解析データの最大サイズ（バイト）。

## async_insert_max_query_number {#async_insert_max_query_number}



型: UInt64

デフォルト値: 450

挿入される前の最大挿入クエリ数。

## async_insert_poll_timeout_ms {#async_insert_poll_timeout_ms}



型: ミリ秒

デフォルト値: 10

非同期挿入キューからデータをポーリングするためのタイムアウト。

## async_insert_use_adaptive_busy_timeout {#async_insert_use_adaptive_busy_timeout}



型: ブール値

デフォルト値: 1

trueに設定されている場合、非同期挿入に対して適応的なビジータイムアウトを使用します。

## async_query_sending_for_remote {#async_query_sending_for_remote}



型: ブール値

デフォルト値: 1

リモートクエリを実行中に非同期接続の作成とクエリの送信を有効にします。

デフォルトで有効です。

## async_socket_for_remote {#async_socket_for_remote}



型: ブール値

デフォルト値: 1

リモートクエリを実行中にソケットから非同期読み取りを有効にします。

デフォルトで有効です。

## azure_allow_parallel_part_upload {#azure_allow_parallel_part_upload}



型: ブール値

デフォルト値: 1

複数のスレッドを使用してAzureのマルチパートアップロードを実行します。

## azure_check_objects_after_upload {#azure_check_objects_after_upload}



型: ブール値

デフォルト値: 0

Azure Blobストレージにアップロードされた各オブジェクトが正常にアップロードされたことを確認します。

## azure_create_new_file_on_insert {#azure_create_new_file_on_insert}



型: ブール値

デフォルト値: 0

Azureエンジンテーブルでの各挿入時に新しいファイルを作成するかどうかを有効または無効にします。

## azure_ignore_file_doesnt_exist {#azure_ignore_file_doesnt_exist}



型: ブール値

デフォルト値: 0

特定のキーを読む際にファイルが存在しない場合は無視します。

可能な値:
- 1 — `SELECT` は空の結果を返します。
- 0 — `SELECT` は例外を発生させます。

## azure_list_object_keys_size {#azure_list_object_keys_size}



型: UInt64

デフォルト値: 1000

ListObjectリクエストによってバッチで返される可能性のあるファイルの最大数。

## azure_max_blocks_in_multipart_upload {#azure_max_blocks_in_multipart_upload}



型: UInt64

デフォルト値: 50000

Azureのマルチパートアップロードでのブロックの最大数。

## azure_max_inflight_parts_for_one_file {#azure_max_inflight_parts_for_one_file}



型: UInt64

デフォルト値: 20

マルチパートアップロードリクエストで同時に読み込まれるパーツの最大数。0は無制限を意味します。

## azure_max_single_part_copy_size {#azure_max_single_part_copy_size}



型: UInt64

デフォルト値: 268435456

シングルパートコピーを使用してAzure Blobストレージにコピーするオブジェクトの最大サイズ。

## azure_max_single_part_upload_size {#azure_max_single_part_upload_size}



型: UInt64

デフォルト値: 104857600

シングルパートアップロードを使用してAzure Blobストレージにアップロードするオブジェクトの最大サイズ。

## azure_max_single_read_retries {#azure_max_single_read_retries}



型: UInt64

デフォルト値: 4

Azure Blobストレージの単一読み取り中の最大リトライ回数。

## azure_max_unexpected_write_error_retries {#azure_max_unexpected_write_error_retries}



型: UInt64

デフォルト値: 4

Azure Blobストレージの書き込み中の予期しないエラーが発生した場合の最大リトライ回数。

## azure_max_upload_part_size {#azure_max_upload_part_size}



型: UInt64

デフォルト値: 5368709120

Azure Blobストレージへのマルチパートアップロード中にアップロードされるパートの最大サイズ。

## azure_min_upload_part_size {#azure_min_upload_part_size}



型: UInt64

デフォルト値: 16777216

Azure Blobストレージへのマルチパートアップロード中にアップロードされるパートの最小サイズ。

## azure_sdk_max_retries {#azure_sdk_max_retries}



型: UInt64

デフォルト値: 10

Azure SDKの最大リトライ回数。

## azure_sdk_retry_initial_backoff_ms {#azure_sdk_retry_initial_backoff_ms}



型: UInt64

デフォルト値: 10

Azure SDKでのリトライ間の最小バックオフ時間（ミリ秒）。

## azure_sdk_retry_max_backoff_ms {#azure_sdk_retry_max_backoff_ms}



型: UInt64

デフォルト値: 1000

Azure SDKでのリトライ間の最大バックオフ時間（ミリ秒）。

## azure_skip_empty_files {#azure_skip_empty_files}



型: ブール値

デフォルト値: 0

S3エンジンで空のファイルをスキップするかどうかを有効または無効にします。

可能な値:
- 0 — 空のファイルがリクエストされたフォーマットと互換性がない場合、`SELECT`は例外を発生させます。
- 1 — 空のファイルに対して`SELECT`は空の結果を返します。

## azure_strict_upload_part_size {#azure_strict_upload_part_size}



型: UInt64

デフォルト値: 0

Azure Blobストレージへのマルチパートアップロード中にアップロードされるパートの正確なサイズ。

## azure_throw_on_zero_files_match {#azure_throw_on_zero_files_match}



型: ブール値

デフォルト値: 0

グロブ展開ルールに従って一致するファイルがゼロの場合にエラーを発生させます。

可能な値:
- 1 — `SELECT`は例外を発生させます。
- 0 — `SELECT`は空の結果を返します。

## azure_truncate_on_insert {#azure_truncate_on_insert}



型: ブール値

デフォルト値: 0

Azureエンジンテーブルに挿入する前に切り詰めを有効または無効にします。

## azure_upload_part_size_multiply_factor {#azure_upload_part_size_multiply_factor}



型: UInt64

デフォルト値: 2

azure_multiply_parts_count_threshold分のパーツがAzure Blobストレージにアップロードされるたびに、azure_min_upload_part_sizeをこの因子で乗算します。

## azure_upload_part_size_multiply_parts_count_threshold {#azure_upload_part_size_multiply_parts_count_threshold}



型: UInt64

デフォルト値: 500

この数のパーツがAzure Blobストレージにアップロードされるたびに、azure_min_upload_part_sizeはazure_upload_part_size_multiply_factorで乗算されます。

## backup_restore_batch_size_for_keeper_multi {#backup_restore_batch_size_for_keeper_multi}



型: UInt64

デフォルト値: 1000

バックアップまたはリストア中に[Zoo]Keeperへのマルチリクエストの最大バッチサイズ。

## backup_restore_batch_size_for_keeper_multiread {#backup_restore_batch_size_for_keeper_multiread}



型: UInt64

デフォルト値: 10000

バックアップまたはリストア中に[Zoo]Keeperへのマルチリードリクエストの最大バッチサイズ。

## backup_restore_failure_after_host_disconnected_for_seconds {#backup_restore_failure_after_host_disconnected_for_seconds}



型: UInt64

デフォルト値: 3600

バックアップまたはクラスターのリストア操作中にホストがこの時間の間、ZooKeeper内でそのエフェメラルな『alive』ノードを再作成しなかった場合、全体のバックアップまたはリストアは失敗したと見なされます。
この値は、ホストが失敗後にZooKeeperに再接続するのにかかる合理的な時間よりも大きくする必要があります。
ゼロは無制限を意味します。

## backup_restore_finish_timeout_after_error_sec {#backup_restore_finish_timeout_after_error_sec}



型: UInt64

デフォルト値: 180

イニシエーターが他のホストが「エラー」ノードに反応し、現在のバックアップまたはリストア操作を停止するのを待つ時間。

## backup_restore_keeper_fault_injection_probability {#backup_restore_keeper_fault_injection_probability}



型: Float

デフォルト値: 0

バックアップまたはリストア中のKeeperリクエストに対する故障注入の約確率。有効な値は[0.0f, 1.0f]の間です。

## backup_restore_keeper_fault_injection_seed {#backup_restore_keeper_fault_injection_seed}



型: UInt64

デフォルト値: 0

0 - ランダムシード、さもなければ設定値。

## backup_restore_keeper_max_retries {#backup_restore_keeper_max_retries}



型: UInt64

デフォルト値: 1000

バックアップまたはリストア操作中の[Zoo]Keeper操作の最大リトライ回数。
一時的な[Zoo]Keeperの失敗のために全体の操作が失敗しないように十分大きくする必要があります。

## backup_restore_keeper_max_retries_while_handling_error {#backup_restore_keeper_max_retries_while_handling_error}



型: UInt64

デフォルト値: 20

バックアップまたはリストア操作のエラーを処理中の[Zoo]Keeper操作の最大リトライ回数。

## backup_restore_keeper_max_retries_while_initializing {#backup_restore_keeper_max_retries_while_initializing}



型: UInt64

デフォルト値: 20

バックアップまたはリストア操作の初期化中の[Zoo]Keeper操作の最大リトライ回数。

## backup_restore_keeper_retry_initial_backoff_ms {#backup_restore_keeper_retry_initial_backoff_ms}



型: UInt64

デフォルト値: 100

バックアップまたはリストア中の[Zoo]Keeper操作の初期バックオフタイムアウト。

## backup_restore_keeper_retry_max_backoff_ms {#backup_restore_keeper_retry_max_backoff_ms}



型: UInt64

デフォルト値: 5000

バックアップまたはリストア中の[Zoo]Keeper操作の最大バックオフタイムアウト。

## backup_restore_keeper_value_max_size {#backup_restore_keeper_value_max_size}



型: UInt64

デフォルト値: 1048576

バックアップ中の[Zoo]Keeperノードのデータの最大サイズ。

## backup_restore_s3_retry_attempts {#backup_restore_s3_retry_attempts}



型: UInt64

デフォルト値: 1000

Aws::Client::RetryStrategyの設定、Aws::Clientが自動的にリトライします。0はリトライなしを意味します。これはバックアップ/復元のみに該当します。

## cache_warmer_threads {#cache_warmer_threads}



型: UInt64

デフォルト値: 4

ClickHouse Cloudにのみ利用可能。ファイルキャッシュに新しいデータパーツを推測的にダウンロードするためのバックグラウンドスレッドの数、[cache_populated_by_fetch](merge-tree-settings.md/#cache_populated_by_fetch)が有効な場合。ゼロは無効にします。

## calculate_text_stack_trace {#calculate_text_stack_trace}



型: ブール値

デフォルト値: 1

クエリ実行中の例外が発生した場合にテキストスタックトレースを計算します。これはデフォルトであり、無効にするとパフォーマンスが低下する可能性があります。

## cancel_http_readonly_queries_on_client_close {#cancel_http_readonly_queries_on_client_close}



型: ブール値

デフォルト値: 0

クライアントが応答を待たずに接続を閉じると、HTTP読み取り専用クエリ（例えば、SELECT）をキャンセルします。

クラウドデフォルト値: `1`。

## cast_ipv4_ipv6_default_on_conversion_error {#cast_ipv4_ipv6_default_on_conversion_error}



型: ブール値

デフォルト値: 0

CAST演算子をIPv4、CAST演算子をIPV6型として、toIPv4、toIPv6関数は、変換エラーの際に例外をスローする代わりにデフォルト値を返します。

## cast_keep_nullable {#cast_keep_nullable}



型: ブール値

デフォルト値: 0

[CAST](../../sql-reference/functions/type-conversion-functions.md/#castx-t)操作において`Nullable`データ型の保持を有効または無効にします。

設定が有効な場合、`CAST`関数の引数が`Nullable`であるとき、結果も`Nullable`型に変換されます。設定が無効な場合、結果は常に正確に宛先型になります。

可能な値：

- 0 — `CAST`結果は指定された宛先型を正確に持ちます。
- 1 — 引数型が`Nullable`の場合、`CAST`結果は`Nullable(DestinationDataType)`に変換されます。

**例**

以下のクエリ結果は宛先データ型を正確に示します：

```sql
SET cast_keep_nullable = 0;
SELECT CAST(toNullable(toInt32(0)) AS Int32) as x, toTypeName(x);
```

結果：

```text
┌─x─┬─toTypeName(CAST(toNullable(toInt32(0)), 'Int32'))─┐
│ 0 │ Int32                                             │
└───┴───────────────────────────────────────────────────┘
```

以下のクエリ結果は宛先データ型に`Nullable`修飾子を示します：

```sql
SET cast_keep_nullable = 1;
SELECT CAST(toNullable(toInt32(0)) AS Int32) as x, toTypeName(x);
```

結果：

```text
┌─x─┬─toTypeName(CAST(toNullable(toInt32(0)), 'Int32'))─┐
│ 0 │ Nullable(Int32)                                   │
└───┴───────────────────────────────────────────────────┘
```

**参照**

- [CAST](../../sql-reference/functions/type-conversion-functions.md/#type_conversion_function-cast)関数

## cast_string_to_dynamic_use_inference {#cast_string_to_dynamic_use_inference}



型: ブール値

デフォルト値: 0

StringからDynamicへの変換中に型推論を使用します。

## check_query_single_value_result {#check_query_single_value_result}



型: ブール値

デフォルト値: 1

`MergeTree`ファミリーエンジンの[CHECK TABLE](../../sql-reference/statements/check-table.md/#checking-mergetree-tables)クエリ結果の詳細度を定義します。

可能な値：

- 0 — クエリはテーブルの各データパートのチェック状態を表示します。
- 1 — クエリはテーブルの一般的なチェック状態を表示します。

## check_referential_table_dependencies {#check_referential_table_dependencies}



型: ブール値

デフォルト値: 0

DDLクエリ（DROP TABLEやRENAMEなど）が参照依存関係を壊さないことを確認します。

## check_table_dependencies {#check_table_dependencies}



型: ブール値

デフォルト値: 1

DDLクエリ（DROP TABLEやRENAMEなど）が依存関係を壊さないことを確認します。

## checksum_on_read {#checksum_on_read}



型: ブール値

デフォルト値: 1

読み取り時にチェックサムを検証します。デフォルトで有効であり、常に本番環境で有効にしておくべきです。この設定を無効にすることに期待できる利益はありません。実験やベンチマークのみに使用されるべきです。この設定はMergeTreeファミリーのテーブルにのみ適用されます。ネットワーク越しにデータを受け取る際には、他のテーブルエンジンに対しても常にチェックサムが検証されます。

## cloud_mode {#cloud_mode}



型: ブール値

デフォルト値: 0

クラウドモード。

## cloud_mode_database_engine {#cloud_mode_database_engine}



型: UInt64

デフォルト値: 1

クラウドで許可されるデータベースエンジン。1 - DDLをReplicatedデータベースに書き換え、2 - DDLをSharedデータベースに書き換えます。

## cloud_mode_engine {#cloud_mode_engine}



型: UInt64

デフォルト値: 1

クラウドで許可されるエンジンファミリー。0 - すべてを許可、1 - DDLを*ReplicatedMergeTreeに書き換え、2 - DDLをSharedMergeTreeに書き換えます。公開部分を最小限にするためにUInt64型。

## cluster_for_parallel_replicas {#cluster_for_parallel_replicas}
<BetaBadge/>


型: 文字列

デフォルト値: 

現在のサーバが位置するシャードのクラスター。

## collect_hash_table_stats_during_aggregation {#collect_hash_table_stats_during_aggregation}



型: ブール値

デフォルト値: 1

メモリ割り当てを最適化するためにハッシュテーブルの統計を収集することを有効にします。

## collect_hash_table_stats_during_joins {#collect_hash_table_stats_during_joins}



型: ブール値

デフォルト値: 1

メモリ割り当てを最適化するためにハッシュテーブルの統計を収集することを有効にします。

## compatibility {#compatibility}



型: 文字列

デフォルト値: 

`compatibility`設定はClickHouseに以前のバージョンのデフォルト設定を使用させます。この以前のバージョンは設定として提供されます。

もし設定がデフォルトではない値に設定されている場合、それらの設定は尊重されます（変更されていない設定のみが`compatibility`設定の影響を受けます）。

この設定はClickHouseのバージョン番号を文字列として受け入れます。例えば、`22.3`、`22.8`。空の値はこの設定が無効であることを意味します。

デフォルトで無効です。

:::note
ClickHouse Cloudでは、互換性設定はClickHouse Cloudサポートによって設定される必要があります。設定してもらうには[ケースを開いて](https://clickhouse.cloud/support)ください。
:::

## compatibility_ignore_auto_increment_in_create_table {#compatibility_ignore_auto_increment_in_create_table}



型: ブール値

デフォルト値: 0

真の場合、カラム宣言でAUTO_INCREMENTキーワードを無視します。そうでない場合、エラーを返します。MySQLからの移行を簡素化します。

## compatibility_ignore_collation_in_create_table {#compatibility_ignore_collation_in_create_table}



型: ブール値

デフォルト値: 1

CREATE TABLEにおける照合を無視する互換性設定。

## compile_aggregate_expressions {#compile_aggregate_expressions}



型: ブール値

デフォルト値: 1

集約関数のネイティブコードへのJITコンパイルを有効または無効にします。この設定を有効にするとパフォーマンスが向上する可能性があります。

可能な値：

- 0 — 集約はJITコンパイルなしで実行されます。
- 1 — 集約はJITコンパイルを使用して実行されます。

**参照**

- [min_count_to_compile_aggregate_expression](#min_count_to_compile_aggregate_expression)

## compile_expressions {#compile_expressions}



型: ブール値

デフォルト値: 0

いくつかのスカラ関数と演算子をネイティブコードにコンパイルします。LLVMコンパイラインフラストラクチャのバグのため、AArch64マシンではnullptrのデリファレンスを引き起こすことが知られており、その結果サーバがクラッシュします。この設定は有効にしないでください。

## compile_sort_description {#compile_sort_description}



型: ブール値

デフォルト値: 1

ソート記述をネイティブコードにコンパイルします。

## connect_timeout {#connect_timeout}



型: 秒

デフォルト値: 10

レプリカがない場合の接続タイムアウト。

## connect_timeout_with_failover_ms {#connect_timeout_with_failover_ms}



型: ミリ秒

デフォルト値: 1000

クラスタ定義に「shard」と「replica」セクションが使用されている場合、分散テーブルエンジン用のリモートサーバへの接続のタイムアウト（ミリ秒）。
接続が失敗した場合、さまざまなレプリカへの接続を試みます。

## connect_timeout_with_failover_secure_ms {#connect_timeout_with_failover_secure_ms}



型: ミリ秒

デフォルト値: 1000

最初の健全なレプリカを選択する際の接続タイムアウト（セキュア接続用）。

## connection_pool_max_wait_ms {#connection_pool_max_wait_ms}



型: ミリ秒

デフォルト値: 0

接続プールが満杯のとき、接続を待機するための時間（ミリ秒）。

可能な値：

- 正の整数。
- 0 — 無限タイムアウト。

## connections_with_failover_max_tries {#connections_with_failover_max_tries}



型: UInt64

デフォルト値: 3

分散テーブルエンジンの各レプリカへの接続試行の最大数。

## convert_query_to_cnf {#convert_query_to_cnf}



型: ブール値

デフォルト値: 0

`true`に設定すると、`SELECT`クエリは標準形（CNF）に変換されます。CNFにクエリを再書き換えることで、実行が速くなるシナリオがあります（この[Github issue](https://github.com/ClickHouse/ClickHouse/issues/11749)を参照してください）。

例えば、次の`SELECT`クエリが変更されていないことに注目してください（デフォルトの動作）：

```sql
EXPLAIN SYNTAX
SELECT *
FROM
(
    SELECT number AS x
    FROM numbers(20)
) AS a
WHERE ((x >= 1) AND (x <= 5)) OR ((x >= 10) AND (x <= 15))
SETTINGS convert_query_to_cnf = false;
```

結果は次の通りです：

```response
┌─explain────────────────────────────────────────────────────────┐
│ SELECT x                                                       │
│ FROM                                                           │
│ (                                                              │
│     SELECT number AS x                                         │
│     FROM numbers(20)                                           │
│     WHERE ((x >= 1) AND (x <= 5)) OR ((x >= 10) AND (x <= 15)) │
│ ) AS a                                                         │
│ WHERE ((x >= 1) AND (x <= 5)) OR ((x >= 10) AND (x <= 15))     │
│ SETTINGS convert_query_to_cnf = 0                              │
└────────────────────────────────────────────────────────────────┘
```

`convert_query_to_cnf`を`true`に設定して変更点を見てみましょう：

```sql
EXPLAIN SYNTAX
SELECT *
FROM
(
    SELECT number AS x
    FROM numbers(20)
) AS a
WHERE ((x >= 1) AND (x <= 5)) OR ((x >= 10) AND (x <= 15))
SETTINGS convert_query_to_cnf = true;
```

`WHERE`句がCNFに書き換えられていることに注意してくださいが、結果セットは同一であり、ブール論理は変わっていません：

```response
┌─explain───────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ SELECT x                                                                                                              │
│ FROM                                                                                                                  │
│ (                                                                                                                     │
│     SELECT number AS x                                                                                                │
│     FROM numbers(20)                                                                                                  │
│     WHERE ((x <= 15) OR (x <= 5)) AND ((x <= 15) OR (x >= 1)) AND ((x >= 10) OR (x <= 5)) AND ((x >= 10) OR (x >= 1)) │
│ ) AS a                                                                                                                │
│ WHERE ((x >= 10) OR (x >= 1)) AND ((x >= 10) OR (x <= 5)) AND ((x <= 15) OR (x >= 1)) AND ((x <= 15) OR (x <= 5))     │
│ SETTINGS convert_query_to_cnf = 1                                                                                     │
└───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
```

可能な値: true, false

## count_distinct_implementation {#count_distinct_implementation}



型: 文字列

デフォルト値: uniqExact

`COUNT(DISTINCT ...)`構文を実行する際に使用する`uniq*`関数を指定します。

可能な値：

- [uniq](../../sql-reference/aggregate-functions/reference/uniq.md/#agg_function-uniq)
- [uniqCombined](../../sql-reference/aggregate-functions/reference/uniqcombined.md/#agg_function-uniqcombined)
- [uniqCombined64](../../sql-reference/aggregate-functions/reference/uniqcombined64.md/#agg_function-uniqcombined64)
- [uniqHLL12](../../sql-reference/aggregate-functions/reference/uniqhll12.md/#agg_function-uniqhll12)
- [uniqExact](../../sql-reference/aggregate-functions/reference/uniqexact.md/#agg_function-uniqexact)

## count_distinct_optimization {#count_distinct_optimization}



型: ブール値

デフォルト値: 0

重複排除をグループ化のサブクエリに対するカウントに書き換えます。

## create_if_not_exists {#create_if_not_exists}



型: ブール値

デフォルト値: 0

デフォルトで`CREATE`文に対して`IF NOT EXISTS`を有効にします。この設定または`IF NOT EXISTS`が指定され、指定された名前のテーブルがすでに存在する場合、例外はスローされません。

## create_index_ignore_unique {#create_index_ignore_unique}



型: ブール値

デフォルト値: 0

CREATE UNIQUE INDEXのUNIQUEキーワードを無視します。SQL互換性テストのために作成されています。

## create_replicated_merge_tree_fault_injection_probability {#create_replicated_merge_tree_fault_injection_probability}



型: Float

デフォルト値: 0

ZooKeeperにメタデータを作成した後のテーブル作成中の故障注入の確率。

## create_table_empty_primary_key_by_default {#create_table_empty_primary_key_by_default}



型: ブール値

デフォルト値: 0

ORDER BY及びPRIMARY KEYが指定されていない場合に、空の主キーを持つ*MergeTreeテーブルを作成することを許可します。

## cross_join_min_bytes_to_compress {#cross_join_min_bytes_to_compress}



型: UInt64

デフォルト値: 1073741824

CROSS JOINで圧縮するためのブロックの最小サイズ。ゼロの値はこの閾値を無効にします。このブロックは、行またはバイトの2つの閾値のいずれかが到達したときに圧縮されます。

## cross_join_min_rows_to_compress {#cross_join_min_rows_to_compress}



型: UInt64

デフォルト値: 10000000

CROSS JOINでブロックを圧縮するための最小行数。ゼロの値はこの閾値を無効にします。このブロックは、行またはバイトの2つの閾値のいずれかが到達したときに圧縮されます。

## data_type_default_nullable {#data_type_default_nullable}



型: ブール値

デフォルト値: 0

カラム定義においてNULLまたはNOT NULL修飾子のないデータ型を[Nullable](../../sql-reference/data-types/nullable.md/#data_type-nullable)にします。

可能な値：

- 1 — カラム定義のデータ型はデフォルトで`Nullable`に設定されます。
- 0 — カラム定義のデータ型はデフォルトで`Nullable`ではありません。

## database_atomic_wait_for_drop_and_detach_synchronously {#database_atomic_wait_for_drop_and_detach_synchronously}



型: ブール値

デフォルト値: 0

すべての`DROP`および`DETACH`クエリに修飾子`SYNC`を追加します。

可能な値：

- 0 — クエリは遅延して実行されます。
- 1 — クエリは遅延なしで実行されます。

## database_replicated_allow_explicit_uuid {#database_replicated_allow_explicit_uuid}



型: UInt64

デフォルト値: 0

0 - レプリケートデータベースにおけるテーブルのUUIDを明示的に指定することを許可しません。1 - 許可します。2 - 許可しますが、指定されたUUIDを無視してランダムなUUIDを生成します。

## database_replicated_allow_heavy_create {#database_replicated_allow_heavy_create}



型: ブール値

デフォルト値: 0

レプリケートデータベースエンジンで長時間実行されるDDLクエリ（CREATE AS SELECTおよびPOPULATE）を許可します。ただし、これによりDDLキューが長時間ブロックされる可能性があります。

## database_replicated_allow_only_replicated_engine {#database_replicated_allow_only_replicated_engine}



型: ブール値

デフォルト値: 0

レプリケートエンジンであるデータベース内にのみレプリケートテーブルを作成することを許可します。

## database_replicated_allow_replicated_engine_arguments {#database_replicated_allow_replicated_engine_arguments}



型: UInt64

デフォルト値: 0

0 - レプリケートデータベースにおける*MergeTreeテーブルのZooKeeperパスとレプリカ名を明示的に指定することを許可しません。1 - 許可します。2 - 許可しますが、指定されたパスを無視してデフォルトのパスを使用します。3 - 許可し、警告をログしません。

## database_replicated_always_detach_permanently {#database_replicated_always_detach_permanently}



型: ブール値

デフォルト値: 0

データベースエンジンがレプリケートの場合、DETACH TABLEをDETACH TABLE PERMANENTLYとして実行します。

## database_replicated_enforce_synchronous_settings {#database_replicated_enforce_synchronous_settings}



型: ブール値

デフォルト値: 0

いくつかのクエリに対して非同期待機を強制します（database_atomic_wait_for_drop_and_detach_synchronously、mutation_sync、alter_syncも参照）。これらの設定を有効にすることは推奨されません。

## database_replicated_initial_query_timeout_sec {#database_replicated_initial_query_timeout_sec}



型: UInt64

デフォルト値: 300

初期DDLクエリがレプリケートデータベースが前のDDLキューエントリを処理するのを待つ時間（秒）。

可能な値：

- 正の整数。
- 0 — 無制限。

## decimal_check_overflow {#decimal_check_overflow}



型: ブール値

デフォルト値: 1

10進数の算術/比較操作のオーバーフローをチェックします。

## deduplicate_blocks_in_dependent_materialized_views {#deduplicate_blocks_in_dependent_materialized_views}



型: ブール値

デフォルト値: 0

レプリケートテーブルからデータを受け取るマテリアライズドビューにおける重複排除のチェックを有効または無効にします。

可能な値：

      0 — 無効。
      1 — 有効。

使用方法

デフォルトでは、マテリアライズドビューに対して重複排除は行われず、ソーステーブルで行われます。
ソーステーブルで重複排除のために挿入されたブロックがスキップされると、添付されたマテリアライズドビューには挿入されません。この動作は、素材化されたビューへの挿入を可能にすることを目的としており、挿入されたブロックが素材化されたビューの集約後に同じであっても、ソーステーブルに対する異なるINSERTから派生している場合に適用されます。
同時に、この動作は`INSERT`の冪等性を「壊します」。メインテーブルへの`INSERT`が成功し、マテリアライズドビューへの`INSERT`が失敗した場合（例えば、ClickHouse Keeperとの通信障害など）、クライアントはエラーを受け取り操作を再試行できます。ただし、マテリアライズドビューは、メインテーブルの重複排除により2回目の挿入を受け取らないため、再挿入されません。設定`deduplicate_blocks_in_dependent_materialized_views`はこの動作を変更することを許可します。再試行時には、マテリアライズドビューが再挿入を受け入れ、自身で重複排除チェックを行います。
ソーステーブルのチェック結果を無視し、初回の失敗によって失われた行を挿入します。

## default_materialized_view_sql_security {#default_materialized_view_sql_security}



型: SQLSecurityType

デフォルト値: DEFINER

マテリアライズドビューを作成する際にSQL SECURITYオプションのデフォルト値を設定することを許可します。[SQLセキュリティについての詳細](../../sql-reference/statements/create/view.md/#sql_security)。

デフォルト値は`DEFINER`です。

## default_max_bytes_in_join {#default_max_bytes_in_join}



型: UInt64

デフォルト値: 1000000000

最大バイト数の右側テーブルが制限されている場合、max_bytes_in_joinが設定されていない場合の最大サイズ。

## default_normal_view_sql_security {#default_normal_view_sql_security}



型: SQLSecurityType

デフォルト値: INVOKER

通常のビューを作成する際にデフォルトの`SQL SECURITY`オプションを設定することを許可します。[SQLセキュリティについての詳細](../../sql-reference/statements/create/view.md/#sql_security)。

デフォルト値は`INVOKER`です。

## default_table_engine {#default_table_engine}



型: DefaultTableEngine

デフォルト値: MergeTree

`CREATE`文において`ENGINE`が指定されていない場合に使用されるデフォルトのテーブルエンジン。

可能な値：

- 有効なテーブルエンジン名を表す文字列。

クラウドデフォルト値: `SharedMergeTree`。

**例**

クエリ：

```sql
SET default_table_engine = 'Log';

SELECT name, value, changed FROM system.settings WHERE name = 'default_table_engine';
```

結果：

```response
┌─name─────────────────┬─value─┬─changed─┐
│ default_table_engine │ Log   │       1 │
└──────────────────────┴───────┴─────────┘
```

この例では、`Engine`を指定しない新しいテーブルは、`Log`テーブルエンジンを使用します：

クエリ：

```sql
CREATE TABLE my_table (
    x UInt32,
    y UInt32
);

SHOW CREATE TABLE my_table;
```

結果：

```response
┌─statement────────────────────────────────────────────────────────────────┐
│ CREATE TABLE default.my_table
(
```sql
`x` UInt32,
`y` UInt32
)
ENGINE = Log
└──────────────────────────────────────────────────────────────────────────┘
```

## default_temporary_table_engine {#default_temporary_table_engine}

タイプ: DefaultTableEngine

デフォルト値: Memory

[default_table_engine](#default_table_engine) と同じですが、テンポラリーテーブルの場合です。

この例では、`Engine` を指定しない新しいテンポラリーテーブルは `Log` テーブルエンジンを使用します。

クエリ:

```sql
SET default_temporary_table_engine = 'Log';

CREATE TEMPORARY TABLE my_table (
    x UInt32,
    y UInt32
);

SHOW CREATE TEMPORARY TABLE my_table;
```

結果:

```response
┌─statement────────────────────────────────────────────────────────────────┐
│ CREATE TEMPORARY TABLE default.my_table
(
    `x` UInt32,
    `y` UInt32
)
ENGINE = Log
└──────────────────────────────────────────────────────────────────────────┘
```

## default_view_definer {#default_view_definer}

タイプ: String

デフォルト値: CURRENT_USER

ビュー作成時にデフォルトの `DEFINER` オプションを設定できます。 [SQLセキュリティについての詳細](../../sql-reference/statements/create/view.md/#sql_security)。

デフォルトの値は `CURRENT_USER` です。

## describe_compact_output {#describe_compact_output}

タイプ: Bool

デフォルト値: 0

trueの場合、DESCRIBE クエリの結果にカラム名とタイプのみを含めます。

## describe_extend_object_types {#describe_extend_object_types}

タイプ: Bool

デフォルト値: 0

DESCRIBE クエリの Object 型のカラムの具体的な型を推測します。

## describe_include_subcolumns {#describe_include_subcolumns}

タイプ: Bool

デフォルト値: 0

[DESCRIBE](../../sql-reference/statements/describe-table.md) クエリのサブカラムを描写することを可能にします。例えば、[Tuple](../../sql-reference/data-types/tuple.md)のメンバーや [Map](../../sql-reference/data-types/map.md/#map-subcolumns)、[Nullable](../../sql-reference/data-types/nullable.md/#finding-null) 又は [Array](../../sql-reference/data-types/array.md/#array-size) データ型のサブカラムなど。

可能な値:

- 0 — サブカラムは `DESCRIBE` クエリに含まれません。
- 1 — サブカラムは `DESCRIBE` クエリに含まれます。

**例**

[DESCRIBE](../../sql-reference/statements/describe-table.md) ステートメントの例を参照してください。

## describe_include_virtual_columns {#describe_include_virtual_columns}

タイプ: Bool

デフォルト値: 0

trueの場合、テーブルのバーチャルカラムがDESCRIBEクエリの結果に含まれます。

## dialect {#dialect}

タイプ: Dialect

デフォルト値: clickhouse

クエリを解析するために使用されるダイアレクトです。

## dictionary_validate_primary_key_type {#dictionary_validate_primary_key_type}

タイプ: Bool

デフォルト値: 0

辞書の主キータイプを検証します。デフォルトでは、シンプルなレイアウトのIDタイプはUInt64に暗黙的に変換されます。

## distinct_overflow_mode {#distinct_overflow_mode}

タイプ: OverflowMode

デフォルト値: throw

制限を超えたときに何をするか。

## distributed_aggregation_memory_efficient {#distributed_aggregation_memory_efficient}

タイプ: Bool

デフォルト値: 1

分散集約のメモリ節約モードが有効になっています。

## distributed_background_insert_batch {#distributed_background_insert_batch}

タイプ: Bool

デフォルト値: 0

挿入されたデータをバッチで送信するかどうかを有効/無効にします。

バッチ送信が有効な場合、[Distributed](../../engines/table-engines/special/distributed.md) テーブルエンジンは、挿入されたデータの複数のファイルを1つの操作で送信し、個別に送信するのではなく、クラスターパフォーマンスを改善します。

可能な値:

- 1 — 有効。
- 0 — 無効。

## distributed_background_insert_max_sleep_time_ms {#distributed_background_insert_max_sleep_time_ms}

タイプ: ミリ秒

デフォルト値: 30000

[Distributed](../../engines/table-engines/special/distributed.md) テーブルエンジンがデータを送信するための最大間隔。 [distributed_background_insert_sleep_time_ms](#distributed_background_insert_sleep_time_ms) 設定で設定された間隔の指数的成長を制限します。

可能な値:

- 正の整数ミリ秒。

## distributed_background_insert_sleep_time_ms {#distributed_background_insert_sleep_time_ms}

タイプ: ミリ秒

デフォルト値: 100

[Distributed](../../engines/table-engines/special/distributed.md) テーブルエンジンがデータを送信するための基本間隔。実際の間隔はエラーが発生した場合に指数的に増加します。

可能な値:

- 正の整数ミリ秒。

## distributed_background_insert_split_batch_on_failure {#distributed_background_insert_split_batch_on_failure}

タイプ: Bool

デフォルト値: 0

失敗時にバッチを分割するかどうかを有効/無効にします。

特定のバッチをリモートシャードに送信することが失敗することがあります。これは、`MATERIALIZED VIEW` に `GROUP BY` があるため、「Memory limit exceeded」や類似のエラーによるものです。この場合、再試行しても役に立たない（これはテーブルの分散送信を停止させます）が、そのバッチのファイルを1つずつ送信することでINSERTが成功する可能性があります。

この設定を `1` にすると、こうしたバッチのバッチ処理が無効にします（つまり、失敗したバッチに対して `distributed_background_insert_batch` を一時的に無効にします）。

可能な値:

- 1 — 有効。
- 0 — 無効。

:::note
この設定は、サーバー（マシン）の異常終了や、[Distributed](../../engines/table-engines/special/distributed.md) テーブルエンジンに対して `fsync_after_insert`/`fsync_directories` がない場合に発生する壊れたバッチにも影響します。
:::

:::note
自動バッチ分割に依存しないでください。性能に悪影響を及ぼす可能性があります。
:::

## distributed_background_insert_timeout {#distributed_background_insert_timeout}

タイプ: UInt64

デフォルト値: 0

分散クエリへの挿入のタイムアウト。この設定は、insert_distributed_sync が有効な場合にのみ使用されます。ゼロの値はタイムアウトなしを意味します。

## distributed_cache_bypass_connection_pool {#distributed_cache_bypass_connection_pool}

タイプ: Bool

デフォルト値: 0

ClickHouse Cloud のみ。分散キャッシュ接続プールをバイパスすることを許可します。

## distributed_cache_connect_max_tries {#distributed_cache_connect_max_tries}

タイプ: UInt64

デフォルト値: 20

ClickHouse Cloud のみ。分散キャッシュに接続できなかった場合の接続試行回数。

## distributed_cache_data_packet_ack_window {#distributed_cache_data_packet_ack_window}

タイプ: UInt64

デフォルト値: 5

ClickHouse Cloud のみ。単一分散キャッシュ読み取りリクエストにおけるデータパケットシーケンスのACKを送信するためのウィンドウです。

## distributed_cache_discard_connection_if_unread_data {#distributed_cache_discard_connection_if_unread_data}

タイプ: Bool

デフォルト値: 1

ClickHouse Cloud のみ。未読データが存在する場合は接続を破棄します。

## distributed_cache_fetch_metrics_only_from_current_az {#distributed_cache_fetch_metrics_only_from_current_az}

タイプ: Bool

デフォルト値: 1

ClickHouse Cloud のみ。system.distributed_cache_metrics および system.distributed_cache_events から現在のアベイラビリティゾーンからのみメトリクスを取得します。

## distributed_cache_log_mode {#distributed_cache_log_mode}

タイプ: DistributedCacheLogMode

デフォルト値: on_error

ClickHouse Cloud のみ。system.distributed_cache_log への書き込みモードです。

## distributed_cache_max_unacked_inflight_packets {#distributed_cache_max_unacked_inflight_packets}

タイプ: UInt64

デフォルト値: 10

ClickHouse Cloud のみ。単一分散キャッシュ読み取りリクエストにおける未確認の揮発性パケットの最大数です。

## distributed_cache_min_bytes_for_seek {#distributed_cache_min_bytes_for_seek}

タイプ: Bool

デフォルト値: 0

ClickHouse Cloud のみ。分散キャッシュで検索を行うための最小バイト数。

## distributed_cache_pool_behaviour_on_limit {#distributed_cache_pool_behaviour_on_limit}

タイプ: DistributedCachePoolBehaviourOnLimit

デフォルト値: wait

ClickHouse Cloud のみ。プール制限に達したときの分散キャッシュ接続の動作を特定します。

## distributed_cache_read_alignment {#distributed_cache_read_alignment}

タイプ: UInt64

デフォルト値: 0

ClickHouse Cloud のみ。テスト目的の設定であり、変更しないでください。

## distributed_cache_receive_response_wait_milliseconds {#distributed_cache_receive_response_wait_milliseconds}

タイプ: UInt64

デフォルト値: 60000

ClickHouse Cloud のみ。分散キャッシュからリクエストのデータを受信するまでの待機時間（ミリ秒）です。

## distributed_cache_receive_timeout_milliseconds {#distributed_cache_receive_timeout_milliseconds}

タイプ: UInt64

デフォルト値: 10000

ClickHouse Cloud のみ。分散キャッシュからの応答を受け取るための待機時間（ミリ秒）です。

## distributed_cache_throw_on_error {#distributed_cache_throw_on_error}

タイプ: Bool

デフォルト値: 0

ClickHouse Cloud のみ。分散キャッシュとの通信中に発生した例外を再スローするか、分散キャッシュから受信した例外です。そうでなければ、エラー時に分散キャッシュをスキップします。

## distributed_cache_wait_connection_from_pool_milliseconds {#distributed_cache_wait_connection_from_pool_milliseconds}

タイプ: UInt64

デフォルト値: 100

ClickHouse Cloud のみ。distributed_cache_pool_behaviour_on_limit が wait の場合、接続プールからの接続を受け取るまでの待機時間（ミリ秒）です。

## distributed_connections_pool_size {#distributed_connections_pool_size}

タイプ: UInt64

デフォルト値: 1024

単一のDistributedテーブルに対するすべてのクエリのリモートサーバーとの同時接続の最大数。クラスタ内のサーバー数以上に設定することをお勧めします。

## distributed_ddl_entry_format_version {#distributed_ddl_entry_format_version}

タイプ: UInt64

デフォルト値: 5

分散DDL（ON CLUSTER）クエリの互換性バージョンです。

## distributed_ddl_output_mode {#distributed_ddl_output_mode}

タイプ: DistributedDDLOutputMode

デフォルト値: throw

分散DDLクエリの結果の形式を設定します。

可能な値:

- `throw` — クエリが完了したすべてのホストに対してクエリ実行ステータスを含む結果セットを返します。もしクエリが一部ホストで失敗した場合、最初の例外を再スローします。もしクエリが未完了のホストが存在し、[distributed_ddl_task_timeout](#distributed_ddl_task_timeout) を超えた場合、`TIMEOUT_EXCEEDED` 例外がスローされます。
- `none` — throw に似ていますが、分散DDLクエリは結果セットを返しません。
- `null_status_on_timeout` — 未完了のホストについては、結果セットの一部の行に対して実行ステータスとして `NULL`を返します。
- `never_throw` — `TIMEOUT_EXCEEDED` をスローせず、一部ホストで失敗した場合に例外を再スローしません。
- `none_only_active` — `none` に似ていますが、`Replicated` データベースの非アクティブなレプリカを待ちません。注意: このモードでは、一部のレプリカでクエリが実行されなかったことを特定できず、バックグラウンドで実行される可能性があります。
- `null_status_on_timeout_only_active` — `null_status_on_timeout` に似ていますが、`Replicated` データベースの非アクティブなレプリカを待ちません。
- `throw_only_active` — `throw` に似ていますが、`Replicated` データベースの非アクティブなレプリカを待ちません。

クラウドのデフォルト値: `none`。

## distributed_ddl_task_timeout {#distributed_ddl_task_timeout}

タイプ: Int64

デフォルト値: 180

クラスター内のすべてのホストからのDDLクエリ応答のタイムアウトを設定します。すべてのホストでDDLリクエストが実行されていない場合、応答はタイムアウトエラーを含み、リクエストは非同期モードで実行されます。負の値は無限を意味します。

可能な値:

- 正の整数。
- 0 — 非同期モード。
- 負の整数 — 無限タイムアウト。

## distributed_foreground_insert {#distributed_foreground_insert}

タイプ: Bool

デフォルト値: 0

[Distributed](../../engines/table-engines/special/distributed.md/#distributed) テーブルへの同期データ挿入を有効または無効にします。

デフォルトでは、`Distributed` テーブルにデータを挿入する場合、ClickHouseサーバーはバックグラウンドモードでクラスターノードにデータを送信します。`distributed_foreground_insert=1` の場合、データは同期的に処理され、すべてのシャード（`internal_replication` が true の場合は各シャードの少なくとも1つのレプリカ）にデータが保存された後にのみ `INSERT` 操作が成功します。

可能な値:

- 0 — データはバックグラウンドモードで挿入されます。
- 1 — データは同期モードで挿入されます。

クラウドのデフォルト値: `1`。

**関連情報**

- [分散テーブルエンジン](../../engines/table-engines/special/distributed.md/#distributed)
- [分散テーブルの管理](../../sql-reference/statements/system.md/#query-language-system-distributed)

## distributed_group_by_no_merge {#distributed_group_by_no_merge}

タイプ: UInt64

デフォルト値: 0

分散クエリ処理用に異なるサーバーからの集約状態をマージしないようにします。シャードごとに異なるキーがあることが確実な場合に使用できます。

可能な値:

- `0` — 無効（最終クエリ処理はイニシエーターノードで行われます）。
- `1` - 分散クエリ処理用に異なるサーバーからの集約状態をマージしません（クエリはシャードで完全に処理され、イニシエーターはデータをプロキシするだけです。シャードごとに異なるキーがあることが確実な場合に使用できます）。
- `2` - `1` と同様ですが、`ORDER BY` と `LIMIT` が適用されます（このオプションはリモートノードで完全に処理されたクエリに対しては適用できません）。

**例**

```sql
SELECT *
FROM remote('127.0.0.{2,3}', system.one)
GROUP BY dummy
LIMIT 1
SETTINGS distributed_group_by_no_merge = 1
FORMAT PrettyCompactMonoBlock

┌─dummy─┐
│     0 │
│     0 │
└───────┘
```

```sql
SELECT *
FROM remote('127.0.0.{2,3}', system.one)
GROUP BY dummy
LIMIT 1
SETTINGS distributed_group_by_no_merge = 2
FORMAT PrettyCompactMonoBlock

┌─dummy─┐
│     0 │
└───────┘
```

## distributed_insert_skip_read_only_replicas {#distributed_insert_skip_read_only_replicas}

タイプ: Bool

デフォルト値: 0

分散に対するINSERTクエリで読み取り専用レプリカをスキップすることを可能にします。

可能な値:

- 0 — 通常通りINSERTされ、読み取り専用レプリカに送信されると失敗します。
- 1 — イニシエーターはデータをシャードに送信する前に読み取り専用レプリカをスキップします。

## distributed_product_mode {#distributed_product_mode}

タイプ: DistributedProductMode

デフォルト値: deny

[分散サブクエリ](../../sql-reference/operators/in.md)の動作を変更します。

ClickHouse は、クエリが分散テーブルの積の中で分散テーブルに対する非グローバルサブクエリを含んでいる場合にこの設定を適用します。

制限:

- IN および JOIN サブクエリに対してのみ適用。
- FROM セクションが複数のシャードを含む分散テーブルを使用する場合。
- サブクエリが複数のシャードを含む分散テーブルに関係している場合。
- テーブル値の [remote](../../sql-reference/table-functions/remote.md) 関数には使用されません。

可能な値:

- `deny` — デフォルト値。これらのタイプのサブクエリの使用を禁止します（「ダブル分散 IN / JOIN サブクエリは拒否されている」という例外が返されます）。
- `local` — サブクエリ内でデータベースとテーブルを宛先サーバー（シャード）用のローカルのものに置き換え、通常の `IN` / `JOIN` を保持します。
- `global` — `IN` / `JOIN` クエリを `GLOBAL IN` / `GLOBAL JOIN` に置き換えます。
- `allow` — これらのタイプのサブクエリの使用を許可します。

## distributed_push_down_limit {#distributed_push_down_limit}

タイプ: UInt64

デフォルト値: 1

各シャードでの [LIMIT](#limit) の適用を有効または無効にします。

これにより、次のことを回避します：
- ネットワーク越しに余分な行を送信すること。
- イニシエーターで制限を超えた行を処理すること。

21.9 バージョン以降、`distributed_push_down_limit` が以下の条件のいずれかを満たす場合にのみ、クエリ実行が変更され、正確な結果が取得できなくなることはありません。
- [distributed_group_by_no_merge](#distributed_group_by_no_merge) > 0。
- クエリ **が** `GROUP BY` / `DISTINCT` / `LIMIT BY` を持たず `ORDER BY` / `LIMIT` を持つ。
- クエリ **が** `GROUP BY` / `DISTINCT` / `LIMIT BY` を `ORDER BY` / `LIMIT` と共に持つ場合、かつ：
    - [optimize_skip_unused_shards](#optimize_skip_unused_shards) が有効です。
    - [optimize_distributed_group_by_sharding_key](#optimize_distributed_group_by_sharding_key) が有効です。

可能な値:

- 0 — 無効。
- 1 — 有効。

関連情報:

- [distributed_group_by_no_merge](#distributed_group_by_no_merge)
- [optimize_skip_unused_shards](#optimize_skip_unused_shards)
- [optimize_distributed_group_by_sharding_key](#optimize_distributed_group_by_sharding_key)

## distributed_replica_error_cap {#distributed_replica_error_cap}

タイプ: UInt64

デフォルト値: 1000

- タイプ: 符号なし整数
- デフォルト値: 1000

各レプリカのエラー数はこの値に制限され、単一のレプリカが過剰なエラーを蓄積することを防ぎます。

関連情報:

- [load_balancing](#load_balancing-round_robin)
- [Table engine Distributed](../../engines/table-engines/special/distributed.md)
- [distributed_replica_error_half_life](#distributed_replica_error_half_life)
- [distributed_replica_max_ignored_errors](#distributed_replica_max_ignored_errors)

## distributed_replica_error_half_life {#distributed_replica_error_half_life}

タイプ: 秒

デフォルト値: 60

- タイプ: 秒
- デフォルト値: 60 秒

分散テーブル内のエラーがゼロになる速さを制御します。あるレプリカが一定時間利用できず、5つのエラーが蓄積し、`distributed_replica_error_half_life` が1秒に設定されている場合、そのレプリカは最後のエラーから3秒後に正常と見なされます。

関連情報:

- [load_balancing](#load_balancing-round_robin)
- [Table engine Distributed](../../engines/table-engines/special/distributed.md)
- [distributed_replica_error_cap](#distributed_replica_error_cap)
- [distributed_replica_max_ignored_errors](#distributed_replica_max_ignored_errors)

## distributed_replica_max_ignored_errors {#distributed_replica_max_ignored_errors}

タイプ: UInt64

デフォルト値: 0

- タイプ: 符号なし整数
- デフォルト値: 0

レプリカを選択する際に無視されるエラーの数（`load_balancing` アルゴリズムに従って）。

関連情報:

- [load_balancing](#load_balancing-round_robin)
- [Table engine Distributed](../../engines/table-engines/special/distributed.md)
- [distributed_replica_error_cap](#distributed_replica_error_cap)
- [distributed_replica_error_half_life](#distributed_replica_error_half_life)

## do_not_merge_across_partitions_select_final {#do_not_merge_across_partitions_select_final}

タイプ: Bool

デフォルト値: 0

最終選択において、同一パーティション内のパーツのみをマージします。

## empty_result_for_aggregation_by_constant_keys_on_empty_set {#empty_result_for_aggregation_by_constant_keys_on_empty_set}

タイプ: Bool

デフォルト値: 1

空のセットに対して定数キーで集計する際、空の結果を返します。

## empty_result_for_aggregation_by_empty_set {#empty_result_for_aggregation_by_empty_set}

タイプ: Bool

デフォルト値: 0

空のセットに対してキーなしで集計する際、空の結果を返します。

## enable_adaptive_memory_spill_scheduler {#enable_adaptive_memory_spill_scheduler}
<ExperimentalBadge/>

タイプ: Bool

デフォルト値: 0

プロセッサにデータを適応的に外部ストレージにスピルさせるトリガーをかけます。現在、グレースジョインがサポートされています。

## enable_blob_storage_log {#enable_blob_storage_log}

タイプ: Bool

デフォルト値: 1

blobストレージ操作に関する情報を system.blob_storage_log テーブルに書き込みます。

## enable_deflate_qpl_codec {#enable_deflate_qpl_codec}

タイプ: Bool

デフォルト値: 0

有効になっている場合、DEFLATE_QPL コーデックを使用してカラムを圧縮できます。

## enable_early_constant_folding {#enable_early_constant_folding}

タイプ: Bool

デフォルト値: 1

関数とサブクエリの結果を分析し、定数がある場合にクエリを再作成するクエリ最適化を有効にします。

## enable_extended_results_for_datetime_functions {#enable_extended_results_for_datetime_functions}

タイプ: Bool

デフォルト値: 0

次のタイプの結果を返すことを有効または無効にします：
- `Date32` の拡張範囲（`Date` 型に比較して）を持つ関数 [toStartOfYear](../../sql-reference/functions/date-time-functions.md/#tostartofyear)、[toStartOfISOYear](../../sql-reference/functions/date-time-functions.md/#tostartofisoyear)、[toStartOfQuarter](../../sql-reference/functions/date-time-functions.md/#tostartofquarter)、[toStartOfMonth](../../sql-reference/functions/date-time-functions.md/#tostartofmonth)、[toLastDayOfMonth](../../sql-reference/functions/date-time-functions.md/#tolastdayofmonth)、[toStartOfWeek](../../sql-reference/functions/date-time-functions.md/#tostartofweek)、[toLastDayOfWeek](../../sql-reference/functions/date-time-functions.md/#tolastdayofweek)、[toMonday](../../sql-reference/functions/date-time-functions.md/#tomonday)。
- `DateTime64` の拡張範囲（`DateTime` 型に比較して）を持つ関数 [toStartOfDay](../../sql-reference/functions/date-time-functions.md/#tostartofday)、[toStartOfHour](../../sql-reference/functions/date-time-functions.md/#tostartofhour)、[toStartOfMinute](../../sql-reference/functions/date-time-functions.md/#tostartofminute)、[toStartOfFiveMinutes](../../sql-reference/functions/date-time-functions.md/#tostartoftenminutes)、[toStartOfTenMinutes](../../sql-reference/functions/date-time-functions.md/#tostartoftenminutes)、[toStartOfFifteenMinutes](../../sql-reference/functions/date-time-functions.md/#tostartoffifteenminutes) と [timeSlot](../../sql-reference/functions/date-time-functions.md/#timeslot)。

可能な値:

- 0 — 関数はすべての引数に対して `Date` または `DateTime` を返します。
- 1 — 関数は `Date32` または `DateTime64` 引数に対して `Date32` または `DateTime64` を返し、それ以外は `Date` または `DateTime` を返します。

## enable_filesystem_cache {#enable_filesystem_cache}

タイプ: Bool

デフォルト値: 1

リモートファイルシステムのキャッシュを使用します。この設定はディスクのキャッシュのオン/オフには影響しません（ディスク設定で行う必要があります）が、意図的に特定のクエリでキャッシュをバイパスすることを可能にします。

## enable_filesystem_cache_log {#enable_filesystem_cache_log}

タイプ: Bool

デフォルト値: 0

各クエリのファイルシステムキャッシュログを記録することを許可します。

## enable_filesystem_cache_on_write_operations {#enable_filesystem_cache_on_write_operations}

タイプ: Bool

デフォルト値: 0

書き込み操作時にキャッシュに書き込みます。この設定を実際に機能させるには、ディスク設定にも追加する必要があります。

## enable_filesystem_read_prefetches_log {#enable_filesystem_read_prefetches_log}

タイプ: Bool

デフォルト値: 0

クエリ中に system.filesystem の prefetch_log にログを記録します。これはテストまたはデバッグ専用であり、デフォルトで有効にすることは推奨されません。

## enable_global_with_statement {#enable_global_with_statement}

タイプ: Bool

デフォルト値: 1

UNION クエリおよびすべてのサブクエリに WITH ステートメントを伝播します。

## enable_http_compression {#enable_http_compression}

タイプ: Bool

デフォルト値: 0

HTTPリクエストに対する応答のデータ圧縮を有効または無効にします。

詳細については、[HTTPインターフェースの説明](../../interfaces/http.md)を参照してください。

可能な値:

- 0 — 無効。
- 1 — 有効。

## enable_job_stack_trace {#enable_job_stack_trace}

タイプ: Bool

デフォルト値: 1

ジョブが例外となった場合、ジョブクリエーターのスタックトレースを出力します。

## enable_lightweight_delete {#enable_lightweight_delete}

タイプ: Bool

デフォルト値: 1

MergeTree テーブル用の軽量 DELETE ミューテーションを有効にします。

## enable_memory_bound_merging_of_aggregation_results {#enable_memory_bound_merging_of_aggregation_results}

タイプ: Bool

デフォルト値: 1

集約結果のためのメモリ境界マージ戦略を有効にします。

## enable_multiple_prewhere_read_steps {#enable_multiple_prewhere_read_steps}

タイプ: Bool

デフォルト値: 1

ANDで結合された複数の条件がある場合、WHEREからPREWHEREにより多くの条件を移動し、ディスクから読み取りとフィルタリングを複数のステップで実行します。

## enable_named_columns_in_function_tuple {#enable_named_columns_in_function_tuple}

タイプ: Bool

デフォルト値: 0

すべての名前が一意であり、引用符なしの識別子として扱える場合、function tuple() に名前付きタプルを生成します。

## enable_optimize_predicate_expression {#enable_optimize_predicate_expression}

タイプ: Bool

デフォルト値: 1

`SELECT` クエリにおける述語プッシュダウンをオンにします。

述語プッシュダウンは、分散クエリのネットワークトラフィックを大幅に削減する可能性があります。

可能な値:

- 0 — 無効。
- 1 — 有効。

使用法

以下のクエリを考慮します:

1.  `SELECT count() FROM test_table WHERE date = '2018-10-10'`
2.  `SELECT count() FROM (SELECT * FROM test_table) WHERE date = '2018-10-10'`

`enable_optimize_predicate_expression = 1` の場合、これらのクエリの実行時間は等しいです。ClickHouseはサブクエリを処理するときに`WHERE`を適用します。

`enable_optimize_predicate_expression = 0` の場合、2番目のクエリの実行時間ははるかに長くなります。なぜなら、`WHERE`句はサブクエリが完了した後にすべてのデータに適用されるからです。

## enable_optimize_predicate_expression_to_final_subquery {#enable_optimize_predicate_expression_to_final_subquery}

タイプ: Bool

デフォルト値: 1

最終サブクエリに対して述語をプッシュダウンすることを許可します。

## enable_order_by_all {#enable_order_by_all}

タイプ: Bool

デフォルト値: 1

`ORDER BY ALL` 構文でのソートを有効または無効にします。[ORDER BY](../../sql-reference/statements/select/order-by.md)を参照してください。

可能な値:

- 0 — ORDER BY ALL を無効にします。
- 1 — ORDER BY ALL を有効にします。

**例**

クエリ:

```sql
CREATE TABLE TAB(C1 Int, C2 Int, ALL Int) ENGINE=Memory();

INSERT INTO TAB VALUES (10, 20, 30), (20, 20, 10), (30, 10, 20);

SELECT * FROM TAB ORDER BY ALL; -- ALL が曖昧であるというエラーを返します。

SELECT * FROM TAB ORDER BY ALL SETTINGS enable_order_by_all = 0;
```

結果:

```text
┌─C1─┬─C2─┬─ALL─┐
│ 20 │ 20 │  10 │
│ 30 │ 10 │  20 │
│ 10 │ 20 │  30 │
└────┴─────┴───────┘
```

## enable_parsing_to_custom_serialization {#enable_parsing_to_custom_serialization}

タイプ: Bool

デフォルト値: 1

trueの場合、データはテーブルから得られたシリアル化のヒントに従って、カスタムシリアル化（例: スパース）を使用してカラムに直接解析されます。

## enable_positional_arguments {#enable_positional_arguments}

タイプ: Bool

デフォルト値: 1

[GROUP BY](../../sql-reference/statements/select/group-by.md)、[LIMIT BY](../../sql-reference/statements/select/limit-by.md)、[ORDER BY](../../sql-reference/statements/select/order-by.md) ステートメントに対する位置引数のサポートを有効または無効にします。

可能な値:

- 0 — 位置引数はサポートされていません。
- 1 — 位置引数がサポートされます：カラム番号をカラム名の代わりに使用できます。

**例**

クエリ:

```sql
CREATE TABLE positional_arguments(one Int, two Int, three Int) ENGINE=Memory();

INSERT INTO positional_arguments VALUES (10, 20, 30), (20, 20, 10), (30, 10, 20);

SELECT * FROM positional_arguments ORDER BY 2,3;
```

結果:

```text
┌─one─┬─two─┬─three─┐
│  30 │  10 │   20  │
│  20 │  20 │   10  │
│  10 │  20 │   30  │
└─────┴─────┴───────┘
```

## enable_reads_from_query_cache {#enable_reads_from_query_cache}

タイプ: Bool

デフォルト値: 1

有効になっている場合、`SELECT` クエリの結果は [クエリーキャッシュ](../query-cache.md) から取得されます。

可能な値:

- 0 - 無効
- 1 - 有効

## enable_s3_requests_logging {#enable_s3_requests_logging}

タイプ: Bool

デフォルト値: 0

S3リクエストの非常に明示的なロギングを有効にします。デバッグ専用です。

## enable_scalar_subquery_optimization {#enable_scalar_subquery_optimization}

タイプ: Bool

デフォルト値: 1

trueの場合、スカラーサブクエリが大きなスカラー値の(デシリアライズ/シリアライズ)を避け、同じサブクエリを複数回実行しないようにします。

## enable_sharing_sets_for_mutations {#enable_sharing_sets_for_mutations}

タイプ: Bool

デフォルト値: 1

IN サブクエリ用に構築されたセットオブジェクトの共有を許可します。これにより、メモリ使用量とCPU消費が削減されます。

## enable_software_prefetch_in_aggregation {#enable_software_prefetch_in_aggregation}

タイプ: Bool

デフォルト値: 1

集約処理におけるソフトウェアプリフェッチの使用を有効にします。

## enable_unaligned_array_join {#enable_unaligned_array_join}

タイプ: Bool

デフォルト値: 0

異なるサイズを持つ複数の配列に対するARRAY JOINを許可します。この設定が有効な場合、配列は最も長いものに合わせてサイズ変更されます。

## enable_url_encoding {#enable_url_encoding}

タイプ: Bool

デフォルト値: 1

[URL](../../engines/table-engines/special/url.md) エンジンテーブル内のuriのパスのデコード/エンコードを有効または無効にします。

デフォルトで有効です。

## enable_vertical_final {#enable_vertical_final}

タイプ: Bool

デフォルト値: 1

有効な場合、FINAL中に重複行をマークし、後でフィルタリングして削除することにより重複行を削除します。

## enable_writes_to_query_cache {#enable_writes_to_query_cache}

タイプ: Bool

デフォルト値: 1

有効になっている場合、`SELECT` クエリの結果は [クエリーキャッシュ](../query-cache.md) に保存されます。

可能な値:

- 0 - 無効
- 1 - 有効

## enable_zstd_qat_codec {#enable_zstd_qat_codec}

タイプ: Bool

デフォルト値: 0

有効な場合、ZSTD_QAT コーデックを使用してカラムを圧縮できます。

## enforce_strict_identifier_format {#enforce_strict_identifier_format}

タイプ: Bool

デフォルト値: 0

有効な場合、英数字とアンダースコアを含む識別子のみを許可します。

## engine_file_allow_create_multiple_files {#engine_file_allow_create_multiple_files}

タイプ: Bool

デフォルト値: 0

ファイルエンジンテーブル内で形式に接尾辞（`JSON`、`ORC`、`Parquet`など）がある場合、各挿入時に新しいファイルを作成することを有効または無効にします。有効にすると、各挿入時に次のパターンに従った名前の新しいファイルが作成されます：

`data.Parquet` → `data.1.Parquet` → `data.2.Parquet` など。

可能な値:
- 0 — `INSERT` クエリはファイルの最後に新しいデータを追加します。
- 1 — `INSERT` クエリは新しいファイルを作成します。

## engine_file_empty_if_not_exists {#engine_file_empty_if_not_exists}

タイプ: Bool

デフォルト値: 0

ファイルなしでファイルエンジンテーブルからデータを選択できるようにします。

可能な値:
- 0 — `SELECT` は例外をスローします。
- 1 — `SELECT` は空の結果を返します。
```
## engine_file_skip_empty_files {#engine_file_skip_empty_files}

Type: Bool

Default value: 0

[File](../../engines/table-engines/special/file.md)エンジンテーブルで空のファイルをスキップするかどうかを有効または無効にします。

Possible values:
- 0 — 空のファイルが要求されたフォーマットと互換性がない場合、`SELECT`は例外をスローします。
- 1 — 空のファイルに対して`SELECT`は空の結果を返します。

## engine_file_truncate_on_insert {#engine_file_truncate_on_insert}

Type: Bool

Default value: 0

[File](../../engines/table-engines/special/file.md)エンジンテーブルにデータを挿入する前に切り捨てるかどうかを有効または無効にします。

Possible values:
- 0 — `INSERT`クエリはファイルの末尾に新しいデータを追加します。
- 1 — `INSERT`クエリはファイルの既存の内容を新しいデータに置き換えます。

## engine_url_skip_empty_files {#engine_url_skip_empty_files}

Type: Bool

Default value: 0

[URL](../../engines/table-engines/special/url.md)エンジンテーブルで空のファイルをスキップするかどうかを有効または無効にします。

Possible values:
- 0 — 空のファイルが要求されたフォーマットと互換性がない場合、`SELECT`は例外をスローします。
- 1 — 空のファイルに対して`SELECT`は空の結果を返します。

## except_default_mode {#except_default_mode}

Type: SetOperationMode

Default value: ALL

EXCEPTクエリのデフォルトモードを設定します。可能な値: 空の文字列、'ALL'、'DISTINCT'。空の場合、モードなしのクエリは例外をスローします。

## external_storage_connect_timeout_sec {#external_storage_connect_timeout_sec}

Type: UInt64

Default value: 10

接続タイムアウト（秒）。現在はMySQLにのみ対応しています。

## external_storage_max_read_bytes {#external_storage_max_read_bytes}

Type: UInt64

Default value: 0

外部エンジンのテーブルが履歴データをフラッシュする際の最大バイト数の制限。現在はMySQLテーブルエンジン、データベースエンジン、辞書にのみ対応しています。0の場合、この設定は無効になります。

## external_storage_max_read_rows {#external_storage_max_read_rows}

Type: UInt64

Default value: 0

外部エンジンのテーブルが履歴データをフラッシュする際の最大行数の制限。現在はMySQLテーブルエンジン、データベースエンジン、辞書にのみ対応しています。0の場合、この設定は無効になります。

## external_storage_rw_timeout_sec {#external_storage_rw_timeout_sec}

Type: UInt64

Default value: 300

読み書きのタイムアウト（秒）。現在はMySQLにのみ対応しています。

## external_table_functions_use_nulls {#external_table_functions_use_nulls}

Type: Bool

Default value: 1

[mysql](../../sql-reference/table-functions/mysql.md)、[postgresql](../../sql-reference/table-functions/postgresql.md)、および[odbc](../../sql-reference/table-functions/odbc.md)テーブル関数がNullableカラムを使用する 방법を定義します。

Possible values:

- 0 — テーブル関数は明示的にNullableカラムを使用します。
- 1 — テーブル関数は暗黙的にNullableカラムを使用します。

**Usage**

設定が`0`に設定されている場合、テーブル関数はNullableカラムを作成せず、NULLの代わりにデフォルト値を挿入します。これは配列内のNULL値にも適用されます。

## external_table_strict_query {#external_table_strict_query}

Type: Bool

Default value: 0

trueに設定された場合、外部テーブルへのクエリでローカルフィルターへの変換が禁止されます。

## extract_key_value_pairs_max_pairs_per_row {#extract_key_value_pairs_max_pairs_per_row}

Type: UInt64

Default value: 1000

`extractKeyValuePairs`関数によって生成される最大ペア数。過剰なメモリ消費を防ぐためのセーフガードとして使用されます。

## extremes {#extremes}

Type: Bool

Default value: 0

クエリ結果のカラムにおける極値（最小値と最大値）をカウントするかどうか。0または1を受け入れます。デフォルトは0（無効）です。 
詳細については、「極値」のセクションを参照してください。

## fallback_to_stale_replicas_for_distributed_queries {#fallback_to_stale_replicas_for_distributed_queries}

Type: Bool

Default value: 1

更新されたデータが利用できない場合、古いレプリカへのクエリを強制します。[Replication](../../engines/table-engines/mergetree-family/replication.md)を参照してください。

ClickHouseはテーブルの古いレプリカから最も関連性の高いものを選択します。

レプリケートされたテーブルを指す分散テーブルから`SELECT`を実行する際に使用されます。

デフォルトは1（有効）です。

## filesystem_cache_boundary_alignment {#filesystem_cache_boundary_alignment}

Type: UInt64

Default value: 0

ファイルシステムのキャッシュ境界アライメント。この設定は、非ディスク読み込み（例えば、リモートテーブルエンジン/table関数のキャッシュ）のみに適用されますが、MergeTreeテーブルのストレージ構成には適用されません。値0はアライメントなしを意味します。

## filesystem_cache_enable_background_download_during_fetch {#filesystem_cache_enable_background_download_during_fetch}

Type: Bool

Default value: 1

ClickHouse Cloud専用。ファイルシステムキャッシュにおけるスペース予約のためにキャッシュをロックするまでの待機時間。

## filesystem_cache_enable_background_download_for_metadata_files_in_packed_storage {#filesystem_cache_enable_background_download_for_metadata_files_in_packed_storage}

Type: Bool

Default value: 1

ClickHouse Cloud専用。ファイルシステムキャッシュにおけるスペース予約のためにキャッシュをロックするまでの待機時間。

## filesystem_cache_max_download_size {#filesystem_cache_max_download_size}

Type: UInt64

Default value: 137438953472

単一のクエリによってダウンロードできる最大のリモートファイルシステムキャッシュサイズ。

## filesystem_cache_name {#filesystem_cache_name}

Type: String

Default value: 

ステートレステーブルエンジンまたはデータレイク用に使用されるファイルシステムキャッシュ名。

## filesystem_cache_prefer_bigger_buffer_size {#filesystem_cache_prefer_bigger_buffer_size}

Type: Bool

Default value: 1

ファイルシステムキャッシュが有効な場合、小さなファイルセグメントを書き込まないように大きなバッファサイズを優先します。これにより、キャッシュのパフォーマンスが低下します。一方で、この設定を有効にするとメモリ使用量が増加する可能性があります。

## filesystem_cache_reserve_space_wait_lock_timeout_milliseconds {#filesystem_cache_reserve_space_wait_lock_timeout_milliseconds}

Type: UInt64

Default value: 1000

ファイルシステムキャッシュでのスペース予約のためにキャッシュをロックするまでの待機時間。

## filesystem_cache_segments_batch_size {#filesystem_cache_segments_batch_size}

Type: UInt64

Default value: 20

読み込みバッファがキャッシュから要求できるファイルセグメントの単一バッチのサイズの制限。値が小さすぎるとキャッシュへの過剰な要求が発生し、大きすぎるとキャッシュからの追い出しが遅くなる可能性があります。

## filesystem_cache_skip_download_if_exceeds_per_query_cache_write_limit {#filesystem_cache_skip_download_if_exceeds_per_query_cache_write_limit}

Type: Bool

Default value: 1

クエリキャッシュサイズを超えた場合、リモートファイルシステムからのダウンロードをスキップします。

## filesystem_prefetch_max_memory_usage {#filesystem_prefetch_max_memory_usage}

Type: UInt64

Default value: 1073741824

プレフェッチの最大メモリ使用量。

## filesystem_prefetch_step_bytes {#filesystem_prefetch_step_bytes}

Type: UInt64

Default value: 0

バイト単位のプレフェッチステップ。ゼロは`auto`を意味し、約最適なプレフェッチステップが自動的に推定されますが、100%最適とは限りません。実際の値は、設定`filesystem_prefetch_min_bytes_for_single_read_task`により異なる場合があります。

## filesystem_prefetch_step_marks {#filesystem_prefetch_step_marks}

Type: UInt64

Default value: 0

マーク単位のプレフェッチステップ。ゼロは`auto`を意味し、約最適なプレフェッチステップが自動的に推定されますが、100%最適とは限りません。実際の値は、設定`filesystem_prefetch_min_bytes_for_single_read_task`により異なる場合があります。

## filesystem_prefetches_limit {#filesystem_prefetches_limit}

Type: UInt64

Default value: 200

プレフェッチの最大数。ゼロは制限なしを意味します。プレフェッチの数を制限したい場合、設定`filesystem_prefetches_max_memory_usage`を推奨します。

## final {#final}

Type: Bool

Default value: 0

クエリ内のすべてのテーブルに[FINAL](../../sql-reference/statements/select/from.md/#final-modifier)修飾子を自動的に適用します。[FINAL](../../sql-reference/statements/select/from.md/#final-modifier)が適用可能なテーブル、結合されたテーブル、サブクエリ中のテーブル、および分散テーブルに適用されます。

Possible values:

- 0 - 無効
- 1 - 有効

Example:

```sql
CREATE TABLE test
(
    key Int64,
    some String
)
ENGINE = ReplacingMergeTree
ORDER BY key;

INSERT INTO test FORMAT Values (1, 'first');
INSERT INTO test FORMAT Values (1, 'second');

SELECT * FROM test;
┌─key─┬─some───┐
│   1 │ second │
└─────┴────────┘
┌─key─┬─some──┐
│   1 │ first │
└─────┴───────┘

SELECT * FROM test SETTINGS final = 1;
┌─key─┬─some───┐
│   1 │ second │
└─────┴────────┘

SET final = 1;
SELECT * FROM test;
┌─key─┬─some───┐
│   1 │ second │
└─────┴────────┘
```

## flatten_nested {#flatten_nested}

Type: Bool

Default value: 1

[nested](../../sql-reference/data-types/nested-data-structures/index.md)カラムのデータフォーマットを設定します。

Possible values:

- 1 — ネストされたカラムは別々の配列にフラット化されます。
- 0 — ネストされたカラムはタプルの単一配列として残ります。

**Usage**

設定が`0`に設定されている場合、任意のネストレベルを使用できます。

**Examples**

Query:

``` sql
SET flatten_nested = 1;
CREATE TABLE t_nest (`n` Nested(a UInt32, b UInt32)) ENGINE = MergeTree ORDER BY tuple();

SHOW CREATE TABLE t_nest;
```

Result:

``` text
┌─statement───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ CREATE TABLE default.t_nest
(
    `n.a` Array(UInt32),
    `n.b` Array(UInt32)
)
ENGINE = MergeTree
ORDER BY tuple()
SETTINGS index_granularity = 8192 │
└─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
```

Query:

``` sql
SET flatten_nested = 0;

CREATE TABLE t_nest (`n` Nested(a UInt32, b UInt32)) ENGINE = MergeTree ORDER BY tuple();

SHOW CREATE TABLE t_nest;
```

Result:

``` text
┌─statement──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ CREATE TABLE default.t_nest
(
    `n` Nested(a UInt32, b UInt32)
)
ENGINE = MergeTree
ORDER BY tuple()
SETTINGS index_granularity = 8192 │
└────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
```

## force_aggregate_partitions_independently {#force_aggregate_partitions_independently}

Type: Bool

Default value: 0

最適化を使用することを強制しますが、ヒューリスティックが使用しないことを決定した場合。

## force_aggregation_in_order {#force_aggregation_in_order}

Type: Bool

Default value: 0

設定は、サーバー自体によって分散クエリをサポートするために使用されます。手動で変更しないでください。さもないと、通常の動作が妨げられます（分散集計中にリモートノードでの順序の集計を強制します）。

## force_data_skipping_indices {#force_data_skipping_indices}

Type: String

Default value: 

指定されたデータスキッピングインデックスが使用されなかった場合、クエリの実行を無効にします。

以下の例を考えてみてください：

```sql
CREATE TABLE data
(
    key Int,
    d1 Int,
    d1_null Nullable(Int),
    INDEX d1_idx d1 TYPE minmax GRANULARITY 1,
    INDEX d1_null_idx assumeNotNull(d1_null) TYPE minmax GRANULARITY 1
)
Engine=MergeTree()
ORDER BY key;

SELECT * FROM data_01515;
SELECT * FROM data_01515 SETTINGS force_data_skipping_indices=''; -- クエリは CANNOT_PARSE_TEXT エラーを生成します。
SELECT * FROM data_01515 SETTINGS force_data_skipping_indices='d1_idx'; -- クエリは INDEX_NOT_USED エラーを生成します。
SELECT * FROM data_01515 WHERE d1 = 0 SETTINGS force_data_skipping_indices='d1_idx'; -- Ok.
SELECT * FROM data_01515 WHERE d1 = 0 SETTINGS force_data_skipping_indices='`d1_idx`'; -- Ok（完全なパーサの例）。
SELECT * FROM data_01515 WHERE d1 = 0 AND assumeNotNull(d1_null) = 0 SETTINGS force_data_skipping_indices='`d1_idx`, d1_null_idx'; -- Ok。
```

## force_grouping_standard_compatibility {#force_grouping_standard_compatibility}

Type: Bool

Default value: 1

GROUPING関数が引数が集計キーとして使用されていない場合、1を返すようにします。

## force_index_by_date {#force_index_by_date}

Type: Bool

Default value: 0

インデックスが日付によって使用できない場合、クエリの実行を無効にします。

MergeTreeファミリーのテーブルで機能します。

`force_index_by_date=1`の場合、ClickHouseは、クエリにデータ範囲を制限するために使用できる日付キー条件があるかどうかを確認します。適切な条件がない場合、例外がスローされます。ただし、条件が読み取るデータ量を削減するかどうかは確認しません。たとえば、条件`Date != ' 2000-01-01 '`は、テーブル内のすべてのデータに一致する場合でも受け入れられます（つまり、クエリを実行するには完全なスキャンが必要です）。MergeTreeテーブルにおけるデータ範囲に関する詳細は、[MergeTree](../../engines/table-engines/mergetree-family/mergetree.md)を参照してください。

## force_optimize_projection {#force_optimize_projection}

Type: Bool

Default value: 0

プロジェクション最適化が有効な場合、`SELECT`クエリでの[プロジェクション](../../engines/table-engines/mergetree-family/mergetree.md/#projections)の必須使用を有効または無効にします。

Possible values:

- 0 — プロジェクション最適化は必須ではありません。
- 1 — プロジェクション最適化は必須です。

## force_optimize_projection_name {#force_optimize_projection_name}

Type: String

Default value: 

非空の文字列に設定されている場合、このプロジェクションがクエリで少なくとも1回使用されていることを確認します。

Possible values:

- string: クエリで使用されるプロジェクションの名前。

## force_optimize_skip_unused_shards {#force_optimize_skip_unused_shards}

Type: UInt64

Default value: 0

[optimize_skip_unused_shards](#optimize_skip_unused_shards)が有効で、未使用のシャードのスキップが不可能である場合、クエリの実行を有効または無効にします。スキップが不可能であり、この設定が有効な場合、例外がスローされます。

Possible values:

- 0 — 無効。ClickHouseは例外をスローしません。
- 1 — 有効。テーブルにシャーディングキーがある場合のみクエリの実行が無効になります。
- 2 — 有効。テーブルにシャーディングキーが定義されているかどうかにかかわらずクエリの実行が無効になります。

## force_optimize_skip_unused_shards_nesting {#force_optimize_skip_unused_shards_nesting}

Type: UInt64

Default value: 0

[`force_optimize_skip_unused_shards`](#force_optimize_skip_unused_shards)を制御します（したがって、[`force_optimize_skip_unused_shards`](#force_optimize_skip_unused_shards)が必要です）。分散クエリのネストレベルに依存します（Distributedテーブルが別のDistributedテーブルを参照している場合）。

Possible values:

- 0 - 無効、`force_optimize_skip_unused_shards`は常に機能します。
- 1 — `force_optimize_skip_unused_shards`を最初のレベルのみに有効にします。
- 2 — `force_optimize_skip_unused_shards`を第二レベルまで有効にします。

## force_primary_key {#force_primary_key}

Type: Bool

Default value: 0

プライマリキーによるインデックスが不可能な場合、クエリの実行を無効にします。

MergeTreeファミリーのテーブルで機能します。

`force_primary_key=1`の場合、ClickHouseは、クエリにデータ範囲を制限するために使用できるプライマリキー条件があるかどうかを確認します。適切な条件がない場合、例外がスローされます。ただし、条件が読み取るデータ量を削減するかどうかは確認しません。MergeTreeテーブルにおけるデータ範囲に関する詳細は、[MergeTree](../../engines/table-engines/mergetree-family/mergetree.md)を参照してください。

## force_remove_data_recursively_on_drop {#force_remove_data_recursively_on_drop}

Type: Bool

Default value: 0

DROPクエリでデータを再帰的に削除します。'Directory not empty'エラーを回避しますが、デタッチされたデータを静かに削除する可能性があります。

## formatdatetime_f_prints_scale_number_of_digits {#formatdatetime_f_prints_scale_number_of_digits}

Type: Bool

Default value: 0

関数'formatDateTime'のフォーマッタ'%f'は、固定の6桁の代わりにDateTime64のスケール数だけを印刷します。

## formatdatetime_f_prints_single_zero {#formatdatetime_f_prints_single_zero}

Type: Bool

Default value: 0

関数'formatDateTime'のフォーマッタ'%f'は、整形された値に小数秒がない場合、一つのゼロを印刷します。

## formatdatetime_format_without_leading_zeros {#formatdatetime_format_without_leading_zeros}

Type: Bool

Default value: 0

関数'formatDateTime'のフォーマッタ'%c'、'%l'、および'%k'は、先頭ゼロなしで月と時間を印刷します。

## formatdatetime_parsedatetime_m_is_month_name {#formatdatetime_parsedatetime_m_is_month_name}

Type: Bool

Default value: 1

関数'formatDateTime'および'parseDateTime'のフォーマッタ'%M'は、分の代わりに月の名前を印刷/解析します。

## fsync_metadata {#fsync_metadata}

Type: Bool

Default value: 1

.sqlファイルを書き込む際に[fsync](http://pubs.opengroup.org/onlinepubs/9699919799/functions/fsync.html)を有効または無効にします。デフォルトでは有効です。

サーバーに何百万もの小さなテーブルが常に作成され、破棄される場合は、無効にする意味があります。

## function_implementation {#function_implementation}

Type: String

Default value: 

特定のターゲットまたはバリアント（実験的）に対する関数の実装を選択します。空の場合はすべてを有効にします。

## function_json_value_return_type_allow_complex {#function_json_value_return_type_allow_complex}

Type: Bool

Default value: 0

json_value関数のために複雑な型（構造体、配列、マップなど）を返すことを許可するかどうかを制御します。

```sql
SELECT JSON_VALUE('{"hello":{"world":"!"}}', '$.hello') settings function_json_value_return_type_allow_complex=true

┌─JSON_VALUE('{"hello":{"world":"!"}}', '$.hello')─┐
│ {"world":"!"}                                    │
└──────────────────────────────────────────────────┘

1 row in set. Elapsed: 0.001 sec.
```

Possible values:

- true — 許可。
- false — 不許可。

## function_json_value_return_type_allow_nullable {#function_json_value_return_type_allow_nullable}

Type: Bool

Default value: 0

JSON_VALUE関数に対して値が存在しない場合に`NULL`を返すことを許可するかどうかを制御します。

```sql
SELECT JSON_VALUE('{"hello":"world"}', '$.b') settings function_json_value_return_type_allow_nullable=true;

┌─JSON_VALUE('{"hello":"world"}', '$.b')─┐
│ ᴺᵁᴸᴸ                                   │
└────────────────────────────────────────┘

1 row in set. Elapsed: 0.001 sec.
```

Possible values:

- true — 許可。
- false — 不許可。

## function_locate_has_mysql_compatible_argument_order {#function_locate_has_mysql_compatible_argument_order}

Type: Bool

Default value: 1

関数[locate](../../sql-reference/functions/string-search-functions.md/#locate)の引数の順序を制御します。

Possible values:

- 0 — 関数`locate`は引数`(haystack, needle[, start_pos])`を受け入れます。
- 1 — 関数`locate`は引数`(needle, haystack, [, start_pos])`を受け入れます（MySQL互換の動作）。

## function_range_max_elements_in_block {#function_range_max_elements_in_block}

Type: UInt64

Default value: 500000000

関数[range](../../sql-reference/functions/array-functions.md/#range)によって生成されるデータボリュームの安全しきい値を設定します。データの各ブロック内で生成される値の最大数を定義します（ブロック内の各行の配列サイズの合計）。

Possible values:

- 正の整数。

**See Also**

- [max_block_size](#max_block_size)
- [min_insert_block_size_rows](#min_insert_block_size_rows)

## function_sleep_max_microseconds_per_block {#function_sleep_max_microseconds_per_block}

Type: UInt64

Default value: 3000000

関数`sleep`が各ブロックで許可される最大マイクロ秒数。ユーザーがそれを大きな値で呼び出した場合、例外がスローされます。これは安全しきい値です。

## function_visible_width_behavior {#function_visible_width_behavior}

Type: UInt64

Default value: 1

`visibleWidth`動作のバージョン。0 - コードポイントの数のみをカウント; 1 - ゼロ幅および組み合わさった文字を正しくカウントし、全幅文字を2つとしてカウントし、タブ幅を推定し、削除文字をカウントします。

## geo_distance_returns_float64_on_float64_arguments {#geo_distance_returns_float64_on_float64_arguments}

Type: Bool

Default value: 1

`geoDistance`、`greatCircleDistance`、`greatCircleAngle`関数の4つの引数がすべてFloat64である場合、Float64を返し、内部計算に二重精度を使用します。以前のClickHouseバージョンでは、関数は常にFloat32を返していました。

## glob_expansion_max_elements {#glob_expansion_max_elements}

Type: UInt64

Default value: 1000

許可されている最大アドレス数（外部ストレージ、テーブル関数など）。 

## grace_hash_join_initial_buckets {#grace_hash_join_initial_buckets}
<ExperimentalBadge/>

Type: NonZeroUInt64

Default value: 1

グレースハッシュ結合のバケットの初期数。

## grace_hash_join_max_buckets {#grace_hash_join_max_buckets}
<ExperimentalBadge/>

Type: NonZeroUInt64

Default value: 1024

グレースハッシュ結合のバケットの数の制限。

## group_by_overflow_mode {#group_by_overflow_mode}

Type: OverflowModeGroupBy

Default value: throw

制限を超えた場合の処理方法。

## group_by_two_level_threshold {#group_by_two_level_threshold}

Type: UInt64

Default value: 100000

2段階集計が開始されるキーの数。0 - しきい値は設定されていません。

## group_by_two_level_threshold_bytes {#group_by_two_level_threshold_bytes}

Type: UInt64

Default value: 50000000

集計状態のサイズ（バイト）がこの値を超えると二段階集計が使用され始めます。0 - しきい値は設定されていません。いずれかのしきい値がトリガーされると、二段階集計が使用されます。

## group_by_use_nulls {#group_by_use_nulls}

Type: Bool

Default value: 0

[GROUP BY句](/sql-reference/statements/select/group-by.md)が集計キーの型をどのように扱うかを変更します。
`ROLLUP`、`CUBE`、または`GROUPING SETS`指定子が使用されると、集計キーの一部は結果行を生成するために使用されない場合があります。
これらのキーに対するカラムは、欠落した値を生成するために、この設定に応じてデフォルト値または`NULL`で埋められます。

Possible values:

- 0 — 集計キー型のデフォルト値を使用して欠落した値を生成します。
- 1 — ClickHouseは、SQL標準に従った同じ方法で`GROUP BY`を実行します。集計キーの型は[Nullable](/sql-reference/data-types/nullable.md/#data_type-nullable)に変換されます。該当する集計キーに対するカラムは、このキーが使用されない行のために[NULL](/sql-reference/syntax.md)で埋められます。

さらに:

- [GROUP BY句](/sql-reference/statements/select/group-by.md)

## h3togeo_lon_lat_result_order {#h3togeo_lon_lat_result_order}

Type: Bool

Default value: 0

関数'h3ToGeo'は、trueの場合に(lon, lat)を返します。それ以外の場合は(lat, lon)を返します。

## handshake_timeout_ms {#handshake_timeout_ms}

Type: Milliseconds

Default value: 10000

ハンドシェイク中にレプリカからHelloパケットを受信するためのタイムアウト（ミリ秒）。

## hdfs_create_new_file_on_insert {#hdfs_create_new_file_on_insert}

Type: Bool

Default value: 0

HDFSエンジンテーブルに対する各挿入時に新しいファイルの作成を有効または無効にします。有効にすると、各挿入で以下のパターンに似た名前の新しいHDFSファイルが作成されます。

initial: `data.Parquet.gz` -> `data.1.Parquet.gz` -> `data.2.Parquet.gz` など。

Possible values:
- 0 — `INSERT`クエリはファイルの末尾に新しいデータを追加します。
- 1 — `INSERT`クエリは新しいファイルを作成します。

## hdfs_ignore_file_doesnt_exist {#hdfs_ignore_file_doesnt_exist}

Type: Bool

Default value: 0

特定のキーを読み込む際にファイルが存在しない場合、その不在を無視します。

Possible values:
- 1 — `SELECT`は空の結果を返します。
- 0 — `SELECT`は例外をスローします。

## hdfs_replication {#hdfs_replication}

Type: UInt64

Default value: 0

hdfsファイルが作成されるときに指定できる実際の複製数。

## hdfs_skip_empty_files {#hdfs_skip_empty_files}

Type: Bool

Default value: 0

[HDFS](../../engines/table-engines/integrations/hdfs.md)エンジンテーブルで空のファイルをスキップするかどうかを有効または無効にします。

Possible values:
- 0 — 空のファイルが要求されたフォーマットと互換性がない場合、`SELECT`は例外をスローします。
- 1 — 空のファイルに対して`SELECT`は空の結果を返します。

## hdfs_throw_on_zero_files_match {#hdfs_throw_on_zero_files_match}

Type: Bool

Default value: 0

グロブ拡張ルールに基づいて一致するファイルがゼロの場合にエラーをスローします。

Possible values:
- 1 — `SELECT`は例外をスローします。
- 0 — `SELECT`は空の結果を返します。

## hdfs_truncate_on_insert {#hdfs_truncate_on_insert}

Type: Bool

Default value: 0

hdfsエンジンテーブルに挿入する前に切り捨てを有効または無効にします。無効の場合、HDFSにすでにファイルが存在する際に挿入しようとすると例外がスローされます。

Possible values:
- 0 — `INSERT`クエリはファイルの末尾に新しいデータを追加します。
- 1 — `INSERT`クエリはファイルの既存の内容を新しいデータに置き換えます。

## hedged_connection_timeout_ms {#hedged_connection_timeout_ms}

Type: Milliseconds

Default value: 50

ヘッジリクエストに対してレプリカとの接続を確立するための接続タイムアウト。

## hnsw_candidate_list_size_for_search {#hnsw_candidate_list_size_for_search}
<ExperimentalBadge/>

Type: UInt64

Default value: 256

ベクトル類似性インデックスの検索時の動的候補リストのサイズ、別名'ef_search'。

## hsts_max_age {#hsts_max_age}

Type: UInt64

Default value: 0

HSTSの有効期限。0はHSTSを無効にします。

## http_connection_timeout {#http_connection_timeout}

Type: Seconds

Default value: 1

HTTP接続のタイムアウト（秒）。

Possible values:

- 任意の正の整数。
- 0 - 無効（無限のタイムアウト）。

## http_headers_progress_interval_ms {#http_headers_progress_interval_ms}

Type: UInt64

Default value: 100

HTTPヘッダーX-ClickHouse-Progressを指定された間隔以上で送信しないようにします。

## http_make_head_request {#http_make_head_request}

Type: Bool

Default value: 1

`http_make_head_request`設定は、HTTPからデータを読み取る際にファイルに関する情報（サイズなど）を取得するために`HEAD`リクエストの実行を許可します。デフォルトで有効であるため、サーバーが`HEAD`リクエストをサポートしない場合は、この設定を無効にすることが望ましいです。

## http_max_field_name_size {#http_max_field_name_size}

Type: UInt64

Default value: 131072

HTTPヘッダーにおけるフィールド名の最大長。

## http_max_field_value_size {#http_max_field_value_size}

Type: UInt64

Default value: 131072

HTTPヘッダーにおけるフィールド値の最大長。

## http_max_fields {#http_max_fields}

Type: UInt64

Default value: 1000000

HTTPヘッダーにおける最大フィールド数。

## http_max_multipart_form_data_size {#http_max_multipart_form_data_size}

Type: UInt64

Default value: 1073741824

multipart/form-dataコンテンツのサイズの制限。この設定はURLパラメーターから解析できず、ユーザープロファイル内で設定する必要があります。HTTPフォームデータの読み取り開始前に、コンテンツが解析され、外部テーブルがメモリに作成されます。そして、これがその段階で影響を及ぼす唯一の制限です（最大メモリ使用量と最大実行時間の制限は、HTTPフォームデータの読み取り中には影響しません）。

## http_max_request_param_data_size {#http_max_request_param_data_size}

Type: UInt64

Default value: 10485760

事前定義されたHTTPリクエストにおけるクエリパラメータとして使用されるリクエストデータのサイズの制限。

## http_max_tries {#http_max_tries}

Type: UInt64

Default value: 10

HTTPを介して読み取る最大試行回数。

## http_max_uri_size {#http_max_uri_size}

Type: UInt64

Default value: 1048576

HTTPリクエストの最大URI長を設定します。

Possible values:

- 正の整数。

## http_native_compression_disable_checksumming_on_decompress {#http_native_compression_disable_checksumming_on_decompress}

Type: Bool

Default value: 0

クライアントからのHTTP POSTデータを展開する際にチェックサムの検証を有効または無効にします。ClickHouseネイティブ圧縮形式にのみ使用されます（`gzip`や`deflate`では使用されません）。

詳細については、[HTTPインターフェースの説明](../../interfaces/http.md)を参照してください。

Possible values:

- 0 — 無効。
- 1 — 有効。

## http_receive_timeout {#http_receive_timeout}

Type: Seconds

Default value: 30

HTTP受信タイムアウト（秒）。

Possible values:

- 任意の正の整数。
- 0 - 無効（無限のタイムアウト）。

## http_response_buffer_size {#http_response_buffer_size}

Type: UInt64

Default value: 0

クライアントへのHTTPレスポンスを送信する前にサーバーメモリにバッファするバイト数（http_wait_end_of_queryが有効な場合、ディスクにフラッシュされます）。
デフォルト値: 30

HTTP送信タイムアウト（秒単位）。

可能な値：

- 任意の正の整数。
- 0 - 無効（無限タイムアウト）。

:::note
これはデフォルトプロファイルにのみ適用されます。変更を有効にするにはサーバーの再起動が必要です。
:::

## http_skip_not_found_url_for_globs {#http_skip_not_found_url_for_globs}

タイプ: Bool

デフォルト値: 1

HTTP_NOT_FOUNDエラーを伴うグロブのURLをスキップします。

## http_wait_end_of_query {#http_wait_end_of_query}

タイプ: Bool

デフォルト値: 0

サーバー側でのHTTPレスポンスバッファリングを有効にします。

## http_write_exception_in_output_format {#http_write_exception_in_output_format}

タイプ: Bool

デフォルト値: 1

有効な出力を生成するために出力形式に例外を書き込みます。JSONおよびXML形式で機能します。

## http_zlib_compression_level {#http_zlib_compression_level}

タイプ: Int64

デフォルト値: 3

[enable_http_compression = 1](#enable_http_compression)の場合、HTTPリクエストへのレスポンスのデータ圧縮レベルを設定します。

可能な値：1から9までの数字。

## idle_connection_timeout {#idle_connection_timeout}

タイプ: UInt64

デフォルト値: 3600

指定された秒数の後にアイドルTCP接続を閉じるためのタイムアウト。

可能な値：

- 正の整数（0 - 即座に閉じる、0秒後）。

## ignore_cold_parts_seconds {#ignore_cold_parts_seconds}

タイプ: Int64

デフォルト値: 0

ClickHouse Cloudでのみ使用可能。新しいデータパーツをSELECTクエリから除外し、それらがプリウォーム（[cache_populated_by_fetch](merge-tree-settings.md/#cache_populated_by_fetch)を参照）されているか、または指定された秒数以上古くなるまで除外します。Replicated-/SharedMergeTree専用です。

## ignore_data_skipping_indices {#ignore_data_skipping_indices}

タイプ: String

デフォルト値: 

クエリで使用されている場合に指定されたスキッピングインデックスを無視します。

次の例を考えてみてください：

```sql
CREATE TABLE data
(
    key Int,
    x Int,
    y Int,
    INDEX x_idx x TYPE minmax GRANULARITY 1,
    INDEX y_idx y TYPE minmax GRANULARITY 1,
    INDEX xy_idx (x,y) TYPE minmax GRANULARITY 1
)
Engine=MergeTree()
ORDER BY key;

INSERT INTO data VALUES (1, 2, 3);

SELECT * FROM data;
SELECT * FROM data SETTINGS ignore_data_skipping_indices=''; -- クエリはCANNOT_PARSE_TEXTエラーを生成します。
SELECT * FROM data SETTINGS ignore_data_skipping_indices='x_idx'; -- OK。
SELECT * FROM data SETTINGS ignore_data_skipping_indices='na_idx'; -- OK。

SELECT * FROM data WHERE x = 1 AND y = 1 SETTINGS ignore_data_skipping_indices='xy_idx',force_data_skipping_indices='xy_idx'; -- クエリはINDEX_NOT_USEDエラーを生成します。xy_idxが明示的に無視されるためです。
SELECT * FROM data WHERE x = 1 AND y = 2 SETTINGS ignore_data_skipping_indices='xy_idx';
```

インデックスを無視しないクエリ：
```sql
EXPLAIN indexes = 1 SELECT * FROM data WHERE x = 1 AND y = 2;

Expression ((Projection + Before ORDER BY))
  Filter (WHERE)
    ReadFromMergeTree (default.data)
    Indexes:
      PrimaryKey
        Condition: true
        Parts: 1/1
        Granules: 1/1
      Skip
        Name: x_idx
        Description: minmax GRANULARITY 1
        Parts: 0/1
        Granules: 0/1
      Skip
        Name: y_idx
        Description: minmax GRANULARITY 1
        Parts: 0/0
        Granules: 0/0
      Skip
        Name: xy_idx
        Description: minmax GRANULARITY 1
        Parts: 0/0
        Granules: 0/0
```

`xy_idx`インデックスを無視する場合：
```sql
EXPLAIN indexes = 1 SELECT * FROM data WHERE x = 1 AND y = 2 SETTINGS ignore_data_skipping_indices='xy_idx';

Expression ((Projection + Before ORDER BY))
  Filter (WHERE)
    ReadFromMergeTree (default.data)
    Indexes:
      PrimaryKey
        Condition: true
        Parts: 1/1
        Granules: 1/1
      Skip
        Name: x_idx
        Description: minmax GRANULARITY 1
        Parts: 0/1
        Granules: 0/1
      Skip
        Name: y_idx
        Description: minmax GRANULARITY 1
        Parts: 0/0
        Granules: 0/0
```

MergeTreeファミリーのテーブルで機能します。

## ignore_drop_queries_probability {#ignore_drop_queries_probability}

タイプ: Float

デフォルト値: 0

有効にすると、サーバーは指定された確率で全てのDROPテーブルクエリを無視します（MemoryおよびJOINエンジンの場合、DROPをTRUNCATEに置き換えます）。テスト目的で使用されます。

## ignore_materialized_views_with_dropped_target_table {#ignore_materialized_views_with_dropped_target_table}

タイプ: Bool

デフォルト値: 0

ビューにプッシュされる際に、ドロップされたターゲットテーブルを持つMVを無視します。

## ignore_on_cluster_for_replicated_access_entities_queries {#ignore_on_cluster_for_replicated_access_entities_queries}

タイプ: Bool

デフォルト値: 0

レプリケートされたアクセスエンティティ管理クエリのON CLUSTER句を無視します。

## ignore_on_cluster_for_replicated_named_collections_queries {#ignore_on_cluster_for_replicated_named_collections_queries}

タイプ: Bool

デフォルト値: 0

レプリケートされた名前付きコレクション管理クエリのON CLUSTER句を無視します。

## ignore_on_cluster_for_replicated_udf_queries {#ignore_on_cluster_for_replicated_udf_queries}

タイプ: Bool

デフォルト値: 0

レプリケートされたUDF管理クエリのON CLUSTER句を無視します。

## implicit_select {#implicit_select}

タイプ: Bool

デフォルト値: 0

選択キーが前にないシンプルなSELECTクエリの書き込みを許可します。これにより、計算機スタイルの使用が容易になります。例えば、`1 + 2`は有効なクエリになります。

`clickhouse-local`ではデフォルトで有効になっており、明示的に無効化できます。

## implicit_transaction {#implicit_transaction}
<ExperimentalBadge/>

タイプ: Bool

デフォルト値: 0

有効にしていて、すでにトランザクション内でない場合は、クエリを完全なトランザクション（開始 + コミットまたはロールバック）でラップします。

## input_format_parallel_parsing {#input_format_parallel_parsing}

タイプ: Bool

デフォルト値: 1

データ形式の順序を保持する並列解析を有効または無効にします。サポートされているのは[TSV](../../interfaces/formats.md/#tabseparated)、[TSKV](../../interfaces/formats.md/#tskv)、[CSV](../../interfaces/formats.md/#csv)および[JSONEachRow](../../interfaces/formats.md/#jsoneachrow)形式のみです。

可能な値：

- 1 — 有効。
- 0 — 無効。

## insert_allow_materialized_columns {#insert_allow_materialized_columns}

タイプ: Bool

デフォルト値: 0

設定が有効な場合、INSERTでのマテリアライズカラムを許可します。

## insert_deduplicate {#insert_deduplicate}

タイプ: Bool

デフォルト値: 1

`INSERT`（Replicated*テーブル向け）のブロック重複排除を有効または無効にします。

可能な値：

- 0 — 無効。
- 1 — 有効。

デフォルトでは、`INSERT`文によってレプリケートされたテーブルに挿入されたブロックは重複排除されます（[データレプリケーション](../../engines/table-engines/mergetree-family/replication.md)を参照）。
レプリケートされたテーブルの場合、デフォルトでは各パーティションの最新の100ブロックのみが重複排除されます（[replicated_deduplication_window](merge-tree-settings.md/#replicated-deduplication-window)、[replicated_deduplication_window_seconds](merge-tree-settings.md/#replicated-deduplication-window-seconds)を参照）。
レプリケートされていないテーブルの場合は[non_replicated_deduplication_window](merge-tree-settings.md/#non-replicated-deduplication-window)を参照。

## insert_deduplication_token {#insert_deduplication_token}

タイプ: String

デフォルト値: 

この設定は、MergeTree/ReplicatedMergeTreeで独自の重複排除セマンティクスを提供することをユーザーに許可します。
例えば、各INSERT文で設定に対してユニークな値を提供することにより、同じデータが重複排除されるのを回避できます。

可能な値：

- 任意の文字列

`insert_deduplication_token`は、空でない場合にのみ重複排除に使用されます。

レプリケートされたテーブルの場合、デフォルトでは各パーティションに対して最新の100の挿入のみが重複排除されます（[replicated_deduplication_window](merge-tree-settings.md/#replicated-deduplication-window)、[replicated_deduplication_window_seconds](merge-tree-settings.md/#replicated-deduplication-window-seconds)を参照）。
レプリケートされていないテーブルの場合は[non_replicated_deduplication_window](merge-tree-settings.md/#non_replicated_deduplication-window)を参照。

:::note
`insert_deduplication_token`はパーティションレベルで動作します（`insert_deduplication`チェックサムと同様）。複数のパーティションが同じ`insert_deduplication_token`を持つことができます。
:::

例：

```sql
CREATE TABLE test_table
( A Int64 )
ENGINE = MergeTree
ORDER BY A
SETTINGS non_replicated_deduplication_window = 100;

INSERT INTO test_table SETTINGS insert_deduplication_token = 'test' VALUES (1);

-- 次の挿入はinsert_deduplication_tokenが異なるため重複排除されません
INSERT INTO test_table SETTINGS insert_deduplication_token = 'test1' VALUES (1);

-- 次の挿入はinsert_deduplication_tokenが以前のもののいずれかと同じであるため重複排除されます
INSERT INTO test_table SETTINGS insert_deduplication_token = 'test' VALUES (2);

SELECT * FROM test_table

┌─A─┐
│ 1 │
└───┘
┌─A─┐
│ 1 │
└───┘
```

## insert_keeper_fault_injection_probability {#insert_keeper_fault_injection_probability}

タイプ: Float

デフォルト値: 0

挿入中のキーパリクエストの障害の近似確率。有効な値は[0.0f, 1.0f]の範囲です。

## insert_keeper_fault_injection_seed {#insert_keeper_fault_injection_seed}

タイプ: UInt64

デフォルト値: 0

0 - ランダムシード、その他は設定値。

## insert_keeper_max_retries {#insert_keeper_max_retries}

タイプ: UInt64

デフォルト値: 20

この設定は、レプリケートされたMergeTreeへの挿入中にClickHouse Keeper（またはZooKeeper）リクエストの最大再試行回数を設定します。ネットワークエラー、Keeperセッションタイムアウト、またはリクエストタイムアウトにより失敗したKeeperリクエストのみが再試行と見なされます。

可能な値：

- 正の整数。
- 0 — リトライは無効

クラウドのデフォルト値: `20`。

Keeperリクエストの再試行はあるタイムアウト後に行われます。タイムアウトは次の設定で制御されます：`insert_keeper_retry_initial_backoff_ms`、`insert_keeper_retry_max_backoff_ms`。
最初の再試行は、`insert_keeper_retry_initial_backoff_ms`のタイムアウトの後に行われます。次のタイムアウトは次のように計算されます：
```
timeout = min(insert_keeper_retry_max_backoff_ms, latest_timeout * 2)
```

例えば、`insert_keeper_retry_initial_backoff_ms=100`、`insert_keeper_retry_max_backoff_ms=10000`、`insert_keeper_max_retries=8`の場合、タイムアウトは`100, 200, 400, 800, 1600, 3200, 6400, 10000`になります。

障害耐性のほか、再試行はより良いユーザー体験を提供することを目的としています - 例えば、Keeperがアップグレードのために再起動された場合、INSERT実行中にエラーを返さないことを可能にします。

## insert_keeper_retry_initial_backoff_ms {#insert_keeper_retry_initial_backoff_ms}

タイプ: UInt64

デフォルト値: 100

INSERTクエリの実行中に失敗したKeeperリクエストを再試行するための初期タイムアウト（ミリ秒単位）。

可能な値：

- 正の整数。
- 0 — タイムアウトなし

## insert_keeper_retry_max_backoff_ms {#insert_keeper_retry_max_backoff_ms}

タイプ: UInt64

デフォルト値: 10000

INSERTクエリの実行中に失敗したKeeperリクエストを再試行するための最大タイムアウト（ミリ秒単位）。

可能な値：

- 正の整数。
- 0 — 最大タイムアウトは制限なし

## insert_null_as_default {#insert_null_as_default}

タイプ: Bool

デフォルト値: 1

[nullable](../../sql-reference/data-types/nullable.md/#data_type-nullable)データ型を持つカラムに対して[NULL](../../sql-reference/syntax.md/#null-literal)の代わりに[デフォルト値](../../sql-reference/statements/create/table.md/#create-default-values)を挿入できるようにします。
カラムの型がnullableでない場合、この設定が無効なときにNULLを挿入すると例外が発生します。カラムの型がnullableの場合、NULL値はこの設定に関係なく挿入されます。

この設定は[INSERT ... SELECT](../../sql-reference/statements/insert-into.md/#inserting-the-results-of-select)クエリに適用されます。`SELECT`サブクエリは`UNION ALL`句で連結される場合があります。

可能な値：

- 0 — nullableでないカラムにNULLを挿入すると例外が発生します。
- 1 — NULLの代わりにカラムのデフォルト値が挿入されます。

## insert_quorum {#insert_quorum}

タイプ: UInt64Auto

デフォルト値: 0

:::note
この設定はSharedMergeTreeには適用されません。詳細は[SharedMergeTreeの整合性](/cloud/reference/shared-merge-tree/#consistency)を参照してください。
:::

クオーラム書き込みを有効にします。

- `insert_quorum < 2`の場合、クオーラム書き込みは無効になります。
- `insert_quorum >= 2`の場合、クオーラム書き込みが有効になります。
- `insert_quorum = 'auto'`の場合、過半数の数（`number_of_replicas / 2 + 1`）がクオーラム数として使用されます。

クオーラム書き込み

`INSERT`は、ClickHouseが`insert_quorum_timeout`中に`insert_quorum`のレプリカに対してデータを書き込むことに成功した場合にのみ成功します。何らかの理由で成功した書き込みのレプリカ数が`insert_quorum`に達しない場合、書き込みは失敗と見なされ、ClickHouseはデータがすでに書き込まれた全てのレプリカから挿入されたブロックを削除します。

`insert_quorum_parallel`が無効の場合、クオーラム内の全てのレプリカは一貫性があります。すなわち、これらは全ての以前の`INSERT`クエリからデータを含んでいます（`INSERT`シーケンスは線形化されています）。`insert_quorum`および`insert_quorum_parallel`を無効にして書き込まれたデータを読み取る際に、[select_sequential_consistency](#select_sequential_consistency)を使用して`SELECT`クエリのために逐次整合性を有効にすることができます。

ClickHouseは例外を生成します：

- クエリの実行時に利用可能なレプリカの数が`insert_quorum`未満の場合。
- `insert_quorum_parallel`が無効で、前のブロックが`insert_quorum`のレプリカに挿入されていない状態でデータの書き込みを試みた場合。この状況は、ユーザーが前のINSERTクエリが完了する前に同じテーブルに対して別の`INSERT`クエリを実行しようとした場合に発生することがあります。

次も参照してください：

- [insert_quorum_timeout](#insert_quorum_timeout)
- [insert_quorum_parallel](#insert_quorum_parallel)
- [select_sequential_consistency](#select_sequential_consistency)

## insert_quorum_parallel {#insert_quorum_parallel}

タイプ: Bool

デフォルト値: 1

:::note
この設定はSharedMergeTreeには適用されません。詳細は[SharedMergeTreeの整合性](/cloud/reference/shared-merge-tree/#consistency)を参照してください。
:::

クオーラム`INSERT`クエリの並列性を有効または無効にします。有効にすると、前のクエリがまだ終了していない間に追加の`INSERT`クエリを送信できます。無効にすると、同じテーブルへの追加の書き込みは拒否されます。

可能な値：

- 0 — 無効。
- 1 — 有効。

次も参照してください：

- [insert_quorum](#insert_quorum)
- [insert_quorum_timeout](#insert_quorum_timeout)
- [select_sequential_consistency](#select_sequential_consistency)

## insert_quorum_timeout {#insert_quorum_timeout}

タイプ: ミリ秒

デフォルト値: 600000

クオーラムへの書き込みタイムアウト（ミリ秒単位）。タイムアウトが過ぎても書き込みが行われていない場合、ClickHouseは例外を生成し、クライアントは同じブロックを同じまたは他のレプリカに書き込むためのクエリを繰り返さなければなりません。

次も参照してください：

- [insert_quorum](#insert_quorum)
- [insert_quorum_parallel](#insert_quorum_parallel)
- [select_sequential_consistency](#select_sequential_consistency)

## insert_shard_id {#insert_shard_id}

タイプ: UInt64

デフォルト値: 0

`0`でない場合、データが同期的に挿入される[分散テーブル](../../engines/table-engines/special/distributed.md/#distributed)のシャードを指定します。

`insert_shard_id`の値が不正な場合、サーバーは例外を投げます。

`requested_cluster`上のシャード数を取得するには、サーバーの設定を確認するか、次のクエリを使用できます：

``` sql
SELECT uniq(shard_num) FROM system.clusters WHERE cluster = 'requested_cluster';
```

可能な値：

- 0 — 無効。
- 対応する[分散テーブル](../../engines/table-engines/special/distributed.md/#distributed)の`1`から`shards_num`までの任意の数。

**例**

クエリ：

```sql
CREATE TABLE x AS system.numbers ENGINE = MergeTree ORDER BY number;
CREATE TABLE x_dist AS x ENGINE = Distributed('test_cluster_two_shards_localhost', currentDatabase(), x);
INSERT INTO x_dist SELECT * FROM numbers(5) SETTINGS insert_shard_id = 1;
SELECT * FROM x_dist ORDER BY number ASC;
```

結果：

``` text
┌─number─┐
│      0 │
│      0 │
│      1 │
│      1 │
│      2 │
│      2 │
│      3 │
│      3 │
│      4 │
│      4 │
└────────┘
```

## interactive_delay {#interactive_delay}

タイプ: UInt64

デフォルト値: 100000

リクエスト実行がキャンセルされたかどうかを確認し、進行状況を送信するためのマイクロ秒単位の間隔。

## intersect_default_mode {#intersect_default_mode}

タイプ: SetOperationMode

デフォルト値: ALL

INTERSECTクエリのデフォルトモードを設定します。可能な値：空文字列、'ALL'、'DISTINCT'。空の場合、モードなしのクエリは例外をスローします。

## join_algorithm {#join_algorithm}

タイプ: JoinAlgorithm

デフォルト値: direct,parallel_hash,hash

どの[JOIN](../../sql-reference/statements/select/join.md)アルゴリズムが使用されるかを指定します。

いくつかのアルゴリズムを指定でき、特定のクエリに基づいて種別/厳密さおよびテーブルエンジンに応じて利用可能なものが選択されます。

可能な値：

- grace_hash

 [グレースハッシュジョイン](https://en.wikipedia.org/wiki/Hash_join#Grace_hash_join)が使用されます。グレースハッシュは、メモリ使用量を制限しつつパフォーマンスの高い複雑なジョインを提供するアルゴリズムのオプションを提供します。

 グレースジョインの最初の段階では、右テーブルを読み取り、キー列のハッシュ値に基づいてNバケットに分割します（最初は、Nは`grace_hash_join_initial_buckets`です）。これは、各バケットが独立して処理できるように行われます。最初のバケットから行をメモリ内のハッシュテーブルに追加し、他のバケットはディスクに保存します。ハッシュテーブルがメモリ制限を超えた場合（例：[`max_bytes_in_join`](/operations/settings/query-complexity.md/#max_bytes_in_join)で設定されたように）、バケットの数が増加し、各行の割り当てられたバケットも増加します。現在のバケットに属さない行はフラッシュされ、再割り当てされます。

 `INNER/LEFT/RIGHT/FULL ALL/ANY JOIN`をサポートしています。

- hash

 [ハッシュジョインアルゴリズム](https://en.wikipedia.org/wiki/Hash_join)が使用されます。最も一般的な実装で、すべての組み合わせの種別と厳密さ、複数の結合キーをサポートします。

 `hash`アルゴリズムを使用する場合、右側のJOINの部分はRAMにアップロードされます。

- parallel_hash

 `hash`ジョインの変種で、データをバケットに分割し、複数のハッシュテーブルを同時に構築し、このプロセスを加速します。

 `parallel_hash`アルゴリズムを使用する場合、右側のJOINの部分はRAMにアップロードされます。

- partial_merge

 [ソートマージアルゴリズム](https://en.wikipedia.org/wiki/Sort-merge_join)の変種で、右テーブルのみが完全にソートされています。

 `RIGHT JOIN`および`FULL JOIN`は、`ALL`厳密さ（`SEMI`、`ANTI`、`ANY`、および`ASOF`はサポートされていません）でのみサポートされます。

 `partial_merge`アルゴリズムを使用する場合、ClickHouseはデータをソートし、それをディスクにダンプします。ClickHouseの`partial_merge`アルゴリズムは、従来の実装とは若干異なります。まず、ClickHouseは結合キーで右テーブルをブロックごとにソートし、ソートされたブロックのために最小最大インデックスを作成します。次に、左テーブルの部分を結合キーでソートし、それを右テーブルと結合します。最小最大インデックスは不要な右テーブルブロックをスキップするためにも使用されます。

- direct

 このアルゴリズムは、右テーブルがキー値リクエストをサポートしているストレージの場合に適用できます。

 `direct`アルゴリズムは、左テーブルからの行をキーとして使用して右テーブルでルックアップを実行します。これは、[Dictionary](../../engines/table-engines/special/dictionary.md/#dictionary)や[EmbeddedRocksDB](../../engines/table-engines/integrations/embedded-rocksdb.md)のような特別なストレージでのみサポートされています。また、`LEFT`および`INNER`JOINのみサポートされています。

- auto

 `auto`に設定されると、最初に`hash`ジョインが試みられ、メモリ制限が違反された場合、他のアルゴリズムに動的に切り替えられます。

- full_sorting_merge

 [ソートマージアルゴリズム](https://en.wikipedia.org/wiki/Sort-merge_join)で、ジョインする前に結合したテーブルを完全にソートします。

- prefer_partial_merge

 ClickHouseは可能な場合は常に`partial_merge`ジョインを使用し、そうでない場合は`hash`を使用しようとします。*非推奨*、`partial_merge,hash`と同様。

- default (非推奨)

 過去の値であり、もう今後は使用しないでください。
 `direct,hash`と同じで、つまり、直接結合とハッシュ結合を試みます（この順序で）。

## join_any_take_last_row {#join_any_take_last_row}

タイプ: Bool

デフォルト値: 0

`ANY`厳密さを持つ結合操作の動作を変更します。

:::note
この設定は[Join](../../engines/table-engines/special/join.md)エンジンテーブルに対する`JOIN`操作にのみ適用されます。
:::

可能な値：

- 0 — 右テーブルに一致する行が複数ある場合、最初に見つかった行のみが結合されます。
- 1 — 右テーブルに一致する行が複数ある場合、最後に見つかった行のみが結合されます。

次を参照してください：

- [JOIN句](../../sql-reference/statements/select/join.md/#select-join)
- [Joinテーブルエンジン](../../engines/table-engines/special/join.md)
- [join_default_strictness](#join_default_strictness)

## join_default_strictness {#join_default_strictness}

タイプ: JoinStrictness

デフォルト値: ALL

[JOIN句](../../sql-reference/statements/select/join.md/#select-join)のデフォルトの厳密さを設定します。

可能な値：

- `ALL` — 右テーブルに一致する行が複数ある場合、ClickHouseは一致する行の[直積](https://en.wikipedia.org/wiki/Cartesian_product)を作成します。これは標準SQLからの通常の`JOIN`動作です。
- `ANY` — 右テーブルに一致する行が複数ある場合、最初に見つかった行のみが結合されます。右テーブルに一致する行が1つだけある場合、`ANY`と`ALL`の結果は同じです。
- `ASOF` — 一致が不確定なシーケンスを結合します。
- 空文字列 — クエリに`ALL`または`ANY`が指定されていない場合、ClickHouseは例外をスローします。

## join_on_disk_max_files_to_merge {#join_on_disk_max_files_to_merge}

タイプ: UInt64

デフォルト値: 64

ディスク上でMergeJoin操作が実行されるときの並列ソートに許可されるファイルの数を制限します。

設定の値が大きいほど、より多くのRAMが使用され、より少ないディスクI/Oが必要です。

可能な値：

- 任意の正の整数、2以上から始まる。

## join_output_by_rowlist_perkey_rows_threshold {#join_output_by_rowlist_perkey_rows_threshold}

タイプ: UInt64

デフォルト値: 5

ハッシュジョインで行リストによって出力するかどうかを決定するために、右テーブルのキーごとの平均行数の下限。

## join_overflow_mode {#join_overflow_mode}

タイプ: OverflowMode

デフォルト値: throw

制限が超えたときに何をするか。

## join_to_sort_maximum_table_rows {#join_to_sort_maximum_table_rows}
<ExperimentalBadge/>

タイプ: UInt64

デフォルト値: 10000

左または内部結合で、右テーブルのキーによって再範囲指定するかどうかを決定するための右テーブル内の最大行数。

## join_to_sort_minimum_perkey_rows {#join_to_sort_minimum_perkey_rows}
<ExperimentalBadge/>

タイプ: UInt64

デフォルト値: 40

左または内部結合で、右テーブルのキーによって再範囲指定するかどうかを決定するためのキーごとの平均行数の下限。この設定は、スパースなテーブルキーに対して最適化が適用されないようにします。

## join_use_nulls {#join_use_nulls}

タイプ: Bool

デフォルト値: 0

[JOIN](../../sql-reference/statements/select/join.md)動作のタイプを設定します。テーブルを統合すると、空のセルが現れる場合があります。ClickHouseはこの設定に基づいてそれらを異なって埋めます。

可能な値：

- 0 — 空のセルは対応するフィールド型のデフォルト値で埋められます。
- 1 — `JOIN`は標準SQLと同様に動作します。対応するフィールドの型は[Nullable](../../sql-reference/data-types/nullable.md/#data_type-nullable)に変換され、空のセルは[NULL](../../sql-reference/syntax.md)で埋められます。

## joined_subquery_requires_alias {#joined_subquery_requires_alias}

タイプ: Bool

デフォルト値: 1

正しい名前の修飾のために、結合されたサブクエリおよびテーブル関数にエイリアスを必須とします。

## kafka_disable_num_consumers_limit {#kafka_disable_num_consumers_limit}

タイプ: Bool

デフォルト値: 0

使用可能なCPUコア数に依存するkafka_num_consumers制限を無効にします。

## kafka_max_wait_ms {#kafka_max_wait_ms}

タイプ: ミリ秒

デフォルト値: 5000

リトライ前に[Kafka](../../engines/table-engines/integrations/kafka.md/#kafka)からメッセージを読み取る際の待機時間（ミリ秒単位）。

可能な値：

- 正の整数。
- 0 — 無限タイムアウト。

次も参照してください：

- [Apache Kafka](https://kafka.apache.org/)

## keeper_map_strict_mode {#keeper_map_strict_mode}

タイプ: Bool

デフォルト値: 0

KeeperMap上の操作中に追加のチェックを強制します。例えば、すでに存在するキーへの挿入時に例外をスローします。

## keeper_max_retries {#keeper_max_retries}

タイプ: UInt64

デフォルト値: 10

一般的なKeeper操作の最大再試行数。

## keeper_retry_initial_backoff_ms {#keeper_retry_initial_backoff_ms}

タイプ: UInt64

デフォルト値: 100

一般的なKeeper操作の初期バックオフタイムアウト。

## keeper_retry_max_backoff_ms {#keeper_retry_max_backoff_ms}

タイプ: UInt64

デフォルト値: 5000

一般的なKeeper操作の最大バックオフタイムアウト。

## least_greatest_legacy_null_behavior {#least_greatest_legacy_null_behavior}

タイプ: Bool

デフォルト値: 0

有効にすると、関数'least'および'greatest'はその引数のいずれかがNULLの場合にNULLを返します。

## legacy_column_name_of_tuple_literal {#legacy_column_name_of_tuple_literal}

タイプ: Bool

デフォルト値: 0

大きなタプルリテラルの各要素の名前をハッシュではなくそのカラム名として全てリストします。この設定は互換性の理由からのみ存在します。バージョン21.7よりも低いものから高いものへのクラスタのロールアップデートを行う際に設定することが理にかなっています。

## lightweight_deletes_sync {#lightweight_deletes_sync}

タイプ: UInt64

デフォルト値: 2

[`mutations_sync`](#mutations_sync)と同様ですが、軽量削除の実行のみを制御します。

可能な値：

- 0 - ミューテーションは非同期で実行されます。
- 1 - クエリは現在のサーバーで軽量削除が完了するまで待機します。
- 2 - クエリは全てのレプリカで軽量削除が完了するまで待機します（存在する場合）。

**参照**

- [ALTERクエリの同期性](../../sql-reference/statements/alter/index.md/#synchronicity-of-alter-queries)
- [ミューテーション](../../sql-reference/statements/alter/index.md/#mutations)

## limit {#limit}

タイプ: UInt64

デフォルト値: 0

クエリ結果から取得する最大行数を設定します。これにより、[LIMIT](../../sql-reference/statements/select/limit.md/#limit-clause)句で設定された値が調整され、クエリで指定された制限が、この設定によって設定された制限を超えることはできなくなります。

可能な値：

- 0 — 行数に制限はありません。
- 正の整数。

## live_view_heartbeat_interval {#live_view_heartbeat_interval}
<ExperimentalBadge/>

タイプ: 秒

デフォルト値: 15

ライブクエリが生きていることを示すためのハートビート間隔（秒単位）。

## load_balancing {#load_balancing}

タイプ: LoadBalancing

デフォルト値: random

分散クエリ処理に使用されるレプリカ選択アルゴリズムを指定します。

ClickHouseは次のレプリカ選択アルゴリズムをサポートします：

- [ランダム](#load_balancing-random)（デフォルト）
- [最近接ホスト名](#load_balancing-nearest_hostname)
- [ホスト名レーベンシュタイン距離](#load_balancing-hostname_levenshtein_distance)
- [順番に](#load_balancing-in_order)
- [最初またはランダム](#load_balancing-first_or_random)
- [ラウンドロビン](#load_balancing-round_robin)

次も参照してください：

- [distributed_replica_max_ignored_errors](#distributed_replica_max_ignored_errors)

### ランダム（デフォルト） {#load_balancing-random}

``` sql
load_balancing = random
```

各レプリカのエラー数がカウントされます。クエリは、最も少ないエラーのレプリカに送信され、同数のエラーがある場合はその中のいずれかに送信されます。
欠点：サーバーの近接性は考慮されず、レプリカ間でデータが異なる場合は、異なるデータを得ることになります。

### 最近接ホスト名 {#load_balancing-nearest_hostname}

``` sql
load_balancing = nearest_hostname
```

各レプリカのエラー数がカウントされます。5分ごとに、エラー数は2で割られます。したがって、最近の時間におけるエラー数が指数的に平滑化されて計算されます。最小のエラー数を持つレプリカが1つある場合（つまり、他のレプリカで最近エラーが発生した場合）、クエリはそのレプリカに送信されます。同数の最小エラー数を持つレプリカが複数ある場合、クエリは設定ファイル内のサーバーのホスト名に最も類似したホスト名を持つレプリカに送信されます（同一位置の代替文字数でカウントされ、両方のホスト名の最小長にまで到達します）。

例えば、example01-01-1とexample01-01-2は1つの位置で異なり、example01-01-1とexample01-02-2は2つの場所で異なります。
この方法は原始的に見えるかもしれませんが、ネットワークトポロジーに関する外部データを必要とせず、IPアドレスを比較することも複雑であるため、IPv6アドレスでの比較が行われません。

そのため、同等のレプリカがある場合、名前によって最も近いものが優先されます。
同じサーバーにクエリを送信する際に、障害が発生しない限り、分散クエリが同じサーバーに行くことを仮定できます。そのため、レプリカに異なるデータが配置されている場合でも、クエリは主に同じ結果を返します。

### ホスト名レーベンシュタイン距離 {#load_balancing-hostname_levenshtein_distance}

``` sql
load_balancing = hostname_levenshtein_distance
```

`nearest_hostname`と同様ですが、ホスト名を[レーベンシュタイン距離](https://en.wikipedia.org/wiki/Levenshtein_distance)方式で比較します。例えば：

``` text
example-clickhouse-0-0 ample-clickhouse-0-0
1

example-clickhouse-0-0 example-clickhouse-1-10
2

example-clickhouse-0-0 example-clickhouse-12-0
3
```

### 順番に {#load_balancing-in_order}

``` sql
load_balancing = in_order
```
以下の内容を日本語に翻訳します。

Replicas with the same number of errors are accessed in the same order as they are specified in the configuration.  
この方法は、どのレプリカが好ましいかを正確に知っている場合に適しています。

### First or Random {#load_balancing-first_or_random}

``` sql
load_balancing = first_or_random
```

このアルゴリズムは、セット内で最初のレプリカを選択するか、最初のものが利用できない場合にはランダムなレプリカを選択します。これはクロスレプリケーショントポロジーのセットアップで効果的ですが、他の構成では役に立ちません。

`first_or_random`アルゴリズムは、`in_order`アルゴリズムの問題を解決します。`in_order`では、レプリカが1つダウンすると、次のレプリカには二重の負荷がかかり、残りのレプリカは通常のトラフィックを処理します。`first_or_random`アルゴリズムを使用すると、利用可能なレプリカに対して負荷が均等に分配されます。

最初のレプリカが何であるかを明示的に定義するには、設定`load_balancing_first_offset`を使用します。これにより、レプリカ間でクエリの作業負荷を再バランスさせることができます。

### Round Robin {#load_balancing-round_robin}

``` sql
load_balancing = round_robin
```

このアルゴリズムは、同じ数のエラーを持つレプリカに対してラウンドロビン方式を使用します（`round_robin`ポリシーのクエリのみが考慮されます）。

## load_balancing_first_offset {#load_balancing_first_offset}

Type: UInt64

Default value: 0

FIRST_OR_RANDOM負荷分散戦略が使用されるときに、どのレプリカにクエリを優先的に送信するかを指定します。

## load_marks_asynchronously {#load_marks_asynchronously}

Type: Bool

Default value: 0

MergeTreeマークを非同期にロードします。

## local_filesystem_read_method {#local_filesystem_read_method}

Type: String

Default value: pread_threadpool

ローカルファイルシステムからデータを読み取る方法。read、pread、mmap、io_uring、pread_threadpoolのいずれかです。 'io_uring'メソッドは実験的で、Log、TinyLog、StripeLog、File、Set、Join、および同時読み取りと書き込みが存在する場合の追加可能ファイルを持つ他のテーブルでは機能しません。

## local_filesystem_read_prefetch {#local_filesystem_read_prefetch}

Type: Bool

Default value: 0

ローカルファイルシステムからデータを読み取るときにプリフェッチを使用するかどうかを指定します。

## lock_acquire_timeout {#lock_acquire_timeout}

Type: Seconds

Default value: 120

ロック要求が失敗するまでの秒数を定義します。

ロックタイムアウトは、テーブルに対する読み取り/書き込み操作を実行する際にデッドロックを防ぐために使用されます。タイムアウトが経過し、ロック要求が失敗すると、ClickHouseサーバーは「ロックの試行がタイムアウトしました！デッドロックが回避されました。クライアントは再試行する必要があります。」という例外をスローし、エラーコード`DEADLOCK_AVOIDED`が返されます。

可能な値:
- 正の整数（秒）。
- 0 — ロックタイムアウトなし。

## log_comment {#log_comment}

Type: String

Default value: 

[system.query_log](../system-tables/query_log.md)テーブルの`log_comment`フィールドの値と、サーバーログのコメントテキストを指定します。

これは、サーバーログの可読性を向上させるために使用できます。さらに、[clickhouse-test](../../development/tests.md)を実行した後、`system.query_log`からテストに関連するクエリを選択するのに役立ちます。

可能な値:
- [max_query_size](#max_query_size)を超えない任意の文字列。max_query_sizeを超えた場合、サーバーは例外をスローします。

**例**

クエリ:

``` sql
SET log_comment = 'log_comment test', log_queries = 1;
SELECT 1;
SYSTEM FLUSH LOGS;
SELECT type, query FROM system.query_log WHERE log_comment = 'log_comment test' AND event_date >= yesterday() ORDER BY event_time DESC LIMIT 2;
```

結果:

``` text
┌─type────────┬─query─────┐
│ QueryStart  │ SELECT 1; │
│ QueryFinish │ SELECT 1; │
└─────────────┴───────────┘
```

## log_formatted_queries {#log_formatted_queries}

Type: Bool

Default value: 0

[system.query_log](../../operations/system-tables/query_log.md)システムテーブルにフォーマットされたクエリをログに記録できるようにします（[system.query_log](../../operations/system-tables/query_log.md)内の`formatted_query`カラムに記入されます）。

可能な値:
- 0 — フォーマットされたクエリはシステムテーブルにログされません。
- 1 — フォーマットされたクエリがシステムテーブルにログされます。

## log_processors_profiles {#log_processors_profiles}

Type: Bool

Default value: 1

実行中にプロセッサがデータを待っている時間を`system.processors_profile_log`テーブルに書き込みます。

更に詳しくは:
- [`system.processors_profile_log`](../../operations/system-tables/processors_profile_log.md)
- [`EXPLAIN PIPELINE`](../../sql-reference/statements/explain.md/#explain-pipeline)

## log_profile_events {#log_profile_events}

Type: Bool

Default value: 1

クエリパフォーマンス統計をquery_log, query_thread_logおよびquery_views_logにログします。

## log_queries {#log_queries}

Type: Bool

Default value: 1

クエリロギングを設定します。

この設定でClickHouseに送信されたクエリは、[query_log](../../operations/server-configuration-parameters/settings.md/#query-log)サーバー設定パラメータのルールに従ってログに記録されます。

例:

``` text
log_queries=1
```

## log_queries_cut_to_length {#log_queries_cut_to_length}

Type: UInt64

Default value: 100000

クエリの長さが指定した閾値（バイト）を超える場合、クエリがクエリログに書き込まれる際に切り捨てられます。また、通常のテキストログに印刷されるクエリの長さにも制限があります。

## log_queries_min_query_duration_ms {#log_queries_min_query_duration_ms}

Type: Milliseconds

Default value: 0

有効にされている場合（ゼロ以外）、この設定の値よりも早くクエリが処理されるとログに記録されません（これは、[MySQL Slow Query Log](https://dev.mysql.com/doc/refman/5.7/en/slow-query-log.html)の`long_query_time`のように考えることができ、基本的に次のテーブルでは見つからないことを意味します）:

- `system.query_log`
- `system.query_thread_log`

ログに記録されるのは次のタイプのクエリのみです:
- `QUERY_FINISH`
- `EXCEPTION_WHILE_PROCESSING`

- Type: milliseconds
- Default value: 0（任意のクエリ）

## log_queries_min_type {#log_queries_min_type}

Type: LogQueriesType

Default value: QUERY_START

ログに記録する`query_log`の最小タイプです。

可能な値:
- `QUERY_START` (`=1`)
- `QUERY_FINISH` (`=2`)
- `EXCEPTION_BEFORE_START` (`=3`)
- `EXCEPTION_WHILE_PROCESSING` (`=4`)

特定のエンティティが`query_log`に記録されるのを制限するために使用できます。たとえば、エラーのみが関心のある場合は、`EXCEPTION_WHILE_PROCESSING`を使用できます:

``` text
log_queries_min_type='EXCEPTION_WHILE_PROCESSING'
```

## log_queries_probability {#log_queries_probability}

Type: Float

Default value: 1

ユーザーが特定の確率で無作為に選ばれたサンプルクエリのみを[query_log](../../operations/system-tables/query_log.md)、[query_thread_log](../../operations/system-tables/query_thread_log.md)、および[query_views_log](../../operations/system-tables/query_views_log.md)システムテーブルに書き込むことを許可します。これにより、1秒あたりのクエリボリュームでの負荷を軽減できます。

可能な値:
- 0 — クエリはシステムテーブルに記録されません。
- 0から1の範囲の正の浮動小数点数。たとえば、設定値が`0.5`の場合、おおよそ半分のクエリがシステムテーブルに記録されます。
- 1 — すべてのクエリがシステムテーブルに記録されます。

## log_query_settings {#log_query_settings}

Type: Bool

Default value: 1

クエリ設定をquery_logおよびOpenTelemetryスパンログに記録します。

## log_query_threads {#log_query_threads}

Type: Bool

Default value: 0

クエリスレッドのロギングを設定します。

クエリスレッドは[system.query_thread_log](../../operations/system-tables/query_thread_log.md)テーブルにログされます。この設定は、[log_queries](#log_queries)がtrueのときのみ効果を持ちます。この設定で実行されるClickHouseのクエリスレッドは、[query_thread_log](../../operations/server-configuration-parameters/settings.md/#query_thread_log)サーバー設定パラメータのルールに従ってログに記録されます。

可能な値:
- 0 — 無効。
- 1 — 有効。

**例**

``` text
log_query_threads=1
```

## log_query_views {#log_query_views}

Type: Bool

Default value: 1

クエリビューのロギングを設定します。

この設定が有効な状態でClickHouseによって実行されるクエリに関連付けられたビュー（マテリアライズドビューまたはライブビュー）が、[query_views_log](../../operations/server-configuration-parameters/settings.md/#query_views_log)サーバー設定パラメータにログされます。

例:

``` text
log_query_views=1
```

## low_cardinality_allow_in_native_format {#low_cardinality_allow_in_native_format}

Type: Bool

Default value: 1

[LowCardinality](../../sql-reference/data-types/lowcardinality.md)データ型を[Native](../../interfaces/formats.md/#native)フォーマットで使用することを許可または制限します。

`LowCardinality`の使用が制限されると、ClickHouseサーバーは`LowCardinality`カラムを普通のカラムに変換し、普通のカラムを`LowCardinality`カラムに変換します。

この設定は、主に`LowCardinality`データ型をサポートしないサードパーティクライアントに必要です。

可能な値:
- 1 — `LowCardinality`の使用が制限されません。
- 0 — `LowCardinality`の使用が制限されます。

## low_cardinality_max_dictionary_size {#low_cardinality_max_dictionary_size}

Type: UInt64

Default value: 8192

ストレージファイルシステムに書き込むことができる[LowCardinality](../../sql-reference/data-types/lowcardinality.md)データ型の共有グローバル辞書の最大サイズを行数で設定します。この設定は、辞書の無制限成長によるRAMの問題を防ぎます。最大辞書サイズの制限のためにエンコードできないすべてのデータは、ClickHouseが普通の方法で書き込みます。

可能な値:
- 任意の正の整数。

## low_cardinality_use_single_dictionary_for_part {#low_cardinality_use_single_dictionary_for_part}

Type: Bool

Default value: 0

データパーツのための単一の辞書の使用を有効または無効にします。

デフォルトでは、ClickHouseサーバーは辞書のサイズを監視し、辞書がオーバーフローすると、サーバーは次の辞書の書き込みを開始します。複数の辞書を作成しないようにするには、`low_cardinality_use_single_dictionary_for_part = 1`を設定します。

可能な値:
- 1 — データパーツのための複数の辞書の作成が禁止されます。
- 0 — データパーツのための複数の辞書の作成が禁止されません。

## materialize_skip_indexes_on_insert {#materialize_skip_indexes_on_insert}

Type: Bool

Default value: 1

INSERT時にスキップインデックスを構築および保存するかどうか。無効の場合、スキップインデックスはマージ時または明示的なMATERIALIZE INDEXによって構築および保存されます。

## materialize_statistics_on_insert {#materialize_statistics_on_insert}

Type: Bool

Default value: 1

INSERT時に統計を構築および挿入するかどうか。無効の場合、統計はマージ時または明示的なMATERIALIZE STATISTICSによって構築および保存されます。

## materialize_ttl_after_modify {#materialize_ttl_after_modify}

Type: Bool

Default value: 1

ALTER MODIFY TTLクエリの後に古いデータにTTLを適用します。

## materialized_views_ignore_errors {#materialized_views_ignore_errors}

Type: Bool

Default value: 0

MATERIALIZED VIEWのエラーを無視し、MVsに関係なく元のブロックをテーブルに配信することを許可します。

## max_analyze_depth {#max_analyze_depth}

Type: UInt64

Default value: 5000

インタプリタによって実行される最大分析数。

## max_ast_depth {#max_ast_depth}

Type: UInt64

Default value: 1000

クエリ構文ツリーの最大深さ。パース後にチェックされます。

## max_ast_elements {#max_ast_elements}

Type: UInt64

Default value: 50000

ノードの数でのクエリ構文ツリーの最大サイズ。パース後にチェックされます。

## max_autoincrement_series {#max_autoincrement_series}

Type: UInt64

Default value: 1000

`generateSeriesID`関数によって作成される系列の数の制限。

各系列はKeeperのノードを表すため、数百万系列以上は推奨されません。

## max_backup_bandwidth {#max_backup_bandwidth}

Type: UInt64

Default value: 0

サーバー上の特定のバックアップの最大読み取り速度（バイト/秒）。ゼロは無制限を意味します。

## max_block_size {#max_block_size}

Type: UInt64

Default value: 65409

ClickHouseでは、データはカラムパーツのセットであるブロックによって処理されます。単一のブロックのための内部処理サイクルは効率的ですが、各ブロックを処理する際には目立ったコストがあります。

`max_block_size`設定は、テーブルからデータをロードする際に単一のブロックに含めることが推奨される最大行数を示します。`max_block_size`のサイズのブロックは、テーブルから常にロードされるわけではありません。ClickHouseは取得するデータの量が少ないと判断した場合は、より小さなブロックを処理します。

ブロックサイズは、各ブロックの処理時に著しいコストを避けるために小さすぎてはいけません。また、LIMIT句を含むクエリが最初のブロック処理後に迅速に実行されるように、大きすぎてもいけません。`max_block_size`を設定する際は、大量のカラムを複数スレッドで抽出する際にメモリを過剰に消費しないようにし、少なくともいくつかのキャッシュローカリティを保持することが目標です。

## max_bytes_before_external_group_by {#max_bytes_before_external_group_by}

Type: UInt64

Default value: 0

GROUP BY操作中のメモリ使用量がこのバイトのしきい値を超えた場合、「外部集約」モードを有効にします（ディスクにデータをスピルします）。推奨値はシステムメモリの半分です。

## max_bytes_before_external_sort {#max_bytes_before_external_sort}

Type: UInt64

Default value: 0

ORDER BY操作中のメモリ使用量がこのバイトのしきい値を超えた場合、「外部ソート」モードを有効にします（ディスクにデータをスピルします）。推奨値はシステムメモリの半分です。

## max_bytes_before_remerge_sort {#max_bytes_before_remerge_sort}

Type: UInt64

Default value: 1000000000

LIMITを持つORDER BYの場合、メモリ使用量が指定したしきい値を超えた場合、最終のマージ前にブロックを再マージする追加ステップを実行して上位LIMIT行のみを保持します。

## max_bytes_in_distinct {#max_bytes_in_distinct}

Type: UInt64

Default value: 0

DISTINCTの実行中の状態のメモリ内での最大合計サイズ（非圧縮バイト）。

## max_bytes_in_join {#max_bytes_in_join}

Type: UInt64

Default value: 0

JOINのためのハッシュテーブルの最大サイズ（メモリ内のバイト数）。

## max_bytes_in_set {#max_bytes_in_set}

Type: UInt64

Default value: 0

INセクションの実行から得られたセットの最大サイズ（メモリ内のバイト）。

## max_bytes_ratio_before_external_group_by {#max_bytes_ratio_before_external_group_by}

Type: Double

Default value: 0.5

外部GROUP BYを有効にする前のメモリ使用率。これを0.6に設定すると、メモリ使用量がクエリの許可メモリの60%に達したときに外部GROUP BYが使用されます。

## max_bytes_ratio_before_external_sort {#max_bytes_ratio_before_external_sort}

Type: Double

Default value: 0.5

外部ORDER BYを有効にする前のメモリ使用率。これを0.6に設定すると、メモリ使用量がクエリの許可メモリの60%に達したときに外部ORDER BYが使用されます。

## max_bytes_to_read {#max_bytes_to_read}

Type: UInt64

Default value: 0

最も「深い」ソースからデータを読み取る際の（解凍後の）バイト制限。つまり、最も深いサブクエリ内のみです。リモートサーバーから読み取るときは、リモートサーバーでのみチェックされます。

## max_bytes_to_read_leaf {#max_bytes_to_read_leaf}

Type: UInt64

Default value: 0

分散クエリのためのリーフノードでのバイト読み取り制限（解凍後のバイト）。制限はローカル読み取りにのみ適用され、ルートノードの最終マージステージは除外されます。この設定は、prefer_localhost_replica=1の際に不安定です。

## max_bytes_to_sort {#max_bytes_to_sort}

Type: UInt64

Default value: 0

ORDER BY操作のために処理する必要がある（非圧縮）バイトが指定された量を超える場合、動作は`sort_overflow_mode`によって決定されます。デフォルトでは例外をスローします。

## max_bytes_to_transfer {#max_bytes_to_transfer}

Type: UInt64

Default value: 0

GLOBAL IN/JOINセクションの実行時に取得される外部テーブルの最大サイズ（非圧縮バイト）。

## max_columns_to_read {#max_columns_to_read}

Type: UInt64

Default value: 0

クエリによる読み取りが指定されたカラム数を超える場合、例外がスローされます。ゼロの値は無制限を意味します。この設定は、あまりにも複雑なクエリを防ぐのに役立ちます。

## max_compress_block_size {#max_compress_block_size}

Type: UInt64

Default value: 1048576

テーブルに書き込むための圧縮する前の未圧縮データのブロックの最大サイズ。デフォルトは1,048,576（1 MiB）です。通常、より小さなブロックサイズを指定すると圧縮率がわずかに減少しますが、キャッシュローカリティによる圧縮と解凍速度がわずかに向上し、メモリ消費量が減少します。

:::note
これはエキスパートレベルの設定であり、ClickHouseを始めたばかりの場合は変更しないことをお勧めします。
:::

圧縮用のブロック（バイトから成るメモリのチャンク）をクエリ処理用のブロック（テーブルからの行のセット）と混同しないでください。

## max_concurrent_queries_for_all_users {#max_concurrent_queries_for_all_users}

Type: UInt64

Default value: 0

この設定の値が現在同時に処理されているクエリの数以下の場合、例外をスローします。

例: `max_concurrent_queries_for_all_users`はすべてのユーザーに対して99に設定し、データベース管理者は自身が調査用にクエリを実行できるように100に設定することができます。サーバーが過負荷になっているときでも同様です。

1つのクエリまたはユーザーの設定を変更しても、他のクエリには影響しません。

可能な値:
- 正の整数。
- 0 — 制限なし。

**例**

```xml
<max_concurrent_queries_for_all_users>99</max_concurrent_queries_for_all_users>
```

**参照**

- [max_concurrent_queries](/operations/server-configuration-parameters/settings.md/#max_concurrent_queries)

## max_concurrent_queries_for_user {#max_concurrent_queries_for_user}

Type: UInt64

Default value: 0

ユーザーごとの同時に処理されるクエリの最大数。

可能な値:
- 正の整数。
- 0 — 制限なし。

**例**

```xml
<max_concurrent_queries_for_user>5</max_concurrent_queries_for_user>
```

## max_distributed_connections {#max_distributed_connections}

Type: UInt64

Default value: 1024

単一のDistributedテーブルに対する単一のクエリのためのリモートサーバーとの同時接続の最大数。クラスタ内のサーバー数以下に設定することを推奨します。

以下のパラメータは、Distributedテーブルを作成する際（およびサーバーを起動する際）のみ使用されるため、ランタイム中に変更する理由はありません。

## max_distributed_depth {#max_distributed_depth}

Type: UInt64

Default value: 5

[Distributed](../../engines/table-engines/special/distributed.md)テーブルの再帰クエリの最大深さを制限します。

この値を超えると、サーバーは例外をスローします。

可能な値:
- 正の整数。
- 0 — 無制限の深さ。

## max_download_buffer_size {#max_download_buffer_size}

Type: UInt64

Default value: 10485760

各スレッドごとの並列ダウンロード（例: URLエンジン）のための最大バッファサイズ。

## max_download_threads {#max_download_threads}

Type: MaxThreads

Default value: 4

データをダウンロードするための最大スレッド数（例: URLエンジン）。

## max_estimated_execution_time {#max_estimated_execution_time}

Type: Seconds

Default value: 0

最大クエリ推定実行時間（秒）。

## max_execution_speed {#max_execution_speed}

Type: UInt64

Default value: 0

秒あたりの最大実行行数。

## max_execution_speed_bytes {#max_execution_speed_bytes}

Type: UInt64

Default value: 0

秒あたりの最大実行バイト数。

## max_execution_time {#max_execution_time}

Type: Seconds

Default value: 0

クエリ実行時間が指定された数の秒を超えると、動作は`timeout_overflow_mode`によって決定されます。デフォルトでは例外をスローします。タイムアウトが確認され、クエリはデータ処理中の指定された場所のみで停止できます。現在、集約状態のマージ中やクエリ分析中には停止できず、実際の実行時間はこの設定の値よりも高くなります。

## max_execution_time_leaf {#max_execution_time_leaf}

Type: Seconds

Default value: 0

max_execution_timeと似た意味ですが、分散クエリのリーフノードにのみ適用され、タイムアウト動作は`timeout_overflow_mode_leaf`によって決定されます。デフォルトでは例外をスローします。

## max_expanded_ast_elements {#max_expanded_ast_elements}

Type: UInt64

Default value: 500000

エイリアスとアスタリスクの展開後のノード数でのクエリ構文ツリーの最大サイズ。

## max_fetch_partition_retries_count {#max_fetch_partition_retries_count}

Type: UInt64

Default value: 5

別のホストからパーティションを取得する際のリトライ回数。

## max_final_threads {#max_final_threads}

Type: MaxThreads

Default value: 'auto(12)'

[FINAL](../../sql-reference/statements/select/from.md/#select-from-final)修飾子を使用してクエリデータ読み取りフェーズの最大スレッド数を設定します。

可能な値:
- 正の整数。
- 0または1 — 無効。`SELECT`クエリはシングルスレッドで実行されます。

## max_http_get_redirects {#max_http_get_redirects}

Type: UInt64

Default value: 0

許可されるHTTP GETリダイレクトホップの最大数。悪意のあるサーバーがリクエストを予期しないサービスにリダイレクトするのを防ぐための追加のセキュリティ対策を確保します。\n\nこれは、外部サーバーが別のアドレスにリダイレクトするが、そのアドレスが会社のインフラストラクチャ内に見える場合に発生します。その場合、HTTPリクエストを内部サーバーに送信することで、認証をバイパスして内部ネットワークから内部APIを要求したり、RedisやMemcachedのような他のサービスにクエリを実行したりする可能性があります。内部インフラがない（ローカルホスト上で実行されている何かを含む）場合やサーバーを信頼する場合は、リダイレクトを許可することが安全です。ただし、URLがHTTPを使用している場合で、リモートサーバーだけでなく、ISPや中間のすべてのネットワークも信頼しなければならないことに注意してください。

## max_hyperscan_regexp_length {#max_hyperscan_regexp_length}

Type: UInt64

Default value: 0

[hyperscanマルチマッチ関数](../../sql-reference/functions/string-search-functions.md/#multimatchanyhaystack-pattern1-pattern2-patternn)における各正規表現の最大長を定義します。

可能な値:
- 正の整数。
- 0 - 長さに制限はありません。

**例**

クエリ:

```sql
SELECT multiMatchAny('abcd', ['ab','bcd','c','d']) SETTINGS max_hyperscan_regexp_length = 3;
```

結果:

```text
┌─multiMatchAny('abcd', ['ab', 'bcd', 'c', 'd'])─┐
│                                              1 │
└────────────────────────────────────────────────┘
```

クエリ:

```sql
SELECT multiMatchAny('abcd', ['ab','bcd','c','d']) SETTINGS max_hyperscan_regexp_length = 2;
```

結果:

```text
Exception: Regexp length too large.
```

**参照**

- [max_hyperscan_regexp_total_length](#max_hyperscan_regexp_total_length)

## max_hyperscan_regexp_total_length {#max_hyperscan_regexp_total_length}

Type: UInt64

Default value: 0

各[hyperscanマルチマッチ関数](../../sql-reference/functions/string-search-functions.md/#multimatchanyhaystack-pattern1-pattern2-patternn)におけるすべての正規表現の合計最大長を設定します。

可能な値:
- 正の整数。
- 0 - 長さに制限はありません。

**例**

クエリ:

```sql
SELECT multiMatchAny('abcd', ['a','b','c','d']) SETTINGS max_hyperscan_regexp_total_length = 5;
```

結果:

```text
┌─multiMatchAny('abcd', ['a', 'b', 'c', 'd'])─┐
│                                           1 │
└─────────────────────────────────────────────┘
```

クエリ:

```sql
SELECT multiMatchAny('abcd', ['ab','bc','c','d']) SETTINGS max_hyperscan_regexp_total_length = 5;
```

結果:

```text
Exception: Total regexp lengths too large.
```

**参照**

- [max_hyperscan_regexp_length](#max_hyperscan_regexp_length)

## max_insert_block_size {#max_insert_block_size}

Type: UInt64

Default value: 1048449

テーブルへの挿入用に形成するブロックのサイズ（行数で）。  
この設定は、サーバーがブロックを形成する場合にのみ適用されます。  
例として、HTTPインターフェース経由のINSERTでは、サーバーがデータ形式を解析し、指定されたサイズのブロックを形成します。  
しかし、clickhouse-clientを使用すると、クライアントがデータを自分で解析するため、サーバーでの`max_insert_block_size`設定は挿入されたブロックのサイズに影響しません。  
この設定は、INSERT SELECTを使用する場合にも目的がなく、SELECTの後に形成されるブロックが使用されます。

デフォルトは`max_block_size`よりもわずかに大きいです。  
これは、特定のテーブルエンジン（`*MergeTree`）が各挿入ブロックに対してディスク上にデータパーツを形成するからです。  
また、`*MergeTree`テーブルは挿入中にデータをソートし、大きなブロックサイズによってRAM内でより多くのデータをソートできるようにします。

## max_insert_delayed_streams_for_parallel_write {#max_insert_delayed_streams_for_parallel_write}

Type: UInt64

Default value: 0

最終パーツフラッシュを遅延させる最大ストリーム（カラム）数。デフォルトは自動（基盤ストレージが並列書き込みをサポートする場合は1000、それ以外は無効）。

## max_insert_threads {#max_insert_threads}

Type: UInt64

Default value: 0

`INSERT SELECT`クエリを実行するための最大スレッド数。

可能な値:
- 0（または1） — `INSERT SELECT`の並列実行なし。
- 正の整数。1より大きい。

クラウドのデフォルト値: サービスのサイズに応じて`2`から`4`。

並列の`INSERT SELECT`は、`SELECT`部分が並列で実行される場合にのみ効果があります。詳細は[max_threads](#max_threads)設定を参照してください。  
より高い値はより多くのメモリを消費します。

## max_joined_block_size_rows {#max_joined_block_size_rows}

Type: UInt64

Default value: 65409

JOIN結果の最大ブロックサイズ（結合アルゴリズムがサポートしている場合）。0は無制限を意味します。

## max_limit_for_ann_queries {#max_limit_for_ann_queries}
<ExperimentalBadge/>

Type: UInt64

Default value: 1000000

この設定を超えるLIMITのあるSELECTクエリはベクトル類似性インデックスを使用できません。ベクトル類似性インデックスでのメモリオーバーフローを防ぐのに役立ちます。

## max_live_view_insert_blocks_before_refresh {#max_live_view_insert_blocks_before_refresh}
<ExperimentalBadge/>

Type: UInt64

Default value: 64

マージ可能なブロックがドロップされ、クエリが再実行されるまでに挿入される最大ブロック数を制限します。

## max_local_read_bandwidth {#max_local_read_bandwidth}

Type: UInt64

Default value: 0

ローカルリードの最大速度（バイト/秒）。

## max_local_write_bandwidth {#max_local_write_bandwidth}

Type: UInt64

Default value: 0

ローカル書き込みの最大速度（バイト/秒）。

## max_memory_usage {#max_memory_usage}

Type: UInt64

Default value: 0

単一のクエリ処理のための最大メモリ使用量。ゼロは無制限を意味します。

## max_memory_usage_for_user {#max_memory_usage_for_user}

Type: UInt64

Default value: 0

同時に実行されているユーザーによるすべてのクエリの最大メモリ使用量。ゼロは無制限を意味します。

## max_network_bandwidth {#max_network_bandwidth}

Type: UInt64

Default value: 0

ネットワークを介したデータの転送速度をバイト/秒で制限します。この設定は各クエリに適用されます。

可能な値:
- 正の整数。
- 0 — 帯域幅の制御は無効です。

## max_network_bandwidth_for_all_users {#max_network_bandwidth_for_all_users}

Type: UInt64

Default value: 0

サーバー上のすべての同時実行クエリに対してネットワークを介してデータが送信される速度をバイト/秒で制限します。

可能な値:
- 正の整数。
- 0 — データ速度の制御は無効です。

## max_network_bandwidth_for_user {#max_network_bandwidth_for_user}

Type: UInt64

Default value: 0

単一ユーザーによる同時実行クエリ全体に適用されるネットワークを介してデータが送信される速度をバイト/秒で制限します。

可能な値:
- 正の整数。
- 0 — データ速度の制御は無効です。

## max_network_bytes {#max_network_bytes}

Type: UInt64

Default value: 0

クエリを実行する際にネットワークを介して受信または送信されるデータ量（バイト）を制限します。この設定はそれぞれの個々のクエリに適用されます。

可能な値:
- 正の整数。
- 0 — データ量の制御は無効です。

## max_number_of_partitions_for_independent_aggregation {#max_number_of_partitions_for_independent_aggregation}

Type: UInt64

Default value: 128

最適化を適用するためのテーブル内のパーティションの最大数。

## max_parallel_replicas {#max_parallel_replicas}

Type: NonZeroUInt64

Default value: 1000

クエリ実行時の各シャードの最大レプリカ数。

可能な値:
- 正の整数。

**追加情報**

このオプションは、使用される設定によって異なる結果をもたらすことがあります。

:::note
この設定は、結合やサブクエリが関与している場合に不正確な結果をもたらします。すべてのテーブルが特定の要件を満たさない場合、詳細については[Distributed Subqueries and max_parallel_replicas](../../sql-reference/operators/in.md/#max_parallel_replica-subqueries)を参照してください。
:::

### Parallel processing using `SAMPLE` key

クエリは、複数のサーバーで並列に実行されると、より早く処理されることがあります。しかし、次のような場合にはクエリのパフォーマンスが低下する可能性があります:
- サンプリングキーの位置がパーティショニングキー内で効率的な範囲スキャンを許可していない。
- テーブルにサンプリングキーが追加されると、他のカラムによるフィルタリングが効率的でなくなる。
- サンプリングキーが計算コストの高い式である。
- クラスタの遅延分布に長い尾があるため、より多くのサーバーをクエリすると全体のクエリ遅延が増加する。

### Parallel processing using [parallel_replicas_custom_key](#parallel_replicas_custom_key)

この設定は、任意の複製テーブルに便利です。
## max_parser_backtracks {#max_parser_backtracks}

Type: UInt64

Default value: 1000000

最大パーサーのバックトラック回数（再帰下降パースプロセスで異なる選択肢を試みる回数）。

## max_parser_depth {#max_parser_depth}

Type: UInt64

Default value: 1000

再帰下降パーサーの最大再帰深度を制限します。スタックサイズを制御することができます。

可能な値：

- 正の整数。
- 0 — 再帰深度は無制限です。

## max_parsing_threads {#max_parsing_threads}

Type: MaxThreads

Default value: 'auto(12)'

並列解析をサポートする入力フォーマットでデータを解析するための最大スレッド数。デフォルトでは自動的に決定されます。

## max_partition_size_to_drop {#max_partition_size_to_drop}

Type: UInt64

Default value: 50000000000

クエリ時にパーティションを削除する際の制限。値0は、制限なしでパーティションを削除できることを意味します。

クラウドのデフォルト値：1 TB。

:::note
このクエリ設定は、サーバー設定の同等の値を上書きします。詳細は [max_partition_size_to_drop](/operations/server-configuration-parameters/settings.md/#max-partition-size-to-drop) を参照してください。
:::

## max_partitions_per_insert_block {#max_partitions_per_insert_block}

Type: UInt64

Default value: 100

単一のINSERTブロック内のパーティションの最大数を制限します。ゼロは無制限を意味します。ブロックが多すぎるパーティションを含む場合は例外をスローします。この設定は安全限界であり、大量のパーティションを使用するのは一般的な誤解だからです。

## max_partitions_to_read {#max_partitions_to_read}

Type: Int64

Default value: -1

1つのクエリでアクセスできる最大パーティション数を制限します。 &lt;= 0 は無制限を意味します。

## max_parts_to_move {#max_parts_to_move}

Type: UInt64

Default value: 1000

1つのクエリで移動できるパーツの数を制限します。ゼロは無制限を意味します。

## max_query_size {#max_query_size}

Type: UInt64

Default value: 262144

SQLパーサーによって解析されるクエリ文字列の最大バイト数。
INSERTクエリのVALUES句内のデータは、別のストリームパーサー（O(1) RAMを消費）で処理され、この制限の影響を受けません。

:::note
`max_query_size` はSQLクエリ内で設定できません（例： `SELECT now() SETTINGS max_query_size=10000`）。なぜなら、ClickHouseはクエリを解析するためのバッファを割り当てる必要があり、このバッファサイズは `max_query_size` 設定によって決定され、クエリが実行される前に設定されなければならないからです。
:::

## max_read_buffer_size {#max_read_buffer_size}

Type: UInt64

Default value: 1048576

ファイルシステムから読み取るためのバッファの最大サイズ。

## max_read_buffer_size_local_fs {#max_read_buffer_size_local_fs}

Type: UInt64

Default value: 131072

ローカルファイルシステムから読み取るためのバッファの最大サイズ。0に設定すると、max_read_buffer_sizeが使用されます。

## max_read_buffer_size_remote_fs {#max_read_buffer_size_remote_fs}

Type: UInt64

Default value: 0

リモートファイルシステムから読み取るためのバッファの最大サイズ。0に設定すると、max_read_buffer_sizeが使用されます。

## max_recursive_cte_evaluation_depth {#max_recursive_cte_evaluation_depth}

Type: UInt64

Default value: 1000

再帰CTE評価深度の最大制限。

## max_remote_read_network_bandwidth {#max_remote_read_network_bandwidth}

Type: UInt64

Default value: 0

読み取りのためのネットワーク越しのデータ交換の最大速度（バイト/秒）。

## max_remote_write_network_bandwidth {#max_remote_write_network_bandwidth}

Type: UInt64

Default value: 0

書き込みのためのネットワーク越しのデータ交換の最大速度（バイト/秒）。

## max_replica_delay_for_distributed_queries {#max_replica_delay_for_distributed_queries}

Type: UInt64

Default value: 300

分散クエリ用の遅延レプリカを無効にします。詳細は [Replication](../../engines/table-engines/mergetree-family/replication.md) を参照してください。

秒単位で時間を設定します。レプリカの遅延が設定値以上の場合、そのレプリカは使用されません。

可能な値：

- 正の整数。
- 0 — レプリカの遅延はチェックされません。

任意のレプリカの遅延が非ゼロであるものを使用しないようにするには、このパラメータを1に設定します。

レプリケートされたテーブルを指す分散テーブルから `SELECT` を行う際に使用されます。

## max_result_bytes {#max_result_bytes}

Type: UInt64

Default value: 0

非圧縮のバイト単位での結果サイズの制限。閾値が達成されるとデータブロックの処理が停止しますが、結果の最後のブロックは切り捨てられないため、結果のサイズは閾値よりも大きくなる可能性があります。注意点：メモリ内の結果サイズもこの閾値に考慮されます。結果サイズが小さくても、LowCardinalityカラムの辞書や、AggregateFunctionカラムのアリーナを参照する大きなデータ構造を指している可能性があるため、小さな結果サイズにも関わらず閾値を超える場合があります。この設定はかなり低レベルであり、注意して使用する必要があります。

## max_result_rows {#max_result_rows}

Type: UInt64

Default value: 0

結果サイズに対する行の制限。閾値が達成されるとデータブロックの処理が停止しますが、結果の最後のブロックは切り捨てられないため、結果のサイズは閾値よりも大きくなる可能性があります。

## max_rows_in_distinct {#max_rows_in_distinct}

Type: UInt64

Default value: 0

DISTINCT実行中の最大要素数。

## max_rows_in_join {#max_rows_in_join}

Type: UInt64

Default value: 0

JOINのためのハッシュテーブルの最大サイズ（行数）。

## max_rows_in_set {#max_rows_in_set}

Type: UInt64

Default value: 0

INセクションの実行結果として得られる集合の最大サイズ（要素数）。

## max_rows_in_set_to_optimize_join {#max_rows_in_set_to_optimize_join}

Type: UInt64

Default value: 0

結合する前に互いの行セットでフィルタリングするための最大の集合のサイズ。

可能な値：

- 0 — 無効にする。
- 任意の正の整数。

## max_rows_to_group_by {#max_rows_to_group_by}

Type: UInt64

Default value: 0

GROUP BY中の集約が指定された行数（ユニークなGROUP BYキー）を超える場合、その動作は 'group_by_overflow_mode' により決定されます。デフォルトは - 例外をスローするですが、近似GROUP BYモードにスイッチすることもできます。

## max_rows_to_read {#max_rows_to_read}

Type: UInt64

Default value: 0

最も「深い」ソースから読み取る行の制限。それは、最も深いサブクエリのみです。リモートサーバーから読み取る時には、リモートサーバーでのみチェックされます。

## max_rows_to_read_leaf {#max_rows_to_read_leaf}

Type: UInt64

Default value: 0

分散クエリ用の葉ノードで読み取る行の制限。この制限は、最終マージ段階をルートノードで除外してローカル読み取りにのみ適用されます。この設定はprefer_localhost_replica=1の時に不安定であることに注意してください。

## max_rows_to_sort {#max_rows_to_sort}

Type: UInt64

Default value: 0

ORDER BY操作のために処理される必要があるレコードが指定された数を超える場合、動作は 'sort_overflow_mode' により決定されます。デフォルトは - 例外をスローします。

## max_rows_to_transfer {#max_rows_to_transfer}

Type: UInt64

Default value: 0

GLOBAL IN/JOINセクションが実行された時に送信される外部テーブルの最大サイズ（行数）。

## max_sessions_for_user {#max_sessions_for_user}

Type: UInt64

Default value: 0

ユーザーのための同時セッションの最大数。

## max_size_to_preallocate_for_aggregation {#max_size_to_preallocate_for_aggregation}

Type: UInt64

Default value: 1000000000000

集約前に全てのハッシュテーブルに対して事前に割り当てることが許可されている要素の総数。

## max_size_to_preallocate_for_joins {#max_size_to_preallocate_for_joins}

Type: UInt64

Default value: 1000000000000

結合前に全てのハッシュテーブルに対して事前に割り当てることが許可されている要素の総数。

## max_streams_for_merge_tree_reading {#max_streams_for_merge_tree_reading}

Type: UInt64

Default value: 0

ゼロでない場合、MergeTreeテーブルの読み取りのためのストリームの数を制限します。

## max_streams_multiplier_for_merge_tables {#max_streams_multiplier_for_merge_tables}

Type: Float

Default value: 5

マージテーブルから読み取る際に、より多くのストリームを要求します。ストリームは、マージテーブルで使用されるテーブル全体に分散されます。これにより、スレッド全体に作業がより均等に分配され、マージされたテーブルのサイズが異なる場合に特に役立ちます。

## max_streams_to_max_threads_ratio {#max_streams_to_max_threads_ratio}

Type: Float

Default value: 1

スレッド数よりも多くのソースを使用して、スレッド間で作業をより均等に分配することを可能にします。この設定は一時的な解決策と見なされ、将来的には、ソースの数をスレッドの数と等しくし、各ソースに対して動的に使用可能な作業を選択できるようにすることが可能になります。

## max_subquery_depth {#max_subquery_depth}

Type: UInt64

Default value: 100

クエリが指定された数の入れ子サブクエリを超える場合、例外をスローします。これにより、クラスターのユーザーがクエリのことで気が狂うのを防ぐための健全性チェックを行います。

## max_table_size_to_drop {#max_table_size_to_drop}

Type: UInt64

Default value: 50000000000

クエリ時にテーブルを削除する際の制限。値0は、制限なしで全てのテーブルを削除できることを意味します。

クラウドのデフォルト値：1 TB。

:::note
このクエリ設定は、サーバー設定の同等の値を上書きします。詳細は [max_table_size_to_drop](/operations/server-configuration-parameters/settings.md/#max-table-size-to-drop) を参照してください。
:::

## max_temporary_columns {#max_temporary_columns}

Type: UInt64

Default value: 0

クエリが中間計算の結果としてメモリに生成する一時カラムの数が指定された数を超える場合、例外がスローされます。ゼロ値は無制限を意味します。この設定は、非常に複雑なクエリを防ぐために役立ちます。

## max_temporary_data_on_disk_size_for_query {#max_temporary_data_on_disk_size_for_query}

Type: UInt64

Default value: 0

同時に実行されている全てのクエリに対して、一時ファイルがディスクで消費するデータの最大量（バイト）です。ゼロは無制限を意味します。

## max_temporary_data_on_disk_size_for_user {#max_temporary_data_on_disk_size_for_user}

Type: UInt64

Default value: 0

同時に実行されている全てのユーザークエリに対して、一時ファイルがディスクで消費するデータの最大量（バイト）です。ゼロは無制限を意味します。

## max_temporary_non_const_columns {#max_temporary_non_const_columns}

Type: UInt64

Default value: 0

'max_temporary_columns'設定に類似していますが、非定数カラムのみに適用されます。定数カラムは安価であり、より多くのカラムを許可するのが合理的です。

## max_threads {#max_threads}

Type: MaxThreads

Default value: 'auto(12)'

リモートサーバーからデータを取得するためのスレッドを除いた、クエリ処理スレッドの最大数。このパラメーターは、クエリ処理パイプラインの同じ段階を並行して実行するスレッドに適用されます。
たとえば、テーブルから読み取る際、関数での式の評価、WHEREでのフィルター、GROUP BYのための事前集計が並行して 'max_threads' スレッド数を使用して行える場合、'max_threads' が使用されます。

LIMITのために迅速に完了するクエリにはより低い 'max_threads' を設定できます。必要なエントリ数がすべてのブロックに存在し、max_threads = 8 の場合、8ブロックが取得されますが、1つを読むだけで十分です。

`max_threads` の値が小さいほど、消費されるメモリは少なくなります。

## max_threads_for_indexes {#max_threads_for_indexes}

Type: UInt64

Default value: 0

インデックスを処理するための最大スレッド数。

## max_untracked_memory {#max_untracked_memory}

Type: UInt64

Default value: 4194304

小さな割り当てと解放はスレッドローカル変数にグループ化され、指定された値（絶対値）が大きくなるまで追跡またはプロファイリングされません。値が 'memory_profiler_step' より高い場合、実際には 'memory_profiler_step' に下げられます。

## memory_overcommit_ratio_denominator {#memory_overcommit_ratio_denominator}

Type: UInt64

Default value: 1073741824

これは、ハード制限がグローバルレベルで達成されたときのソフトメモリ制限を表します。
この値は、クエリのオーバーコミット比率を計算するために使用されます。
ゼロは、クエリをスキップすることを意味します。
[メモリオーバーコミット](memory-overcommit.md)についての詳細をご覧ください。

## memory_overcommit_ratio_denominator_for_user {#memory_overcommit_ratio_denominator_for_user}

Type: UInt64

Default value: 1073741824

これは、ハード制限がユーザーレベルで達成されたときのソフトメモリ制限を表します。
この値は、クエリのオーバーコミット比率を計算するために使用されます。
ゼロは、クエリをスキップすることを意味します。
[メモリオーバーコミット](memory-overcommit.md)についての詳細をご覧ください。

## memory_profiler_sample_max_allocation_size {#memory_profiler_sample_max_allocation_size}

Type: UInt64

Default value: 0

指定された値以下のサイズのランダムな割り当てを確率 `memory_profiler_sample_probability` で収集します。0は無効を意味します。期待通りにこの閾値が機能するために、'max_untracked_memory' を0に設定することをお勧めします。

## memory_profiler_sample_min_allocation_size {#memory_profiler_sample_min_allocation_size}

Type: UInt64

Default value: 0

指定された値以上のサイズのランダムな割り当てを確率 `memory_profiler_sample_probability` で収集します。0は無効を意味します。期待通りにこの閾値が機能するために、'max_untracked_memory' を0に設定することをお勧めします。

## memory_profiler_sample_probability {#memory_profiler_sample_probability}

Type: Float

Default value: 0

ランダムな割り当てと解放を収集し、それらを'system.trace_log'に 'MemorySample' trace_typeで書き込みます。確率は、割り当てのサイズにかかわらず、すべての割り当て/解放に対して適用されます（これは `memory_profiler_sample_min_allocation_size` と `memory_profiler_sample_max_allocation_size` で変更できます）。サンプリングは、追跡されていないメモリの量が 'max_untracked_memory' を超えたときのみ発生します。追加の詳細なサンプリングを行うには、'max_untracked_memory' を0に設定することをお勧めします。

## memory_profiler_step {#memory_profiler_step}

Type: UInt64

Default value: 4194304

メモリプロファイラーのステップを設定します。クエリのメモリ使用量が、バイト単位での各次のステップより大きくなるたびに、メモリプロファイラーは割り当てスタックトレースを収集し、[trace_log](../../operations/system-tables/trace_log.md/#system_tables-trace_log) に書き込みます。

可能な値：

- 正の整数バイト数。

- メモリプロファイラーをオフにするための0。

## memory_tracker_fault_probability {#memory_tracker_fault_probability}

Type: Float

Default value: 0

`例外安全性` のテスト - 指定された確率でメモリを割り当てるたびに例外をスローします。

## memory_usage_overcommit_max_wait_microseconds {#memory_usage_overcommit_max_wait_microseconds}

Type: UInt64

Default value: 5000000

ユーザーレベルでメモリがオーバーコミットされた場合、スレッドがメモリが解放されるまで待機する最大時間（マイクロ秒）。
タイムアウトが達成され、メモリが解放されない場合、例外がスローされます。
[メモリオーバーコミット](memory-overcommit.md)についての詳細をご覧ください。

## merge_table_max_tables_to_look_for_schema_inference {#merge_table_max_tables_to_look_for_schema_inference}

Type: UInt64

Default value: 1000

明示的なスキーマなしで `Merge` テーブルを作成する場合や、`merge` テーブル関数を使用する場合、指定された数のマッチングテーブルの結合としてスキーマを推測します。
テーブルの数が多すぎる場合、最初に指定された数のテーブルからスキーマが推測されます。

## merge_tree_coarse_index_granularity {#merge_tree_coarse_index_granularity}

Type: UInt64

Default value: 8

データを検索する際、ClickHouseはインデックスファイル内のデータマークをチェックします。必要なキーが特定の範囲に存在することがわかった場合、ClickHouseはこの範囲を `merge_tree_coarse_index_granularity` サブレンジに分割し、そこで再帰的に必要なキーを検索します。

可能な値：

- 任意の正の偶数。

## merge_tree_compact_parts_min_granules_to_multibuffer_read {#merge_tree_compact_parts_min_granules_to_multibuffer_read}

Type: UInt64

Default value: 16

ClickHouse Cloudでのみ利用可能。MergeTreeテーブルのコンパクトパーツのストライプ内のグラニュール数を指定し、マルチバッファリーダーを使用して並行読み取りとプリフェッチをサポートします。リモートFSから読み取る場合、マルチバッファリーダーを使用すると、読み取りリクエストの数が増加します。

## merge_tree_determine_task_size_by_prewhere_columns {#merge_tree_determine_task_size_by_prewhere_columns}

Type: Bool

Default value: 1

読み取りタスクのサイズを決定するために、前のカラムサイズのみを使用するかどうか。

## merge_tree_max_bytes_to_use_cache {#merge_tree_max_bytes_to_use_cache}

Type: UInt64

Default value: 2013265920

ClickHouseが1つのクエリで `merge_tree_max_bytes_to_use_cache` バイト以上を読み取る必要がある場合、未圧縮ブロックのキャッシュを使用しません。

未圧縮ブロックのキャッシュは、クエリに対して抽出されたデータを保存します。ClickHouseはこのキャッシュを使用して、繰り返し行われる小さなクエリに対する応答を速めます。この設定は、大量のデータを読み取るクエリによってキャッシュが無駄になるのを防ぎます。[uncompressed_cache_size](../../operations/server-configuration-parameters/settings.md/#server-settings-uncompressed_cache_size) サーバー設定は、未圧縮ブロックのキャッシュサイズを定義します。

可能な値：

- 任意の正の整数。

## merge_tree_max_rows_to_use_cache {#merge_tree_max_rows_to_use_cache}

Type: UInt64

Default value: 1048576

ClickHouseが1つのクエリで `merge_tree_max_rows_to_use_cache` 行以上を読み取る必要がある場合、未圧縮ブロックのキャッシュを使用しません。

未圧縮ブロックのキャッシュは、クエリに対して抽出されたデータを保存します。ClickHouseはこのキャッシュを使用して、繰り返し行われる小さなクエリに対する応答を速めます。この設定は、大量のデータを読み取るクエリによってキャッシュが無駄になるのを防ぎます。[uncompressed_cache_size](../../operations/server-configuration-parameters/settings.md/#server-settings-uncompressed_cache_size) サーバー設定は、未圧縮ブロックのキャッシュサイズを定義します。

可能な値：

- 任意の正の整数。

## merge_tree_min_bytes_for_concurrent_read {#merge_tree_min_bytes_for_concurrent_read}

Type: UInt64

Default value: 251658240

1つの [MergeTree](../../engines/table-engines/mergetree-family/mergetree.md)エンジンテーブルから読み取るバイト数が `merge_tree_min_bytes_for_concurrent_read` を超える場合、ClickHouseはこのファイルから複数のスレッドで並行します。

可能な値：

- 正の整数。

## merge_tree_min_bytes_for_concurrent_read_for_remote_filesystem {#merge_tree_min_bytes_for_concurrent_read_for_remote_filesystem}

Type: UInt64

Default value: 0

リモートファイルシステムから読み取る際に、[MergeTree](../../engines/table-engines/mergetree-family/mergetree.md)エンジンが読み取りを並列化できる前に、1つのファイルから読み取るバイトの最小数。 この設定の使用は推奨されません。

可能な値：

- 正の整数。

## merge_tree_min_bytes_for_seek {#merge_tree_min_bytes_for_seek}

Type: UInt64

Default value: 0

1つのファイル内の2つの読み取るデータブロック間の距離が `merge_tree_min_bytes_for_seek` バイト未満である場合、ClickHouseは両方のブロックを含むファイルの範囲を逐次的に読み取り、余分なシークを回避します。

可能な値：

- 任意の正の整数。

## merge_tree_min_bytes_per_task_for_remote_reading {#merge_tree_min_bytes_per_task_for_remote_reading}

Type: UInt64

Default value: 2097152

タスクごとの最小バイト数を読み取ります。

## merge_tree_min_read_task_size {#merge_tree_min_read_task_size}

Type: UInt64

Default value: 8

タスクサイズに関する厳しい下限（グラヌール数が少なく、使用可能なスレッド数が多くても、より小さなタスクを割り当てることはありません）。

## merge_tree_min_rows_for_concurrent_read {#merge_tree_min_rows_for_concurrent_read}

Type: UInt64

Default value: 163840

1つの [MergeTree](../../engines/table-engines/mergetree-family/mergetree.md) テーブルから読み取る行数が `merge_tree_min_rows_for_concurrent_read` を超える場合、ClickHouseはこのファイルから複数のスレッドによる並行読み取りを試みます。

可能な値：

- 正の整数。

## merge_tree_min_rows_for_concurrent_read_for_remote_filesystem {#merge_tree_min_rows_for_concurrent_read_for_remote_filesystem}

Type: UInt64

Default value: 0

リモートファイルシステムから読み取る際に、[MergeTree](../../engines/table-engines/mergetree-family/mergetree.md)エンジンが読み取りを並列化できる前に、1つのファイルから読み取る行の最小数。この設定の使用は推奨されません。

可能な値：

- 正の整数。

## merge_tree_min_rows_for_seek {#merge_tree_min_rows_for_seek}

Type: UInt64

Default value: 0

1つのファイル内の2つの読み取るデータブロック間の距離が `merge_tree_min_rows_for_seek` 行未満である場合、ClickHouseはファイルをシークせずにデータを逐次的に読み取ります。

可能な値：

- 任意の正の整数。

## merge_tree_read_split_ranges_into_intersecting_and_non_intersecting_injection_probability {#merge_tree_read_split_ranges_into_intersecting_and_non_intersecting_injection_probability}

Type: Float

Default value: 0

`PartsSplitter` のテスト - MergeTreeを読み取るたびに、指定された確率で範囲を重複と非重複に分割します。

## merge_tree_use_const_size_tasks_for_remote_reading {#merge_tree_use_const_size_tasks_for_remote_reading}

Type: Bool

Default value: 1

リモートテーブルからの読み取りのために定サイズタスクを使用するかどうか。

## merge_tree_use_deserialization_prefixes_cache {#merge_tree_use_deserialization_prefixes_cache}

Type: Bool

Default value: 1

MergeTree内のWideパーツからの読み取り中に、ファイルプレフィックスからのカラムメタデータのキャッシュを有効にします。

## merge_tree_use_prefixes_deserialization_thread_pool {#merge_tree_use_prefixes_deserialization_thread_pool}

Type: Bool

Default value: 1

MergeTree内のWideパーツにおける並列プレフィックス読み取りのためにスレッドプールの使用を有効にします。スレッドプールのサイズは、サーバー設定 `max_prefixes_deserialization_thread_pool_size` によって制御されます。

## merge_tree_use_v1_object_and_dynamic_serialization {#merge_tree_use_v1_object_and_dynamic_serialization}

Type: Bool

Default value: 0

有効にすると、MergeTreeでJSONおよびDynamicタイプのV1シリアライズバージョンが使用され、V2ではなくなります。この設定を変更するには、サーバーを再起動する必要があります。

## metrics_perf_events_enabled {#metrics_perf_events_enabled}

Type: Bool

Default value: 0

有効にすると、クエリの実行中にいくつかのパフォーマンスイベントが測定されます。

## metrics_perf_events_list {#metrics_perf_events_list}

Type: String

Default value: 

カンマ区切りのパフォーマンスメトリックリストが、クエリの実行中に測定されます。空はすべてのイベントを意味します。使用可能なイベントに関しては、ソース内の PerfEventInfo を参照してください。

## min_bytes_to_use_direct_io {#min_bytes_to_use_direct_io}

Type: UInt64

Default value: 0

ストレージディスクへのダイレクトI/Oアクセスを使用するための最小データ量。

ClickHouseは、テーブルからデータを読み取る際にこの設定を使用します。読み取るすべてのデータの総格納量が `min_bytes_to_use_direct_io` バイトを超える場合、ClickHouseはストレージディスクから `O_DIRECT` オプションでデータを読み取ります。

可能な値：

- 0 — ダイレクトI/Oは無効。
- 正の整数。

## min_bytes_to_use_mmap_io {#min_bytes_to_use_mmap_io}

Type: UInt64

Default value: 0

これは実験的な設定です。カーネルからユーザースペースにデータをコピーせずに大きなファイルを読み取るための最小メモリ量を設定します。推奨される閾値は約64MBです。なぜなら、[mmap/munmap](https://en.wikipedia.org/wiki/Mmap) は速度が遅いからです。これは大きなファイルに対してのみ意味があり、データがページキャッシュ内に存在する場合にのみ役立ちます。

可能な値：

- 正の整数。
- 0 — 大きなファイルはカーネルからユーザースペースにデータをコピーしてのみ読み取ります。

## min_chunk_bytes_for_parallel_parsing {#min_chunk_bytes_for_parallel_parsing}

Type: NonZeroUInt64

Default value: 10485760

- Type: unsigned int
- Default value: 1 MiB

各スレッドが並行して解析する最小チャンクサイズ（バイト数）。

## min_compress_block_size {#min_compress_block_size}

Type: UInt64

Default value: 65536

[MergeTree](../../engines/table-engines/mergetree-family/mergetree.md)テーブルのための設定です。クエリ処理中のレイテンシを削減する目的で、次のマークを書き込む際にブロックが `min_compress_block_size` 以上のサイズの場合、そのブロックは圧縮されます。デフォルトは65,536です。

未圧縮データが `max_compress_block_size` 未満の場合、ブロックの実際のサイズは、この値以上であり、かつ1つのマークのためのデータ量以上である必要があります。

例を見てみましょう。テーブル作成中に `index_granularity` が8192に設定されたと仮定します。

UInt32型のカラムを書き込む（値ごとに4バイト）。8192行を書き込むと、データ合計は32 KBになります。 `min_compress_block_size` = 65,536とすることで、マーク２つごとに圧縮ブロックが形成されます。

平均して60バイトの値を持つString型のURLカラムを書き込む場合。8192行を書き込むと、平均して500 KB未満のデータになります。このサイズは65,536を超えるため、各マークごとの圧縮ブロックが形成されます。この場合、ディスクからデータを読み取る際、1つのマークの範囲では、余分なデータが解凍されないことを意味します。

:::note
これは専門家向けの設定であり、ClickHouseの使用を始めたばかりの場合は変更しないでください。
:::

## min_count_to_compile_aggregate_expression {#min_count_to_compile_aggregate_expression}

Type: UInt64

Default value: 3

JITコンパイルを開始するための最小同一集約式の数。この設定は [compile_aggregate_expressions](#compile_aggregate_expressions) が有効になっている場合にのみ機能します。

可能な値：

- 正の整数。
- 0 — 同一の集約式は常にJITコンパイルされます。

## min_count_to_compile_expression {#min_count_to_compile_expression}

Type: UInt64

Default value: 3

同じ式が実行される前の最小個数が、その式がコンパイルされることになります。

## min_count_to_compile_sort_description {#min_count_to_compile_sort_description}

Type: UInt64

Default value: 3

JITコンパイルされる前に同一のソート説明の数。

## min_execution_speed {#min_execution_speed}

Type: UInt64

Default value: 0

秒間の実行行数の最小数。

## min_execution_speed_bytes {#min_execution_speed_bytes}

Type: UInt64

Default value: 0

秒間の実行バイト数の最小数。

## min_external_sort_block_bytes {#min_external_sort_block_bytes}

Type: UInt64

Default value: 104857600

ディスクにダンプされる外部ソートの最小ブロックサイズ（バイト単位）。あまりにも多くのファイルを避けるために。

## min_external_table_block_size_bytes {#min_external_table_block_size_bytes}

Type: UInt64

Default value: 268402944

ブロックが十分に大きくない場合、外部テーブルに渡されるブロックを指定されたバイト数に圧縮します。

## min_external_table_block_size_rows {#min_external_table_block_size_rows}

Type: UInt64

Default value: 1048449

ブロックが十分に大きくない場合、外部テーブルに渡されるブロックを指定された行数に圧縮します。

## min_free_disk_bytes_to_perform_insert {#min_free_disk_bytes_to_perform_insert}

Type: UInt64

Default value: 0

挿入を実行するための最小空きディスクスペース（バイト）。

## min_free_disk_ratio_to_perform_insert {#min_free_disk_ratio_to_perform_insert}

Type: Float

Default value: 0

挿入を実行するための最小空きディスクスペースの比率。

## min_free_disk_space_for_temporary_data {#min_free_disk_space_for_temporary_data}

Type: UInt64

Default value: 0

外部ソートや集約に使用される一時データを書き込み中に保持する必要のある最小ディスクスペース（バイト）。

## min_hit_rate_to_use_consecutive_keys_optimization {#min_hit_rate_to_use_consecutive_keys_optimization}

Type: Float

Default value: 0.5

集約における連続キー最適化のために使用されるキャッシュの最小ヒット率を保持するために必要です。

## min_insert_block_size_bytes {#min_insert_block_size_bytes}

Type: UInt64

Default value: 268402944

`INSERT` クエリによってテーブルに挿入できるブロック内の最小バイト数をセットします。より小さいサイズのブロックは、大きなものに圧縮されます。

可能な値：

- 正の整数。
- 0 — 圧縮を無効にする。

## min_insert_block_size_bytes_for_materialized_views {#min_insert_block_size_bytes_for_materialized_views}

Type: UInt64

Default value: 0

`INSERT` クエリによってテーブルに挿入できるブロック内の最小バイト数をセットします。より小さいサイズのブロックは、大きなものに圧縮されます。この設定は、[materialized view](../../sql-reference/statements/create/view.md) に挿入されるブロックのみに適用されます。この設定を調整することで、マテリアライズドビューへのプッシュ時にブロックの圧縮を制御し、メモリ使用量を過剰に無駄にしないようにします。

可能な値：

- 任意の正の整数。
- 0 — 圧縮を無効にする。

**参照：**

- [min_insert_block_size_bytes](#min_insert_block_size_bytes)

## min_insert_block_size_rows {#min_insert_block_size_rows}

Type: UInt64

Default value: 1048449

`INSERT` クエリによってテーブルに挿入できるブロック内の最小行数を設定します。より小さいサイズのブロックは、大きなものに圧縮されます。

可能な値：

- 正の整数。
- 0 — 圧縮を無効にする。

## min_insert_block_size_rows_for_materialized_views {#min_insert_block_size_rows_for_materialized_views}

Type: UInt64

Default value: 0
```html
挿入クエリによってテーブルに挿入できるブロック内の最小行数を設定します。小さなサイズのブロックは大きなブロックに圧縮されます。この設定は、[マテリアライズドビュー](../../sql-reference/statements/create/view.md)に挿入されるブロックにのみ適用されます。この設定を調整することで、マテリアライズドビューへのプッシュ中にブロックの圧縮を制御し、過剰なメモリ使用を回避します。

可能な値：

- 正の整数
- 0 — 圧縮無効

**関連項目**

- [min_insert_block_size_rows](#min_insert_block_size_rows)

## min_joined_block_size_bytes {#min_joined_block_size_bytes}



タイプ: UInt64

デフォルト値: 524288

JOIN結果の最小ブロックサイズ（結合アルゴリズムがサポートしている場合）。0は無制限を意味します。

## mongodb_throw_on_unsupported_query {#mongodb_throw_on_unsupported_query}



タイプ: Bool

デフォルト値: 1

有効にすると、MongoDBクエリが構築できない場合、MongoDBテーブルはエラーを返します。そうでない場合、ClickHouseはテーブル全体を読み込み、ローカルで処理します。このオプションは、レガシー実装または 'allow_experimental_analyzer=0' の場合には適用されません。

## move_all_conditions_to_prewhere {#move_all_conditions_to_prewhere}



タイプ: Bool

デフォルト値: 1

すべての有効な条件をWHEREからPREWHEREに移動します。

## move_primary_key_columns_to_end_of_prewhere {#move_primary_key_columns_to_end_of_prewhere}



タイプ: Bool

デフォルト値: 1

主キー列を含むPREWHERE条件をANDチェーンの最後に移動します。これらの条件は主キー分析中に考慮される傾向があるため、PREWHEREフィルタリングにはあまり寄与しません。

## multiple_joins_try_to_keep_original_names {#multiple_joins_try_to_keep_original_names}



タイプ: Bool

デフォルト値: 0

複数の結合を再書き込みする際に、最上位の式リストに別名を追加しません。

## mutations_execute_nondeterministic_on_initiator {#mutations_execute_nondeterministic_on_initiator}



タイプ: Bool

デフォルト値: 0

真の場合、定数の非決定論的関数（例：関数 `now()`）はイニシエーターで実行され、`UPDATE` および `DELETE` クエリ内のリテラルに置き換えられます。これは、定数の非決定論的関数を使用してミューテーションを実行中にレプリカでデータを同期しておくのに役立ちます。デフォルト値: `false`。

## mutations_execute_subqueries_on_initiator {#mutations_execute_subqueries_on_initiator}



タイプ: Bool

デフォルト値: 0

真の場合、スカラサブクエリはイニシエーターで実行され、`UPDATE` および `DELETE` クエリ内のリテラルに置き換えられます。デフォルト値: `false`。

## mutations_max_literal_size_to_replace {#mutations_max_literal_size_to_replace}



タイプ: UInt64

デフォルト値: 16384

`UPDATE` および `DELETE` クエリ内で置き換えられる直列化されたリテラルの最大サイズ（バイト単位）。上記の二つの設定のうち少なくとも一つが有効な場合にのみ適用されます。デフォルト値: 16384（16 KiB）。

## mutations_sync {#mutations_sync}



タイプ: UInt64

デフォルト値: 0

`synchronously` に `ALTER TABLE ... UPDATE|DELETE|MATERIALIZE INDEX|MATERIALIZE PROJECTION|MATERIALIZE COLUMN` クエリを実行できるようにします（[ミューテーション](../../sql-reference/statements/alter/index.md/#mutations)）。

可能な値：

- 0 - ミューテーションは非同期で実行。
- 1 - クエリは現在のサーバー上のすべてのミューテーションが完了するのを待ちます。
- 2 - クエリはすべてのレプリカ（存在する場合）でのすべてのミューテーションが完了するのを待ちます。

## mysql_datatypes_support_level {#mysql_datatypes_support_level}



タイプ: MySQLDataTypesSupport

デフォルト値: 

MySQLタイプが対応するClickHouseタイプに変換される方法を定義します。 `decimal`、`datetime64`、`date2Date32`または`date2String`の任意の組み合わせでカンマ区切りのリスト。
- `decimal`: 精度が許す場合、`NUMERIC`および`DECIMAL`タイプを`Decimal`に変換します。
- `datetime64`: 精度が`0`でない場合、`DATETIME`および`TIMESTAMP`タイプを`DateTime64`に変換します。
- `date2Date32`: `DATE`を`Date32`に変換し、`Date`ではなくこの優先度を使用します。
- `date2String`: `DATE`を`String`に変換し、`Date`ではなくこの優先度を使用します。`datetime64`により上書きされます。

## mysql_map_fixed_string_to_text_in_show_columns {#mysql_map_fixed_string_to_text_in_show_columns}



タイプ: Bool

デフォルト値: 1

有効にすると、[FixedString](../../sql-reference/data-types/fixedstring.md)型は[SHOW COLUMNS](../../sql-reference/statements/show.md/#show_columns)で`TEXT`として表示されます。

MySQLワイヤプロトコルを介して接続しているときにのみ影響します。

- 0 - `BLOB`を使用。
- 1 - `TEXT`を使用。

## mysql_map_string_to_text_in_show_columns {#mysql_map_string_to_text_in_show_columns}



タイプ: Bool

デフォルト値: 1

有効にすると、[String](../../sql-reference/data-types/string.md)型は[SHOW COLUMNS](../../sql-reference/statements/show.md/#show_columns)で`TEXT`として表示されます。

MySQLワイヤプロトコルを介して接続しているときにのみ影響します。

- 0 - `BLOB`を使用。
- 1 - `TEXT`を使用。

## mysql_max_rows_to_insert {#mysql_max_rows_to_insert}



タイプ: UInt64

デフォルト値: 65536

MySQLストレージエンジンのMySQLバッチ挿入における最大行数。

## network_compression_method {#network_compression_method}



タイプ: String

デフォルト値: LZ4

サーバー間およびサーバーと[clickhouse-client](../../interfaces/cli.md)間の通信に使用されるデータ圧縮方式を設定します。

可能な値：

- `LZ4` — LZ4圧縮方式を設定します。
- `ZSTD` — ZSTD圧縮方式を設定します。

**関連項目**

- [network_zstd_compression_level](#network_zstd_compression_level)

## network_zstd_compression_level {#network_zstd_compression_level}



タイプ: Int64

デフォルト値: 1

ZSTD圧縮のレベルを調整します。[network_compression_method](#network_compression_method)が`ZSTD`に設定されているときにのみ使用されます。

可能な値：

- 1から15の正の整数。

## normalize_function_names {#normalize_function_names}



タイプ: Bool

デフォルト値: 1

関数名を標準名に正規化します。

## number_of_mutations_to_delay {#number_of_mutations_to_delay}



タイプ: UInt64

デフォルト値: 0

変異したテーブルが未完了のミューテーションを少なくともその数持っている場合、テーブルのミューテーションを人工的に遅くします。0 - 無効。

## number_of_mutations_to_throw {#number_of_mutations_to_throw}



タイプ: UInt64

デフォルト値: 0

変異したテーブルが未完了のミューテーションを少なくともその数持っている場合、'Too many mutations ...'例外をスローします。0 - 無効。

## odbc_bridge_connection_pool_size {#odbc_bridge_connection_pool_size}



タイプ: UInt64

デフォルト値: 16

ODBCブリッジ内の各接続設定文字列に対する接続プールサイズ。

## odbc_bridge_use_connection_pooling {#odbc_bridge_use_connection_pooling}



タイプ: Bool

デフォルト値: 1

ODBCブリッジで接続プーリングを使用します。falseに設定すると、毎回新しい接続が作成されます。

## offset {#offset}



タイプ: UInt64

デフォルト値: 0

クエリから行を返す前にスキップする行数を設定します。これは、[OFFSET](../../sql-reference/statements/select/offset.md/#offset-fetch)句によって設定されたオフセットを調整し、これらの二つの値を合算します。

可能な値：

- 0 — 行をスキップしない。
- 正の整数。

**例**

入力テーブル:

``` sql
CREATE TABLE test (i UInt64) ENGINE = MergeTree() ORDER BY i;
INSERT INTO test SELECT number FROM numbers(500);
```

クエリ:

``` sql
SET limit = 5;
SET offset = 7;
SELECT * FROM test LIMIT 10 OFFSET 100;
```
結果:

``` text
┌───i─┐
│ 107 │
│ 108 │
│ 109 │
└─────┘
```

## opentelemetry_start_trace_probability {#opentelemetry_start_trace_probability}



タイプ: Float

デフォルト値: 0

ClickHouseが実行されたクエリのトレースを開始できる確率を設定します（親の[トレースコンテキスト](https://www.w3.org/TR/trace-context/)が供給されていない場合）。

可能な値：

- 0 — 実行されたすべてのクエリのトレースが無効になります（親トレースコンテキストが供給されていない場合）。
- [0..1]の範囲内の正の浮動小数点数。たとえば、設定値が`0.5`の場合、ClickHouseは平均して半分のクエリでトレースを開始できます。
- 1 — 実行されたすべてのクエリのトレースが有効になります。

## opentelemetry_trace_processors {#opentelemetry_trace_processors}



タイプ: Bool

デフォルト値: 0

プロセッサに対してOpenTelemetryスパンを収集します。

## optimize_aggregation_in_order {#optimize_aggregation_in_order}



タイプ: Bool

デフォルト値: 0

[MergeTree](../../engines/table-engines/mergetree-family/mergetree.md)テーブルでデータを適切な順序で集約するための[GROUP BY](../../sql-reference/statements/select/group-by.md)最適化を有効にします。

可能な値：

- 0 — `GROUP BY`最適化は無効。
- 1 — `GROUP BY`最適化は有効。

**関連項目**

- [GROUP BYの最適化](../../sql-reference/statements/select/group-by.md/#aggregation-in-order)

## optimize_aggregators_of_group_by_keys {#optimize_aggregators_of_group_by_keys}



タイプ: Bool

デフォルト値: 1

SELECTセクションのGROUP BYキーの最小/最大/任意/任意最後の集約器を排除します。

## optimize_and_compare_chain {#optimize_and_compare_chain}



タイプ: Bool

デフォルト値: 1

フィルタリング能力を向上させるためにANDチェーン内の定数比較を補充します。サポートされる演算子は`<`、`<=`、`>`、`>=`、`=`とその混合です。たとえば、`(a < b) AND (b < c) AND (c < 5)`は`(a < b) AND (b < c) AND (c < 5) AND (b < 5) AND (a < 5)`に書き換えられます。

## optimize_append_index {#optimize_append_index}



タイプ: Bool

デフォルト値: 0

インデックス条件を追加するために[制約](../../sql-reference/statements/create/table.md/#constraints)を使用します。デフォルトは`false`です。

可能な値：

- true、false

## optimize_arithmetic_operations_in_aggregate_functions {#optimize_arithmetic_operations_in_aggregate_functions}



タイプ: Bool

デフォルト値: 1

集約関数の外に算術演算を移動します。

## optimize_count_from_files {#optimize_count_from_files}



タイプ: Bool

デフォルト値: 1

異なる入力形式のファイルから行数をカウントする最適化を有効または無効にします。これは、テーブル関数/エンジン`file`/`s3`/`url`/`hdfs`/`azureBlobStorage`に適用されます。

可能な値：

- 0 — 最適化無効。
- 1 — 最適化有効。

## optimize_distinct_in_order {#optimize_distinct_in_order}



タイプ: Bool

デフォルト値: 1

明示的なソートのプレフィックスを形成するDISTINCTのカラムがある場合にDISTINCTの最適化を有効にします。たとえば、マージツリーやORDER BYステートメントにおけるソートキーのプレフィックスです。

## optimize_distributed_group_by_sharding_key {#optimize_distributed_group_by_sharding_key}



タイプ: Bool

デフォルト値: 1

イニシエーターサーバーでのコストのかかる集約を回避して、`GROUP BY sharding_key`クエリを最適化します（これにより、イニシエーターサーバーのクエリのメモリ使用量が削減されます）。

以下のタイプのクエリがサポートされています（それらのすべての組み合わせ）：

- `SELECT DISTINCT [..., ]sharding_key[, ...] FROM dist`
- `SELECT ... FROM dist GROUP BY sharding_key[, ...]`
- `SELECT ... FROM dist GROUP BY sharding_key[, ...] ORDER BY x`
- `SELECT ... FROM dist GROUP BY sharding_key[, ...] LIMIT 1`
- `SELECT ... FROM dist GROUP BY sharding_key[, ...] LIMIT 1 BY x`

以下のクエリタイプはサポートされていません（後でそのうちのいくつかが追加される可能性があります）：

- `SELECT ... GROUP BY sharding_key[, ...] WITH TOTALS`
- `SELECT ... GROUP BY sharding_key[, ...] WITH ROLLUP`
- `SELECT ... GROUP BY sharding_key[, ...] WITH CUBE`
- `SELECT ... GROUP BY sharding_key[, ...] SETTINGS extremes=1`

可能な値：

- 0 — 無効。
- 1 — 有効。

関連項目：

- [distributed_group_by_no_merge](#distributed_group_by_no_merge)
- [distributed_push_down_limit](#distributed_push_down_limit)
- [optimize_skip_unused_shards](#optimize_skip_unused_shards)

:::note
現在、これは `optimize_skip_unused_shards` を必要とします（その理由は、1日にデフォルトで有効にされる可能性があり、データがDistributedテーブルを介して挿入された場合、つまり、データがsharding_keyに基づいて分散されている場合にのみ正しく機能するためです）。
:::

## optimize_extract_common_expressions {#optimize_extract_common_expressions}



タイプ: Bool

デフォルト値: 1

WHERE、PREWHERE、ON、HAVINGおよびQUALIFY式の論理式から一般的な式を抽出できるようにします。論理式`(A AND B) OR (A AND C)`は`A AND (B OR C)`に書き換えられ、この最適化により：
- 単純なフィルタリング式におけるインデックスの利用
- 内部結合の最適化へのクロス

## optimize_functions_to_subcolumns {#optimize_functions_to_subcolumns}



タイプ: Bool

デフォルト値: 1

特定の関数をサブカラムの読み取りに変換する最適化を有効または無効にします。これにより、読み取るデータ量が減少します。

これらの関数は変換される可能性があります：

- [length](../../sql-reference/functions/array-functions.md/#array_functions-length)を使用して[サイズ0](../../sql-reference/data-types/array.md/#array-size)サブカラムを読み取ります。
- [empty](../../sql-reference/functions/array-functions.md/#function-empty)を使用して[サイズ0](../../sql-reference/data-types/array.md/#array-size)サブカラムを読み取ります。
- [notEmpty](../../sql-reference/functions/array-functions.md/#function-notempty)を使用して[サイズ0](../../sql-reference/data-types/array.md/#array-size)サブカラムを読み取ります。
- [isNull](../../sql-reference/operators/index.md/#operator-is-null)を使用して[null](../../sql-reference/data-types/nullable.md/#finding-null)サブカラムを読み取ります。
- [isNotNull](../../sql-reference/operators/index.md/#is-not-null)を使用して[null](../../sql-reference/data-types/nullable.md/#finding-null)サブカラムを読み取ります。
- [count](../../sql-reference/aggregate-functions/reference/count.md)を使用して[null](../../sql-reference/data-types/nullable.md/#finding-null)サブカラムを読み取ります。
- [mapKeys](../../sql-reference/functions/tuple-map-functions.md/#mapkeys)を使用して[keys](../../sql-reference/data-types/map.md/#map-subcolumns)サブカラムを読み取ります。
- [mapValues](../../sql-reference/functions/tuple-map-functions.md/#mapvalues)を使用して[values](../../sql-reference/data-types/map.md/#map-subcolumns)サブカラムを読み取ります。

可能な値：

- 0 — 最適化無効。
- 1 — 最適化有効。

## optimize_group_by_constant_keys {#optimize_group_by_constant_keys}



タイプ: Bool

デフォルト値: 1

ブロック内のすべてのキーが定数の際にGROUP BYを最適化します。

## optimize_group_by_function_keys {#optimize_group_by_function_keys}



タイプ: Bool

デフォルト値: 1

GROUP BYセクション内の他のキーの関数を排除します。

## optimize_if_chain_to_multiif {#optimize_if_chain_to_multiif}



タイプ: Bool

デフォルト値: 0

if(cond1, then1, if(cond2, ...))チェーンをmultiIfに置き換えます。現在、数値型の場合には有益ではありません。

## optimize_if_transform_strings_to_enum {#optimize_if_transform_strings_to_enum}



タイプ: Bool

デフォルト値: 0

IfとTransformの文字列型引数を列挙型に置き換えます。分散クエリ内で不整合な変更を引き起こす可能性があるため、デフォルトでは無効です。

## optimize_injective_functions_in_group_by {#optimize_injective_functions_in_group_by}



タイプ: Bool

デフォルト値: 1

GROUP BYセクション内で引数のInjective関数を引数に置き換えます。

## optimize_injective_functions_inside_uniq {#optimize_injective_functions_inside_uniq}



タイプ: Bool

デフォルト値: 1

uniq*()関数内の1つの引数のInjective関数を削除します。

## optimize_min_equality_disjunction_chain_length {#optimize_min_equality_disjunction_chain_length}



タイプ: UInt64

デフォルト値: 3

最適化のための式 `expr = x1 OR ... expr = xN` の最小長さ。

## optimize_min_inequality_conjunction_chain_length {#optimize_min_inequality_conjunction_chain_length}



タイプ: UInt64

デフォルト値: 3

最適化のための式 `expr <> x1 AND ... expr <> xN` の最小長さ。

## optimize_move_to_prewhere {#optimize_move_to_prewhere}



タイプ: Bool

デフォルト値: 1

[SELECT](../../sql-reference/statements/select/index.md)クエリでの自動[PREWHERE](../../sql-reference/statements/select/prewhere.md)最適化を有効または無効にします。

`*MergeTree`テーブルのみに機能します。

可能な値：

- 0 — 自動`PREWHERE`最適化無効。
- 1 — 自動`PREWHERE`最適化有効。

## optimize_move_to_prewhere_if_final {#optimize_move_to_prewhere_if_final}



タイプ: Bool

デフォルト値: 0

`FINAL`修飾子を持つ[SELECT](../../sql-reference/statements/select/index.md)クエリでの自動[PREWHERE](../../sql-reference/statements/select/prewhere.md)最適化を有効または無効にします。

`*MergeTree`テーブルのみに機能します。

可能な値：

- 0 — `FINAL`修飾子を持つ`SELECT`クエリでの自動`PREWHERE`最適化無効。
- 1 — `FINAL`修飾子を持つ`SELECT`クエリでの自動`PREWHERE`最適化有効。

**関連項目**

- [optimize_move_to_prewhere](#optimize_move_to_prewhere)設定。

## optimize_multiif_to_if {#optimize_multiif_to_if}



タイプ: Bool

デフォルト値: 1

条件が一つだけの'multiIf'を'if'に置き換えます。

## optimize_normalize_count_variants {#optimize_normalize_count_variants}



タイプ: Bool

デフォルト値: 1

意味的にcount()と等しい集約関数をcount()として書き換えます。

## optimize_on_insert {#optimize_on_insert}



タイプ: Bool

デフォルト値: 1

挿入の前にデータ変換を有効または無効にします。この設定は、ブロック内でマージが完了したかのように動作します（テーブルエンジンによります）。

可能な値：

- 0 — 無効。
- 1 — 有効。

**例**

有効と無効の違い：

クエリ:

```sql
SET optimize_on_insert = 1;

CREATE TABLE test1 (`FirstTable` UInt32) ENGINE = ReplacingMergeTree ORDER BY FirstTable;

INSERT INTO test1 SELECT number % 2 FROM numbers(5);

SELECT * FROM test1;

SET optimize_on_insert = 0;

CREATE TABLE test2 (`SecondTable` UInt32) ENGINE = ReplacingMergeTree ORDER BY SecondTable;

INSERT INTO test2 SELECT number % 2 FROM numbers(5);

SELECT * FROM test2;
```

結果:

``` text
┌─FirstTable─┐
│          0 │
│          1 │
└────────────┘

┌─SecondTable─┐
│           0 │
│           0 │
│           0 │
│           1 │
│           1 │
└─────────────┘
```

この設定は、[マテリアライズドビュー](../../sql-reference/statements/create/view.md/#materialized)の動作に影響を与えます。

## optimize_or_like_chain {#optimize_or_like_chain}



タイプ: Bool

デフォルト値: 0

複数のOR LIKEをmultiMatchAnyに最適化します。この最適化は、場合によってはインデックス解析を破壊するため、デフォルトでは有効にすべきではありません。

## optimize_read_in_order {#optimize_read_in_order}



タイプ: Bool

デフォルト値: 1

[MergeTree](../../engines/table-engines/mergetree-family/mergetree.md)テーブルからデータを読み取る際の[ORDER BY](../../sql-reference/statements/select/order-by.md/#optimize_read_in_order)最適化を有効にします。

可能な値：

- 0 — `ORDER BY`最適化無効。
- 1 — `ORDER BY`最適化有効。

**関連項目**

- [ORDER BY句](../../sql-reference/statements/select/order-by.md/#optimize_read_in_order)

## optimize_read_in_window_order {#optimize_read_in_window_order}



タイプ: Bool

デフォルト値: 1

MergeTreeテーブル内でデータを適切な順序で読み取るためのウィンドウ句内でのORDER BY最適化を有効にします。

## optimize_redundant_functions_in_order_by {#optimize_redundant_functions_in_order_by}



タイプ: Bool

デフォルト値: 1

ORDER BY内で引数がORDER BYにも存在する場合、ORDER BYから関数を削除します。

## optimize_respect_aliases {#optimize_respect_aliases}



タイプ: Bool

デフォルト値: 1

trueに設定されていると、WHERE/GROUP BY/ORDER BYでのエイリアスを尊重し、これによりパーティションプルーニング/セカンダリインデックス/optimize_aggregation_in_order/optimize_read_in_order/optimize_trivial_countを助けます。

## optimize_rewrite_aggregate_function_with_if {#optimize_rewrite_aggregate_function_with_if}



タイプ: Bool

デフォルト値: 1

引数としてif式を持つ集約関数を同等のものとして書き換えます。
たとえば、`avg(if(cond, col, null))`は`avgOrNullIf(cond, col)`に書き換えることができます。これにより、パフォーマンスが向上する可能性があります。

:::note
これは、アナライザーと一緒にサポートされています（`enable_analyzer = 1`）。
:::

## optimize_rewrite_array_exists_to_has {#optimize_rewrite_array_exists_to_has}



タイプ: Bool

デフォルト値: 0

arrayExists()関数をhas()に書き換え、論理的に同等のときに行います。たとえば、`arrayExists(x -> x = 1, arr)`は`has(arr, 1)`に書き換えられる可能性があります。

## optimize_rewrite_sum_if_to_count_if {#optimize_rewrite_sum_if_to_count_if}



タイプ: Bool

デフォルト値: 1

sumIf()およびsum(if())関数を、論理的に同等である場合はcountIf()関数に書き換えます。

## optimize_skip_merged_partitions {#optimize_skip_merged_partitions}



タイプ: Bool

デフォルト値: 0

1つのパーツのレベルが0を超えていて、期限切れTTLを持たない場合に`OPTIMIZE TABLE ... FINAL`クエリの最適化を有効または無効にします。

- `OPTIMIZE TABLE ... FINAL SETTINGS optimize_skip_merged_partitions=1`

デフォルトでは、`OPTIMIZE TABLE ... FINAL`クエリは、単一のパーツであってもそれを書き換えます。

可能な値：

- 1 — 最適化を有効にします。
- 0 — 最適化を無効にします。

## optimize_skip_unused_shards {#optimize_skip_unused_shards}



タイプ: Bool

デフォルト値: 0

`WHERE/PREWHERE`にshardingキー条件を持つ[SELECT](../../sql-reference/statements/select/index.md)クエリに対する未使用シャードのスキップを有効または無効にします（データがshardingキーによって分散されていると仮定する場合、さもなければクエリは不正確な結果をもたらします）。

可能な値：

- 0 — 無効。
- 1 — 有効。

## optimize_skip_unused_shards_limit {#optimize_skip_unused_shards_limit}



タイプ: UInt64

デフォルト値: 1000

shardingキー値の数に対する制限、制限に達すると`optimize_skip_unused_shards`がオフになります。

値が多すぎると処理に多くの時間がかかる可能性がありますが、利益は疑わしいです。なぜなら、`IN (...)`内に多くの値がある場合、ほとんどの場合、クエリはどちらにしてもすべてのシャードに送信されるからです。

## optimize_skip_unused_shards_nesting {#optimize_skip_unused_shards_nesting}



タイプ: UInt64

デフォルト値: 0

[`optimize_skip_unused_shards`](#optimize_skip_unused_shards)（したがって，`optimize_skip_unused_shards`がまだ必要です）を分散クエリの入れ子のレベルによって制御します（`Distributed`テーブルが別の`Distributed`テーブルを参照している場合）。

可能な値：

- 0 — 無効、`optimize_skip_unused_shards`は常に機能します。
- 1 — 第一レベルに対してのみ`optimize_skip_unused_shards`を有効にします。
- 2 — 第二レベルまで`optimize_skip_unused_shards`を有効にします。

## optimize_skip_unused_shards_rewrite_in {#optimize_skip_unused_shards_rewrite_in}



タイプ: Bool

デフォルト値: 1

リモートシャードのクエリ内でINを再書き換え、シャードに属さない値を除外します（`optimize_skip_unused_shards`が必要です）。

可能な値：

- 0 — 無効。
- 1 — 有効。

## optimize_sorting_by_input_stream_properties {#optimize_sorting_by_input_stream_properties}



タイプ: Bool

デフォルト値: 1

入力ストリームのソートプロパティによるソートを最適化します。

## optimize_substitute_columns {#optimize_substitute_columns}



タイプ: Bool

デフォルト値: 0

列の置き換えのために[制約](../../sql-reference/statements/create/table.md/#constraints)を使用します。デフォルトは`false`です。

可能な値：

- true、false

## optimize_syntax_fuse_functions {#optimize_syntax_fuse_functions}



タイプ: Bool

デフォルト値: 0

同一の引数を持つ集約関数を結合できるようにします。これは、同一の引数を持つ少なくとも二つの集約関数を含むクエリを、[sum](../../sql-reference/aggregate-functions/reference/sum.md/#agg_function-sum)、[count](../../sql-reference/aggregate-functions/reference/count.md/#agg_function-count)または[avg](../../sql-reference/aggregate-functions/reference/avg.md/#agg_function-avg)に書き換えます。

可能な値：

- 0 — 同一の引数を持つ関数は結合されません。
- 1 — 同一の引数を持つ関数が結合されます。

**例**

クエリ:

``` sql
CREATE TABLE fuse_tbl(a Int8, b Int8) Engine = Log;
SET optimize_syntax_fuse_functions = 1;
EXPLAIN SYNTAX SELECT sum(a), sum(b), count(b), avg(b) from fuse_tbl FORMAT TSV;
```

結果:

``` text
SELECT
    sum(a),
    sumCount(b).1,
    sumCount(b).2,
    (sumCount(b).1) / (sumCount(b).2)
FROM fuse_tbl
```

## optimize_throw_if_noop {#optimize_throw_if_noop}



タイプ: Bool

デフォルト値: 0

[OPTIMIZE](../../sql-reference/statements/optimize.md)クエリがマージを実行しなかった場合に例外をスローするかどうかを有効または無効にします。

デフォルトでは、`OPTIMIZE`は何もしなかった場合でも正常に返されます。この設定を使用すると、これらの状況を区別し、例外メッセージで理由を取得できます。

可能な値：

- 1 — 例外をスローすることが有効。
- 0 — 例外をスローすることが無効。

## optimize_time_filter_with_preimage {#optimize_time_filter_with_preimage}



タイプ: Bool

デフォルト値: 1

関数を変換することによって、DateおよびDateTimeの述語を最適化します。これは、比較の代わりに行います（例： `toYear(col) = 2023 -> col >= '2023-01-01' AND col <= '2023-12-31'`）。

## optimize_trivial_approximate_count_query {#optimize_trivial_approximate_count_query}



タイプ: Bool

デフォルト値: 0

そのような推定をサポートするストレージ（たとえば、EmbeddedRocksDB）のトリビアルカウント最適化のために近似値を使用します。

可能な値：

   - 0 — 最適化無効。
   - 1 — 最適化有効。

## optimize_trivial_count_query {#optimize_trivial_count_query}



タイプ: Bool

デフォルト値: 1

MergeTreeからのメタデータを使用して、`SELECT count() FROM table`のトリビアルクエリの最適化を有効または無効にします。行レベルのセキュリティを使用する必要がある場合、この設定を無効にします。

可能な値：

   - 0 — 最適化無効。
   - 1 — 最適化有効。

関連項目：

- [optimize_functions_to_subcolumns](#optimize_functions_to_subcolumns)

## optimize_trivial_insert_select {#optimize_trivial_insert_select}



タイプ: Bool

デフォルト値: 0

トリビアル 'INSERT INTO table SELECT ... FROM TABLES' クエリを最適化します。

## optimize_uniq_to_count {#optimize_uniq_to_count}



タイプ: Bool

デフォルト値: 1

uniqおよびそのバリエーション（uniqUpToを除く）を、サブクエリにdistinctまたはgroup by句がある場合にcountに書き換えます。

## optimize_use_implicit_projections {#optimize_use_implicit_projections}



タイプ: Bool

デフォルト値: 1

SELECTクエリを実行するための暗黙的なプロジェクションを自動的に選択します。

## optimize_use_projections {#optimize_use_projections}



タイプ: Bool

デフォルト値: 1

`SELECT`クエリの処理中に[プロジェクション](../../engines/table-engines/mergetree-family/mergetree.md/#projections)最適化を有効または無効にします。

可能な値：

- 0 — プロジェクション最適化は無効。
- 1 — プロジェクション最適化は有効。

## optimize_using_constraints {#optimize_using_constraints}



タイプ: Bool

デフォルト値: 0

クエリ最適化のために[制約](../../sql-reference/statements/create/table.md/#constraints)を使用します。デフォルトは`false`です。

可能な値：

- true、false

## os_thread_priority {#os_thread_priority}



タイプ: Int64

デフォルト値: 0

クエリを実行するスレッドの優先度（[nice](https://en.wikipedia.org/wiki/Nice_(Unix))）を設定します。OSスケジューラは、この優先度を考慮して、各利用可能なCPUコア上で次に実行するスレッドを選択します。

:::note
この設定を使用するには、`CAP_SYS_NICE`権限を設定する必要があります。`clickhouse-server`パッケージは、インストール中にこれを設定します。一部の仮想環境では、`CAP_SYS_NICE`権限を設定できない場合があります。この場合、`clickhouse-server`は起動時にその旨をメッセージとして表示します。
:::

可能な値：

- 値は`[-20, 19]`の範囲内で設定できます。

値が低いほど優先度が高くなります。低い`nice`優先度の値を持つスレッドは、高い値のスレッドよりも頻繁に実行されます。高い値は、短期的な対話型クエリが到着したときに、リソースをすばやく譲ることができるため、長時間実行される非対話型クエリには好ましいです。

## output_format_compression_level {#output_format_compression_level}



タイプ: UInt64

デフォルト値: 3

クエリ出力が圧縮されている場合のデフォルト圧縮レベル。この設定は、`SELECT`クエリが`INTO OUTFILE`を持っている場合、またはテーブル関数`file`、`url`、`hdfs`、`s3`、または`azureBlobStorage`に書き込んでいる場合に適用されます。

可能な値: `1`から`22`まで。

## output_format_compression_zstd_window_log {#output_format_compression_zstd_window_log}



タイプ: UInt64

デフォルト値: 0

出力圧縮方式が`zstd`のときに使用できます。これが`0`より大きい場合、この設定は圧縮ウィンドウサイズ（2の冪）を明示的に設定し、zstd圧縮の長距離モードを有効にします。これにより、より良い圧縮率を達成できることがあります。

可能な値：非負の数。値が小さすぎたり大きすぎたりすると、`zstdlib`が例外をスローします。典型的な値は、`20`（ウィンドウサイズ = `1MB`）から`30`（ウィンドウサイズ = `1GB`）です。

## output_format_parallel_formatting {#output_format_parallel_formatting}



タイプ: Bool

デフォルト値: 1

データ形式の並行フォーマットを有効または無効にします。[TSV](../../interfaces/formats.md/#tabseparated)、[TSKV](../../interfaces/formats.md/#tskv)、[CSV](../../interfaces/formats.md/#csv)、および[JSONEachRow](../../interfaces/formats.md/#jsoneachrow)形式のみサポートされています。

可能な値：

- 1 — 有効。
- 0 — 無効。

## page_cache_inject_eviction {#page_cache_inject_eviction}



タイプ: Bool

デフォルト値: 0

ユーザースペースのページキャッシュは、時折ランダムにページの無効化を行います。テストを目的としています。

## parallel_distributed_insert_select {#parallel_distributed_insert_select}



タイプ: UInt64

デフォルト値: 0

並行分散`INSERT ... SELECT`クエリを有効にします。
```
```markdown
もし `INSERT INTO distributed_table_a SELECT ... FROM distributed_table_b` クエリを実行し、両方のテーブルが同じクラスターを使用し、両方のテーブルが [レプリケート](../../engines/table-engines/mergetree-family/replication.md) されているか非レプリケートである場合、このクエリは各シャードでローカルに処理されます。

可能な値:

- 0 — 無効。
- 1 — `SELECT` は分散エンジンの基になるテーブルの各シャードで実行されます。
- 2 — `SELECT` と `INSERT` は分散エンジンの基になるテーブルの各シャードで実行されます。

## parallel_replica_offset {#parallel_replica_offset}
<BetaBadge/>

タイプ: UInt64

デフォルト値: 0

これは内部設定であり、直接使用すべきではありません。'parallel replicas' モードの実装の詳細を表します。この設定は、クエリ処理に参加しているレプリカのインデックスへの分散クエリのために、イニシエーターサーバーによって自動的に設定されます。

## parallel_replicas_allow_in_with_subquery {#parallel_replicas_allow_in_with_subquery}
<BetaBadge/>

タイプ: Bool

デフォルト値: 1

trueの場合、INのためのサブクエリはすべてのフォロワーレプリカで実行されます。

## parallel_replicas_count {#parallel_replicas_count}
<BetaBadge/>

タイプ: UInt64

デフォルト値: 0

これは内部設定であり、直接使用すべきではありません。'parallel replicas' モードの実装の詳細を表します。この設定は、クエリ処理に参加している平行レプリカの数のためにイニシエーターサーバーによって自動的に設定されます。

## parallel_replicas_custom_key {#parallel_replicas_custom_key}
<BetaBadge/>

タイプ: String

デフォルト値: 

特定のテーブルにおけるレプリカ間での作業を分割するために使用できる任意の整数式。
値は任意の整数式にすることができます。

主キーを使用したシンプルな式が推奨されます。

この設定が、複数のレプリカを持つ単一のシャードからなるクラスターで使用されると、そのレプリカは仮想シャードに変換されます。
そうでない場合は、`SAMPLE` キーと同様に動作し、各シャードの複数のレプリカを使用します。

## parallel_replicas_custom_key_range_lower {#parallel_replicas_custom_key_range_lower}
<BetaBadge/>

タイプ: UInt64

デフォルト値: 0

フィルタータイプ `range` がカスタム範囲 `[parallel_replicas_custom_key_range_lower, INT_MAX]` に基づいてレプリカ間で均等に作業を分割することを許可します。

[parallel_replicas_custom_key_range_upper](#parallel_replicas_custom_key_range_upper) と一緒に使用すると、フィルターは `[parallel_replicas_custom_key_range_lower, parallel_replicas_custom_key_range_upper]` の範囲でレプリカ間で均等に作業を分割できます。

注: この設定は、クエリ処理中に追加のデータをフィルターすることはありません。むしろ、平行処理のために範囲フィルターが範囲 `[0, INT_MAX]` を分割するポイントを変更します。

## parallel_replicas_custom_key_range_upper {#parallel_replicas_custom_key_range_upper}
<BetaBadge/>

タイプ: UInt64

デフォルト値: 0

フィルタータイプ `range` がカスタム範囲 `[0, parallel_replicas_custom_key_range_upper]` に基づいてレプリカ間で均等に作業を分割することを許可します。値が0の場合は上限が無効になります。カスタムキー式の最大値に設定されます。

[parallel_replicas_custom_key_range_lower](#parallel_replicas_custom_key_range_lower) と一緒に使用すると、フィルターは `[parallel_replicas_custom_key_range_lower, parallel_replicas_custom_key_range_upper]` の範囲でレプリカ間で均等に作業を分割できます。

注: この設定は、クエリ処理中に追加のデータをフィルターすることはありません。むしろ、平行処理のために範囲フィルターが範囲 `[0, INT_MAX]` を分割するポイントを変更します。

## parallel_replicas_for_non_replicated_merge_tree {#parallel_replicas_for_non_replicated_merge_tree}
<BetaBadge/>

タイプ: Bool

デフォルト値: 0

trueの場合、ClickHouseは非レプリケートのMergeTreeテーブルに対しても平行レプリカアルゴリズムを使用します。

## parallel_replicas_index_analysis_only_on_coordinator {#parallel_replicas_index_analysis_only_on_coordinator}
<BetaBadge/>

タイプ: Bool

デフォルト値: 1

インデックス分析はレプリカコーディネーターのみに行われ、他のレプリカではスキップされます。これは、平行レプリカローカルプランが有効な場合のみ有効です。

## parallel_replicas_local_plan {#parallel_replicas_local_plan}
<BetaBadge/>

タイプ: Bool

デフォルト値: 1

ローカルレプリカのためにローカルプランを構築します。

## parallel_replicas_mark_segment_size {#parallel_replicas_mark_segment_size}
<BetaBadge/>

タイプ: UInt64

デフォルト値: 0

パーツは仮想的にセグメントに分割され、平行読み取りのためにレプリカ間で分配されます。この設定はこれらのセグメントのサイズを制御します。自分が何をしているか確信が持てるまでは変更をおすすめしません。値は [128; 16384] の範囲である必要があります。

## parallel_replicas_min_number_of_rows_per_replica {#parallel_replicas_min_number_of_rows_per_replica}
<BetaBadge/>

タイプ: UInt64

デフォルト値: 0

クエリで使用されるレプリカの数を (推定行数 / min_number_of_rows_per_replica) に制限します。最大値は 'max_parallel_replicas' によって制限されます。

## parallel_replicas_mode {#parallel_replicas_mode}
<BetaBadge/>

タイプ: ParallelReplicasMode

デフォルト値: read_tasks

カスタムキーを使用した平行レプリカに対して使用するフィルターの種類です。デフォルトは、カスタムキーに対して剰余演算を使用します。範囲 - カスタムキーの値タイプのすべての可能な値を使用してカスタムキーに対して範囲フィルターを使用します。

## parallel_replicas_prefer_local_join {#parallel_replicas_prefer_local_join}
<BetaBadge/>

タイプ: Bool

デフォルト値: 1

trueの場合、JOIN が平行レプリカアルゴリズムで実行でき、右JOIN部分のすべてのストレージが *MergeTree の場合、ローカルJOIN が使用され、GLOBAL JOIN ではなくなります。

## parallel_view_processing {#parallel_view_processing}

タイプ: Bool

デフォルト値: 0

添付されたビューに対して順次にではなく同時にプッシュを有効にします。

## parallelize_output_from_storages {#parallelize_output_from_storages}

タイプ: Bool

デフォルト値: 1

ストレージからの読み取りステップの出力を並列化します。これは、ストレージから読み取った後にクエリ処理を並列化することを可能にします。

## parsedatetime_parse_without_leading_zeros {#parsedatetime_parse_without_leading_zeros}

タイプ: Bool

デフォルト値: 1

関数 'parseDateTime' でのフォーマッター '%c', '%l' および '%k' は、先頭のゼロなしで月と時間を解析します。

## partial_merge_join_left_table_buffer_bytes {#partial_merge_join_left_table_buffer_bytes}

タイプ: UInt64

デフォルト値: 0

0でない場合、部分マージJOINにおける左側テーブルの左テーブルブロックをより大きなものにグループ化します。結合スレッドごとに指定したメモリの最大2倍を使用します。

## partial_merge_join_rows_in_right_blocks {#partial_merge_join_rows_in_right_blocks}

タイプ: UInt64

デフォルト値: 65536

[JOIN](../../sql-reference/statements/select/join.md) クエリの部分マージJOINアルゴリズムにおける右側結合データブロックのサイズを制限します。

ClickHouse サーバーは:

1. 右側結合データを、指定した行数までのブロックに分割します。
2. 各ブロックを最小値および最大値でインデックスします。
3. 可能であれば、準備されたブロックをディスクにアンロードします。

可能な値:

- 任意の正の整数。推奨される値の範囲: \[1000, 100000\]。

## partial_result_on_first_cancel {#partial_result_on_first_cancel}

タイプ: Bool

デフォルト値: 0

キャンセル後に部分結果を返すことを許可します。

## parts_to_delay_insert {#parts_to_delay_insert}

タイプ: UInt64

デフォルト値: 0

もし、宛先テーブルが単一のパーティション内にこの数以上のアクティブパーツを含む場合、テーブルへの挿入を人工的に遅縮します。

## parts_to_throw_insert {#parts_to_throw_insert}

タイプ: UInt64

デフォルト値: 0

宛先テーブルの単一のパーティション内にこの番号を超えるアクティブパーツがある場合、「Too many parts ...」という例外をスローします。

## periodic_live_view_refresh {#periodic_live_view_refresh}

タイプ: 秒

デフォルト値: 60

定期的に更新されるライブビューが強制的に更新される間隔。

## poll_interval {#poll_interval}

タイプ: UInt64

デフォルト値: 10

サーバーのクエリ待機ループで指定された秒数だけブロックします。

## postgresql_connection_attempt_timeout {#postgresql_connection_attempt_timeout}

タイプ: UInt64

デフォルト値: 2

PostgreSQLエンドポイントへの接続の単一試行の際の接続タイムアウト（秒）。

この値は接続URLの `connect_timeout` パラメータとして渡されます。

## postgresql_connection_pool_auto_close_connection {#postgresql_connection_pool_auto_close_connection}

タイプ: Bool

デフォルト値: 0

接続をプールに戻す前に接続を閉じます。

## postgresql_connection_pool_retries {#postgresql_connection_pool_retries}

タイプ: UInt64

デフォルト値: 2

PostgreSQLテーブルエンジンおよびデータベースエンジンのための接続プールのプッシュ/ポップの再試行回数。

## postgresql_connection_pool_size {#postgresql_connection_pool_size}

タイプ: UInt64

デフォルト値: 16

PostgreSQLテーブルエンジンおよびデータベースエンジンのための接続プールのサイズ。

## postgresql_connection_pool_wait_timeout {#postgresql_connection_pool_wait_timeout}

タイプ: UInt64

デフォルト値: 5000

PostgreSQLテーブルエンジンおよびデータベースエンジンのための空のプールでのプッシュ/ポップのタイムアウト。

デフォルトでは空のプールでブロックします。

## postgresql_fault_injection_probability {#postgresql_fault_injection_probability}

タイプ: Float

デフォルト値: 0

内部（レプリケーション用）のPostgreSQLクエリが失敗するおおよその確率。有効な値は [0.0f, 1.0f] の範囲にあります。

## prefer_column_name_to_alias {#prefer_column_name_to_alias}

タイプ: Bool

デフォルト値: 0

クエリ式および句でエイリアスの代わりに元のカラム名を使用することを有効または無効にします。特にエイリアスがカラム名と同じ場合に重要です。 詳しくは [Expression Aliases](../../sql-reference/syntax.md/#notes-on-usage) を参照してください。この設定を有効にすると、ClickHouseのエイリアス構文ルールが他のほとんどのデータベースエンジンとより互換性があるようになります。

可能な値:

- 0 — カラム名がエイリアスに置き換えられます。
- 1 — カラム名はエイリアスに置き換えられません。

**例**

有効と無効の違い:

クエリ:

```sql
SET prefer_column_name_to_alias = 0;
SELECT avg(number) AS number, max(number) FROM numbers(10);
```

結果:

```text
サーバーからの例外を受け取りました (バージョン 21.5.1):
コード: 184. DB::Exception: localhost:9000 から受信。 DB::Exception: 集約関数 avg(number) はクエリ内の他の集約関数内に見つかりました: avg(number) AS number を処理中。
```

クエリ:

```sql
SET prefer_column_name_to_alias = 1;
SELECT avg(number) AS number, max(number) FROM numbers(10);
```

結果:

```text
┌─number─┬─max(number)─┐
│    4.5 │           9 │
└────────┴─────────────┘
```

## prefer_external_sort_block_bytes {#prefer_external_sort_block_bytes}

タイプ: UInt64

デフォルト値: 16744704

外部ソート用の最大ブロックバイトを優先し、マージ中のメモリ使用量を減らします。

## prefer_global_in_and_join {#prefer_global_in_and_join}

タイプ: Bool

デフォルト値: 0

`IN`/`JOIN` 演算子を `GLOBAL IN`/`GLOBAL JOIN` に置き換えることを有効にします。

可能な値:

- 0 — 無効。 `IN`/`JOIN` 演算子は `GLOBAL IN`/`GLOBAL JOIN` に置き換えられません。
- 1 — 有効。 `IN`/`JOIN` 演算子は `GLOBAL IN`/`GLOBAL JOIN` に置き換えられます。

**使用方法**

`SET distributed_product_mode=global` で分散テーブルのクエリの動作を変更できますが、これはローカルテーブルや外部リソースのテーブルには適していません。この設定は `prefer_global_in_and_join` に関係しています。

たとえば、ローカルテーブルを含むクエリサービスノードがある場合、それらは分配に適しません。分散処理中にデータを流動的に散らす必要があります — `GLOBAL` キーワードで。

`prefer_global_in_and_join` の別の使用例は、外部エンジンによって作成されたテーブルにアクセスすることです。この設定は、結合中に外部ソースへの呼び出し回数を減少させるのに役立ちます: クエリごとに1回の呼び出しだけでよいのです。

**参照:**

- [Distributed subqueries](../../sql-reference/operators/in.md/#select-distributed-subqueries) 確に `GLOBAL IN`/`GLOBAL JOIN` をどう使うかに関しての詳細。

## prefer_localhost_replica {#prefer_localhost_replica}

タイプ: Bool

デフォルト値: 1

分散クエリを処理するときにローカルホストレプリカを好んで使用することを有効/無効にします。

可能な値:

- 1 — ローカルホストレプリカが存在する場合、ClickHouse は常にそのクエリを送信します。
- 0 — ClickHouse は [load_balancing](#load_balancing) 設定によって指定されたバランス戦略を使用します。

:::note
[parallel_replicas_custom_key](#parallel_replicas_custom_key) を使用せずに [max_parallel_replicas](#max_parallel_replicas) を使用する場合は、この設定を無効にしてください。
[parallel_replicas_custom_key](#parallel_replicas_custom_key) が設定されている場合、この設定は複数のレプリカを含む複数のシャードのクラスターで使用されている場合にのみ無効にしてください。
単一のシャードと複数のレプリカのクラスターでは、この設定を無効にしても悪影響が出ます。
:::

## prefer_warmed_unmerged_parts_seconds {#prefer_warmed_unmerged_parts_seconds}

タイプ: Int64

デフォルト値: 0

ClickHouse Cloud にのみ利用可能。マージされていないパーツがこの秒数未満で新しく、事前にウォームアップされていない場合（[cache_populated_by_fetch](merge-tree-settings.md/#cache_populated_by_fetch) を見てください）、それでもソースパーツがすべて利用可能で事前にウォームアップされている場合、SELECT クエリはそれらのパーツから読み取ります。これは Replicated-/SharedMergeTree のみです。

## preferred_block_size_bytes {#preferred_block_size_bytes}

タイプ: UInt64

デフォルト値: 1000000

この設定はクエリ処理のためのデータブロックサイズを調整し、より粗い 'max_block_size' 設定に対する追加の微調整を表します。もしカラムが大きく、'max_block_size' 行がある場合、そのブロックサイズは指定されたバイト数より大きくなる可能性があるため、CPUキャッシュのローカリティを向上させるためにサイズが小さくなります。

## preferred_max_column_in_block_size_bytes {#preferred_max_column_in_block_size_bytes}

タイプ: UInt64

デフォルト値: 0

読み取り中のブロック内の最大カラムサイズの制限。キャッシュミスの数を減らすのに役立ちます。L2キャッシュサイズに近い必要があります。

## preferred_optimize_projection_name {#preferred_optimize_projection_name}

タイプ: String

デフォルト値: 

空でない文字列に設定されている場合、ClickHouseはクエリに指定されたプロジェクションを適用しようとします。

可能な値:

- 文字列: 好みのプロジェクションの名前

## prefetch_buffer_size {#prefetch_buffer_size}

タイプ: UInt64

デフォルト値: 1048576

ファイルシステムから読み取るためのプレフェッチバッファの最大サイズ。

## print_pretty_type_names {#print_pretty_type_names}

タイプ: Bool

デフォルト値: 1

`DESCRIBE` クエリおよび `toTypeName()` 関数で深く入れ子になったタイプ名をインデント付きのきれいな形式で印刷することを許可します。

例:

```sql
CREATE TABLE test (a Tuple(b String, c Tuple(d Nullable(UInt64), e Array(UInt32), f Array(Tuple(g String, h Map(String, Array(Tuple(i String, j UInt64))))), k Date), l Nullable(String))) ENGINE=Memory;
DESCRIBE TABLE test FORMAT TSVRaw SETTINGS print_pretty_type_names=1;
```

```
a   Tuple(
    b String,
    c Tuple(
        d Nullable(UInt64),
        e Array(UInt32),
        f Array(Tuple(
            g String,
            h Map(
                String,
                Array(Tuple(
                    i String,
                    j UInt64
                ))
            )
        )),
        k Date
    ),
    l Nullable(String)
)
```

## priority {#priority}

タイプ: UInt64

デフォルト値: 0

クエリの優先度。1は最も高く、より高い値は低い優先度を表します。0は優先度を使用しません。

## push_external_roles_in_interserver_queries {#push_external_roles_in_interserver_queries}

タイプ: Bool

デフォルト値: 1

クエリを実行する際に、発信元から他のノードにユーザーロールをプッシュすることを有効にします。

## query_cache_compress_entries {#query_cache_compress_entries}

タイプ: Bool

デフォルト値: 1

[クエリキャッシュ](../query-cache.md)内のエントリを圧縮します。これにより、クエリキャッシュのメモリ消費が削減されますが、キャッシュへの挿入および読み取りが遅くなります。

可能な値:

- 0 - 無効
- 1 - 有効

## query_cache_max_entries {#query_cache_max_entries}

タイプ: UInt64

デフォルト値: 0

現在のユーザーが [クエリキャッシュ](../query-cache.md) に保存できるクエリ結果の最大数。0は無制限を意味します。

可能な値:

- 正の整数 >= 0.

## query_cache_max_size_in_bytes {#query_cache_max_size_in_bytes}

タイプ: UInt64

デフォルト値: 0

現在のユーザーが [クエリキャッシュ](../query-cache.md) に割り当てられる最大メモリ量（バイト単位）。0は無制限を意味します。

可能な値:

- 正の整数 >= 0.

## query_cache_min_query_duration {#query_cache_min_query_duration}

タイプ: ミリ秒

デフォルト値: 0

結果が [クエリキャッシュ](../query-cache.md) に保存されるために、クエリが実行される必要がある最小の持続時間（ミリ秒単位）。

可能な値:

- 正の整数 >= 0.

## query_cache_min_query_runs {#query_cache_min_query_runs}

タイプ: UInt64

デフォルト値: 0

`SELECT` クエリが [クエリキャッシュ](../query-cache.md) に結果を保存する前に、実行されなければならない最小回数。

可能な値:

- 正の整数 >= 0.

## query_cache_nondeterministic_function_handling {#query_cache_nondeterministic_function_handling}

タイプ: QueryCacheNondeterministicFunctionHandling

デフォルト値: throw

[クエリキャッシュ](../query-cache.md)が `rand()` や `now()` のような非決定論的関数を含む `SELECT` クエリをどのように扱うかを制御します。

可能な値:

- `'throw'` - 例外をスローし、クエリ結果をキャッシュしません。
- `'save'` - クエリ結果をキャッシュします。
- `'ignore'` - クエリ結果をキャッシュせず、例外をスローしません。

## query_cache_share_between_users {#query_cache_share_between_users}

タイプ: Bool

デフォルト値: 0

有効にすると、[クエリキャッシュ](../query-cache.md)にキャッシュされた `SELECT` クエリの結果は他のユーザーによって読み取ることができます。
セキュリティ上の理由から、この設定を有効にすることは推奨されません。

可能な値:

- 0 - 無効
- 1 - 有効

## query_cache_squash_partial_results {#query_cache_squash_partial_results}

タイプ: Bool

デフォルト値: 1

部分結果ブロックを [max_block_size](#max_block_size) のサイズのブロックに圧縮します。これにより [query cache](../query-cache.md) への挿入の性能が低下しますが、キャッシュエントリの圧縮可能性が向上します（[query_cache_compress_entries](#query_cache_compress_entries) を参照）。

可能な値:

- 0 - 無効
- 1 - 有効

## query_cache_system_table_handling {#query_cache_system_table_handling}

タイプ: QueryCacheSystemTableHandling

デフォルト値: throw

[クエリキャッシュ](../query-cache.md) がシステムテーブルに対する `SELECT` クエリをどのように扱うか（すなわち `system.*` および `information_schema.*` 内のテーブル）。

可能な値:

- `'throw'` - 例外をスローし、クエリ結果をキャッシュしません。
- `'save'` - クエリ結果をキャッシュします。
- `'ignore'` - クエリ結果をキャッシュせず、例外をスローしません。

## query_cache_tag {#query_cache_tag}

タイプ: String

デフォルト値: 

[クエリキャッシュ](../query-cache.md)エントリのラベルとして機能する文字列。
異なるタグを持つ同じクエリは、クエリキャッシュによって別々に扱われます。

可能な値:

- 任意の文字列

## query_cache_ttl {#query_cache_ttl}

タイプ: 秒

デフォルト値: 60

この時間（秒単位）が経過した後、[クエリキャッシュ](../query-cache.md)のエントリは陳腐化します。

可能な値:

- 正の整数 >= 0.

## query_metric_log_interval {#query_metric_log_interval}

タイプ: Int64

デフォルト値: -1

個々のクエリに対する [query_metric_log](../../operations/system-tables/query_metric_log.md) の収集間隔（ミリ秒単位）。

負の値が設定された場合、[query_metric_log設定](../../operations/server-configuration-parameters/settings.md/#query_metric_log) の値 `collect_interval_milliseconds` から値を取得するか、存在しない場合はデフォルトで1000になります。

単一のクエリの収集を無効にするには、 `query_metric_log_interval` を0に設定してください。

デフォルト値: -1

## query_plan_aggregation_in_order {#query_plan_aggregation_in_order}

タイプ: Bool

デフォルト値: 1

クエリプランレベルの最適化で、順序での集約を切り替えます。
設定 [query_plan_enable_optimizations](#query_plan_enable_optimizations) が 1 の場合にのみ有効です。

:::note
これは開発者によるデバッグ専用の専門的な設定です。この設定は将来、後方互換性のない方法で変更される可能性があります。
:::

可能な値:

- 0 - 無効
- 1 - 有効

## query_plan_convert_outer_join_to_inner_join {#query_plan_convert_outer_join_to_inner_join}

タイプ: Bool

デフォルト値: 1

フィルタがJOINの後に常にデフォルト値をフィルタリングする場合、OUTER JOINをINNER JOINに変換できるようにします。

## query_plan_enable_multithreading_after_window_functions {#query_plan_enable_multithreading_after_window_functions}

タイプ: Bool

デフォルト値: 1

ウィンドウ関数を評価した後に並行ストリーム処理を可能にするために多スレッド処理を有効にします。

## query_plan_enable_optimizations {#query_plan_enable_optimizations}

タイプ: Bool

デフォルト値: 1

クエリプランレベルでの最適化を切り替えます。

:::note
これは開発者によるデバッグ専用の専門的な設定です。この設定は将来、後方互換性のない方法で変更される可能性があります。
:::

可能な値:

- 0 - クエリプランレベルでのすべての最適化を無効化
- 1 - クエリプランレベルでの最適化を有効化（ただし、個々の最適化はそれぞれの設定で無効にすることができます）

## query_plan_execute_functions_after_sorting {#query_plan_execute_functions_after_sorting}

タイプ: Bool

デフォルト値: 1

ソートステップの後に式を移動させるクエリプランレベルの最適化を切り替えます。
設定 [query_plan_enable_optimizations](#query_plan_enable_optimizations) が 1 の場合にのみ有効です。

:::note
これは開発者によるデバッグ専用の専門的な設定です。この設定は将来、後方互換性のない方法で変更される可能性があります。
:::

可能な値:

- 0 - 無効
- 1 - 有効

## query_plan_filter_push_down {#query_plan_filter_push_down}

タイプ: Bool

デフォルト値: 1

実行プラン内でフィルターを下に移動させるクエリプランレベルの最適化を切り替えます。
設定 [query_plan_enable_optimizations](#query_plan_enable_optimizations) が 1 の場合にのみ有効です。

:::note
これは開発者によるデバッグ専用の専門的な設定です。この設定は将来、後方互換性のない方法で変更される可能性があります。
:::

可能な値:

- 0 - 無効
- 1 - 有効

## query_plan_join_swap_table {#query_plan_join_swap_table}

タイプ: BoolAuto

デフォルト値: auto

    クエリプラン内で結合テーブルのどちら側がビルドテーブル（ハッシュ結合のためにハッシュテーブルに挿入されるテーブルとも呼ばれる）であるべきかを決定します。この設定は、結合ON句を持つ`ALL`結合の厳密性に対してのみサポートされています。可能な値は以下の通りです:
    - 'auto': プランナーがビルドテーブルに使用するテーブルを決定します。
    - 'false': テーブルを交換しません（右側のテーブルがビルドテーブルです）。
    - 'true': 常にテーブルを交換します（左側のテーブルがビルドテーブルです）。

## query_plan_lift_up_array_join {#query_plan_lift_up_array_join}

タイプ: Bool

デフォルト値: 1

ARRAY JOINを実行プランの上に移動させるクエリプランレベルの最適化を切り替えます。
設定 [query_plan_enable_optimizations](#query_plan_enable_optimizations) が 1 の場合にのみ有効です。

:::note
これは開発者によるデバッグ専用の専門的な設定です。この設定は将来、後方互換性のない方法で変更される可能性があります。
:::

可能な値:

- 0 - 無効
- 1 - 有効

## query_plan_lift_up_union {#query_plan_lift_up_union}

タイプ: Bool

デフォルト値: 1

クエリプラン内のより大きなサブツリーをユニオンに移動させ、さらなる最適化を可能にするクエリプランレベルの最適化を切り替えます。
設定 [query_plan_enable_optimizations](#query_plan_enable_optimizations) が 1 の場合にのみ有効です。

:::note
これは開発者によるデバッグ専用の専門的な設定です。この設定は将来、後方互換性のない方法で変更される可能性があります。
:::

可能な値:

- 0 - 無効
- 1 - 有効

## query_plan_max_optimizations_to_apply {#query_plan_max_optimizations_to_apply}

タイプ: UInt64

デフォルト値: 10000

クエリプランに適用される最適化の総数を制限します。 [query_plan_enable_optimizations](#query_plan_enable_optimizations) 設定を参照してください。
複雑なクエリのために長い最適化時間を避けるために有用です。
実際の最適化数がこの設定を超える場合、例外がスローされます。

:::note
これは開発者によるデバッグ専用の専門的な設定です。この設定は将来、後方互換性のない方法で変更される可能性があります。
:::

## query_plan_merge_expressions {#query_plan_merge_expressions}

タイプ: Bool

デフォルト値: 1

連続するフィルターを結合するクエリプランレベルの最適化を切り替えます。
設定 [query_plan_enable_optimizations](#query_plan_enable_optimizations) が 1 の場合にのみ有効です。

:::note
これは開発者によるデバッグ専用の専門的な設定です。この設定は将来、後方互換性のない方法で変更される可能性があります。
:::

可能な値:

- 0 - 無効
- 1 - 有効

## query_plan_merge_filters {#query_plan_merge_filters}

タイプ: Bool

デフォルト値: 1

クエリプラン内でフィルターを結合することを許可します。

## query_plan_optimize_prewhere {#query_plan_optimize_prewhere}

タイプ: Bool

デフォルト値: 1

サポートされているストレージのために、フィルターをPREWHERE式にプッシュダウンすることを許可します。

## query_plan_push_down_limit {#query_plan_push_down_limit}

タイプ: Bool

デフォルト値: 1

実行プラン内でLIMITを下に移動させるクエリプランレベルの最適化を切り替えます。
設定 [query_plan_enable_optimizations](#query_plan_enable_optimizations) が 1 の場合にのみ有効です。

:::note
これは開発者によるデバッグ専用の専門的な設定です。この設定は将来、後方互換性のない方法で変更される可能性があります。
:::

可能な値:

- 0 - 無効
- 1 - 有効

## query_plan_read_in_order {#query_plan_read_in_order}

タイプ: Bool

デフォルト値: 1

順序での読み取りの最適化クエリプランレベルの最適化を切り替えます。
設定 [query_plan_enable_optimizations](#query_plan_enable_optimizations) が 1 の場合にのみ有効です。

:::note
これは開発者によるデバッグ専用の専門的な設定です。この設定は将来、後方互換性のない方法で変更される可能性があります。
:::

可能な値:

- 0 - 無効
- 1 - 有効

## query_plan_remove_redundant_distinct {#query_plan_remove_redundant_distinct}

タイプ: Bool

デフォルト値: 1

冗長なDISTINCTステップを削除するクエリプランレベルの最適化を切り替えます。
設定 [query_plan_enable_optimizations](#query_plan_enable_optimizations) が 1 の場合にのみ有効です。

:::note
これは開発者によるデバッグ専用の専門的な設定です。この設定は将来、後方互換性のない方法で変更される可能性があります。
:::

可能な値:

- 0 - 無効
- 1 - 有効

## query_plan_remove_redundant_sorting {#query_plan_remove_redundant_sorting}

タイプ: Bool

デフォルト値: 1

冗長なソートステップを削除するクエリプランレベルの最適化を切り替えます。たとえば、サブクエリ内での操作です。
設定 [query_plan_enable_optimizations](#query_plan_enable_optimizations) が 1 の場合にのみ有効です。

:::note
これは開発者によるデバッグ専用の専門的な設定です。この設定は将来、後方互換性のない方法で変更される可能性があります。
:::

可能な値:

- 0 - 無効
- 1 - 有効

## query_plan_reuse_storage_ordering_for_window_functions {#query_plan_reuse_storage_ordering_for_window_functions}

タイプ: Bool

デフォルト値: 1

ウィンドウ関数のソート時にストレージのソートを再利用するクエリプランレベルの最適化を切り替えます。
設定 [query_plan_enable_optimizations](#query_plan_enable_optimizations) が 1 の場合にのみ有効です。

:::note
これは開発者によるデバッグ専用の専門的な設定です。この設定は将来、後方互換性のない方法で変更される可能性があります。
:::

可能な値:

- 0 - 無効
- 1 - 有効

## query_plan_split_filter {#query_plan_split_filter}

タイプ: Bool

デフォルト値: 1

:::note
これは開発者によるデバッグ専用の専門的な設定です。この設定は将来、後方互換性のない方法で変更される可能性があります。
:::

フィルターを式に分割するクエリプランレベルの最適化を切り替えます。
設定 [query_plan_enable_optimizations](#query_plan_enable_optimizations) が 1 の場合にのみ有効です。

可能な値:

- 0 - 無効
- 1 - 有効

## query_plan_try_use_vector_search {#query_plan_try_use_vector_search}

タイプ: Bool

デフォルト値: 1

ベクトル類似インデックスを使用しようとするクエリプランレベルの最適化を切り替えます。
設定 [query_plan_enable_optimizations](#query_plan_enable_optimizations) が 1 の場合にのみ有効です。

:::note
これは開発者によるデバッグ専用の専門的な設定です。この設定は将来、後方互換性のない方法で変更される可能性があります。
:::

可能な値:

- 0 - 無効
- 1 - 有効

## query_plan_use_new_logical_join_step {#query_plan_use_new_logical_join_step}

タイプ: Bool

デフォルト値: 1

クエリプランで新しい論理結合ステップを使用します。

## query_profiler_cpu_time_period_ns {#query_profiler_cpu_time_period_ns}

タイプ: UInt64

デフォルト値: 1000000000

[クエリプロファイラー](../../operations/optimizing-performance/sampling-query-profiler.md)のCPUクロックタイマーの期間を設定します。このタイマーはCPU時間のみをカウントします。

可能な値:
```
- 正の整数のナノ秒。

    推奨値：

            - 単一のクエリの場合は、10000000（1秒に100回）ナノ秒以上。
            - クラスター全体のプロファイリングには、1000000000（1秒に1回）。

- タイマーをオフにするには0を設定します。

**ClickHouse Cloudでは一時的に無効化されています。**

参照：

- システムテーブル [trace_log](../../operations/system-tables/trace_log.md/#system_tables-trace_log)

## query_profiler_real_time_period_ns {#query_profiler_real_time_period_ns}



タイプ: UInt64

デフォルト値: 1000000000

[クエリプロファイラー](../../operations/optimizing-performance/sampling-query-profiler.md)のリアルクロックタイマーの期間を設定します。リアルクロックタイマーは壁時計の時間をカウントします。

可能な値：

- 正の整数、ナノ秒単位。

    推奨値：

            - 単一のクエリの場合は、10000000（1秒に100回）ナノ秒以下。
            - クラスター全体のプロファイリングには、1000000000（1秒に1回）。

- タイマーをオフにするには0を設定します。

**ClickHouse Cloudでは一時的に無効化されています。**

参照：

- システムテーブル [trace_log](../../operations/system-tables/trace_log.md/#system_tables-trace_log)

## queue_max_wait_ms {#queue_max_wait_ms}



タイプ: ミリ秒

デフォルト値: 0

同時リクエストの数が最大を超えた場合のリクエストキュー内での待機時間。

## rabbitmq_max_wait_ms {#rabbitmq_max_wait_ms}



タイプ: ミリ秒

デフォルト値: 5000

再試行前にRabbitMQから読み取るための待機時間。

## read_backoff_max_throughput {#read_backoff_max_throughput}



タイプ: UInt64

デフォルト値: 1048576

スローレイドの際にスレッドの数を減少させるための設定。読み取り帯域幅が毎秒このバイト数未満になった場合のイベントをカウントします。

## read_backoff_min_concurrency {#read_backoff_min_concurrency}



タイプ: UInt64

デフォルト値: 1

スローレイドの際に最小限のスレッド数を維持しようとする設定。

## read_backoff_min_events {#read_backoff_min_events}



タイプ: UInt64

デフォルト値: 2

スローレイドの際にスレッドの数を減少させるための設定。スレッドの数が減少するまでのイベントの数。

## read_backoff_min_interval_between_events_ms {#read_backoff_min_interval_between_events_ms}



タイプ: ミリ秒

デフォルト値: 1000

スローレイドの際にスレッドの数を減少させるための設定。前のイベントから一定の時間が経過していない場合は、そのイベントに注意を払わない。

## read_backoff_min_latency_ms {#read_backoff_min_latency_ms}



タイプ: ミリ秒

デフォルト値: 1000

スローレイドの際にスレッドの数を減少させるための設定。少なくともその分の時間がかかった読み取りのみに注意を払う。

## read_from_filesystem_cache_if_exists_otherwise_bypass_cache {#read_from_filesystem_cache_if_exists_otherwise_bypass_cache}



タイプ: Bool

デフォルト値: 0

パッシブモードでファイルシステムキャッシュを使用することを許可します - 既存のキャッシュエントリの利益を得ますが、キャッシュに新しいエントリを追加しません。この設定を重いアドホッククエリに設定し、短期のリアルタイムクエリには無効にすると、重いクエリによるキャッシュのスラッシングを避け、全体的なシステム効率を向上させることができます。

## read_from_page_cache_if_exists_otherwise_bypass_cache {#read_from_page_cache_if_exists_otherwise_bypass_cache}



タイプ: Bool

デフォルト値: 0

ファイルシステムキャッシュが存在しない場合、ページキャッシュをパッシブモードで使用します。不使用の場合は、read_from_filesystem_cache_if_exists_otherwise_bypass_cacheと同様です。

## read_in_order_two_level_merge_threshold {#read_in_order_two_level_merge_threshold}



タイプ: UInt64

デフォルト値: 100

プライマリキーの順序でマルチスレッド読み取りを実行する際に事前マージステップを実行するために読み取るべき最小部品数。

## read_in_order_use_buffering {#read_in_order_use_buffering}



タイプ: Bool

デフォルト値: 1

プライマリキーの順序で読み取る際にマージ前にバッファリングを使用します。これによりクエリの実行の並列性が向上します。

## read_in_order_use_virtual_row {#read_in_order_use_virtual_row}



タイプ: Bool

デフォルト値: 0

プライマリキーの順序またはその単調関数のファッションで読み込む際に仮想行を使用します。これは複数のパーツを検索する際に、関係するもののみがタッチされる場合に便利です。

## read_overflow_mode {#read_overflow_mode}



タイプ: OverflowMode

デフォルト値: throw

制限を超えた場合に何をするか。

## read_overflow_mode_leaf {#read_overflow_mode_leaf}



タイプ: OverflowMode

デフォルト値: throw

リーフ制限を超えた場合に何をするか。

## read_priority {#read_priority}



タイプ: Int64

デフォルト値: 0

ローカルファイルシステムまたはリモートファイルシステムからデータを読み取るための優先度。ローカルファイルシステムに対する `pread_threadpool` メソッドと、リモートファイルシステムに対する `threadpool` メソッドのみがサポートされています。

## read_through_distributed_cache {#read_through_distributed_cache}



タイプ: Bool

デフォルト値: 0

ClickHouse Cloudのみ。分散キャッシュからの読み取りを許可します。

## readonly {#readonly}



タイプ: UInt64

デフォルト値: 0

0 - 読み取り専用の制限なし。1 - 読み取りリクエストのみ、明示的に許可された設定の変更も可能。2 - 読み取りリクエストのみ、設定の変更も可能だが、'readonly'設定は除外。

## receive_data_timeout_ms {#receive_data_timeout_ms}



タイプ: ミリ秒

デフォルト値: 2000

初回データパケットまたはレプリカからの進行状況のあるパケットを受信するための接続タイムアウト。

## receive_timeout {#receive_timeout}



タイプ: 秒

デフォルト値: 300

ネットワークからデータを受信するためのタイムアウト（秒単位）。この間にバイトが受信されなかった場合、例外がスローされます。クライアントでこの設定を設定すると、ソケットの 'send_timeout' もサーバーの対応する接続側に設定されます。

## regexp_max_matches_per_row {#regexp_max_matches_per_row}



タイプ: UInt64

デフォルト値: 1000

行あたりの単一正規表現に対する最大一致数を設定します。[extractAllGroupsHorizontal](../../sql-reference/functions/string-search-functions.md/#extractallgroups-horizontal)関数を使用する際に、メモリオーバーロードを防ぐために使用します。

可能な値：

- 正の整数。

## reject_expensive_hyperscan_regexps {#reject_expensive_hyperscan_regexps}



タイプ: Bool

デフォルト値: 1

ハイパースキャンで評価するのが高コストになりそうなパターンを拒否します（NFA状態の爆発による）。

## remerge_sort_lowered_memory_bytes_ratio {#remerge_sort_lowered_memory_bytes_ratio}



タイプ: Float

デフォルト値: 2

リマージ後のメモリ使用量がこの比率で減少しない場合、リマージは無効になります。

## remote_filesystem_read_method {#remote_filesystem_read_method}



タイプ: 文字列

デフォルト値: threadpool

リモートファイルシステムからデータを読み取る方法、readまたはthreadpoolのいずれか。

## remote_filesystem_read_prefetch {#remote_filesystem_read_prefetch}



タイプ: Bool

デフォルト値: 1

リモートファイルシステムからデータを読み取る際にプレファッチを使用する必要があります。

## remote_fs_read_backoff_max_tries {#remote_fs_read_backoff_max_tries}



タイプ: UInt64

デフォルト値: 5

バックオフで読み取るための最大試行回数。

## remote_fs_read_max_backoff_ms {#remote_fs_read_max_backoff_ms}



タイプ: UInt64

デフォルト値: 10000

リモートディスクからデータを読み取る際の最大待機時間。

## remote_read_min_bytes_for_seek {#remote_read_min_bytes_for_seek}



タイプ: UInt64

デフォルト値: 4194304

リモート読み取り（url、s3）でシークを行うために必要な最小バイト数。

## rename_files_after_processing {#rename_files_after_processing}



タイプ: 文字列

デフォルト値: 

- **タイプ:** 文字列

- **デフォルト値:** 空文字列

この設定では、`file` テーブル関数によって処理されたファイルの名前変更パターンを指定できます。このオプションが設定されると、`file` テーブル関数で読み取られるすべてのファイルは、処理が成功した場合に指定されたパターンに従ってプレースホルダーと共に名前が変更されます。

### プレースホルダー

- `%a` — 完全な元のファイル名（例: "sample.csv"）。
- `%f` — 拡張子なしの元のファイル名（例: "sample"）。
- `%e` — ドット付きの元のファイル拡張子（例: ".csv"）。
- `%t` — タイムスタンプ（マイクロ秒）。
- `%%` — パーセント記号 ("%")。

### 例
- オプション: `--rename_files_after_processing="processed_%f_%t%e"`

- クエリ: `SELECT * FROM file('sample.csv')`

`sample.csv` の読み取りが成功すると、ファイル名は `processed_sample_1683473210851438.csv` に変更されます。

## replace_running_query {#replace_running_query}



タイプ: Bool

デフォルト値: 0

HTTPインターフェースを使用する際、'query_id'パラメーターを渡すことができます。これはクエリ識別子として機能する任意の文字列です。
同じユーザーから同じ 'query_id' のクエリがすでに存在する場合、動作は 'replace_running_query' パラメーターに依存します。

`0`（デフォルト）– 例外をスローします（同じ 'query_id' のクエリがすでに実行中であれば、そのクエリを実行させません）。

`1` – 古いクエリをキャンセルし、新しいものを実行開始します。

セグメンテーション条件の提案を実装するためにこのパラメータを1に設定します。次の文字を入力した場合、古いクエリがまだ終了していなければ、キャンセルされる必要があります。

## replace_running_query_max_wait_ms {#replace_running_query_max_wait_ms}



タイプ: ミリ秒

デフォルト値: 5000

[replace_running_query](#replace_running_query) 設定が有効な場合、同じ `query_id` を持つクエリの実行が終了するまでの待機時間。

可能な値：

- 正の整数。
- 0 – 新しいクエリを実行することを許可しない例外をスローします。サーバーが同じ `query_id` のクエリをすでに実行している場合。

## replication_wait_for_inactive_replica_timeout {#replication_wait_for_inactive_replica_timeout}



タイプ: Int64

デフォルト値: 120

[ALTER](../../sql-reference/statements/alter/index.md)、[OPTIMIZE](../../sql-reference/statements/optimize.md) または [TRUNCATE](../../sql-reference/statements/truncate.md)クエリを実行するまで、非アクティブなレプリカを待つ時間（秒単位）を指定します。

可能な値：

- 0 – 待機しません。
- 負の整数 – 無制限に待機します。
- 正の整数 – 待機する秒数。

## restore_replace_external_dictionary_source_to_null {#restore_replace_external_dictionary_source_to_null}



タイプ: Bool

デフォルト値: 0

復元時に外部辞書ソースをNullに置き換えます。テスト目的に便利です。

## restore_replace_external_engines_to_null {#restore_replace_external_engines_to_null}



タイプ: Bool

デフォルト値: 0

テスト目的。すべての外部エンジンをNullに置き換えて外部接続を開始しないようにします。

## restore_replace_external_table_functions_to_null {#restore_replace_external_table_functions_to_null}



タイプ: Bool

デフォルト値: 0

テスト目的。すべての外部テーブル関数をNullに置き換えて外部接続を開始しないようにします。

## result_overflow_mode {#result_overflow_mode}



タイプ: OverflowMode

デフォルト値: throw

制限を超えた場合に何をするか。

## rewrite_count_distinct_if_with_count_distinct_implementation {#rewrite_count_distinct_if_with_count_distinct_implementation}



タイプ: Bool

デフォルト値: 0

`countDistinctIf`を[ count_distinct_implementation](#count_distinct_implementation) 設定で書き換えることを許可します。

可能な値：

- true — 許可。
- false — 不許可。

## s3_allow_parallel_part_upload {#s3_allow_parallel_part_upload}



タイプ: Bool

デフォルト値: 1

s3のマルチパートアップロードに複数スレッドを使用します。これにより、わずかにメモリ使用量が増加する可能性があります。

## s3_check_objects_after_upload {#s3_check_objects_after_upload}



タイプ: Bool

デフォルト値: 0

アップロードされた各オブジェクトをヘッドリクエストでチェックして、アップロードが成功したことを確認します。

## s3_connect_timeout_ms {#s3_connect_timeout_ms}



タイプ: UInt64

デフォルト値: 1000

s3ディスクからのホストへの接続タイムアウト。

## s3_create_new_file_on_insert {#s3_create_new_file_on_insert}



タイプ: Bool

デフォルト値: 0

s3エンジンテーブルに対する各挿入時に新しいファイルを作成するかどうかを有効または無効にします。有効にすると、各挿入時に、新しいS3オブジェクトが次のパターンに従って作成されます：

初期: `data.Parquet.gz` -> `data.1.Parquet.gz` -> `data.2.Parquet.gz` など。

可能な値：
- 0 — `INSERT` クエリはファイルの末尾に新しいデータを追加します。
- 1 — `INSERT` クエリは新しいファイルを作成します。

## s3_disable_checksum {#s3_disable_checksum}



タイプ: Bool

デフォルト値: 0

ファイルをS3に送信する際にチェックサムを計算しません。これにより、ファイルへの過剰な処理を避けることで書き込みが高速化されます。これは、MergeTreeテーブルのデータはClickHouseによってチェックサムされるため、ほとんど安全です。また、S3にHTTPSでアクセスする際には、TLSレイヤーによりネットワークを介して転送中の整合性が提供されます。S3上での追加のチェックサムは深さのある防御となります。

## s3_ignore_file_doesnt_exist {#s3_ignore_file_doesnt_exist}



タイプ: Bool

デフォルト値: 0

特定のキーを読み取る際にファイルが存在しない場合は無視します。

可能な値：
- 1 — `SELECT` は空の結果を返します。
- 0 — `SELECT` は例外をスローします。

## s3_list_object_keys_size {#s3_list_object_keys_size}



タイプ: UInt64

デフォルト値: 1000

ListObjectリクエストによってバッチで返される可能性のあるファイルの最大数。

## s3_max_connections {#s3_max_connections}



タイプ: UInt64

デフォルト値: 1024

サーバーあたりの最大接続数。

## s3_max_get_burst {#s3_max_get_burst}



タイプ: UInt64

デフォルト値: 0

リクエストによる制限に達する前に同時に発行できる最大リクエスト数。デフォルト（0）は `s3_max_get_rps` と等しい。

## s3_max_get_rps {#s3_max_get_rps}



タイプ: UInt64

デフォルト値: 0

スロットリング前のS3 GETリクエストの速度制限。一時的には無制限を意味します。

## s3_max_inflight_parts_for_one_file {#s3_max_inflight_parts_for_one_file}



タイプ: UInt64

デフォルト値: 20

マルチパートアップロードリクエストで同時にロードされるパーツの最大数。0は無制限を意味します。

## s3_max_part_number {#s3_max_part_number}



タイプ: UInt64

デフォルト値: 10000

s3アップロードパートの最大パート番号。

## s3_max_put_burst {#s3_max_put_burst}



タイプ: UInt64

デフォルト値: 0

リクエストによる制限に達する前に同時に発行できる最大リクエスト数。デフォルト（0）は `s3_max_put_rps` と等しい。

## s3_max_put_rps {#s3_max_put_rps}



タイプ: UInt64

デフォルト値: 0

スロットリング前のS3 PUTリクエストの速度制限。一時的には無制限を意味します。

## s3_max_redirects {#s3_max_redirects}



タイプ: UInt64

デフォルト値: 10

許可される最大のS3リダイレクトホップ数。

## s3_max_single_operation_copy_size {#s3_max_single_operation_copy_size}



タイプ: UInt64

デフォルト値: 33554432

s3における単一コピー操作の最大サイズ。

## s3_max_single_part_upload_size {#s3_max_single_part_upload_size}



タイプ: UInt64

デフォルト値: 33554432

単一パートアップロードを使用してS3にアップロードできるオブジェクトの最大サイズ。

## s3_max_single_read_retries {#s3_max_single_read_retries}



タイプ: UInt64

デフォルト値: 4

単一S3読み取りの最大リトライ回数。

## s3_max_unexpected_write_error_retries {#s3_max_unexpected_write_error_retries}



タイプ: UInt64

デフォルト値: 4

S3書き込み中の予期しないエラーが発生した場合の最大リトライ回数。

## s3_max_upload_part_size {#s3_max_upload_part_size}



タイプ: UInt64

デフォルト値: 5368709120

マルチパートアップロード中にS3にアップロードするパーツの最大サイズ。

## s3_min_upload_part_size {#s3_min_upload_part_size}



タイプ: UInt64

デフォルト値: 16777216

マルチパートアップロード中にS3にアップロードするパーツの最小サイズ。

## s3_request_timeout_ms {#s3_request_timeout_ms}



タイプ: UInt64

デフォルト値: 30000

S3とのデータ送信および受信に関するアイドルタイムアウト。この期間中に単一のTCP読み取りまたは書き込みコールがブロックされている場合は失敗します。

## s3_retry_attempts {#s3_retry_attempts}



タイプ: UInt64

デフォルト値: 100

Aws::Client::RetryStrategy用の設定。Aws::Clientは自動的にリトライを実行します。0はリトライしないことを意味します。

## s3_skip_empty_files {#s3_skip_empty_files}



タイプ: Bool

デフォルト値: 1

[S3](../../engines/table-engines/integrations/s3.md)エンジンのテーブルで空のファイルをスキップするかどうかを有効または無効にします。

可能な値：
- 0 — 空のファイルがリクエストされた形式と互換性がない場合、`SELECT`は例外をスローします。
- 1 — 空のファイルに対して`SELECT`は空の結果を返します。

## s3_strict_upload_part_size {#s3_strict_upload_part_size}



タイプ: UInt64

デフォルト値: 0

マルチパートアップロード中にS3にアップロードするパーツの正確なサイズ（一部の実装では可変サイズパーツをサポートしていません）。

## s3_throw_on_zero_files_match {#s3_throw_on_zero_files_match}



タイプ: Bool

デフォルト値: 0

ListObjectsリクエストがファイルを一致させることができない場合にエラーをスローします。

## s3_truncate_on_insert {#s3_truncate_on_insert}



タイプ: Bool

デフォルト値: 0

S3エンジンテーブルへの挿入の前に切り捨てを有効または無効にします。無効にすると、既存のS3オブジェクトが既に存在する場合は挿入試行時に例外がスローされます。

可能な値：
- 0 — `INSERT` クエリはファイル末尾に新しいデータを追加します。
- 1 — `INSERT` クエリは新しいデータでファイルの既存の内容を置き換えます。

## s3_upload_part_size_multiply_factor {#s3_upload_part_size_multiply_factor}



タイプ: UInt64

デフォルト値: 2

s3_multiply_parts_count_thresholdから単一の書き込みでアップロードされた各s3_min_upload_part_sizeをこのファクターで掛け算します。

## s3_upload_part_size_multiply_parts_count_threshold {#s3_upload_part_size_multiply_parts_count_threshold}



タイプ: UInt64

デフォルト値: 500

この数のパーツがS3にアップロードされた各回ごとに、s3_min_upload_part_sizeがs3_upload_part_size_multiply_factorによって掛け算されます。

## s3_use_adaptive_timeouts {#s3_use_adaptive_timeouts}



タイプ: Bool

デフォルト値: 1

`true`に設定されると、すべてのs3リクエストに対して最初の2回の試行が低い送信および受信タイムアウトで行われます。
`false`に設定されると、すべての試行が同一のタイムアウトで行われます。

## s3_validate_request_settings {#s3_validate_request_settings}



タイプ: Bool

デフォルト値: 1

s3リクエスト設定の検証を有効にします。

可能な値：
- 1 — 設定を検証します。
- 0 — 設定を検証しません。

## s3queue_default_zookeeper_path {#s3queue_default_zookeeper_path}



タイプ: 文字列

デフォルト値: /clickhouse/s3queue/

S3Queueエンジンのデフォルトのzookeeperパスプレフィックス。

## s3queue_enable_logging_to_s3queue_log {#s3queue_enable_logging_to_s3queue_log}



タイプ: Bool

デフォルト値: 0

system.s3queue_logへの書き込みを有効にします。この値はテーブル設定で上書き可能です。

## s3queue_migrate_old_metadata_to_buckets {#s3queue_migrate_old_metadata_to_buckets}



タイプ: Bool

デフォルト値: 0

S3Queueテーブルの古いメタデータ構造を新しいものに移行します。

## schema_inference_cache_require_modification_time_for_url {#schema_inference_cache_require_modification_time_for_url}



タイプ: Bool

デフォルト値: 1

最終変更時間の検証（Last-Modifiedヘッダーを持つURL）についてキャッシュ内のスキーマを使用します。

## schema_inference_use_cache_for_azure {#schema_inference_use_cache_for_azure}



タイプ: Bool

デフォルト値: 1

azureテーブル関数を使用している間、スキーマ推論のキャッシュを使用します。

## schema_inference_use_cache_for_file {#schema_inference_use_cache_for_file}



タイプ: Bool

デフォルト値: 1

ファイルテーブル関数を使用している間、スキーマ推論のキャッシュを使用します。

## schema_inference_use_cache_for_hdfs {#schema_inference_use_cache_for_hdfs}



タイプ: Bool

デフォルト値: 1

hdfsテーブル関数を使用している間、スキーマ推論のキャッシュを使用します。

## schema_inference_use_cache_for_s3 {#schema_inference_use_cache_for_s3}



タイプ: Bool

デフォルト値: 1

s3テーブル関数を使用している間、スキーマ推論のキャッシュを使用します。

## schema_inference_use_cache_for_url {#schema_inference_use_cache_for_url}



タイプ: Bool

デフォルト値: 1

urlテーブル関数を使用している間、スキーマ推論のキャッシュを使用します。

## select_sequential_consistency {#select_sequential_consistency}



タイプ: UInt64

デフォルト値: 0

:::note
この設定はSharedMergeTreeとReplicatedMergeTreeの間で動作が異なります。`select_sequential_consistency`のSharedMergeTreeにおける動作の詳細については、[SharedMergeTreeの一貫性](/cloud/reference/shared-merge-tree/#consistency)を参照してください。
:::

`SELECT`クエリのための逐次整合性を有効または無効にします。`insert_quorum_parallel`が無効である必要があります（デフォルトでは有効）。

可能な値：

- 0 — 無効。
- 1 — 有効。

使用方法

逐次整合性が有効になっている場合、ClickHouseはクライアントが、`insert_quorum`を使用して実行されたすべての以前の`INSERT`クエリからデータを含むレプリカにのみ`SELECT`クエリを実行することを許可します。クライアントが部分的なレプリカを参照する場合、ClickHouseは例外を生成します。SELECTクエリは、まだクオーラムのレプリカに書き込まれていないデータを含めません。

`insert_quorum_parallel`が有効になっている場合（デフォルト）、`select_sequential_consistency`は機能しません。これは、並列の`INSERT`クエリが異なるセットのクオーラムレプリカに書き込まれる可能性があるため、単一のレプリカがすべての書き込みを受け取ったと保証するものがないからです。

参照：

- [insert_quorum](#insert_quorum)
- [insert_quorum_timeout](#insert_quorum_timeout)
- [insert_quorum_parallel](#insert_quorum_parallel)

## send_logs_level {#send_logs_level}



タイプ: LogsLevel

デフォルト値: fatal

指定された最小レベルのサーバーテキストログをクライアントに送信します。有効な値: 'trace', 'debug', 'information', 'warning', 'error', 'fatal', 'none'

## send_logs_source_regexp {#send_logs_source_regexp}



タイプ: 文字列

デフォルト値: 

指定された正規表現と一致するログソース名を持つサーバーテキストログを送信します。空である場合、すべてのソースが対象となります。

## send_progress_in_http_headers {#send_progress_in_http_headers}



タイプ: Bool

デフォルト値: 0

`clickhouse-server`レスポンスの `X-ClickHouse-Progress` HTTPレスポンスヘッダーを有効または無効にします。

詳細については、[HTTPインターフェースの説明](../../interfaces/http.md)を参照してください。

可能な値：

- 0 — 無効。
- 1 — 有効。

## send_timeout {#send_timeout}



タイプ: 秒

デフォルト値: 300

ネットワークにデータを送信するためのタイムアウト（秒単位）。クライアントがデータを送信する必要がありますが、この間にバイトを送信できない場合、例外がスローされます。クライアントでこの設定を指定した場合、ソケットの 'receive_timeout' もサーバーの対応する接続側に設定されます。

## session_timezone {#session_timezone}
<BetaBadge/>


タイプ: タイムゾーン

デフォルト値: 

現在のセッションまたはクエリの暗黙のタイムゾーンを設定します。
暗黙のタイムゾーンは、明示的に指定されていないDateTime/DateTime64型の値に適用されます。
この設定は、グローバルに構成された（サーバーレベルの）暗黙のタイムゾーンよりも優先されます。
''（空文字列）の値は、現在のセッションまたはクエリの暗黙のタイムゾーンが[サーバータイムゾーン](../server-configuration-parameters/settings.md/#timezone)と等しいことを意味します。

`timeZone()`および`serverTimeZone()`関数を使用して、セッションタイムゾーンとサーバータイムゾーンを取得できます。

可能な値：

- `system.time_zones`からの任意のタイムゾーン名（例：`Europe/Berlin`、`UTC`または`Zulu`）。

例：

```sql
SELECT timeZone(), serverTimeZone() FORMAT CSV

"Europe/Berlin","Europe/Berlin"
```

```sql
SELECT timeZone(), serverTimeZone() SETTINGS session_timezone = 'Asia/Novosibirsk' FORMAT CSV

"Asia/Novosibirsk","Europe/Berlin"
```

明示的に指定されていないタイムゾーンを持つ内部DateTimeに対して、セッションタイムゾーン 'America/Denver' を割り当てます：

```sql
SELECT toDateTime64(toDateTime64('1999-12-12 23:23:23.123', 3), 3, 'Europe/Zurich') SETTINGS session_timezone = 'America/Denver' FORMAT TSV

1999-12-13 07:23:23.123
```

:::warning
すべての関数がDateTime/DateTime64を解析する際に`session_timezone`を尊重するわけではありません。これにより、微妙なエラーが発生する可能性があります。
以下の例と説明を参照してください。
:::

```sql
CREATE TABLE test_tz (`d` DateTime('UTC')) ENGINE = Memory AS SELECT toDateTime('2000-01-01 00:00:00', 'UTC');

SELECT *, timeZone() FROM test_tz WHERE d = toDateTime('2000-01-01 00:00:00') SETTINGS session_timezone = 'Asia/Novosibirsk'
0 rows in set.

SELECT *, timeZone() FROM test_tz WHERE d = '2000-01-01 00:00:00' SETTINGS session_timezone = 'Asia/Novosibirsk'
┌───────────────────d─┬─timeZone()───────┐
│ 2000-01-01 00:00:00 │ Asia/Novosibirsk │
└─────────────────────┴──────────────────┘
```

これは異なる解析パイプラインによるものです：

- 明示的に指定されたタイムゾーンなしで使用される`toDateTime()`は、最初の`SELECT`クエリで`session_timezone`とグローバルタイムゾーンを尊重します。
- 2番目のクエリでは、文字列からDateTimeが解析され、既存のカラム`d`の型とタイムゾーンを継承します。したがって、`session_timezone`およびグローバルタイムゾーンの設定は尊重されません。

**参照：**

- [timezone](../server-configuration-parameters/settings.md/#timezone)

## set_overflow_mode {#set_overflow_mode}



タイプ: OverflowMode

デフォルト値: throw

制限を超えた場合に何をするか。

## shared_merge_tree_sync_parts_on_partition_operations {#shared_merge_tree_sync_parts_on_partition_operations}



タイプ: Bool

デフォルト値: 1

SMTテーブルのMOVE|REPLACE|ATTACHパーティション操作後にデータパーツのセットを自動的に同期します。クラウド専用。

## short_circuit_function_evaluation {#short_circuit_function_evaluation}



タイプ: ShortCircuitFunctionEvaluation

デフォルト値: enable

[if](../../sql-reference/functions/conditional-functions.md/#if)、[multiIf](../../sql-reference/functions/conditional-functions.md/#multiif)、[and](../../sql-reference/functions/logical-functions.md/#logical-and-function)、および [or](../../sql-reference/functions/logical-functions.md/#logical-or-function)関数を[短絡方式](https://en.wikipedia.org/wiki/Short-circuit_evaluation)で計算することを許可します。これにより、これらの関数内の複雑な式の実行が最適化され、予期しない例外（ゼロでの除算など）を防ぐことができます。

可能な値：

- `enable` — 適切な関数に対して短絡関数評価を有効にします（例外をスローしたり、計算が重いものがある場合）。
- `force_enable` — すべての関数に対して短絡関数評価を有効にします。
- `disable` — 短絡関数評価を無効にします。

## short_circuit_function_evaluation_for_nulls {#short_circuit_function_evaluation_for_nulls}



タイプ: Bool

デフォルト値: 1

引数にNULL値が含まれる行に対して、Nullable引数を持つ関数を実行することを許可します。引数のNULL値の比率がshort_circuit_function_evaluation_for_nulls_thresholdを超えた場合のみ適用されます。NULL値を含む行に対してNULL値を返す関数にのみ適用されます。

## short_circuit_function_evaluation_for_nulls_threshold {#short_circuit_function_evaluation_for_nulls_threshold}



タイプ: ダブル

デフォルト値: 1

引数にNullable引数を持つ関数を、すべての引数に対して非NULL値の行でのみ実行するためのNULL値の比率の閾値。short_circuit_function_evaluation_for_nullsが有効なときに適用されます。
NULL値を含む行の比率がこの閾値を超えると、これらのNULL値を含む行は評価されません。

## show_table_uuid_in_table_create_query_if_not_nil {#show_table_uuid_in_table_create_query_if_not_nil}



タイプ: Bool

デフォルト値: 0

`SHOW TABLE` クエリの表示を設定します。

可能な値：

- 0 — クエリはテーブルUUIDなしで表示されます。
- 1 — クエリはテーブルUUIDとともに表示されます。

## single_join_prefer_left_table {#single_join_prefer_left_table}



タイプ: Bool

デフォルト値: 1

識別子のあいまいさがある場合、単一のJOINでは左側のテーブルを優先します。

## skip_redundant_aliases_in_udf {#skip_redundant_aliases_in_udf}



タイプ: Bool

デフォルト値: 0

ユーザー定義関数内で冗長なエイリアスが使用されないように（置き換えられないように）します。

可能な値：

- 1 — エイリアスがスキップされます（置き換えられます）。
- 0 — エイリアスはスキップされません（置き換えられません）。

**例**

有効と無効の違い：

クエリ：

```sql
SET skip_redundant_aliases_in_udf = 0;
CREATE FUNCTION IF NOT EXISTS test_03274 AS ( x ) -> ((x + 1 as y, y + 2));

EXPLAIN SYNTAX SELECT test_03274(4 + 2);
```

結果：

```text
SELECT ((4 + 2) + 1 AS y, y + 2)
```

クエリ：

```sql
SET skip_redundant_aliases_in_udf = 1;
CREATE FUNCTION IF NOT EXISTS test_03274 AS ( x ) -> ((x + 1 as y, y + 2));

EXPLAIN SYNTAX SELECT test_03274(4 + 2);
```

結果：

```text
SELECT ((4 + 2) + 1, ((4 + 2) + 1) + 2)
```

## skip_unavailable_shards {#skip_unavailable_shards}



タイプ: Bool

デフォルト値: 0

使用できないシャードの静かにスキップを有効または無効にします。

シャードは、すべてのレプリカが使用できない場合は使用できないと見なされます。レプリカが使用できないのは次のような場合です：

- ClickHouseがレプリカに接続できない理由。

    レプリカへの接続時、ClickHouseはいくつかの試行を行います。これらの試行がすべて失敗した場合、そのレプリカは使用できないと見なされます。

- レプリカをDNSを通じて解決できません。

    レプリカのホスト名がDNSを通じて解決できない場合、次の状況を示す可能性があります。

    - レプリカのホストにはDNSレコードがない可能性があります。これは、[Kubernetes](https://kubernetes.io)のような動的DNSを持つシステムで発生することがあります。ノードがダウンタイム中に解決できない場合、これはエラーではありません。

    - 構成エラー。ClickHouseの構成ファイルに誤ったホスト名が含まれています。

可能な値：

- 1 — スキップを有効化。

    シャードが使用できない場合、ClickHouseは部分的なデータに基づく結果を返し、ノードの可用性の問題を報告しません。

- 0 — スキップ無効。

    シャードが使用できない場合、ClickHouseは例外をスローします。

## sleep_after_receiving_query_ms {#sleep_after_receiving_query_ms}



タイプ: ミリ秒

デフォルト値: 0

TCPHandlerでクエリを受信した後のスリープ時間。

## sleep_in_send_data_ms {#sleep_in_send_data_ms}



タイプ: ミリ秒

デフォルト値: 0

TCPHandlerでデータを送信中のスリープ時間。

## sleep_in_send_tables_status_ms {#sleep_in_send_tables_status_ms}



タイプ: ミリ秒

デフォルト値: 0

TCPHandlerでテーブルステータス応答を送信中のスリープ時間。

## sort_overflow_mode {#sort_overflow_mode}



タイプ: OverflowMode

デフォルト値: throw

制限を超えた場合に何をするか。
## split_intersecting_parts_ranges_into_layers_final {#split_intersecting_parts_ranges_into_layers_final}

タイプ: Bool

デフォルト値: 1

FINAL最適化中に交差するパーツ範囲をレイヤに分割します。

## split_parts_ranges_into_intersecting_and_non_intersecting_final {#split_parts_ranges_into_intersecting_and_non_intersecting_final}

タイプ: Bool

デフォルト値: 1

FINAL最適化中にパーツ範囲を交差するものとしないものに分割します。

## splitby_max_substrings_includes_remaining_string {#splitby_max_substrings_includes_remaining_string}

タイプ: Bool

デフォルト値: 0

引数 `max_substrings` が 0 より大きい場合に、関数 [splitBy*()](../../sql-reference/functions/splitting-merging-functions.md) が結果配列の最後の要素に残りの文字列を含むかどうかを制御します。

可能な値:

- `0` - 残りの文字列は結果配列の最後の要素に含まれません。
- `1` - 残りの文字列は結果配列の最後の要素に含まれます。これはSparkの[`split()`](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.split.html)関数およびPythonの['string.split()'](https://docs.python.org/3/library/stdtypes.html#str.split)メソッドの動作です。

## stop_refreshable_materialized_views_on_startup {#stop_refreshable_materialized_views_on_startup}
<ExperimentalBadge/>

タイプ: Bool

デフォルト値: 0

サーバ起動時に、まるで `SYSTEM STOP VIEWS` のようにリフレッシュ可能なマテリアライズドビューのスケジュールを防ぎます。その後、`SYSTEM START VIEWS`または`SYSTEM START VIEW <name>`で手動で開始することができます。また、新しく作成されたビューにも適用されます。リフレッシュ不可能なマテリアライズドビューには影響しません。

## storage_file_read_method {#storage_file_read_method}

タイプ: LocalFSReadMethod

デフォルト値: pread

ストレージファイルからデータを読み取る方法。`read`、`pread`、`mmap` のいずれかです。mmapメソッドはclickhouse-serverには適用されません（これはclickhouse-local用です）。

## storage_system_stack_trace_pipe_read_timeout_ms {#storage_system_stack_trace_pipe_read_timeout_ms}

タイプ: ミリ秒

デフォルト値: 100

`system.stack_trace` テーブルをクエリする際にスレッドから情報を受信するためのパイプから読み取る最大時間。この設定はテスト目的で使用され、ユーザーによって変更されるべきではありません。

## stream_flush_interval_ms {#stream_flush_interval_ms}

タイプ: ミリ秒

デフォルト値: 7500

タイムアウトの場合、またはスレッドが [max_insert_block_size](#max_insert_block_size) 行を生成する場合にストリーミングを持つテーブルに対して機能します。

デフォルト値は7500です。

値が小さいほど、データがテーブルにフラッシュされる頻度が増します。値を低く設定しすぎるとパフォーマンスが悪化します。

## stream_like_engine_allow_direct_select {#stream_like_engine_allow_direct_select}

タイプ: Bool

デフォルト値: 0

Kafka、RabbitMQ、FileLog、Redis Streams、NATSエンジンに対して直接SELECTクエリを許可します。マテリアライズドビューが添付されている場合、この設定が有効でもSELECTクエリは許可されません。

## stream_like_engine_insert_queue {#stream_like_engine_insert_queue}

タイプ: 文字列

デフォルト値: 

ストリーム状のエンジンが複数のキューから読み取る場合、書き込み時に挿入するキューを1つ選択する必要があります。Redis StreamsおよびNATSによって使用されます。

## stream_poll_timeout_ms {#stream_poll_timeout_ms}

タイプ: ミリ秒

デフォルト値: 500

ストリーミングストレージからのデータのポーリングに関するタイムアウト。

## system_events_show_zero_values {#system_events_show_zero_values}

タイプ: Bool

デフォルト値: 0

[`system.events`](../../operations/system-tables/events.md) からゼロ値のイベントを選択できるようにします。

一部の監視システムでは、メトリック値がゼロであっても、各チェックポイントにすべてのメトリック値を渡す必要があります。

可能な値:

- 0 — 無効。
- 1 — 有効。

**例**

クエリ

```sql
SELECT * FROM system.events WHERE event='QueryMemoryLimitExceeded';
```

結果

```text
Ok.
```

クエリ
```sql
SET system_events_show_zero_values = 1;
SELECT * FROM system.events WHERE event='QueryMemoryLimitExceeded';
```

結果

```text
┌─event────────────────────┬─value─┬─description───────────────────────────────────────────┐
│ QueryMemoryLimitExceeded │     0 │ クエリのメモリ制限を超えた回数。                   │
└──────────────────────────┴───────┴───────────────────────────────────────────────────────┘
```

## table_function_remote_max_addresses {#table_function_remote_max_addresses}

タイプ: UInt64

デフォルト値: 1000

[remote](../../sql-reference/table-functions/remote.md) 関数用のパターンから生成されるアドレスの最大数を設定します。

可能な値:

- 正の整数。

## tcp_keep_alive_timeout {#tcp_keep_alive_timeout}

タイプ: 秒

デフォルト値: 290

TCPがキープアライブプローブを送信し始める前に接続がアイドルの状態である必要がある時間（秒）。

## temporary_data_in_cache_reserve_space_wait_lock_timeout_milliseconds {#temporary_data_in_cache_reserve_space_wait_lock_timeout_milliseconds}

タイプ: UInt64

デフォルト値: 600000

ファイルシステムキャッシュ内の一時データのためのスペース予約のためにキャッシュをロックする待機時間。

## temporary_files_codec {#temporary_files_codec}

タイプ: 文字列

デフォルト値: LZ4

ディスク上のソートおよび結合操作で使用される一時ファイルの圧縮コーデックを設定します。

可能な値:

- LZ4 — [LZ4](https://en.wikipedia.org/wiki/LZ4_(compression_algorithm))圧縮が適用されます。
- NONE — 圧縮は適用されません。

## throw_if_deduplication_in_dependent_materialized_views_enabled_with_async_insert {#throw_if_deduplication_in_dependent_materialized_views_enabled_with_async_insert}

タイプ: Bool

デフォルト値: 1

`async_insert` とともに `deduplicate_blocks_in_dependent_materialized_views` 設定が有効な場合、INSERTクエリで例外をスローします。これにより、これらの機能が一緒に動作できないことが保証されます。

## throw_if_no_data_to_insert {#throw_if_no_data_to_insert}

タイプ: Bool

デフォルト値: 1

空のINSERTを許可するかどうか、デフォルトで有効（空の挿入にエラーをスロー）。これは、[`clickhouse-client`](/interfaces/cli) または [gRPCインターフェース](/interfaces/grpc) を使用したINSERTにのみ適用されます。

## throw_on_error_from_cache_on_write_operations {#throw_on_error_from_cache_on_write_operations}

タイプ: Bool

デフォルト値: 0

書き込み操作（INSERT、マージ）のキャッシング時にキャッシュからのエラーを無視します。

## throw_on_max_partitions_per_insert_block {#throw_on_max_partitions_per_insert_block}

タイプ: Bool

デフォルト値: 1

max_partitions_per_insert_blockと併用します。真の場合（デフォルト）、max_partitions_per_insert_blockに達すると例外がスローされます。偽の場合、この制限に達したINSERTクエリの詳細とパーティション数がログに記録されます。これは、max_partitions_per_insert_blockの変更がユーザーに与える影響を理解しようとする場合に役立ちます。

## throw_on_unsupported_query_inside_transaction {#throw_on_unsupported_query_inside_transaction}
<ExperimentalBadge/>

タイプ: Bool

デフォルト値: 1

トランザクション内でサポートされていないクエリが使用された場合に例外をスローします。

## timeout_before_checking_execution_speed {#timeout_before_checking_execution_speed}

タイプ: 秒

デフォルト値: 10

指定された時間が経過した後に速度が過度に低くないことを確認します。

## timeout_overflow_mode {#timeout_overflow_mode}

タイプ: OverflowMode

デフォルト値: throw

制限を超えた場合の対処方法。

## timeout_overflow_mode_leaf {#timeout_overflow_mode_leaf}

タイプ: OverflowMode

デフォルト値: throw

リーフ制限を超えた場合の対処方法。

## totals_auto_threshold {#totals_auto_threshold}

タイプ: Float

デフォルト値: 0.5

`totals_mode = 'auto'` の閾値。
「WITH TOTALS修飾子」のセクションを参照してください。

## totals_mode {#totals_mode}

タイプ: TotalsMode

デフォルト値: after_having_exclusive

HAVINGが存在する場合や、max_rows_to_group_by および group_by_overflow_mode = ‘any’ が存在する場合のTOTALSの計算方法。
「WITH TOTALS修飾子」のセクションを参照してください。

## trace_profile_events {#trace_profile_events}

タイプ: Bool

デフォルト値: 0

プロファイルイベントの各更新時にスタックトレースの収集を有効または無効にし、プロファイルイベントの名前および増分の値と共に[trace_log](../../operations/system-tables/trace_log.md/#system_tables-trace_log)に送信します。

可能な値:

- 1 — プロファイルイベントのトレースが有効。
- 0 — プロファイルイベントのトレースが無効。

## transfer_overflow_mode {#transfer_overflow_mode}

タイプ: OverflowMode

デフォルト値: throw

制限を超えた場合の対処方法。

## transform_null_in {#transform_null_in}

タイプ: Bool

デフォルト値: 0

[IN](../../sql-reference/operators/in.md)演算子に対する[NULL](../../sql-reference/syntax.md/#null-literal)値の等価性を有効にします。

デフォルトでは、`NULL`値は比較できません。なぜなら、`NULL`は未定義の値を意味するからです。したがって、比較`expr = NULL`は常に`false`を返さなければなりません。この設定が有効な場合、`NULL = NULL`は`IN`演算子に対して`true`を返します。

可能な値:

- 0 — `IN`演算子での`NULL`値の比較は`false`を返します。
- 1 — `IN`演算子での`NULL`値の比較は`true`を返します。

**例**

`null_in`テーブルを考えてみましょう:

``` text
┌──idx─┬─────i─┐
│    1 │     1 │
│    2 │  NULL │
│    3 │     3 │
└──────┴───────┘
```

クエリ:

``` sql
SELECT idx, i FROM null_in WHERE i IN (1, NULL) SETTINGS transform_null_in = 0;
```

結果:

``` text
┌──idx─┬────i─┐
│    1 │    1 │
└──────┴──────┘
```

クエリ:

``` sql
SELECT idx, i FROM null_in WHERE i IN (1, NULL) SETTINGS transform_null_in = 1;
```

結果:

``` text
┌──idx─┬─────i─┐
│    1 │     1 │
│    2 │  NULL │
└──────┴───────┘
```

**参照**

- [IN演算子でのNULL処理](../../sql-reference/operators/in.md/#in-null-processing)

## traverse_shadow_remote_data_paths {#traverse_shadow_remote_data_paths}

タイプ: Bool

デフォルト値: 0

クエリ `system.remote_data_paths` の際に、実際のテーブルデータに加えて凍結データ（シャドウディレクトリ）を走査します。

## union_default_mode {#union_default_mode}

タイプ: SetOperationMode

デフォルト値: 

`SELECT`クエリ結果を組み合わせるモードを設定します。この設定は、`UNION ALL` または `UNION DISTINCT` を明示的に指定せずに [UNION](../../sql-reference/statements/select/union.md) で共有される際にのみ使用されます。

可能な値:

- `'DISTINCT'` — ClickHouseはクエリを組み合わせる結果として重複行を削除します。
- `'ALL'` — ClickHouseは重複行を含むすべての行を結果として出力します。
- `''` — ClickHouseは`UNION`で使用されるときに例外を生成します。

例は[UNION](../../sql-reference/statements/select/union.md)を参照してください。

## unknown_packet_in_send_data {#unknown_packet_in_send_data}

タイプ: UInt64

デフォルト値: 0

N番目のデータパケットではなく、未知のパケットを送信します。

## use_async_executor_for_materialized_views {#use_async_executor_for_materialized_views}

タイプ: Bool

デフォルト値: 0

マテリアライズドビュークエリの非同期および潜在的にマルチスレッド実行を使用し、INSERT中にビュー処理を加速することができますが、メモリを多く消費します。

## use_cache_for_count_from_files {#use_cache_for_count_from_files}

タイプ: Bool

デフォルト値: 1

テーブル関数 `file`/`s3`/`url`/`hdfs`/`azureBlobStorage` からのカウント時に行数のキャッシングを有効にします。

デフォルトで有効です。

## use_client_time_zone {#use_client_time_zone}

タイプ: Bool

デフォルト値: 0

DateTime文字列値を解釈するために、サーバーのタイムゾーンの代わりにクライアントのタイムゾーンを使用します。

## use_compact_format_in_distributed_parts_names {#use_compact_format_in_distributed_parts_names}

タイプ: Bool

デフォルト値: 1

`Distributed` エンジンを持つテーブルに対して背景 (`distributed_foreground_insert`) 挿入用のブロックを格納するためにコンパクトフォーマットを使用します。

可能な値:

- 0 — `user[:password]@host:port#default_database` ディレクトリ形式を使用します。
- 1 — `[shard{shard_index}[_replica{replica_index}]]` ディレクトリ形式を使用します。

:::note
- `use_compact_format_in_distributed_parts_names=0` の場合、クラスター定義の変更はバックグラウンド挿入に適用されません。
- `use_compact_format_in_distributed_parts_names=1` の場合、クラスター定義のノードの順序を変更すると、`shard_index`/`replica_index`が変更されるため、注意が必要です。
:::

## use_concurrency_control {#use_concurrency_control}

タイプ: Bool

デフォルト値: 1

サーバーの同時実行制御を尊重します（`concurrent_threads_soft_limit_num` および `concurrent_threads_soft_limit_ratio_to_cores` グローバルサーバー設定を参照）。無効にすると、サーバーが過負荷の場合でもスレッドの数が増加します（通常の使用には推奨されず、主にテストで必要です）。

## use_hedged_requests {#use_hedged_requests}

タイプ: Bool

デフォルト値: 1

リモートクエリに対するヘッジリクエストロジックを有効にします。これにより、クエリのために異なるレプリカとの多くの接続を確立することができます。
既存の接続が `hedged_connection_timeout` 内に確立されなかった場合、またはデータが `receive_data_timeout` 内に受信されなかった場合に新しい接続が有効化されます。クエリは最初に非空進捗パケット（またはデータパケット、`allow_changing_replica_until_first_data_packet`が有効な場合）を送信する接続を使用します。その他の接続はキャンセルされます。`max_parallel_replicas > 1` を持つクエリがサポートされています。

デフォルトで有効です。

Cloudではデフォルトで無効です。

## use_hive_partitioning {#use_hive_partitioning}

タイプ: Bool

デフォルト値: 1

これが有効な場合、ClickHouseはファイル状のテーブルエンジン [File](../../engines/table-engines/special/file.md/#hive-style-partitioning)/[S3](../../engines/table-engines/integrations/s3.md/#hive-style-partitioning)/[URL](../../engines/table-engines/special/url.md/#hive-style-partitioning)/[HDFS](../../engines/table-engines/integrations/hdfs.md/#hive-style-partitioning)/[AzureBlobStorage](../../engines/table-engines/integrations/azureBlobStorage.md/#hive-style-partitioning) 内のパス (`/name=value/`) におけるHiveスタイルのパーティショニングを検出し、クエリ内でパーティションカラムを仮想カラムとして使用できるようにします。これらの仮想カラムは、パーティションのパスと同じ名前を持ちますが、`_` で始まります。

## use_iceberg_partition_pruning {#use_iceberg_partition_pruning}

タイプ: Bool

デフォルト値: 0

Icebergテーブルに対してIcebergパーティションプルーニングを使用します。

## use_index_for_in_with_subqueries {#use_index_for_in_with_subqueries}

タイプ: Bool

デフォルト値: 1

IN演算子の右側にサブクエリまたはテーブル式がある場合、インデックスを使用しようとします。

## use_index_for_in_with_subqueries_max_values {#use_index_for_in_with_subqueries_max_values}

タイプ: UInt64

デフォルト値: 0

フィルタリングにテーブルインデックスを使用するためのIN演算子の右側のセットの最大サイズ。これにより、大きなクエリの準備に余分なデータ構造を使用することによるパフォーマンスの低下とメモリの使用を回避します。ゼロは制限がないことを意味します。

## use_json_alias_for_old_object_type {#use_json_alias_for_old_object_type}

タイプ: Bool

デフォルト値: 0

有効にすると、古い [Object('json')](../../sql-reference/data-types/json.md) タイプを作成するために `JSON` データ型エイリアスが使用されます。新しい [JSON](../../sql-reference/data-types/newjson.md) タイプの代わりです。

## use_local_cache_for_remote_storage {#use_local_cache_for_remote_storage}

タイプ: Bool

デフォルト値: 1

HDFSやS3などのリモートストレージ用のローカルキャッシュを使用します。これはリモートテーブルエンジン専用です。

## use_page_cache_for_disks_without_file_cache {#use_page_cache_for_disks_without_file_cache}

タイプ: Bool

デフォルト値: 0

ファイルシステムキャッシュが有効でないリモートディスクのためにユーザースペースページキャッシュを使用します。

## use_query_cache {#use_query_cache}

タイプ: Bool

デフォルト値: 0

有効にすると、`SELECT`クエリは[クエリキャッシュ](../query-cache.md)を利用できる場合があります。[enable_reads_from_query_cache](#enable_reads_from_query_cache) と [enable_writes_to_query_cache](#enable_writes_to_query_cache)パラメータは、キャッシュの使用方法をより詳細に制御します。

可能な値:

- 0 - 無効
- 1 - 有効

## use_skip_indexes {#use_skip_indexes}

タイプ: Bool

デフォルト値: 1

クエリ実行時にデータスキッピングインデックスを使用します。

可能な値:

- 0 — 無効。
- 1 — 有効。

## use_skip_indexes_if_final {#use_skip_indexes_if_final}

タイプ: Bool

デフォルト値: 0

FINAL修飾子を持つクエリを実行する際にスキップインデックスが使用されるかどうかを制御します。

デフォルトでは、この設定は無効になっています。なぜなら、スキップインデックスが最新のデータを含む行（グラニュール）を除外する可能性があるため、不正確な結果を引き起こす可能性があるからです。有効にすると、FINAL修飾子を使用した場合でもスキップインデックスが適用され、パフォーマンスが向上する可能性がありますが、最近の更新を見逃すリスクがあります。

可能な値:

- 0 — 無効。
- 1 — 有効。

## use_structure_from_insertion_table_in_table_functions {#use_structure_from_insertion_table_in_table_functions}

タイプ: UInt64

デフォルト値: 2

データからのスキーマ推論の代わりに挿入テーブルからの構造を使用します。可能な値: 0 - 無効、1 - 有効、2 - 自動

## use_uncompressed_cache {#use_uncompressed_cache}

タイプ: Bool

デフォルト値: 0

非圧縮ブロックのキャッシュを使用するかどうか。0または1を受け入れます。デフォルトでは、0（無効）です。
非圧縮キャッシュを使用すると（MergeTreeファミリのテーブルのみ）、大量の短いクエリで作業するときにレイテンシが大幅に低下し、スループットが向上することがあります。この設定は、頻繁に短いリクエストを送信するユーザーに対して有効にしてください。また、[uncompressed_cache_size](../../operations/server-configuration-parameters/settings.md/#server-settings-uncompressed_cache_size)設定パラメータ（設定ファイルでのみ設定）に注意を払います - 非圧縮キャッシュブロックのサイズ。デフォルトは8 GiBです。非圧縮キャッシュは必要に応じて充填され、最も使用されないデータが自動的に削除されます。

ある程度大きなデータ量（100万行以上）を読み取るクエリの場合、真に小さなクエリのためにスペースを保存するために非圧縮キャッシュが自動的に無効になります。このため、`use_uncompressed_cache`設定は常に1に設定しておくことができます。

## use_variant_as_common_type {#use_variant_as_common_type}

タイプ: Bool

デフォルト値: 0

引数タイプに共通のタイプがない場合、[if](../../sql-reference/functions/conditional-functions.md/#if)/[multiIf](../../sql-reference/functions/conditional-functions.md/#multiif)/[array](../../sql-reference/functions/array-functions.md)/[map](../../sql-reference/functions/tuple-map-functions.md)関数の結果タイプとして`Variant`タイプを使用することを許可します。

例:

```sql
SET use_variant_as_common_type = 1;
SELECT toTypeName(if(number % 2, number, range(number))) as variant_type FROM numbers(1);
SELECT if(number % 2, number, range(number)) as variant FROM numbers(5);
```

```text
┌─variant_type───────────────────┐
│ Variant(Array(UInt64), UInt64) │
└────────────────────────────────┘
┌─variant───┐
│ []        │
│ 1         │
│ [0,1]     │
│ 3         │
│ [0,1,2,3] │
└───────────┘
```

```sql
SET use_variant_as_common_type = 1;
SELECT toTypeName(multiIf((number % 4) = 0, 42, (number % 4) = 1, [1, 2, 3], (number % 4) = 2, 'Hello, World!', NULL)) AS variant_type FROM numbers(1);
SELECT multiIf((number % 4) = 0, 42, (number % 4) = 1, [1, 2, 3], (number % 4) = 2, 'Hello, World!', NULL) AS variant FROM numbers(4);
```

```text
─variant_type─────────────────────────┐
│ Variant(Array(UInt8), String, UInt8) │
└──────────────────────────────────────┘

┌─variant───────┐
│ 42            │
│ [1,2,3]       │
│ Hello, World! │
│ ᴺᵁᴸᴸ          │
└───────────────┘
```

```sql
SET use_variant_as_common_type = 1;
SELECT toTypeName(array(range(number), number, 'str_' || toString(number))) as array_of_variants_type from numbers(1);
SELECT array(range(number), number, 'str_' || toString(number)) as array_of_variants FROM numbers(3);
```

```text
┌─array_of_variants_type────────────────────────┐
│ Array(Variant(Array(UInt64), String, UInt64)) │
└───────────────────────────────────────────────┘

┌─array_of_variants─┐
│ [[],0,'str_0']    │
│ [[0],1,'str_1']   │
│ [[0,1],2,'str_2'] │
└───────────────────┘
```

```sql
SET use_variant_as_common_type = 1;
SELECT toTypeName(map('a', range(number), 'b', number, 'c', 'str_' || toString(number))) as map_of_variants_type from numbers(1);
SELECT map('a', range(number), 'b', number, 'c', 'str_' || toString(number)) as map_of_variants FROM numbers(3);
```

```text
┌─map_of_variants_type────────────────────────────────┐
│ Map(String, Variant(Array(UInt64), String, UInt64)) │
└─────────────────────────────────────────────────────┘

┌─map_of_variants───────────────┐
│ {'a':[],'b':0,'c':'str_0'}    │
│ {'a':[0],'b':1,'c':'str_1'}   │
│ {'a':[0,1],'b':2,'c':'str_2'} │
└───────────────────────────────┘
```

## use_with_fill_by_sorting_prefix {#use_with_fill_by_sorting_prefix}

タイプ: Bool

デフォルト値: 1

ORDER BY句におけるWITH FILL列の前のカラムがソートプレフィックスを形成します。ソートプレフィックス内の異なる値を持つ行は独立して埋められます。

## validate_enum_literals_in_operators {#validate_enum_literals_in_operators}

タイプ: Bool

デフォルト値: 0

有効にすると、`IN`、`NOT IN`、`==`、`!=`のような演算子での列挙型リテラルを列挙型タイプに対して検証し、リテラルが有効な列挙型値でない場合に例外をスローします。

## validate_mutation_query {#validate_mutation_query}

タイプ: Bool

デフォルト値: 1

ミューテーションクエリを受け付ける前に検証します。ミューテーションはバックグラウンドで実行され、無効なクエリを実行するとミューテーションがスタックし、手動での介入が必要になります。

互換性のないバグに遭遇した場合のみ、この設定を変更してください。

## validate_polygons {#validate_polygons}

タイプ: Bool

デフォルト値: 1

ポリゴンが自己交差または自己接触している場合に、[pointInPolygon](../../sql-reference/functions/geo/index.md/#pointinpolygon)関数で例外をスローすることを有効または無効にします。

可能な値:

- 0 — 例外のスローが無効。 `pointInPolygon` は無効なポリゴンを受け入れ、それに対して不正確な結果を返す可能性があります。
- 1 — 例外のスローが有効。

## wait_changes_become_visible_after_commit_mode {#wait_changes_become_visible_after_commit_mode}
<ExperimentalBadge/>

タイプ: TransactionsWaitCSNMode

デフォルト値: wait_unknown

コミットされた変更が最新のスナップショットで実際に可視化されるのを待ちます。

## wait_for_async_insert {#wait_for_async_insert}

タイプ: Bool

デフォルト値: 1

真の場合、非同期挿入の処理を待機します。

## wait_for_async_insert_timeout {#wait_for_async_insert_timeout}

タイプ: 秒

デフォルト値: 120

非同期挿入の処理を待つためのタイムアウト。

## wait_for_window_view_fire_signal_timeout {#wait_for_window_view_fire_signal_timeout}
<ExperimentalBadge/>

タイプ: 秒

デフォルト値: 10

イベント時間処理におけるウィンドウビューの発火信号を待つためのタイムアウト。

## window_view_clean_interval {#window_view_clean_interval}
<ExperimentalBadge/>

タイプ: 秒

デフォルト値: 60

古いデータを解放するためのウィンドウビューのクリーン間隔（秒）。

## window_view_heartbeat_interval {#window_view_heartbeat_interval}
<ExperimentalBadge/>

タイプ: 秒

デフォルト値: 15

ウォッチクエリが生きていることを示すためのハートビート間隔（秒）。

## workload {#workload}

タイプ: 文字列

デフォルト値: default

リソースにアクセスするために使用されるワークロードの名前。

## write_through_distributed_cache {#write_through_distributed_cache}

タイプ: Bool

デフォルト値: 0

ClickHouse Cloudのみ。分散キャッシュへの書き込みを許可します（S3への書き込みも分散キャッシュによって行われます）。

## zstd_window_log_max {#zstd_window_log_max}

タイプ: Int64

デフォルト値: 0

ZSTDの最大ウィンドウログを選択することを許可します（これはMergeTreeファミリには使用されません）。
