---
slug: /engines/table-engines/integrations/s3
sidebar_position: 180
sidebar_label: S3
title: "S3 テーブルエンジン"
description: "このエンジンは Amazon S3 エコシステムとの統合を提供します。HDFS エンジンに似ていますが、S3 特有の機能を提供します。"
---

# S3 テーブルエンジン

このエンジンは [Amazon S3](https://aws.amazon.com/s3/) エコシステムとの統合を提供します。このエンジンは [HDFS](../../../engines/table-engines/special/file.md#table_engines-hdfs) エンジンに似ていますが、S3 特有の機能を提供します。

## 例 {#example}

``` sql
CREATE TABLE s3_engine_table (name String, value UInt32)
    ENGINE=S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/test-data.csv.gz', 'CSV', 'gzip')
    SETTINGS input_format_with_names_use_header = 0;

INSERT INTO s3_engine_table VALUES ('one', 1), ('two', 2), ('three', 3);

SELECT * FROM s3_engine_table LIMIT 2;
```

```text
┌─name─┬─value─┐
│ one  │     1 │
│ two  │     2 │
└──────┴───────┘
```

## テーブルの作成 {#creating-a-table}

``` sql
CREATE TABLE s3_engine_table (name String, value UInt32)
    ENGINE = S3(path [, NOSIGN | aws_access_key_id, aws_secret_access_key,] format, [compression])
    [PARTITION BY expr]
    [SETTINGS ...]
```

### エンジンパラメータ {#parameters}

- `path` — ファイルへのパスを持つバケットの URL。読み取り専用モードでは、以下のワイルドカードに対応しています: `*`, `**`, `?`, `{abc,def}` および `{N..M}` ここで `N`, `M` は数値、`'abc'`, `'def'` は文字列です。詳細については [以下](#wildcards-in-path) を参照してください。
- `NOSIGN` - このキーワードが資格情報の代わりに提供されると、すべてのリクエストは署名されません。
- `format` — ファイルの [フォーマット](../../../interfaces/formats.md#formats)。
- `aws_access_key_id`, `aws_secret_access_key` - [AWS](https://aws.amazon.com/) アカウントユーザーの長期資格情報。リクエストの認証に使用できます。このパラメータはオプションです。資格情報が指定されていない場合、構成ファイルから使用されます。詳細については [S3 を使用したデータストレージ](../mergetree-family/mergetree.md#table_engine-mergetree-s3) を参照してください。
- `compression` — 圧縮タイプ。サポートされる値: `none`, `gzip/gz`, `brotli/br`, `xz/LZMA`, `zstd/zst`。このパラメータはオプションです。デフォルトでは、ファイル拡張子によって圧縮が自動的に検出されます。

### データキャッシュ {#data-cache}

`S3` テーブルエンジンは、ローカルディスク上でのデータキャッシングをサポートしています。
[このセクション](/operations/storing-data.md/#using-local-cache) でファイルシステムキャッシュの構成オプションと使用法を確認してください。
キャッシングは、パスとストレージオブジェクトの ETag に基づいて行われるため、ClickHouse は古いキャッシュバージョンを読みません。

キャッシングを有効にするには、設定 `filesystem_cache_name = '<name>'` および `enable_filesystem_cache = 1` を使用します。

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_3.csv', 'minioadmin', 'minioadminpassword', 'CSV')
SETTINGS filesystem_cache_name = 'cache_for_s3', enable_filesystem_cache = 1;
```

構成ファイルでキャッシュを定義する方法は二つあります。

1. ClickHouse 構成ファイルに以下のセクションを追加します:

``` xml
<clickhouse>
    <filesystem_caches>
        <cache_for_s3>
            <path>キャッシュディレクトリへのパス</path>
            <max_size>10Gi</max_size>
        </cache_for_s3>
    </filesystem_caches>
</clickhouse>
```

2. ClickHouse の `storage_configuration` セクションからキャッシュ設定（したがってキャッシュストレージ）を再利用します。[ここで説明されています](/operations/storing-data.md/#using-local-cache)。

### PARTITION BY {#partition-by}

`PARTITION BY` — オプションです。ほとんどの場合、パーティションキーは不要ですが、必要な場合は大抵、月ごとのパーティションキー以上の細かさは一般的に不要です。パーティショニングはクエリの速度を向上させません（ORDER BY 式とは対照的に）。あまりにも細かいパーティショニングを使用してはいけません。顧客識別子や名前でデータをパーティショニングしないでください（代わりに、ORDER BY 式の最初のカラムに顧客識別子や名前を配置します）。

月ごとのパーティショニングには、`toYYYYMM(date_column)` 式を使用します。ここで `date_column` は [Date](/sql-reference/data-types/date.md) 型のカラムです。パーティション名は `"YYYYMM"` フォーマットになります。

### パーティションデータのクエリ {#querying-partitioned-data}

この例では [docker compose レシピ](https://github.com/ClickHouse/examples/tree/5fdc6ff72f4e5137e23ea075c88d3f44b0202490/docker-compose-recipes/recipes/ch-and-minio-S3) を使用します。このレシピは ClickHouse と MinIO を統合します。エンドポイントと認証値を置き換えることで、同じクエリを S3 を使用して再現できるはずです。

`ENGINE` 構成の S3 エンドポイントで、パラメータトークン `{_partition_id}` が S3 オブジェクト（ファイル名）の一部として使用されており、SELECT クエリはそれらの結果オブジェクト名（例: `test_3.csv`）に対して選択されていることに注意してください。

:::note
例に示すように、パーティショニングされた S3 テーブルからのクエリは
現時点では直接サポートされていませんが、S3 テーブル関数を使用することで個別のパーティションをクエリすることで実現できます。

S3 にパーティショニングされたデータを書き込む主なユースケースは、そのデータを別の
ClickHouse システムに転送することを可能にすることです（たとえば、オンプレミスシステムから ClickHouse Cloud への移行）。ClickHouse データセットはしばしば非常に大きいため、ネットワークの信頼性が時々不完全であることからデータセットをサブセットに分割して転送することが理にかなっているため、パーティショニングされた書き込みを行います。
:::

#### テーブルの作成 {#create-the-table}
```sql
CREATE TABLE p
(
    `column1` UInt32,
    `column2` UInt32,
    `column3` UInt32
)
ENGINE = S3(
# highlight-next-line
           'http://minio:10000/clickhouse//test_{_partition_id}.csv',
           'minioadmin',
           'minioadminpassword',
           'CSV')
PARTITION BY column3
```

#### データの挿入 {#insert-data}
```sql
INSERT INTO p VALUES (1, 2, 3), (3, 2, 1), (78, 43, 45)
```

#### パーティション 3 からの選択 {#select-from-partition-3}

:::tip
このクエリは S3 テーブル関数を使用します
:::

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_3.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```
```response
┌─c1─┬─c2─┬─c3─┐
│  1 │  2 │  3 │
└────┴────┴────┘
```

#### パーティション 1 からの選択 {#select-from-partition-1}
```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_1.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```
```response
┌─c1─┬─c2─┬─c3─┐
│  3 │  2 │  1 │
└────┴────┴────┘
```

#### パーティション 45 からの選択 {#select-from-partition-45}
```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_45.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```
```response
┌─c1─┬─c2─┬─c3─┐
│ 78 │ 43 │ 45 │
└────┴────┴────┘
```

#### 制限 {#limitation}

`Select * from p` を実行しようとするかもしれませんが、上記の通り、このクエリは失敗します。前述のクエリを使用してください。

```sql
SELECT * FROM p
```
```response
サーバーからの例外を受信しました (バージョン 23.4.1):
コード: 48. DB::Exception: localhost:9000 から受信。DB::Exception: パーティショニングされた S3 ストレージからの読み取りはまだ実装されていません。 (NOT_IMPLEMENTED)
```

## 仮想カラム {#virtual-columns}

- `_path` — ファイルへのパス。タイプ: `LowCardinality(String)`。
- `_file` — ファイル名。タイプ: `LowCardinality(String)`。
- `_size` — ファイルのサイズ（バイト単位）。タイプ: `Nullable(UInt64)`。サイズが不明な場合、値は `NULL`。
- `_time` — ファイルの最終更新時刻。タイプ: `Nullable(DateTime)`。時刻が不明な場合、値は `NULL`。
- `_etag` — ファイルの ETag。タイプ: `LowCardinality(String)`。etag が不明な場合、値は `NULL`。

仮想カラムに関する詳細は [こちら](../../../engines/table-engines/index.md#table_engines-virtual_columns) を参照してください。

## 実装の詳細 {#implementation-details}

- 読み取りと書き込みは並列で可能です。
- サポートされていないもの:
    - `ALTER` および `SELECT...SAMPLE` 操作。
    - インデックス。
    - [ゼロコピー](../../../operations/storing-data.md#zero-copy) レプリケーションは可能ですが、サポートされていません。

  :::note ゼロコピー レプリケーションは生産準備が整っていません。
  ゼロコピー レプリケーションは ClickHouse バージョン 22.8 以降ではデフォルトで無効になっています。この機能は生産利用には推奨されません。
  :::

## パス内のワイルドカード {#wildcards-in-path}

`path` 引数は、bash のようなワイルドカードを使用して複数のファイルを指定できます。処理されるファイルは存在し、全体のパスパターンに一致している必要があります。ファイルのリストは `SELECT` の際に決定されます（`CREATE` の瞬間ではありません）。

- `*` — `/` を除く任意の文字の任意の数（空文字を含む）に置き換えます。
- `**` — `/` を含む任意の文字の任意の数（空文字を含む）に置き換えます。
- `?` — 任意の単一の文字に置き換えます。
- `{some_string,another_string,yet_another_one}` — 文字列 `'some_string', 'another_string', 'yet_another_one'` のいずれかに置き換えます。
- `{N..M}` — N から M までの範囲の任意の数（両端の境界を含む）に置き換えます。N および M は先頭にゼロを付けることができます（例: `000..078`）。

`{}` を使用した構文は、[remote](../../../sql-reference/table-functions/remote.md) テーブル関数に類似しています。

:::note
ファイルのリストに先頭ゼロ付きの数値範囲が含まれている場合、各桁に対してブレースを使用するか、`?` を使用してください。
:::

**ワイルドカードを使用した例 1**

`file-000.csv`, `file-001.csv`, ... , `file-999.csv` という名前のファイルを持つテーブルを作成します:

``` sql
CREATE TABLE big_table (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/my_folder/file-{000..999}.csv', 'CSV');
```

**ワイルドカードを使用した例 2**

以下の URI で CSV フォーマットのいくつかのファイルがあると仮定します:

- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_1.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_2.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_3.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_1.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_2.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_3.csv'

すべての6つのファイルで構成されるテーブルを作成するには、いくつかの方法があります。

1. ファイルの後ろの数字の範囲を指定します:

``` sql
CREATE TABLE table_with_range (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/some_file_{1..3}', 'CSV');
```

2. `some_file_` プレフィックスのすべてのファイルを取得します（両方のフォルダーにそのようなプレフィックスの余分なファイルが存在してはいけません）:

``` sql
CREATE TABLE table_with_question_mark (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/some_file_?', 'CSV');
```

3. 両方のフォルダー内のすべてのファイルを取得します（すべてのファイルがクエリで説明されたフォーマットとスキーマを満たす必要があります）:

``` sql
CREATE TABLE table_with_asterisk (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/*', 'CSV');
```

## ストレージ設定 {#storage-settings}

- [s3_truncate_on_insert](/operations/settings/settings.md#s3_truncate_on_insert) - 挿入する前にファイルを切り捨てることを許可します。デフォルトでは無効です。
- [s3_create_new_file_on_insert](/operations/settings/settings.md#s3_create_new_file_on_insert) - フォーマットにサフィックスがある場合、各挿入時に新しいファイルを作成することを許可します。デフォルトでは無効です。
- [s3_skip_empty_files](/operations/settings/settings.md#s3_skip_empty_files) - 読み取り時に空のファイルをスキップします。デフォルトでは有効です。

## S3関連設定 {#settings}

以下の設定は、クエリ実行前に設定できるか、構成ファイルに配置できます。

- `s3_max_single_part_upload_size` — S3 への単一パートアップロードでの最大オブジェクトサイズ。デフォルト値は `32Mb`。
- `s3_min_upload_part_size` — [S3 マルチパートアップロード](https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html) 中のパートの最小サイズ。デフォルト値は `16Mb`。
- `s3_max_redirects` — 許可される最大 S3 リダイレクトホップ数。デフォルト値は `10`。
- `s3_single_read_retries` — 単一読み取り中の最大試行回数。デフォルト値は `4`。
- `s3_max_put_rps` — スロットリング前の秒間最大 PUT リクエスト数。デフォルト値は `0`（無制限）。
- `s3_max_put_burst` — リクエストごとの制限に達する前に同時に発行できる最大リクエスト数。デフォルトでは (`0` 値) `s3_max_put_rps` に等しい。
- `s3_max_get_rps` — スロットリング前の秒間最大 GET リクエスト数。デフォルト値は `0`（無制限）。
- `s3_max_get_burst` — リクエストごとの制限に達する前に同時に発行できる最大リクエスト数。デフォルトでは (`0` 値) `s3_max_get_rps` に等しい。
- `s3_upload_part_size_multiply_factor` - この因子で `s3_min_upload_part_size` を乗算します。デフォルト値は `2`。
- `s3_upload_part_size_multiply_parts_count_threshold` - S3 にこの数のパーツをアップロードするたびに、`s3_min_upload_part_size` が `s3_upload_part_size_multiply_factor` で乗算されます。デフォルト値は `500`。
- `s3_max_inflight_parts_for_one_file` - 一つのオブジェクトに対して同時に実行できる PUT リクエストの数を制限します。この数は制限されるべきです。値 `0` は無制限を意味します。デフォルト値は `20`。各インフライトパートは、最初の `s3_upload_part_size_multiply_factor` パーツに対して `s3_min_upload_part_size` サイズのバッファを持ち、ファイルが大きくなると、`upload_part_size_multiply_factor` に従い追加のサイズが増加します。デフォルト設定での1つのアップロードファイルは、8G 未満のファイルで最大 `320Mb` を消費します。より大きなファイルでは消費量が大きくなります。

セキュリティの考慮: 悪意のあるユーザーが任意の S3 URL を指定できる場合、`s3_max_redirects` はゼロに設定して [SSRF](https://en.wikipedia.org/wiki/Server-side_request_forgery) 攻撃を回避する必要があります。または、代わりに `remote_host_filter` をサーバー構成に指定する必要があります。

## エンドポイントベースの設定 {#endpoint-settings}

以下の設定は、指定されたエンドポイント（URL の正確なプレフィックスで一致）に対して構成ファイルに指定できます。

- `endpoint` — エンドポイントのプレフィックスを指定します。必須です。
- `access_key_id` および `secret_access_key` — 指定されたエンドポイントで使用する資格情報を指定します。オプションです。
- `use_environment_credentials` — `true` に設定されている場合、S3 クライアントは環境変数および [Amazon EC2](https://en.wikipedia.org/wiki/Amazon_Elastic_Compute_Cloud) メタデータから資格情報を取得しようとします。オプションで、デフォルト値は `false` です。
- `region` — S3 リージョン名を指定します。オプションです。
- `use_insecure_imds_request` — `true` に設定されている場合、S3 クライアントは Amazon EC2 メタデータから資格情報を取得する際に不安定な IMDS リクエストを使用します。オプションで、デフォルト値は `false` です。
- `expiration_window_seconds` — 有効期限ベースの資格情報が失効したかどうかを確認するための猶予期間。オプションで、デフォルト値は `120` です。
- `no_sign_request` - すべての資格情報を無視して、リクエストに署名しないようにします。公開バケットにアクセスするのに便利です。
- `header` — 指定された HTTP ヘッダーを指定されたエンドポイントへのリクエストに追加します。オプションで、複数回指定できます。
- `access_header` - 他の出所からの資格情報がない場合に、指定された HTTP ヘッダーを指定されたエンドポイントへのリクエストに追加します。
- `server_side_encryption_customer_key_base64` — 指定されている場合、S3 オブジェクトへの SSE-C 暗号化のために必要なヘッダーが設定されます。オプションです。
- `server_side_encryption_kms_key_id` - 指定されている場合、[SSE-KMS 暗号化](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html) のために必要なヘッダーが設定されます。空文字列が指定されている場合、AWS 管理の S3 キーが使用されます。オプションです。
- `server_side_encryption_kms_encryption_context` - `server_side_encryption_kms_key_id` とともに指定されている場合、SSE-KMS に対して指定された暗号化コンテキストヘッダーが設定されます。オプションです。
- `server_side_encryption_kms_bucket_key_enabled` - `server_side_encryption_kms_key_id` とともに指定されている場合、S3 バケットキーを SSE-KMS 用に有効にするためのヘッダーが設定されます。オプションで、`true` または `false` として指定できます（バケットレベルの設定に一致します）。
- `max_single_read_retries` — 単一読み取り中の最大試行回数。デフォルト値は `4` です。オプションです。
- `max_put_rps`, `max_put_burst`, `max_get_rps` および `max_get_burst` - 特定のエンドポイントで使用するためのスロットリング設定（上記の説明を参照）。オプションです。

**例:**

``` xml
<s3>
    <endpoint-name>
        <endpoint>https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/</endpoint>
        <!-- <access_key_id>ACCESS_KEY_ID</access_key_id> -->
        <!-- <secret_access_key>SECRET_ACCESS_KEY</secret_access_key> -->
        <!-- <region>us-west-1</region> -->
        <!-- <use_environment_credentials>false</use_environment_credentials> -->
        <!-- <use_insecure_imds_request>false</use_insecure_imds_request> -->
        <!-- <expiration_window_seconds>120</expiration_window_seconds> -->
        <!-- <no_sign_request>false</no_sign_request> -->
        <!-- <header>Authorization: Bearer SOME-TOKEN</header> -->
        <!-- <server_side_encryption_customer_key_base64>BASE64-ENCODED-KEY</server_side_encryption_customer_key_base64> -->
        <!-- <server_side_encryption_kms_key_id>KMS_KEY_ID</server_side_encryption_kms_key_id> -->
        <!-- <server_side_encryption_kms_encryption_context>KMS_ENCRYPTION_CONTEXT</server_side_encryption_kms_encryption_context> -->
        <!-- <server_side_encryption_kms_bucket_key_enabled>true</server_side_encryption_kms_bucket_key_enabled> -->
        <!-- <max_single_read_retries>4</max_single_read_retries> -->
    </endpoint-name>
</s3>
```

## アーカイブの取り扱い {#working-with-archives}

以下の URI でいくつかのアーカイブファイルが S3 にあると仮定します。

- 'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-10.csv.zip'
- 'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-11.csv.zip'
- 'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-12.csv.zip'

これらのアーカイブからデータを抽出するには、次のように使用できます。:: の後にワイルドカードも使用できます。

``` sql
SELECT *
FROM s3(
   'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-1{0..2}.csv.zip :: *.csv'
);
```

:::note 
ClickHouse は次の三つのアーカイブフォーマットをサポートします:
ZIP
TAR
7Z
ZIP および TAR アーカイブは、任意のサポートされたストレージ場所からアクセスできますが、7Z アーカイブは ClickHouse がインストールされているローカルファイルシステムからのみ読み取ることができます。  
:::


## 公開バケットへのアクセス {#accessing-public-buckets}

ClickHouse はさまざまなタイプのソースから資格情報を取得しようとします。
時には、一部の公開バケットへのアクセス時に、クライアントが `403` エラーコードを返す問題が発生することがあります。
この問題は `NOSIGN` キーワードを使用することで回避でき、クライアントはすべての資格情報を無視してリクエストに署名しません。

``` sql
CREATE TABLE big_table (name String, value UInt32)
    ENGINE = S3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/aapl_stock.csv', NOSIGN, 'CSVWithNames');
```

## パフォーマンスの最適化 {#optimizing-performance}

`s3` 関数のパフォーマンスを最適化するための詳細については、[私たちの詳細ガイド](/integrations/s3/performance) を参照してください。

## 関連情報 {#see-also}

- [s3 テーブル関数](../../../sql-reference/table-functions/s3.md)
