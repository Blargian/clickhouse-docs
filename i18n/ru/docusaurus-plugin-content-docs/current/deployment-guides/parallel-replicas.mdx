---
slug: /deployment-guides/parallel-replicas
title: 'Параллельные реплики'
keywords: ['параллельная реплика']
description: 'В этом руководстве мы сначала обсудим, как ClickHouse распределяет запрос по нескольким шардам через распределенные таблицы, а затем как запрос может использовать несколько реплик для своего выполнения.'
---

import Image from '@theme/IdealImage';
import BetaBadge from '@theme/badges/BetaBadge';
import image_1 from '@site/static/images/deployment-guides/parallel-replicas-1.png'
import image_2 from '@site/static/images/deployment-guides/parallel-replicas-2.png'
import image_3 from '@site/static/images/deployment-guides/parallel-replicas-3.png'
import image_4 from '@site/static/images/deployment-guides/parallel-replicas-4.png'
import image_5 from '@site/static/images/deployment-guides/parallel-replicas-5.png'
import image_6 from '@site/static/images/deployment-guides/parallel-replicas-6.png'
import image_7 from '@site/static/images/deployment-guides/parallel-replicas-7.png'
import image_8 from '@site/static/images/deployment-guides/parallel-replicas-8.png'
import image_9 from '@site/static/images/deployment-guides/parallel-replicas-9.png'

<BetaBadge/>
## Введение {#introduction}

ClickHouse обрабатывает запросы чрезвычайно быстро, но как эти запросы распределяются и параллелизуются между несколькими серверами? 

> В этом руководстве мы сначала обсудим, как ClickHouse распределяет запрос по нескольким шардам через распределенные таблицы, а затем как запрос может использовать несколько реплик для своего выполнения.
## Шардированная архитектура {#sharded-architecture}

В архитектуре с отсутствием общей памяти кластеры обычно делятся на несколько шардов, каждый из которых содержит подмножество общих данных. Распределенная таблица располагается над этими шардами, предоставляя единый взгляд на полные данные.

Чтения могут быть направлены в локальную таблицу. Выполнение запроса будет происходить только на указанном шарде, или он может быть отправлен в распределенную таблицу, и в этом случае каждый шард выполнит заданные запросы. Сервер, на котором был запрошен доступ к распределенной таблице, агрегирует данные и возвращает ответ клиенту:

<Image img={image_1} size="md" alt="шардированная архитектура" />

На рисунке выше визуализируется, что происходит, когда клиент запрашивает распределенную таблицу:

<ol className="docs-ordered-list">
    <li>
        Запрос SELECT отправляется в распределенную таблицу на узел произвольно (через стратегию round-robin или после маршрутизации на определенный сервер балансировщиком нагрузки). Этот узел теперь будет действовать как координатор.
    </li>
    <li>
        Узел находит каждый шард, который должен выполнить запрос, с помощью информации, указанной в распределенной таблице, и запрос отправляется каждому шард.
    </li>
    <li>
        Каждый шард считывает, фильтрует и агрегирует данные локально, а затем отправляет обратно объединяемое состояние координатору.
    </li>
    <li>
        Координирующий узел объединяет данные и затем отправляет ответ клиенту.
    </li>
</ol>

Когда мы добавляем реплики в mix, процесс довольно похож, единственное различие состоит в том, что только одна реплика из каждого шарда выполнит запрос. Это означает, что больше запросов может быть обработано параллельно.
## Не-шардированная архитектура {#non-sharded-architecture}

ClickHouse Cloud имеет совершенно другую архитектуру, чем представленная выше. (См. ["Архитектура ClickHouse Cloud"](https://clickhouse.com/docs/cloud/reference/architecture) для получения дополнительных сведений). С разделением вычислений и хранения, и с практически неограниченным объемом хранения необходимость в шардировании становится менее важной.

На рисунке ниже показана архитектура ClickHouse Cloud:

<Image img={image_2} size="md" alt="не-шардированная архитектура" />

Эта архитектура позволяет нам добавлять и удалять реплики почти мгновенно, обеспечивая очень высокую масштабируемость кластера. Кластер ClickHouse Keeper (показан справа) обеспечивает наличие единого источника правды для метаданных. Реплики могут извлекать метаданные из кластера ClickHouse Keeper и все поддерживать одинаковые данные. Сами данные хранятся в объектном хранилище, а кеш SSD позволяет ускорить запросы.

Но как мы теперь можем распределить выполнение запросов между несколькими серверами? В шардированной архитектуре это было довольно очевидно, поскольку каждый шард мог фактически выполнить запрос на подмножестве данных. Как это работает при отсутствии шардирования?
## Введение в параллельные реплики {#introducing-parallel-replicas}

Чтобы параллелизовать выполнение запросов через несколько серверов, нам сначала нужно назначить один из наших серверов координатором. Координатор — это тот, кто создает список задач, которые необходимо выполнить, обеспечивает их выполнение, агрегирует и возвращает результат клиенту. Как и в большинстве распределенных систем, эту роль будет исполнять узел, который получает начальный запрос. Мы также должны определить единицу работы. В шардированной архитектуре единицей работы является шард, подмножество данных. В параллельных репликах мы будем использовать небольшую часть таблицы, которая называется [гранулы](/docs/guides/best-practices/sparse-primary-indexes#data-is-organized-into-granules-for-parallel-data-processing), в качестве единицы работы.

Теперь давайте посмотрим, как это работает на практике с помощью рисунка ниже:

<Image img={image_3} size="md" alt="Параллельные реплики" />

С параллельными репликами:

<ol className="docs-ordered-list">
    <li>
        Запрос от клиента отправляется на один узел после прохождения через балансировщик нагрузки. Этот узел становится координатором для этого запроса.
    </li>
    <li>
        Узел анализирует индекс каждой части и выбирает правильные части и гранулы для обработки.
    </li>
    <li>
        Координатор разбивает рабочую нагрузку на набор гранул, которые могут быть назначены различным репликам.
    </li>
    <li>
        Каждый набор гранул обрабатывается соответствующими репликами, и объединяемое состояние отправляется координатору, когда они заканчивают.
    </li>
    <li>
        Наконец, координатор объединяет все результаты от реплик и затем возвращает ответ клиенту.
    </li>
</ol>

Шаги выше описывают, как работают параллельные реплики теоретически. Однако на практике существует множество факторов, которые могут помешать такой логике работать идеально:

<ol className="docs-ordered-list">
    <li>
        Некоторые реплики могут быть недоступны.
    </li>
    <li>
        Репликация в ClickHouse асинхронная, некоторые реплики могут не иметь одинаковых частей в какой-то момент времени.
    </li>
    <li>
        Необходимо как-то обрабатывать задержку между репликами.
    </li>
    <li>
        Кеш файловой системы варьируется от реплики к реплике в зависимости от активности каждой реплики, что означает, что случайное назначение задач может привести к менее оптимальной производительности из-за локальности кеша.
    </li>
</ol>

Мы исследуем, как эти факторы преодолеваются в следующих разделах.
### Объявления {#announcements}

Чтобы решить (1) и (2) из списка выше, мы ввели концепцию объявления. Давайте визуализируем, как это работает, с помощью рисунка ниже:

<Image img={image_4} size="md" alt="Объявления" />

<ol className="docs-ordered-list">
    <li>
        Запрос от клиента отправляется на один узел после прохождения через балансировщик нагрузки. Узел становится координатором для этого запроса.
    </li>
    <li>
        Координирующий узел отправляет запрос на получение объявлений от всех реплик в кластере. Реплики могут иметь несколько разные представления текущего набора частей для таблицы. В результате нам нужно собрать эту информацию, чтобы избежать неправильных решений по расписанию.
    </li>
    <li>
        Координирующий узел затем использует объявления, чтобы определить набор гранул, которые могут быть назначены различным репликам. Здесь, например, мы видим, что никакие гранулы из части 3 не были назначены реплике 2, потому что эта реплика не предоставила эту часть в своем объявлении. Также отмечаем, что никакие задачи не были назначены реплике 3, потому что реплика не предоставила объявление.
    </li>
    <li>
        После того как каждая реплика обработала запрос на своем подмножестве гранул и объединяемое состояние было отправлено обратно координатору, координатор объединяет результаты и ответ отправляется клиенту.
    </li>
</ol>
### Динамическое координирование {#dynamic-coordination}

Чтобы решить проблему задержки, мы добавили динамическое координирование. Это означает, что все гранулы не отправляются реплике в одном запросе, но каждая реплика сможет запрашивать новую задачу (набор гранул для обработки) у координатора. Координатор даст реплике набор гранул на основе полученного объявления.

Предположим, что мы находимся на этапе процесса, когда все реплики отправили объявление со всеми частями.

Рисунок ниже визуализирует, как работает динамическое координирование:

<Image img={image_5} size="md" alt="Динамическое координирование - часть 1" />

<ol className="docs-ordered-list">
    <li>
        Реплики сообщают координатору, что они могут обрабатывать задачи, они также могут указать, сколько работы они могут выполнить.
    </li>
    <li>
        Координатор назначает задачи репликам.
    </li>
</ol>

<Image img={image_6} size="md" alt="Динамическое координирование - часть 2" />

<ol className="docs-ordered-list">
    <li>
        Реплики 1 и 2 могут очень быстро закончить свою задачу. Они запросят еще одну задачу у координатора.
    </li>
    <li>
        Координатор назначает новые задачи реплике 1 и 2.
    </li>
</ol>

<Image img={image_7} size="md" alt="Динамическое координирование - часть 3" />

<ol className="docs-ordered-list">
    <li>
        Все реплики теперь закончили обработку своей задачи. Они запрашивают больше задач.
    </li>
    <li>
        Координатор, используя объявления, проверяет, какие задачи остаются для обработки, но не осталось оставшихся задач.
    </li>
    <li>
        Координатор сообщает репликам, что все было обработано. Он теперь объединит все объединяемые состояния и ответит на запрос.
    </li>
</ol>
### Управление локальностью кеша {#managing-cache-locality}

Последним оставшимся потенциальным вопросом является то, как мы обрабатываем локальность кеша. Если запрос выполняется несколько раз, как мы можем гарантировать, что одна и та же задача будет направлена к одной и той же реплике? В предыдущем примере у нас были назначены следующие задачи:

<table>
    <thead>
        <tr>
            <th></th>
            <th>Реплика 1</th>
            <th>Реплика 2</th>
            <th>Реплика 3</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Часть 1</td>
            <td>g1, g6, g7</td>
            <td>g2, g4, g5</td>
            <td>g3</td>
        </tr>
        <tr>
            <td>Часть 2</td>
            <td>g1</td>
            <td>g2, g4, g5</td>
            <td>g3</td>
        </tr>
        <tr>
            <td>Часть 3</td>
            <td>g1, g6</td>
            <td>g2, g4, g5</td>
            <td>g3</td>
        </tr>
    </tbody>
</table>

Чтобы гарантировать, что одни и те же задачи назначаются одним и тем же репликам и могут воспользоваться кешем, происходит две вещи. Вычисляется хеш части + набор гранул (задача). Применяется деление по модулю количества реплик для назначения задач.

На бумаге это звучит хорошо, но на практике резкая нагрузка на одну реплику или ухудшение сети могут привести к задержке, если одна и та же реплика постоянно используется для выполнения определенных задач. Если `max_parallel_replicas` меньше, чем количество реплик, то случайные реплики выбираются для выполнения запроса.
### Кража задач {#task-stealing}

Если какая-либо реплика обрабатывает задачи медленнее других, другие реплики попытаются "украсть" задачи, которые в принципе принадлежат этой реплике по хешу, чтобы снизить задержку.
### Ограничения {#limitations}

У этой функции есть известные ограничения, из которых основные документированы в этом разделе.

:::note
Если вы обнаружите проблему, которая не является одной из перечисленных ограничений, и подозреваете, что параллельные реплики являются причиной, пожалуйста, откройте проблему на GitHub, используя метку `comp-parallel-replicas`.
:::

| Ограничение                                    | Описание                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
|-----------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Сложные запросы                                 | В настоящее время параллельные реплики работают довольно хорошо для простых запросов. Сложные конструкции, такие как CTE, подзапросы, JOIN, не плоские запросы и т. д., могут негативно сказаться на производительности запросов.                                                                                                                                                                                                                   |
| Малые запросы                                 | Если вы выполняете запрос, который не обрабатывает много строк, выполнение его на нескольких репликах может не дать лучшего времени выполнения, учитывая, что время сети на координацию между репликами может привести к дополнительным циклам в выполнении запроса. Вы можете ограничить эти проблемы, используя настройку: [`parallel_replicas_min_number_of_rows_per_replica`](/docs/operations/settings/settings#parallel_replicas_min_number_of_rows_per_replica).  |
| Параллельные реплики отключены с FINAL      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| Данные с высокой кардинальностью и сложная агрегация | Высоко кардинальная агрегация, которая требует отправки большого объема данных, может значительно замедлить ваши запросы.                                                                                                                                                                                                                                                                                                                                                                     |
| Совместимость с новым аналитиком           | Новый аналитик может значительно замедлить или ускорить выполнение запроса в конкретных сценариях.                                                                                                                                                                                                                                                                                                                                                                       |
## Настройки, связанные с параллельными репликами {#settings-related-to-parallel-replicas}

| Настройка                                            | Описание                                                                                                                                                                                                                                                         |
|----------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `enable_parallel_replicas`                         | `0`: отключено<br/> `1`: включено <br/>`2`: Принудительно использование параллельной реплики, выбросит исключение, если не используется.                                                                                                                                                          |
| `cluster_for_parallel_replicas`                    | Имя кластера, которое следует использовать для параллельной репликации; если вы используете ClickHouse Cloud, используйте `default`.                                                                                                                                                                 |
| `max_parallel_replicas`                            | Максимальное количество реплик, используемых для выполнения запроса на нескольких репликах, если указано число меньше количества реплик в кластере, узлы выбираются случайным образом. Это значение также может быть пересмотрено, чтобы учесть горизонтальное масштабирование. |
| `parallel_replicas_min_number_of_rows_per_replica` | Помогает ограничить количество реплик, используемых на основе числа строк, которые необходимо обработать. Количество используемых реплик определяется: <br/> `предполагаемое количество строк для чтения` / `минимальное количество строк на реплику`.                                                               |
| `allow_experimental_analyzer`                      | `0`: используйте старый анализатор<br/> `1`: используйте новый анализатор. <br/><br/>Поведение параллельных реплик может изменяться в зависимости от используемого анализатора.                                                                                                                                    |
## Расследование проблем с параллельными репликами {#investigating-issues-with-parallel-replicas}

Вы можете проверить, какие настройки используются для каждого запроса в таблице 
[`system.query_log`](/docs/operations/system-tables/query_log). Вы также можете
посмотреть на таблицу [`system.events`](/docs/operations/system-tables/events),
чтобы увидеть все события, произошедшие на сервере, и вы можете использовать
табличную функцию [`clusterAllReplicas`](/docs/sql-reference/table-functions/cluster), чтобы увидеть таблицы на всех репликах
(если вы облачный пользователь, используйте `default`).

```sql title="Запрос"
SELECT
   hostname(),
   *
FROM clusterAllReplicas('default', system.events)
WHERE event ILIKE '%ParallelReplicas%'
```
<details>
<summary>Ответ</summary>
```response title="Ответ"
┌─hostname()───────────────────────┬─event──────────────────────────────────────────┬─value─┬─description──────────────────────────────────────────────────────────────────────────────────────────┐
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasHandleRequestMicroseconds      │   438 │ Время, затраченное на обработку запросов на метки от реплик                                             │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasHandleAnnouncementMicroseconds │   558 │ Время, затраченное на обработку объявлений реплик                                                      │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasReadUnassignedMarks            │   240 │ Сумма для всех реплик, сколько незанятых меток было запланировано                                      │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasReadAssignedForStealingMarks   │     4 │ Сумма для всех реплик, сколько запланированных меток было назначено для кражи по консистентному хешу   │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasStealingByHashMicroseconds     │     5 │ Время, затраченное на сбор сегментов, предназначенных для кражи по хешу                                  │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasProcessingPartsMicroseconds    │     5 │ Время, затраченное на обработку частей данных                                                           │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasStealingLeftoversMicroseconds  │     3 │ Время, затраченное на сбор сиротских сегментов                                                          │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasUsedCount                      │     2 │ Количество реплик, использованных для выполнения запроса с задачно-ориентированными параллельными репликами │
│ c-crimson-vd-86-server-rdhnsx3-0 │ ParallelReplicasAvailableCount                 │     6 │ Количество реплик, доступных для выполнения запроса с задачно-ориентированными параллельными репликами  │
└──────────────────────────────────┴────────────────────────────────────────────────┴───────┴──────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─hostname()───────────────────────┬─event──────────────────────────────────────────┬─value─┬─description──────────────────────────────────────────────────────────────────────────────────────────┐
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasHandleRequestMicroseconds      │   698 │ Время, затраченное на обработку запросов на метки от реплик                                             │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasHandleAnnouncementMicroseconds │   644 │ Время, затраченное на обработку объявлений реплик                                                      │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasReadUnassignedMarks            │   190 │ Сумма для всех реплик, сколько незанятых меток было запланировано                                      │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasReadAssignedForStealingMarks   │    54 │ Сумма для всех реплик, сколько запланированных меток было назначено для кражи по консистентному хешу   │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasStealingByHashMicroseconds     │     8 │ Время, затраченное на сбор сегментов, предназначенных для кражи по хешу                                  │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasProcessingPartsMicroseconds    │     4 │ Время, затраченное на обработку частей данных                                                           │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasStealingLeftoversMicroseconds  │     2 │ Время, затраченное на сбор сиротских сегментов                                                          │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasUsedCount                      │     2 │ Количество реплик, использованных для выполнения запроса с задачно-ориентированными параллельными репликами │
│ c-crimson-vd-86-server-e9kp5f0-0 │ ParallelReplicasAvailableCount                 │     6 │ Количество реплик, доступных для выполнения запроса с задачно-ориентированными параллельными репликами  │
└──────────────────────────────────┴────────────────────────────────────────────────┴───────┴──────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─hostname()───────────────────────┬─event──────────────────────────────────────────┬─value─┬─description──────────────────────────────────────────────────────────────────────────────────────────┐
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasHandleRequestMicroseconds      │   620 │ Время, затраченное на обработку запросов на метки от реплик                                             │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasHandleAnnouncementMicroseconds │   656 │ Время, затраченное на обработку объявлений реплик                                                      │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasReadUnassignedMarks            │     1 │ Сумма для всех реплик, сколько незанятых меток было запланировано                                      │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasReadAssignedForStealingMarks   │     1 │ Сумма для всех реплик, сколько запланированных меток было назначено для кражи по консистентному хешу   │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasStealingByHashMicroseconds     │     4 │ Время, затраченное на сбор сегментов, предназначенных для кражи по хешу                                  │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasProcessingPartsMicroseconds    │     3 │ Время, затраченное на обработку частей данных                                                           │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasStealingLeftoversMicroseconds  │     1 │ Время, затраченное на сбор сиротских сегментов                                                          │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasUsedCount                      │     2 │ Количество реплик, использованных для выполнения запроса с задачно-ориентированными параллельными репликами │
│ c-crimson-vd-86-server-ybtm18n-0 │ ParallelReplicasAvailableCount                 │    12 │ Количество реплик, доступных для выполнения запроса с задачно-ориентированными параллельными репликами  │
└──────────────────────────────────┴────────────────────────────────────────────────┴───────┴──────────────────────────────────────────────────────────────────────────────────────────────────────┘
┌─hostname()───────────────────────┬─event──────────────────────────────────────────┬─value─┬─description──────────────────────────────────────────────────────────────────────────────────────────┐
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasHandleRequestMicroseconds      │   696 │ Время, затраченное на обработку запросов на метки от реплик                                             │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasHandleAnnouncementMicroseconds │   717 │ Время, затраченное на обработку объявлений реплик                                                      │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasReadUnassignedMarks            │     2 │ Сумма для всех реплик, сколько незанятых меток было запланировано                                      │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasReadAssignedForStealingMarks   │     2 │ Сумма для всех реплик, сколько запланированных меток было назначено для кражи по консистентному хешу   │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasStealingByHashMicroseconds     │    10 │ Время, затраченное на сбор сегментов, предназначенных для кражи по хешу                                  │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasProcessingPartsMicroseconds    │     6 │ Время, затраченное на обработку частей данных                                                           │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasStealingLeftoversMicroseconds  │     2 │ Время, затраченное на сбор сиротских сегментов                                                          │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasUsedCount                      │     2 │ Количество реплик, использованных для выполнения запроса с задачно-ориентированными параллельными репликами │
│ c-crimson-vd-86-server-16j1ncj-0 │ ParallelReplicasAvailableCount                 │    12 │ Количество реплик, доступных для выполнения запроса с задачно-ориентированными параллельными репликами  │
└──────────────────────────────────┴────────────────────────────────────────────────┴───────┴──────────────────────────────────────────────────────────────────────────────────────────────────────┘
```
</details>

Таблица [`system.text_log`](/docs/operations/system-tables/text_log) также 
содержит информацию о выполнении запросов с использованием параллельных реплик:

```sql title="Запрос"
SELECT message
FROM clusterAllReplicas('default', system.text_log)
WHERE query_id = 'ad40c712-d25d-45c4-b1a1-a28ba8d4019c'
ORDER BY event_time_microseconds ASC
```

<details>
<summary>Ответ</summary>
```response title="Ответ"
┌─message────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ (from 54.218.178.249:59198) SELECT * FROM session_events WHERE type='type2' LIMIT 10 SETTINGS allow_experimental_parallel_reading_from_replicas=2; (stage: Complete)                                                                                       │
│ Запрос SELECT __table1.clientId AS clientId, __table1.sessionId AS sessionId, __table1.pageId AS pageId, __table1.timestamp AS timestamp, __table1.type AS type FROM default.session_events AS __table1 WHERE __table1.type = 'type2' LIMIT _CAST(10, 'UInt64') SETTINGS allow_experimental_parallel_reading_from_replicas = 2 до стадии Complete │
│ Доступ разрешен: SELECT(clientId, sessionId, pageId, timestamp, type) ON default.session_events                                                                                                                                                             │
│ Запрос SELECT __table1.clientId AS clientId, __table1.sessionId AS sessionId, __table1.pageId AS pageId, __table1.timestamp AS timestamp, __table1.type AS type FROM default.session_events AS __table1 WHERE __table1.type = 'type2' LIMIT _CAST(10, 'UInt64') до стадии WithMergeableState только анализ │
│ Доступ разрешен: SELECT(clientId, sessionId, pageId, timestamp, type) ON default.session_events                                                                                                                                                             │
│ Запрос SELECT __table1.clientId AS clientId, __table1.sessionId AS sessionId, __table1.pageId AS pageId, __table1.timestamp AS timestamp, __table1.type AS type FROM default.session_events AS __table1 WHERE __table1.type = 'type2' LIMIT _CAST(10, 'UInt64') с стадии FetchColumns до стадии WithMergeableState только анализ │
│ Запрос SELECT __table1.clientId AS clientId, __table1.sessionId AS sessionId, __table1.pageId AS pageId, __table1.timestamp AS timestamp, __table1.type AS type FROM default.session_events AS __table1 WHERE __table1.type = 'type2' LIMIT _CAST(10, 'UInt64') SETTINGS allow_experimental_parallel_reading_from_replicas = 2 до стадии WithMergeableState только анализ │
│ Доступ разрешен: SELECT(clientId, sessionId, pageId, timestamp, type) ON default.session_events                                                                                                                                                             │
│ Запрос SELECT __table1.clientId AS clientId, __table1.sessionId AS sessionId, __table1.pageId AS pageId, __table1.timestamp AS timestamp, __table1.type AS type FROM default.session_events AS __table1 WHERE __table1.type = 'type2' LIMIT _CAST(10, 'UInt64') SETTINGS allow_experimental_parallel_reading_from_replicas = 2 с стадии FetchColumns до стадии WithMergeableState только анализ │
│ Запрос SELECT __table1.clientId AS clientId, __table1.sessionId AS sessionId, __table1.pageId AS pageId, __table1.timestamp AS timestamp, __table1.type AS type FROM default.session_events AS __table1 WHERE __table1.type = 'type2' LIMIT _CAST(10, 'UInt64') SETTINGS allow_experimental_parallel_reading_from_replicas = 2 с стадии WithMergeableState до стадии Complete │
│ Запрашиваемое количество реплик (100) больше, чем реальное количество доступное в кластере (6). Будет использовано последнее число для выполнения запроса.                                                                                                       │
│ Начальный запрос от реплики 4: 2 части: [part all_0_2_1 с диапазонами [(0, 182)], part all_3_3_0 с диапазонами [(0, 62)]]----------
Получено от 4 реплики
                                                                                                   │
│ Состояние чтения полностью инициализировано: часть all_0_2_1 с диапазонами [(0, 182)] в репликах [4]; часть all_3_3_0 с диапазонами [(0, 62)] в репликах [4]                                                                                                            │
│ Отправлены начальные запросы: 1 Количество реплик: 6                                                                                                                                                                                                                 │
│ Начальный запрос от реплики 2: 2 части: [part all_0_2_1 с диапазонами [(0, 182)], part all_3_3_0 с диапазонами [(0, 62)]]----------
Получено от 2 реплики
                                                                                                   │
│ Отправлены начальные запросы: 2 Количество реплик: 6                                                                                                                                                                                                                 │
│ Обработка запроса от реплики 4, минимальный размер меток 240                                                                                                                                                                                                 │
│ Буду отвечать реплике 4 с 1 частями: [part all_0_2_1 с диапазонами [(128, 182)]]. Завершено: false; mine_marks=0, stolen_by_hash=54, stolen_rest=0                                                                                                       │
│ Начальный запрос от реплики 1: 2 части: [part all_0_2_1 с диапазонами [(0, 182)], part all_3_3_0 с диапазонами [(0, 62)]]----------
Получено от 1 реплики
                                                                                                   │
│ Отправлены начальные запросы: 3 Количество реплик: 6                                                                                                                                                                                                                 │
│ Обработка запроса от реплики 4, минимальный размер меток 240                                                                                                                                                                                                 │
│ Буду отвечать реплике 4 с 2 частями: [part all_0_2_1 с диапазонами [(0, 128)], part all_3_3_0 с диапазонами [(0, 62)]]. Завершено: false; mine_marks=0, stolen_by_hash=0, stolen_rest=190                                                                  │
│ Начальный запрос от реплики 0: 2 части: [part all_0_2_1 с диапазонами [(0, 182)], part all_3_3_0 с диапазонами [(0, 62)]]----------
Получено от 0 реплики
                                                                                                   │
│ Отправлены начальные запросы: 4 Количество реплик: 6                                                                                                                                                                                                                 │
│ Начальный запрос от реплики 5: 2 части: [part all_0_2_1 с диапазонами [(0, 182)], part all_3_3_0 с диапазонами [(0, 62)]]----------
Получено от 5 реплики
                                                                                                   │
│ Отправлены начальные запросы: 5 Количество реплик: 6                                                                                                                                                                                                                 │
│ Обработка запроса от реплики 2, минимальный размер меток 240                                                                                                                                                                                                 │
│ Буду отвечать реплике 2 с 0 частями: []. Завершено: true; mine_marks=0, stolen_by_hash=0, stolen_rest=0                                                                                                                                                │
│ Начальный запрос от реплики 3: 2 части: [part all_0_2_1 с диапазонами [(0, 182)], part all_3_3_0 с диапазонами [(0, 62)]]----------
Получено от 3 реплики
                                                                                                   │
│ Отправлены начальные запросы: 6 Количество реплик: 6                                                                                                                                                                                                                 │
│ Всего строк для чтения: 2000000                                                                                                                                                                                                                                │
│ Обработка запроса от реплики 5, минимальный размер меток 240                                                                                                                                                                                                 │
│ Буду отвечать реплике 5 с 0 частями: []. Завершено: true; mine_marks=0, stolen_by_hash=0, stolen_rest=0                                                                                                                                                │
│ Обработка запроса от реплики 0, минимальный размер меток 240                                                                                                                                                                                                 │
│ Буду отвечать реплике 0 с 0 частями: []. Завершено: true; mine_marks=0, stolen_by_hash=0, stolen_rest=0                                                                                                                                                │
│ Обработка запроса от реплики 1, минимальный размер меток 240                                                                                                                                                                                                 │
│ Буду отвечать реплике 1 с 0 частями: []. Завершено: true; mine_marks=0, stolen_by_hash=0, stolen_rest=0                                                                                                                                                │
│ Обработка запроса от реплики 3, минимальный размер меток 240                                                                                                                                                                                                 │
│ Буду отвечать реплике 3 с 0 частями: []. Завершено: true; mine_marks=0, stolen_by_hash=0, stolen_rest=0                                                                                                                                                │
│ (c-crimson-vd-86-server-rdhnsx3-0.c-crimson-vd-86-server-headless.ns-crimson-vd-86.svc.cluster.local:9000) Отмена запроса, поскольку достаточно данных уже прочитано                                                                                              │
│ Прочитано 81920 строк, 5.16 MiB за 0.013166 сек., 6222087.194288318 строк/сек., 391.63 MiБ/сек.                                                                                                                                                                   │
│ Координация завершена: Статистика: реплика 0 - {requests: 2 marks: 0 assigned_to_me: 0 stolen_by_hash: 0 stolen_unassigned: 0}; реплика 1 - {requests: 2 marks: 0 assigned_to_me: 0 stolen_by_hash: 0 stolen_unassigned: 0}; реплика 2 - {requests: 2 marks: 0 assigned_to_me: 0 stolen_by_hash: 0 stolen_unassigned: 0}; реплика 3 - {requests: 2 marks: 0 assigned_to_me: 0 stolen_by_hash: 0 stolen_unassigned: 0}; реплика 4 - {requests: 3 marks: 244 assigned_to_me: 0 stolen_by_hash: 54 stolen_unassigned: 190}; реплика 5 - {requests: 2 marks: 0 assigned_to_me: 0 stolen_by_hash: 0 stolen_unassigned: 0} │
│ Пиковое использование памяти (для запроса): 1.81 MiБ.                                                                                                                                                                                                                   │
│ Обработано за 0.024095586 сек.                                                                                                                                                                                                                              │
└────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
```
</details>

Наконец, вы также можете использовать `EXPLAIN PIPELINE`. Он демонстрирует, как ClickHouse 
будет выполнять запрос и какие ресурсы будут использованы для 
выполнения запроса. Рассмотрим следующий запрос в качестве примера:

```sql
SELECT count(), uniq(pageId) , min(timestamp), max(timestamp) 
FROM session_events 
WHERE type='type3' 
GROUP BY toYear(timestamp) LIMIT 10
```

Давайте посмотрим на конвейер запроса без параллельной реплики:

```sql title="EXPLAIN PIPELINE (без параллельной реплики)"
EXPLAIN PIPELINE graph = 1, compact = 0 
SELECT count(), uniq(pageId) , min(timestamp), max(timestamp) 
FROM session_events 
WHERE type='type3' 
GROUP BY toYear(timestamp) 
LIMIT 10 
SETTINGS allow_experimental_parallel_reading_from_replicas=0 
FORMAT TSV;
```

<Image img={image_8} size="lg" alt="EXPLAIN без parallel_replica" />

А теперь с параллельной репликой:

```sql title="EXPLAIN PIPELINE (с параллельной репликой)"
EXPLAIN PIPELINE graph = 1, compact = 0 
SELECT count(), uniq(pageId) , min(timestamp), max(timestamp) 
FROM session_events 
WHERE type='type3' 
GROUP BY toYear(timestamp) 
LIMIT 10 
SETTINGS allow_experimental_parallel_reading_from_replicas=2 
FORMAT TSV;
```

<Image img={image_9} size="lg" alt="EXPLAIN с parallel_replica"/>
