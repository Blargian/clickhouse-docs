---
title: セッション設定
sidebar_label: セッション設定
slug: /operations/settings/settings
toc_max_heading_level: 2
description: テーブルに見られる設定。
---

import ExperimentalBadge from '@theme/badges/ExperimentalBadge';
import BetaBadge from '@theme/badges/BetaBadge';
import CloudAvailableBadge from '@theme/badges/CloudAvailableBadge';

<!-- Autogenerated -->
以下のすべての設定は、テーブル [system.settings](/docs/operations/system-tables/settings) にも存在します。これらの設定は [source](https://github.com/ClickHouse/ClickHouse/blob/master/src/Core/Settings.cpp) から自動生成されます。
## add_http_cors_header {#add_http_cors_header}



タイプ: Bool

デフォルト値: 0

HTTP CORS ヘッダーを追加します。
## additional_result_filter {#additional_result_filter}



タイプ: String

デフォルト値:

`SELECT` クエリの結果に適用する追加のフィルター式です。この設定は、サブクエリには適用されません。

**例**

``` sql
INSERT INTO table_1 VALUES (1, 'a'), (2, 'bb'), (3, 'ccc'), (4, 'dddd');
SElECT * FROM table_1;
```
```response
┌─x─┬─y────┐
│ 1 │ a    │
│ 2 │ bb   │
│ 3 │ ccc  │
│ 4 │ dddd │
└───┴──────┘
```
```sql
SELECT *
FROM table_1
SETTINGS additional_result_filter = 'x != 2'
```
```response
┌─x─┬─y────┐
│ 1 │ a    │
│ 3 │ ccc  │
│ 4 │ dddd │
└───┴──────┘
```
## additional_table_filters {#additional_table_filters}



タイプ: Map

デフォルト値: {}

指定されたテーブルから読み取った後に適用される追加のフィルター式です。

**例**

``` sql
INSERT INTO table_1 VALUES (1, 'a'), (2, 'bb'), (3, 'ccc'), (4, 'dddd');
SELECT * FROM table_1;
```
```response
┌─x─┬─y────┐
│ 1 │ a    │
│ 2 │ bb   │
│ 3 │ ccc  │
│ 4 │ dddd │
└───┴──────┘
```
```sql
SELECT *
FROM table_1
SETTINGS additional_table_filters = {'table_1': 'x != 2'}
```
```response
┌─x─┬─y────┐
│ 1 │ a    │
│ 3 │ ccc  │
│ 4 │ dddd │
└───┴──────┘
```
## aggregate_functions_null_for_empty {#aggregate_functions_null_for_empty}



タイプ: Bool

デフォルト値: 0

クエリ内のすべての集約関数を再書き換え、[-OrNull](../../sql-reference/aggregate-functions/combinators.md/#agg-functions-combinator-ornull) サフィックスを追加するかどうかを制御します。SQL標準互換のために有効にします。分散クエリに対して一貫した結果を取得するためにクエリの再書き換えを介して実装されています（[count_distinct_implementation](#count_distinct_implementation) 設定に似ています）。

可能な値:

- 0 — 無効。
- 1 — 有効。

**例**

以下の集約関数を含むクエリを考えてみましょう：
```sql
SELECT SUM(-1), MAX(0) FROM system.one WHERE 0;
```

`aggregate_functions_null_for_empty = 0` の場合の結果は次のようになります：
```text
┌─SUM(-1)─┬─MAX(0)─┐
│       0 │      0 │
└─────────┴────────┘
```

`aggregate_functions_null_for_empty = 1` の場合の結果は次のようになります：
```text
┌─SUMOrNull(-1)─┬─MAXOrNull(0)─┐
│          NULL │         NULL │
└───────────────┴──────────────┘
```
## aggregation_in_order_max_block_bytes {#aggregation_in_order_max_block_bytes}



タイプ: UInt64

デフォルト値: 50000000

主キーの順序で集約中に蓄積される最大バイト数のブロック。ブロックサイズを小さくすると、集約の最終的なマージステージをより多く並列化できます。
## aggregation_memory_efficient_merge_threads {#aggregation_memory_efficient_merge_threads}



タイプ: UInt64

デフォルト値: 0

メモリ効率モードで中間集約結果をマージするために使用するスレッドの数。大きいほど、より多くのメモリが消費されます。0は、'max_threads' と同じです。
## allow_aggregate_partitions_independently {#allow_aggregate_partitions_independently}



タイプ: Bool

デフォルト値: 0

パーティションキーがグループ化キーに適合する場合に、別のスレッドでパーティションの独立した集約を有効にします。パーティションの数がコアの数に近く、パーティションのサイズがほぼ同じ場合に有益です。
## allow_archive_path_syntax {#allow_archive_path_syntax}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 1

ファイル/S3 エンジン/テーブル関数は、アーカイブに正しい拡張子がある場合、`<archive> :: <file>\` として `::` を持つパスを解析します。
## allow_asynchronous_read_from_io_pool_for_merge_tree {#allow_asynchronous_read_from_io_pool_for_merge_tree}



タイプ: Bool

デフォルト値: 0

MergeTree テーブルから読み取るためにバックグラウンドI/Oプールを使用します。この設定は、I/O バウンドのクエリの性能を向上させる可能性があります。
## allow_changing_replica_until_first_data_packet {#allow_changing_replica_until_first_data_packet}



タイプ: Bool

デフォルト値: 0

有効になっている場合、ヘッジリクエストでは、最初のデータパケットを受信するまでに新しい接続を開始できます。すでに進捗があっても（ただし進捗が `receive_data_timeout` タイムアウトまで更新されていない場合）、そうでなければ進捗を開始した後はレプリカを変更することは無効になります。
## allow_create_index_without_type {#allow_create_index_without_type}



タイプ: Bool

デフォルト値: 0

TYPEなしでCREATE INDEXクエリを許可します。このクエリは無視されます。SQL互換性のテストのために作成されました。
## allow_custom_error_code_in_throwif {#allow_custom_error_code_in_throwif}



タイプ: Bool

デフォルト値: 0

関数 throwIf() にカスタムエラーコードを有効にします。true の場合、スローされた例外には予期しないエラーコードが含まれる可能性があります。
## allow_ddl {#allow_ddl}



タイプ: Bool

デフォルト値: 1

これが true に設定されている場合、ユーザーはDDLクエリを実行できるようになります。
## allow_deprecated_database_ordinary {#allow_deprecated_database_ordinary}



タイプ: Bool

デフォルト値: 0

非推奨の Ordinary エンジンを持つデータベースの作成を許可します。
## allow_deprecated_error_prone_window_functions {#allow_deprecated_error_prone_window_functions}



タイプ: Bool

デフォルト値: 0

エラーを引き起こしやすい非推奨のウィンドウ関数（neighbor、runningAccumulate、runningDifferenceStartingWithFirstValue、runningDifference）の使用を許可します。
## allow_deprecated_snowflake_conversion_functions {#allow_deprecated_snowflake_conversion_functions}



タイプ: Bool

デフォルト値: 0

`snowflakeToDateTime`、`snowflakeToDateTime64`、`dateTimeToSnowflake`、`dateTime64ToSnowflake` の関数は非推奨であり、デフォルトで無効になっています。`snowflakeIDToDateTime`、`snowflakeIDToDateTime64`、`dateTimeToSnowflakeID`、`dateTime64ToSnowflakeID` の関数を代わりに使用してください。

非推奨の関数を再有効化するには（例えば、移行期間中）、この設定を `true` に設定してください。
## allow_deprecated_syntax_for_merge_tree {#allow_deprecated_syntax_for_merge_tree}



タイプ: Bool

デフォルト値: 0

非推奨のエンジン定義構文を使用して *MergeTree テーブルを作成することを許可します。
## allow_distributed_ddl {#allow_distributed_ddl}



タイプ: Bool

デフォルト値: 1

これが true に設定されている場合、ユーザーは分散DDLクエリを実行できるようになります。
## allow_drop_detached {#allow_drop_detached}



タイプ: Bool

デフォルト値: 0

ALTER TABLE ... DROP DETACHED PART[ITION] ... クエリを許可します。
## allow_execute_multiif_columnar {#allow_execute_multiif_columnar}



タイプ: Bool

デフォルト値: 1

columnar に対して multiIf 関数を実行することを許可します。
## allow_experimental_analyzer {#allow_experimental_analyzer}



タイプ: Bool

デフォルト値: 1

新しいクエリアナライザーを許可します。
## allow_experimental_codecs {#allow_experimental_codecs}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

これが true に設定されている場合、実験的な圧縮コーデックを指定することを許可します（ただし、現在そのようなものはありませんので、このオプションは何も機能しません）。
## allow_experimental_database_iceberg {#allow_experimental_database_iceberg}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

実験的なデータベースエンジン Iceberg を許可します。
## allow_experimental_database_materialized_postgresql {#allow_experimental_database_materialized_postgresql}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

Engine=MaterializedPostgreSQL(...) でデータベースを作成することを許可します。
## allow_experimental_dynamic_type {#allow_experimental_dynamic_type}
<BetaBadge/>


タイプ: Bool

デフォルト値: 0

[Dynamic](../../sql-reference/data-types/dynamic.md) データ型の作成を許可します。
## allow_experimental_full_text_index {#allow_experimental_full_text_index}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

これが true に設定されている場合、実験的な全文検索インデックスを使用することを許可します。
## allow_experimental_funnel_functions {#allow_experimental_funnel_functions}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

ファネル分析のための実験的な関数を有効にします。
## allow_experimental_hash_functions {#allow_experimental_hash_functions}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

実験的なハッシュ関数を有効にします。
## allow_experimental_inverted_index {#allow_experimental_inverted_index}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

これが true に設定されている場合、実験的なインバーテッドインデックスを使用することを許可します。
## allow_experimental_join_condition {#allow_experimental_join_condition}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

左と右の両方のテーブルの列を含む不等式条件との結合をサポートします。例：`t1.y < t2.y`。
## allow_experimental_join_right_table_sorting {#allow_experimental_join_right_table_sorting}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

これが true に設定されている場合、`join_to_sort_minimum_perkey_rows` および `join_to_sort_maximum_table_rows` の条件が満たされると、右テーブルをキーによって再配置して、左または内部ハッシュ結合の性能を向上させます。
## allow_experimental_json_type {#allow_experimental_json_type}
<BetaBadge/>


タイプ: Bool

デフォルト値: 0

[JSON](../../sql-reference/data-types/newjson.md) データ型の作成を許可します。
## allow_experimental_kafka_offsets_storage_in_keeper {#allow_experimental_kafka_offsets_storage_in_keeper}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

Kafka 関連のオフセットを ClickHouse Keeper に保存する実験的な機能を許可します。有効にすると、ClickHouse Keeper パスとレプリカ名を Kafka テーブルエンジンに指定できます。その結果、通常の Kafka エンジンの代わりに、コミットされたオフセットを主に ClickHouse Keeper に保存する新しいタイプのストレージエンジンが使用されます。
## allow_experimental_kusto_dialect {#allow_experimental_kusto_dialect}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

Kusto Query Language (KQL) - SQL の代替を有効にします。
## allow_experimental_live_view {#allow_experimental_live_view}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

非推奨の LIVE VIEW の作成を許可します。

可能な値:

- 0 — ライブビューの操作は無効です。
- 1 — ライブビューの操作は有効です。
## allow_experimental_materialized_postgresql_table {#allow_experimental_materialized_postgresql_table}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

MaterializedPostgreSQL テーブルエンジンを使用することを許可します。デフォルトでは無効ですが、この機能は実験的です。
## allow_experimental_nlp_functions {#allow_experimental_nlp_functions}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

自然言語処理のための実験的な関数を有効にします。
## allow_experimental_object_type {#allow_experimental_object_type}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

廃止された Object データ型を許可します。
## allow_experimental_parallel_reading_from_replicas {#allow_experimental_parallel_reading_from_replicas}
<BetaBadge/>


タイプ: UInt64

デフォルト値: 0

SELECT クエリの実行のために各シャードから最大 `max_parallel_replicas` レプリカに対して読み取りを使用します。読み取りは並列化され、動的に調整されます。0 - 無効、1 - 有効、失敗時には静かに無効、2 - 有効、失敗時には例外をスローします。
## allow_experimental_prql_dialect {#allow_experimental_prql_dialect}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

PRQL - SQL の代替を有効にします。
## allow_experimental_query_deduplication {#allow_experimental_query_deduplication}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

パート UUID に基づく SELECT クエリの実験的なデデュプリケーション。
## allow_experimental_shared_set_join {#allow_experimental_shared_set_join}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

ClickHouse Cloud のみ。ShareSet と SharedJoin の作成を許可します。
## allow_experimental_statistics {#allow_experimental_statistics}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

[統計](../../engines/table-engines/mergetree-family/mergetree.md/#table_engine-mergetree-creating-a-table)を持つ列の定義や [統計の操作](../../engines/table-engines/mergetree-family/mergetree.md/#column-statistics)を許可します。
## allow_experimental_time_series_table {#allow_experimental_time_series_table}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

[TimeSeries](../../engines/table-engines/integrations/time-series.md) テーブルエンジンを持つテーブルの作成を許可します。

可能な値:

- 0 — [TimeSeries](../../engines/table-engines/integrations/time-series.md) テーブルエンジンは無効です。
- 1 — [TimeSeries](../../engines/table-engines/integrations/time-series.md) テーブルエンジンは有効です。
## allow_experimental_ts_to_grid_aggregate_function {#allow_experimental_ts_to_grid_aggregate_function}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

Prometheus ライクな時系列再サンプリングのための実験的な tsToGrid 集約関数。Cloud のみ。
## allow_experimental_variant_type {#allow_experimental_variant_type}
<BetaBadge/>


タイプ: Bool

デフォルト値: 0

[Variant](../../sql-reference/data-types/variant.md) データ型の作成を許可します。
## allow_experimental_vector_similarity_index {#allow_experimental_vector_similarity_index}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

実験的なベクトル類似性インデックスを許可します。
## allow_experimental_window_view {#allow_experimental_window_view}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

WINDOW VIEW を有効にします。成熟していません。
## allow_general_join_planning {#allow_general_join_planning}



タイプ: Bool

デフォルト値: 1

より一般的な結合計画アルゴリズムを許可し、より複雑な条件を処理できますが、ハッシュ結合でのみ動作します。ハッシュ結合が無効な場合、この設定の値に関係なく通常の結合計画アルゴリズムが使用されます。
## allow_get_client_http_header {#allow_get_client_http_header}



タイプ: Bool

デフォルト値: 0

クライアントが現在のHTTPリクエストヘッダーの値を取得できるようにする関数 `getClientHTTPHeader` を使用することを許可します。これはセキュリティ上の理由からデフォルトでは無効であり、`Cookie` などの一部のヘッダーには機密情報が含まれている可能性があります。`X-ClickHouse-*` および `Authentication` ヘッダーは常に制限されているため、この関数で取得することはできません。
## allow_hyperscan {#allow_hyperscan}



タイプ: Bool

デフォルト値: 1

Hyperscan ライブラリを使用する関数を許可します。潜在的に長いコンパイル時間や過剰なリソース使用を避けるために無効にします。
## allow_introspection_functions {#allow_introspection_functions}



タイプ: Bool

デフォルト値: 0

クエリプロファイリング用の [イントロスペクション関数](../../sql-reference/functions/introspection.md) を有効または無効にします。

可能な値:

- 1 — イントロスペクション関数が有効。
- 0 — イントロスペクション関数が無効。

**関連情報**

- [サンプリングクエリプロファイラー](../../operations/optimizing-performance/sampling-query-profiler.md)
- システムテーブル [trace_log](../../operations/system-tables/trace_log.md/#system_tables-trace_log)
## allow_materialized_view_with_bad_select {#allow_materialized_view_with_bad_select}



タイプ: Bool

デフォルト値: 1

存在しないテーブルやカラムを参照するSELECTクエリでCREATE MATERIALIZED VIEWを許可します。文法的には有効でなければなりません。リフレッシュ可能なMVには適用されません。MVスキーマをSELECTクエリから推論する必要がある場合（すなわち、CREATE にカラムリストや TO テーブルがない場合）には適用されません。ソーステーブルの前にMVを作成するために使用できます。
## allow_named_collection_override_by_default {#allow_named_collection_override_by_default}



タイプ: Bool

デフォルト値: 1

デフォルトで名前付きコレクションのフィールドの上書きを許可します。
## allow_non_metadata_alters {#allow_non_metadata_alters}



タイプ: Bool

デフォルト値: 1

テーブルのメタデータだけでなく、ディスク上のデータに影響を与えるALTERを実行することを許可します。
## allow_nonconst_timezone_arguments {#allow_nonconst_timezone_arguments}



タイプ: Bool

デフォルト値: 0

toTimeZone()、fromUnixTimestamp*()、snowflakeToDateTime*() などの特定の時間関連関数において、定数でないタイムゾーン引数を許可します。
## allow_nondeterministic_mutations {#allow_nondeterministic_mutations}



タイプ: Bool

デフォルト値: 0

ユーザーレベルの設定で、レプリケートテーブル上で `dictGet` などの非決定的関数を使用して変更を行うことを許可します。

例えば、辞書はノード間で同期されていない可能性があるため、そこから値を取得する変更はデフォルトではレプリケートテーブルでは許可されていません。この設定を有効にすると、その動作が許可され、すべてのノード間でデータが同期されていることを保証するのはユーザーの責任となります。

**例**

``` xml
<profiles>
    <default>
        <allow_nondeterministic_mutations>1</allow_nondeterministic_mutations>

        <!-- ... -->
    </default>

    <!-- ... -->

</profiles>
```
## allow_nondeterministic_optimize_skip_unused_shards {#allow_nondeterministic_optimize_skip_unused_shards}



タイプ: Bool

デフォルト値: 0

シャーディングキーにおいて非決定的（`rand` や `dictGet` など、後者はアップデート時にいくつかの注意が必要）関数を許可します。

可能な値:

- 0 — 不許可。
- 1 — 許可。
## allow_not_comparable_types_in_comparison_functions {#allow_not_comparable_types_in_comparison_functions}



タイプ: Bool

デフォルト値: 0

比較関数 `equal/less/greater/etc` で比較できないタイプ（JSON/Object/AggregateFunctionなど）の使用を許可または制限します。
## allow_not_comparable_types_in_order_by {#allow_not_comparable_types_in_order_by}



タイプ: Bool

デフォルト値: 0

ORDER BY キーで比較できないタイプ（JSON/Object/AggregateFunctionなど）の使用を許可または制限します。
## allow_prefetched_read_pool_for_local_filesystem {#allow_prefetched_read_pool_for_local_filesystem}



タイプ: Bool

デフォルト値: 0

すべてのパーツがローカルファイルシステムにある場合、プリフェッチしたスレッドプールを優先します。
## allow_prefetched_read_pool_for_remote_filesystem {#allow_prefetched_read_pool_for_remote_filesystem}



タイプ: Bool

デフォルト値: 1

すべてのパーツがリモートファイルシステムにある場合、プリフェッチしたスレッドプールを優先します。
## allow_push_predicate_ast_for_distributed_subqueries {#allow_push_predicate_ast_for_distributed_subqueries}



タイプ: Bool

デフォルト値: 1

分析器が有効な状態で、分散サブクエリに対してASTレベルでプッシュ条件を許可します。
## allow_push_predicate_when_subquery_contains_with {#allow_push_predicate_when_subquery_contains_with}



タイプ: Bool

デフォルト値: 1

サブクエリに WITH 句が含まれている場合にプッシュ条件を許可します。
## allow_reorder_prewhere_conditions {#allow_reorder_prewhere_conditions}



タイプ: Bool

デフォルト値: 1

WHERE から PREWHERE に条件を移動する際、フィルタリングを最適化するためにそれらを再配置できるようにします。
## allow_settings_after_format_in_insert {#allow_settings_after_format_in_insert}



タイプ: Bool

デフォルト値: 0

INSERT クエリ内の `FORMAT` の後に `SETTINGS` を許可するかどうかを制御します。これは推奨されません。なぜなら、`SETTINGS` の一部を値として解釈する可能性があるからです。

例:

```sql
INSERT INTO FUNCTION null('foo String') SETTINGS max_threads=1 VALUES ('bar');
```

ただし、以下のクエリは `allow_settings_after_format_in_insert` が必要です：

```sql
SET allow_settings_after_format_in_insert=1;
INSERT INTO FUNCTION null('foo String') VALUES ('bar') SETTINGS max_threads=1;
```

可能な値:

- 0 — 不許可。
- 1 — 許可。

:::note
この設定は、古い構文に依存するユースケースがある場合のみ、後方互換性のために使用してください。
:::
## allow_simdjson {#allow_simdjson}



タイプ: Bool

デフォルト値: 1

AVX2 命令が使用可能な場合、'JSON*' 関数において simdjson ライブラリを使用することを許可します。無効にすると rapidjson が使用されます。
## allow_statistics_optimize {#allow_statistics_optimize}
<ExperimentalBadge/>


タイプ: Bool

デフォルト値: 0

クエリの最適化に統計を使用することを許可します。
## allow_suspicious_codecs {#allow_suspicious_codecs}



タイプ: Bool

デフォルト値: 0

これが true に設定されている場合、意味のない圧縮コーデックを指定することを許可します。
## allow_suspicious_fixed_string_types {#allow_suspicious_fixed_string_types}



タイプ: Bool

デフォルト値: 0

CREATE TABLE 文において、n > 256 の型 FixedString(n) のカラムを作成することを許可します。長さが 256 以上の FixedString は疑わしく、誤用を示す可能性が高いです。
## allow_suspicious_indices {#allow_suspicious_indices}



タイプ: Bool

デフォルト値: 0

同一の式を持つプライマリ/セカンダリインデックスおよびソートキーを拒否します。
## allow_suspicious_low_cardinality_types {#allow_suspicious_low_cardinality_types}



タイプ: Bool

デフォルト値: 0

固定サイズが 8 バイト以下のデータ型に対して、[LowCardinality](../../sql-reference/data-types/lowcardinality.md)の使用を許可または制限します：数値データ型および `FixedString(8_bytes_or_less)`。

小さな固定値に対して `LowCardinality` を使用することは通常効率的ではありません。なぜなら、ClickHouse は各行に対して数値インデックスを保存するためです。その結果：

- ディスクスペースの使用量が増加する可能性があります。
- RAM の消費量が、辞書のサイズに応じて増加する可能性があります。
- いくつかの関数は、追加のコーディングやエンコーディングの操作により遅くなる可能性があります。

[MergeTree](../../engines/table-engines/mergetree-family/mergetree.md)-エンジンテーブルのマージ時間が、上記の理由により増加する可能性があります。

可能な値:

- 1 — `LowCardinality` の使用に制限なし。
- 0 — `LowCardinality` の使用が制限される。
## allow_suspicious_primary_key {#allow_suspicious_primary_key}



タイプ: Bool

デフォルト値: 0

MergeTree に対して疑わしい `PRIMARY KEY`/`ORDER BY` を許可します（すなわち SimpleAggregateFunction）。
## allow_suspicious_ttl_expressions {#allow_suspicious_ttl_expressions}



タイプ: Bool

デフォルト値: 0

テーブルのカラムに依存しない TTL 式を拒否します。これは大抵の場合、ユーザーのエラーを示します。
## allow_suspicious_types_in_group_by {#allow_suspicious_types_in_group_by}



タイプ: Bool

デフォルト値: 0

GROUP BY キーで [Variant](../../sql-reference/data-types/variant.md) および [Dynamic](../../sql-reference/data-types/dynamic.md) タイプの使用を許可または制限します。
## allow_suspicious_types_in_order_by {#allow_suspicious_types_in_order_by}



タイプ: Bool

デフォルト値: 0

ORDER BY キーで [Variant](../../sql-reference/data-types/variant.md) および [Dynamic](../../sql-reference/data-types/dynamic.md) タイプの使用を許可または制限します。
## allow_suspicious_variant_types {#allow_suspicious_variant_types}



タイプ: Bool

デフォルト値: 0

CREATE TABLE 文において、類似の変種型（異なる数値型または日付型など）を持つ Variant 型を指定することを許可します。この設定を有効にすると、類似したタイプの値での作業に曖昧さが生じる可能性があります。
## allow_unrestricted_reads_from_keeper {#allow_unrestricted_reads_from_keeper}



タイプ: Bool

デフォルト値: 0

条件なしで system.zookeeper テーブルからの無制限の読み取りを許可します。便利ですが、zookeeper にとって安全ではありません。
## alter_move_to_space_execute_async {#alter_move_to_space_execute_async}



タイプ: Bool

デフォルト値: 0

ALTER TABLE MOVE ... TO [DISK|VOLUME] を非同期で実行します。
## alter_partition_verbose_result {#alter_partition_verbose_result}



タイプ: Bool

デフォルト値: 0

パーティションおよびパーツに対する操作が正常に適用された情報の表示を有効または無効にします。
[ATTACH PARTITION|PART](../../sql-reference/statements/alter/partition.md/#alter_attach-partition) および [FREEZE PARTITION](../../sql-reference/statements/alter/partition.md/#alter_freeze-partition) に適用されます。

可能な値:

- 0 — 冗長性を無効にします。
- 1 — 冗長性を有効にします。

**例**

```sql
CREATE TABLE test(a Int64, d Date, s String) ENGINE = MergeTree PARTITION BY toYYYYMDECLARE(d) ORDER BY a;
INSERT INTO test VALUES(1, '2021-01-01', '');
INSERT INTO test VALUES(1, '2021-01-01', '');
ALTER TABLE test DETACH PARTITION ID '202101';

ALTER TABLE test ATTACH PARTITION ID '202101' SETTINGS alter_partition_verbose_result = 1;

┌─command_type─────┬─partition_id─┬─part_name────┬─old_part_name─┐
│ ATTACH PARTITION │ 202101       │ 202101_7_7_0 │ 202101_5_5_0  │
│ ATTACH PARTITION │ 202101       │ 202101_8_8_0 │ 202101_6_6_0  │
└──────────────────┴──────────────┴──────────────┴───────────────┘

ALTER TABLE test FREEZE SETTINGS alter_partition_verbose_result = 1;

┌─command_type─┬─partition_id─┬─part_name────┬─backup_name─┬─backup_path───────────────────┬─part_backup_path────────────────────────────────────────────┐
│ FREEZE ALL   │ 202101       │ 202101_7_7_0 │ 8           │ /var/lib/clickhouse/shadow/8/ │ /var/lib/clickhouse/shadow/8/data/default/test/202101_7_7_0 │
│ FREEZE ALL   │ 202101       │ 202101_8_8_0 │ 8           │ /var/lib/clickhouse/shadow/8/ │ /var/lib/clickhouse/shadow/8/data/default/test/202101_8_8_0 │
└──────────────┴──────────────┴──────────────┴─────────────┴───────────────────────────────┴─────────────────────────────────────────────────────────────┘
```
## alter_sync {#alter_sync}



タイプ: UInt64

デフォルト値: 1

[ALTER](../../sql-reference/statements/alter/index.md)、[OPTIMIZE](../../sql-reference/statements/optimize.md)、または [TRUNCATE](../../sql-reference/statements/truncate.md) クエリによってレプリカで実行されたアクションの待機を設定できるようにします。

可能な値:

- 0 — 待機しない。
- 1 — 自身の実行を待機。
- 2 — すべてを待機。

Cloud のデフォルト値: `0`。

:::note
`alter_sync` は `Replicated` テーブルにのみ適用され、非 `Replicated` テーブルの変更には影響しません。
:::
## analyze_index_with_space_filling_curves {#analyze_index_with_space_filling_curves}



タイプ: Bool

デフォルト値: 1

テーブルがそのインデックスにスペースフィリングカーブ（例：`ORDER BY mortonEncode(x, y)` または `ORDER BY hilbertEncode(x, y)`）を持ち、クエリがその引数に関する条件を持っている場合（例：`x >= 10 AND x <= 20 AND y >= 20 AND y <= 30`）、インデックス分析にスペースフィリングカーブを使用します。
## analyzer_compatibility_join_using_top_level_identifier {#analyzer_compatibility_join_using_top_level_identifier}



タイプ: Bool

デフォルト値: 0

JOIN USING で識別子を投影から解決するように強制します（例えば、`SELECT a + 1 AS b FROM t1 JOIN t2 USING (b)` の場合、結合は `t1.a + 1 = t2.b` によって行われ、`t1.b = t2.b` とはならない）。
## any_join_distinct_right_table_keys {#any_join_distinct_right_table_keys}



タイプ: Bool

デフォルト値: 0

`ANY INNER|LEFT JOIN` 操作において、旧式の ClickHouse サーバーの動作を有効にします。

:::note
この設定は、旧式の `JOIN` 動作に依存する場合の後方互換性のためだけに使用してください。
:::

旧式の動作が有効になっている場合：

- `t1 ANY LEFT JOIN t2` と `t2 ANY RIGHT JOIN t1` 操作の結果は等しくなく、ClickHouse は多対一の左から右へのテーブルキーのマッピングを使います。
- `ANY INNER JOIN` 操作の結果は、`SEMI LEFT JOIN` 操作と同様に、左テーブルのすべての行を含みます。

旧式の動作が無効になっている場合：

- `t1 ANY LEFT JOIN t2` と `t2 ANY RIGHT JOIN t1` 操作の結果は等しくなり、ClickHouse は `ANY RIGHT JOIN` 操作において一対多のキーのマッピングを提供するロジックを使用します。
- `ANY INNER JOIN` 操作の結果は、左テーブルと右テーブルの両方からのキーごとに1行を含みます。

可能な値：

- 0 — 旧式の動作は無効です。
- 1 — 旧式の動作は有効です。

関連情報：

- [JOIN の厳密さ](../../sql-reference/statements/select/join.md/#join-settings)
## apply_deleted_mask {#apply_deleted_mask}



タイプ: Bool

デフォルト値: 1

軽量DELETEで削除された行をフィルタリングすることを可能にします。無効にすると、そのような行を読むことができるクエリが可能になります。これはデバッグや「元に戻す」シナリオに便利です。
## apply_mutations_on_fly {#apply_mutations_on_fly}



タイプ: Bool

デフォルト値: 0

true の場合、データパートに具現化されていない修正（UPDATE と DELETE）が SELECT に適用されます。
## apply_settings_from_server {#apply_settings_from_server}



タイプ: Bool

デフォルト値: 1

クライアントがサーバーからの設定を受け入れるべきかどうか。

これはクライアント側で実行される操作にのみ影響し、特に INSERT 入力データの解析とクエリ結果のフォーマットに影響します。クエリの実行のほとんどはサーバー側で行われ、この設定の影響を受けません。

通常、この設定はユーザープロファイル（users.xml や `ALTER USER` などのクエリ）で設定するべきであり、クライアント（クライアント コマンドライン引数、`SET` クエリ、または `SELECT` クエリの `SETTINGS` セクション）を通じて設定しないでください。クライアントを通じて false に変更できますが、true に変更することはできません（ユーザープロファイルに `apply_settings_from_server = false` があると、サーバーは設定を送信しないため）。

最初は（24.12）、サーバー設定（`send_settings_to_client`）がありましたが、後にこのクライアント設定に置き換えられ、使いやすさが向上しました。
## asterisk_include_alias_columns {#asterisk_include_alias_columns}



タイプ: Bool

デフォルト値: 0

ワイルドカードクエリ（`SELECT *`）に [ALIAS](../../sql-reference/statements/create/table.md/#alias) カラムを含めます。

可能な値：

- 0 - 無効
- 1 - 有効
## asterisk_include_materialized_columns {#asterisk_include_materialized_columns}



タイプ: Bool

デフォルト値: 0

ワイルドカードクエリ（`SELECT *`）に [MATERIALIZED](../../sql-reference/statements/create/table.md/#materialized) カラムを含めます。

可能な値：

- 0 - 無効
- 1 - 有効
## async_insert {#async_insert}



タイプ: Bool

デフォルト値: 0

true の場合、INSERT クエリからのデータがキューに保存され、後でバックグラウンドでテーブルにフラッシュされます。wait_for_async_insert が false の場合、INSERT クエリはほぼ即座に処理されますが、そうでない場合、クライアントはデータがテーブルにフラッシュされるまで待機します。
## async_insert_busy_timeout_decrease_rate {#async_insert_busy_timeout_decrease_rate}



タイプ: Double

デフォルト値: 0.2

適応的な非同期挿入タイムアウトが減少する際の指数的成長率
## async_insert_busy_timeout_increase_rate {#async_insert_busy_timeout_increase_rate}



タイプ: Double

デフォルト値: 0.2

適応的な非同期挿入タイムアウトが増加する際の指数的成長率

## async_insert_busy_timeout_max_ms {#async_insert_busy_timeout_max_ms}

Type: ミリ秒

Default value: 200

最初のデータが現れてからの各クエリごとに収集されたデータをダンプするまでの最大待機時間です。
## async_insert_busy_timeout_min_ms {#async_insert_busy_timeout_min_ms}

Type: ミリ秒

Default value: 50

async_insert_use_adaptive_busy_timeoutが有効になっている場合に、最初のデータが現れてからの各クエリごとに収集されたデータをダンプするまでの最小待機時間です。これは、適応アルゴリズムの初期値としても機能します。
## async_insert_deduplicate {#async_insert_deduplicate}

Type: Bool

Default value: 0

複製テーブルにおける非同期INSERTクエリの場合、挿入ブロックの重複排除を実施するかどうかを指定します。
## async_insert_max_data_size {#async_insert_max_data_size}

Type: UInt64

Default value: 10485760

挿入される前に各クエリごとに収集された未解析データの最大サイズ（バイト）です。
## async_insert_max_query_number {#async_insert_max_query_number}

Type: UInt64

Default value: 450

挿入される前のINSERTクエリの最大数です。
## async_insert_poll_timeout_ms {#async_insert_poll_timeout_ms}

Type: ミリ秒

Default value: 10

非同期挿入キューからのデータをポーリングするためのタイムアウトです。
## async_insert_use_adaptive_busy_timeout {#async_insert_use_adaptive_busy_timeout}

Type: Bool

Default value: 1

trueに設定されている場合、非同期INSERT用に適応的ビジータイムアウトを使用します。
## async_query_sending_for_remote {#async_query_sending_for_remote}

Type: Bool

Default value: 1

リモートクエリを実行する際に、非同期接続の作成とクエリの送信を可能にします。

デフォルトで有効です。
## async_socket_for_remote {#async_socket_for_remote}

Type: Bool

Default value: 1

リモートクエリを実行する際に、ソケットからの非同期読み取りを可能にします。

デフォルトで有効です。
## azure_allow_parallel_part_upload {#azure_allow_parallel_part_upload}

Type: Bool

Default value: 1

Azureマルチパートアップロード用に複数スレッドを使用します。
## azure_check_objects_after_upload {#azure_check_objects_after_upload}

Type: Bool

Default value: 0

アップロードした各オブジェクトがAzure Blobストレージで正常にアップロードされたかを確認します。
## azure_create_new_file_on_insert {#azure_create_new_file_on_insert}

Type: Bool

Default value: 0

Azureエンジンテーブルでの各挿入時に新しいファイルを作成するかどうかを有効または無効にします。
## azure_ignore_file_doesnt_exist {#azure_ignore_file_doesnt_exist}

Type: Bool

Default value: 0

特定のキーを読み取る際に、ファイルが存在しない場合は無視します。

可能な値:
- 1 — `SELECT`は空の結果を返します。
- 0 — `SELECT`は例外をスローします。
## azure_list_object_keys_size {#azure_list_object_keys_size}

Type: UInt64

Default value: 1000

ListObjectリクエストでバッチで返される可能性があるファイルの最大数です。
## azure_max_blocks_in_multipart_upload {#azure_max_blocks_in_multipart_upload}

Type: UInt64

Default value: 50000

Azure用のマルチパートアップロードにおけるブロックの最大数です。
## azure_max_inflight_parts_for_one_file {#azure_max_inflight_parts_for_one_file}

Type: UInt64

Default value: 20

マルチパートアップロードリクエストで同時にロードされるパーツの最大数です。0は無制限を意味します。
## azure_max_single_part_copy_size {#azure_max_single_part_copy_size}

Type: UInt64

Default value: 268435456

単一パートコピーを使用してAzure Blobストレージにコピーするオブジェクトの最大サイズです。
## azure_max_single_part_upload_size {#azure_max_single_part_upload_size}

Type: UInt64

Default value: 104857600

単一パートアップロードを使用してAzure Blobストレージにアップロードするオブジェクトの最大サイズです。
## azure_max_single_read_retries {#azure_max_single_read_retries}

Type: UInt64

Default value: 4

単一のAzure Blobストレージの読み取りにおける最大再試行回数です。
## azure_max_unexpected_write_error_retries {#azure_max_unexpected_write_error_retries}

Type: UInt64

Default value: 4

Azure Blobストレージの書き込み中に予期しないエラーが発生した場合の最大再試行回数です。
## azure_max_upload_part_size {#azure_max_upload_part_size}

Type: UInt64

Default value: 5368709120

Azure Blobストレージにマルチパートアップロードを行う際のパートの最大サイズです。
## azure_min_upload_part_size {#azure_min_upload_part_size}

Type: UInt64

Default value: 16777216

Azure Blobストレージにマルチパートアップロードを行う際のパートの最小サイズです。
## azure_sdk_max_retries {#azure_sdk_max_retries}

Type: UInt64

Default value: 10

Azure SDKにおける最大再試行回数です。
## azure_sdk_retry_initial_backoff_ms {#azure_sdk_retry_initial_backoff_ms}

Type: UInt64

Default value: 10

Azure SDKでの再試行間の最小バックオフです。
## azure_sdk_retry_max_backoff_ms {#azure_sdk_retry_max_backoff_ms}

Type: UInt64

Default value: 1000

Azure SDKでの再試行間の最大バックオフです。
## azure_skip_empty_files {#azure_skip_empty_files}

Type: Bool

Default value: 0

S3エンジンで空のファイルをスキップするかどうかを有効または無効にします。

可能な値:
- 0 — 空のファイルがリクエストされた形式と互換性がない場合、`SELECT`は例外をスローします。
- 1 — 空のファイルに対して、`SELECT`は空の結果を返します。
## azure_strict_upload_part_size {#azure_strict_upload_part_size}

Type: UInt64

Default value: 0

Azure Blobストレージにマルチパートアップロードを行う際のパートの正確なサイズです。
## azure_throw_on_zero_files_match {#azure_throw_on_zero_files_match}

Type: Bool

Default value: 0

グロブ展開ルールに従って一致するファイルがゼロの場合にエラーをスローします。

可能な値:
- 1 — `SELECT`は例外をスローします。
- 0 — `SELECT`は空の結果を返します。
## azure_truncate_on_insert {#azure_truncate_on_insert}

Type: Bool

Default value: 0

Azureエンジンテーブルでの挿入前のトランケートを有効または無効にします。
## azure_upload_part_size_multiply_factor {#azure_upload_part_size_multiply_factor}

Type: UInt64

Default value: 2

azure_multiply_parts_count_thresholdのパーツが単一の書き込みからAzure Blobストレージにアップロードされるたびに、azure_min_upload_part_sizeをこの係数で乗算します。
## azure_upload_part_size_multiply_parts_count_threshold {#azure_upload_part_size_multiply_parts_count_threshold}

Type: UInt64

Default value: 500

この数のパーツがAzure Blobストレージにアップロードされるたびに、azure_min_upload_part_sizeがazure_upload_part_size_multiply_factorで乗算されます。
## backup_restore_batch_size_for_keeper_multi {#backup_restore_batch_size_for_keeper_multi}

Type: UInt64

Default value: 1000

バックアップまたは復元中の[Zoo]Keeperへのマルチリクエストの最大バッチサイズです。
## backup_restore_batch_size_for_keeper_multiread {#backup_restore_batch_size_for_keeper_multiread}

Type: UInt64

Default value: 10000

バックアップまたは復元中の[Zoo]Keeperへのマルチリードリクエストの最大バッチサイズです。
## backup_restore_failure_after_host_disconnected_for_seconds {#backup_restore_failure_after_host_disconnected_for_seconds}

Type: UInt64

Default value: 3600

BACKUP ON CLUSTERまたはRESTORE ON CLUSTER操作中にホストがこの量の時間、ZooKeeper内の一時的な「alive」ノードを再生成しない場合、全体のバックアップまたは復元は失敗と見なされます。
この値はホストが失敗後にZooKeeperに再接続するために合理的な時間よりも大きくする必要があります。
ゼロは無制限を意味します。
## backup_restore_finish_timeout_after_error_sec {#backup_restore_finish_timeout_after_error_sec}

Type: UInt64

Default value: 180

イニシエーターが、他のホストが「error」ノードに反応し、現在のBACKUP ON CLUSTERまたはRESTORE ON CLUSTER操作の作業を停止するまでに待機するべき時間です。
## backup_restore_keeper_fault_injection_probability {#backup_restore_keeper_fault_injection_probability}

Type: Float

Default value: 0

バックアップまたは復元中のキーパーリクエストに対する障害注入の確率です。妥当な値は[0.0f, 1.0f]の間です。
## backup_restore_keeper_fault_injection_seed {#backup_restore_keeper_fault_injection_seed}

Type: UInt64

Default value: 0

0 - ランダムシード、そうでない場合は設定値です。
## backup_restore_keeper_max_retries {#backup_restore_keeper_max_retries}

Type: UInt64

Default value: 1000

BACKUPまたはRESTORE操作中の[Zoo]Keeper操作の最大再試行回数です。
一時的な[Zoo]Keeperの失敗によって全体の操作が失敗しないようにするために十分大きくする必要があります。
## backup_restore_keeper_max_retries_while_handling_error {#backup_restore_keeper_max_retries_while_handling_error}

Type: UInt64

Default value: 20

BACKUP ON CLUSTERまたはRESTORE ON CLUSTER操作のエラーを処理している間の[Zoo]Keeper操作の最大再試行回数です。
## backup_restore_keeper_max_retries_while_initializing {#backup_restore_keeper_max_retries_while_initializing}

Type: UInt64

Default value: 20

BACKUP ON CLUSTERまたはRESTORE ON CLUSTER操作の初期化中の[Zoo]Keeper操作の最大再試行回数です。
## backup_restore_keeper_retry_initial_backoff_ms {#backup_restore_keeper_retry_initial_backoff_ms}

Type: UInt64

Default value: 100

バックアップまたは復元中の[Zoo]Keeper操作に対する初期のバックオフタイムアウトです。
## backup_restore_keeper_retry_max_backoff_ms {#backup_restore_keeper_retry_max_backoff_ms}

Type: UInt64

Default value: 5000

バックアップまたは復元中の[Zoo]Keeper操作に対する最大バックオフタイムアウトです。
## backup_restore_keeper_value_max_size {#backup_restore_keeper_value_max_size}

Type: UInt64

Default value: 1048576

バックアップ中の[Zoo]Keeperノードのデータの最大サイズです。
## backup_restore_s3_retry_attempts {#backup_restore_s3_retry_attempts}

Type: UInt64

Default value: 1000

Aws::Client::RetryStrategyの設定で、Aws::Clientは自ら再試行を行い、0は再試行なしを意味します。これはバックアップ/復元のみに適用されます。
## cache_warmer_threads {#cache_warmer_threads}

Type: UInt64

Default value: 4

ClickHouse Cloudのみで使用可能です。[cache_populated_by_fetch](merge-tree-settings.md/#cache_populated_by_fetch)が有効になっている場合に、ファイルキャッシュに新しいデータパーツを投機的にダウンロードするためのバックグラウンドスレッドの数です。ゼロは無効を意味します。
## calculate_text_stack_trace {#calculate_text_stack_trace}

Type: Bool

Default value: 1

クエリ実行中の例外が発生した場合にテキストスタックトレースを計算します。これはデフォルトです。大量の間違ったクエリが実行される場合には、シンボルのルックアップが遅くなる可能性があります。通常のケースでは、このオプションを無効にすべきではありません。
## cancel_http_readonly_queries_on_client_close {#cancel_http_readonly_queries_on_client_close}

Type: Bool

Default value: 0

クライアントが応答を待たずに接続を閉じた場合、HTTPの読み取り専用クエリ（例: SELECT）をキャンセルします。

クラウドのデフォルト値: `1`。
## cast_ipv4_ipv6_default_on_conversion_error {#cast_ipv4_ipv6_default_on_conversion_error}

Type: Bool

Default value: 0

IPv4やIPV6型へのCAST演算子、およびtoIPv4、toIPv6関数は、変換エラーが発生した場合に例外をスローする代わりにデフォルト値を返します。
## cast_keep_nullable {#cast_keep_nullable}

Type: Bool

Default value: 0

[CAST](../../sql-reference/functions/type-conversion-functions.md/#castx-t)操作における`Nullable`データ型の保持を有効または無効にします。

設定が有効な場合、`CAST`関数の引数が`Nullable`の場合、結果も`Nullable`型に変換されます。設定が無効な場合、結果は常に指定された宛先タイプになります。

可能な値:

- 0 — `CAST`結果は指定された宛先タイプと正確に一致します。
- 1 — 引数の型が`Nullable`の場合、`CAST`結果は`Nullable(DestinationDataType)`に変換されます。

**例**

以下のクエリは宛先データ型と正確に一致します:

```sql
SET cast_keep_nullable = 0;
SELECT CAST(toNullable(toInt32(0)) AS Int32) as x, toTypeName(x);
```

結果:

```text
┌─x─┬─toTypeName(CAST(toNullable(toInt32(0)), 'Int32'))─┐
│ 0 │ Int32                                             │
└───┴───────────────────────────────────────────────────┘
```

以下のクエリは宛先データ型に`Nullable`の修飾が加わります:

```sql
SET cast_keep_nullable = 1;
SELECT CAST(toNullable(toInt32(0)) AS Int32) as x, toTypeName(x);
```

結果:

```text
┌─x─┬─toTypeName(CAST(toNullable(toInt32(0)), 'Int32'))─┐
│ 0 │ Nullable(Int32)                                   │
└───┴───────────────────────────────────────────────────┘
```

**関連情報**

- [CAST](../../sql-reference/functions/type-conversion-functions.md/#type_conversion_function-cast)関数
## cast_string_to_dynamic_use_inference {#cast_string_to_dynamic_use_inference}

Type: Bool

Default value: 0

文字列から動的への変換時に型推論を使用します。
## check_query_single_value_result {#check_query_single_value_result}

Type: Bool

Default value: 1

`MergeTree`ファミリーエンジンの[CHECK TABLE](../../sql-reference/statements/check-table.md/#checking-mergetree-tables)クエリ結果に対する詳細レベルを定義します。

可能な値:

- 0 — クエリはテーブルの各個別データパートのチェック状態を表示します。
- 1 — クエリは一般的なテーブルチェックの状態を表示します。
## check_referential_table_dependencies {#check_referential_table_dependencies}

Type: Bool

Default value: 0

DDLクエリ（DROP TABLEやRENAMEなど）が参照依存関係を壊さないことを確認します。
## check_table_dependencies {#check_table_dependencies}

Type: Bool

Default value: 1

DDLクエリ（DROP TABLEやRENAMEなど）が依存関係を壊さないことを確認します。
## checksum_on_read {#checksum_on_read}

Type: Bool

Default value: 1

読み取り時のチェックサムを検証します。デフォルトで有効であり、本番環境では常に有効にしておくべきです。この設定を無効にしてもメリットは期待できません。実験やベンチマークでのみ使用されるべきです。この設定はMergeTreeファミリーのテーブルにのみ適用されます。ネットワーク経由でデータを受信する際には、すべてのテーブルエンジンに対してチェックサムが常に検証されます。
## cloud_mode {#cloud_mode}

Type: Bool

Default value: 0

クラウドモード
## cloud_mode_database_engine {#cloud_mode_database_engine}

Type: UInt64

Default value: 1

Cloudで許可されているデータベースエンジン。1 - DDLをReplicatedデータベースを使用するように書き換え、2 - DDLをSharedデータベースを使用するように書き換えます。
## cloud_mode_engine {#cloud_mode_engine}

Type: UInt64

Default value: 1

Cloudで許可されているエンジンファミリー。0 - すべてを許可、1 - DDLを*ReplicatedMergeTreeを使用するように書き換え、2 - DDLをSharedMergeTreeを使用するように書き換えます。UInt64は公開部分を最小限にします。
## cluster_for_parallel_replicas {#cluster_for_parallel_replicas}
<BetaBadge/>

Type: String

Default value:

現在のサーバーが位置するシャードのクラスタです。
## collect_hash_table_stats_during_aggregation {#collect_hash_table_stats_during_aggregation}

Type: Bool

Default value: 1

メモリの割り当てを最適化するためにハッシュテーブルの統計を収集することを有効にします。
## collect_hash_table_stats_during_joins {#collect_hash_table_stats_during_joins}

Type: Bool

Default value: 1

メモリの割り当てを最適化するためにハッシュテーブルの統計を収集することを有効にします。
## compatibility {#compatibility}

Type: String

Default value:

`compatibility`設定により、ClickHouseは指定された以前のバージョンのClickHouseのデフォルト設定を使用します。

設定が非デフォルト値に設定されている場合、その設定は尊重されます（変更されていない設定のみが`compatibility`設定の影響を受けます）。

この設定は、`22.3`、`22.8`のような文字列形式のClickHouseバージョン番号を受け取ります。空の値は、この設定が無効であることを意味します。

デフォルトでは無効になっています。

:::note
ClickHouse Cloudでは、互換性設定はClickHouse Cloudサポートによって設定される必要があります。設定してもらうには[ケースをオープン](https://clickhouse.cloud/support)してください。
:::
## compatibility_ignore_auto_increment_in_create_table {#compatibility_ignore_auto_increment_in_create_table}

Type: Bool

Default value: 0

真の場合、カラム宣言内のAUTO_INCREMENTキーワードを無視します。そうでない場合はエラーを返します。MySQLからの移行を簡素化します。
## compatibility_ignore_collation_in_create_table {#compatibility_ignore_collation_in_create_table}

Type: Bool

Default value: 1

CREATE TABLE時の照合順序を無視します。
## compile_aggregate_expressions {#compile_aggregate_expressions}

Type: Bool

Default value: 1

集約関数のJITコンパイルをネイティブコードに対して有効または無効にします。この設定を有効にするとパフォーマンスが向上する可能性があります。

可能な値:

- 0 — 集約はJITコンパイルなしで行われます。
- 1 — 集約はJITコンパイルを使用して行われます。

**関連情報**

- [min_count_to_compile_aggregate_expression](#min_count_to_compile_aggregate_expression)
## compile_expressions {#compile_expressions}

Type: Bool

Default value: 0

一部のスカラ関数と演算子をネイティブコードにコンパイルします。LLVMコンパイラインフラのバグにより、AArch64マシンではnullptr参照の原因となり、結果としてサーバーがクラッシュすることが知られています。この設定を有効にしないでください。
## compile_sort_description {#compile_sort_description}

Type: Bool

Default value: 1

ソート説明をネイティブコードにコンパイルします。
## connect_timeout {#connect_timeout}

Type: 秒

Default value: 10

レプリカがない場合の接続タイムアウトです。
## connect_timeout_with_failover_ms {#connect_timeout_with_failover_ms}

Type: ミリ秒

Default value: 1000

クラスタ定義で「シャード」および「レプリカ」セクションが使用されている場合、分散テーブルエンジンのリモートサーバーへの接続のタイムアウト（ミリ秒）です。
接続に失敗した場合、さまざまなレプリカへの接続が何度も試みられます。
## connect_timeout_with_failover_secure_ms {#connect_timeout_with_failover_secure_ms}

Type: ミリ秒

Default value: 1000

最初の正常なレプリカを選択するための接続タイムアウト（セキュア接続用）。
## connection_pool_max_wait_ms {#connection_pool_max_wait_ms}

Type: ミリ秒

Default value: 0

接続プールがいっぱいのときの接続待機時間（ミリ秒）。

可能な値:

- 正の整数。
- 0 — 無限のタイムアウト。
## connections_with_failover_max_tries {#connections_with_failover_max_tries}

Type: UInt64

Default value: 3

分散テーブルエンジンの各レプリカへの接続試行の最大回数です。
## convert_query_to_cnf {#convert_query_to_cnf}

Type: Bool

Default value: 0

`true`に設定すると、`SELECT`クエリは共起正常形（CNF）に変換されます。CNFでクエリを書き換えることで、実行が速くなるシナリオがあります（この[Githubの問題](https://github.com/ClickHouse/ClickHouse/issues/11749)を参照してください）。

以下の `SELECT` クエリが修正されていないことに注目してください（デフォルトの動作）:

```sql
EXPLAIN SYNTAX
SELECT *
FROM
(
    SELECT number AS x
    FROM numbers(20)
) AS a
WHERE ((x >= 1) AND (x <= 5)) OR ((x >= 10) AND (x <= 15))
SETTINGS convert_query_to_cnf = false;
```

結果は次のとおりです:

```response
┌─explain────────────────────────────────────────────────────────┐
│ SELECT x                                                       │
│ FROM                                                           │
│ (                                                              │
│     SELECT number AS x                                         │
│     FROM numbers(20)                                           │
│     WHERE ((x >= 1) AND (x <= 5)) OR ((x >= 10) AND (x <= 15)) │
│ ) AS a                                                         │
│ WHERE ((x >= 1) AND (x <= 5)) OR ((x >= 10) AND (x <= 15))     │
│ SETTINGS convert_query_to_cnf = 0                              │
└────────────────────────────────────────────────────────────────┘
```

`convert_query_to_cnf`を`true`に設定し、どのように変わるか見てみましょう:

```sql
EXPLAIN SYNTAX
SELECT *
FROM
(
    SELECT number AS x
    FROM numbers(20)
) AS a
WHERE ((x >= 1) AND (x <= 5)) OR ((x >= 10) AND (x <= 15))
SETTINGS convert_query_to_cnf = true;
```

`WHERE`句がCNFに書き換えられていることに注目してくださいが、結果セットは同じでありブール論理は変更されていません:

```response
┌─explain───────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ SELECT x                                                                                                              │
│ FROM                                                                                                                  │
│ (                                                                                                                     │
│     SELECT number AS x                                                                                                │
│     FROM numbers(20)                                                                                                  │
│     WHERE ((x <= 15) OR (x <= 5)) AND ((x <= 15) OR (x >= 1)) AND ((x >= 10) OR (x <= 5)) AND ((x >= 10) OR (x >= 1)) │
│ ) AS a                                                                                                                │
│ WHERE ((x >= 10) OR (x >= 1)) AND ((x >= 10) OR (x <= 5)) AND ((x <= 15) OR (x >= 1)) AND ((x <= 15) OR (x <= 5))     │
│ SETTINGS convert_query_to_cnf = 1                                                                                     │
└───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
```

可能な値: true, false
## count_distinct_implementation {#count_distinct_implementation}

Type: String

Default value: uniqExact

COUNT(DISTINCT ...)を実行するために使用される`uniq*`関数を指定します。

可能な値:

- [uniq](../../sql-reference/aggregate-functions/reference/uniq.md/#agg_function-uniq)
- [uniqCombined](../../sql-reference/aggregate-functions/reference/uniqcombined.md/#agg_function-uniqcombined)
- [uniqCombined64](../../sql-reference/aggregate-functions/reference/uniqcombined64.md/#agg_function-uniqcombined64)
- [uniqHLL12](../../sql-reference/aggregate-functions/reference/uniqhll12.md/#agg_function-uniqhll12)
- [uniqExact](../../sql-reference/aggregate-functions/reference/uniqexact.md/#agg_function-uniqexact)
## count_distinct_optimization {#count_distinct_optimization}

Type: Bool

Default value: 0

count distinctをグループ化のサブクエリに書き換えます。
## create_if_not_exists {#create_if_not_exists}

Type: Bool

Default value: 0

`CREATE`ステートメントにデフォルトで`IF NOT EXISTS`を有効にします。この設定または`IF NOT EXISTS`のいずれかが指定され、指定された名前のテーブルがすでに存在する場合、例外はスローされません。
## create_index_ignore_unique {#create_index_ignore_unique}

Type: Bool

Default value: 0

CREATE UNIQUE INDEXにおけるUNIQUEキーワードを無視します。SQL互換性テストのために作成されました。
## create_replicated_merge_tree_fault_injection_probability {#create_replicated_merge_tree_fault_injection_probability}

Type: Float

Default value: 0

メタデータをZooKeeperに作成した後のテーブル作成中の障害注入の確率です。
## create_table_empty_primary_key_by_default {#create_table_empty_primary_key_by_default}

Type: Bool

Default value: 0

ORDER BYおよびPRIMARY KEYが指定されていない場合に、*MergeTreeテーブルを空の主キーで作成することを許可します。
## cross_join_min_bytes_to_compress {#cross_join_min_bytes_to_compress}

Type: UInt64

Default value: 1073741824

CROSS JOINで圧縮するためのブロックの最小サイズです。ゼロの値はこのしきい値を無効にすることを意味します。行またはバイトのいずれかのしきい値に達したときに、このブロックが圧縮されます。
## cross_join_min_rows_to_compress {#cross_join_min_rows_to_compress}

Type: UInt64

Default value: 10000000

CROSS JOINでブロックを圧縮するための最小行数です。ゼロの値はこのしきい値を無効にすることを意味します。行またはバイトのいずれかのしきい値に達したときに、このブロックが圧縮されます。
## data_type_default_nullable {#data_type_default_nullable}

Type: Bool

Default value: 0

カラム定義内で明示的な修飾子[NULLまたはNOT NULL](../../sql-reference/statements/create/table.md/#null-modifiers)がなくてもデータ型を[Nullable](../../sql-reference/data-types/nullable.md/#data_type-nullable)にすることを許可します。

可能な値:

- 1 — カラム定義のデータ型はデフォルトで`Nullable`に設定されます。
- 0 — カラム定義のデータ型はデフォルトで`Nullable`ではなく設定されます。
## database_atomic_wait_for_drop_and_detach_synchronously {#database_atomic_wait_for_drop_and_detach_synchronously}

Type: Bool

Default value: 0

すべての`DROP`および`DETACH`クエリに`SYNC`修飾子を追加します。

可能な値:

- 0 — クエリは遅延して実行されます。
- 1 — クエリは遅延せずに実行されます。
## database_replicated_allow_explicit_uuid {#database_replicated_allow_explicit_uuid}

Type: UInt64

Default value: 0

0 - 複製データベースのテーブルにUUIDを明示的に指定することを許可しません。1 - 許可します。2 - 許可しますが、指定されたUUIDを無視してランダムに生成します。
## database_replicated_allow_heavy_create {#database_replicated_allow_heavy_create}

Type: Bool

Default value: 0

複製データベースエンジンでの長時間実行されるDDLクエリ（CREATE AS SELECTやPOPULATE）の実行を許可します。これはDDLキューを長時間ブロックする可能性があります。
## database_replicated_allow_only_replicated_engine {#database_replicated_allow_only_replicated_engine}

Type: Bool

Default value: 0

複製データベースエンジンを持つデータベースで複製テーブルのみを作成することを許可します。
## database_replicated_allow_replicated_engine_arguments {#database_replicated_allow_replicated_engine_arguments}

Type: UInt64

Default value: 0

0 - 複製データベースの*MergeTreeテーブルに対してZooKeeperパスとレプリカ名を明示的に指定することを許可しません。1 - 許可します。2 - 許可しますが、指定されたパスを無視してデフォルトのものを使用します。3 - 許可し、警告をログに記録しません。
## database_replicated_always_detach_permanently {#database_replicated_always_detach_permanently}

Type: Bool

Default value: 0

データベースエンジンが複製されている場合、DETACH TABLEをDETACH TABLE PERMANENTLYとして実行します。
## database_replicated_enforce_synchronous_settings {#database_replicated_enforce_synchronous_settings}

Type: Bool

Default value: 0

いくつかのクエリの同期待機を強制します（database_atomic_wait_for_drop_and_detach_synchronously、mutation_sync、alter_syncを参照）。これらの設定を有効にすることは推奨されません。
## database_replicated_initial_query_timeout_sec {#database_replicated_initial_query_timeout_sec}

Type: UInt64

Default value: 300

最初のDDLクエリが複製データベースに対して前のDDLキューエントリを処理するのを待つ時間（秒）を設定します。

可能な値:

- 正の整数。
- 0 — 無制限。
## decimal_check_overflow {#decimal_check_overflow}

Type: Bool

Default value: 1

10進数の算術/比較操作のオーバーフローをチェックします。
## deduplicate_blocks_in_dependent_materialized_views {#deduplicate_blocks_in_dependent_materialized_views}

Type: Bool

Default value: 0

複製テーブルからデータを受け取るマテリアライズドビューの重複チェックを有効または無効にします。

可能な値:

      0 — 無効。
      1 — 有効。

使用法

デフォルトでは、マテリアライズドビューのための重複排除は行われませんが、ソーステーブルで行われます。
挿入されたブロックがソーステーブルでの重複排除によりスキップされると、添付されたマテリアライズドビューには挿入されません。この動作は、ソーステーブルへの異なるINSERTから派生したブロックが、マテリアライズドビューで集約された後に同一であるケースを処理するために、高度に集約されたデータをマテリアライズドビューに挿入できるようにするために存在します。
同時に、この動作は`INSERT`の冪等性を「壊します」。メインテーブルへの`INSERT`が成功し、マテリアライズドビューへの`INSERT`が失敗した場合（例: ClickHouse Keeperとの通信失敗）、クライアントはエラーを取得し、操作を再試行できます。しかし、マテリアライズドビューは最初の失敗により重複排除によって二度目の挿入を受け取らないことになります。設定`deduplicate_blocks_in_dependent_materialized_views`はこの動作を変更することを許可します。再試行時、マテリアライズドビューは再度挿入を受け取り、ソーステーブルのチェック結果を無視して自ら重複排除チェックを行い、最初の失敗により失われた行を挿入します。
## default_materialized_view_sql_security {#default_materialized_view_sql_security}

Type: SQLSecurityType

Default value: DEFINER

マテリアライズドビューを作成する際にSQL SECURITYオプションのデフォルト値を設定することを許可します。[SQLセキュリティに関する詳細](../../sql-reference/statements/create/view.md/#sql_security)。

デフォルト値は`DEFINER`です。
## default_max_bytes_in_join {#default_max_bytes_in_join}

Type: UInt64

Default value: 1000000000

制限が必要だがmax_bytes_in_joinが設定されていない場合の、右側のテーブルの最大サイズです。
## default_normal_view_sql_security {#default_normal_view_sql_security}

Type: SQLSecurityType

Default value: INVOKER

通常のビューを作成する際のデフォルトの`SQL SECURITY`オプションを設定することを許可します。[SQLセキュリティに関する詳細](../../sql-reference/statements/create/view.md/#sql_security)。

デフォルト値は`INVOKER`です。
```
## default_table_engine {#default_table_engine}

タイプ: DefaultTableEngine

デフォルト値: MergeTree

`CREATE` ステートメントに `ENGINE` が設定されていない場合に使用されるデフォルトのテーブルエンジン。

可能な値:

- 有効なテーブルエンジン名を表す文字列

クラウドデフォルト値: `SharedMergeTree`。

**例**

クエリ:

```sql
SET default_table_engine = 'Log';

SELECT name, value, changed FROM system.settings WHERE name = 'default_table_engine';
```

結果:

```response
┌─name─────────────────┬─value─┬─changed─┐
│ default_table_engine │ Log   │       1 │
└──────────────────────┴───────┴─────────┘
```

この例では、`Engine` を指定しない新しいテーブルは `Log` テーブルエンジンを使用します:

クエリ:

```sql
CREATE TABLE my_table (
    x UInt32,
    y UInt32
);

SHOW CREATE TABLE my_table;
```

結果:

```response
┌─statement────────────────────────────────────────────────────────────────┐
│ CREATE TABLE default.my_table
(
    `x` UInt32,
    `y` UInt32
)
ENGINE = Log
└──────────────────────────────────────────────────────────────────────────┘
```
## default_temporary_table_engine {#default_temporary_table_engine}

タイプ: DefaultTableEngine

デフォルト値: Memory

一時テーブル用の[default_table_engine](#default_table_engine)と同様です。

この例では、`Engine` を指定しない新しい一時テーブルは `Log` テーブルエンジンを使用します:

クエリ:

```sql
SET default_temporary_table_engine = 'Log';

CREATE TEMPORARY TABLE my_table (
    x UInt32,
    y UInt32
);

SHOW CREATE TEMPORARY TABLE my_table;
```

結果:

```response
┌─statement────────────────────────────────────────────────────────────────┐
│ CREATE TEMPORARY TABLE default.my_table
(
    `x` UInt32,
    `y` UInt32
)
ENGINE = Log
└──────────────────────────────────────────────────────────────────────────┘
```
## default_view_definer {#default_view_definer}

タイプ: String

デフォルト値: CURRENT_USER

ビュー作成時のデフォルト `DEFINER` オプションを設定できます。 [SQL セキュリティの詳細](../../sql-reference/statements/create/view.md/#sql_security)。

デフォルト値は `CURRENT_USER` です。

## describe_compact_output {#describe_compact_output}

タイプ: Bool

デフォルト値: 0

真の場合、DESCRIBE クエリの結果にカラム名とタイプのみを含めます。

## describe_extend_object_types {#describe_extend_object_types}

タイプ: Bool

デフォルト値: 0

DESCRIBE クエリにおいて Object 型のカラムの具体的な型を推測します。

## describe_include_subcolumns {#describe_include_subcolumns}

タイプ: Bool

デフォルト値: 0

[DESCRIBE](../../sql-reference/statements/describe-table.md) クエリのサブカラムの記述を有効にします。例えば、[Tuple](../../sql-reference/data-types/tuple.md) のメンバーや、[Map](../../sql-reference/data-types/map.md/#map-subcolumns) のサブカラム、[Nullable](../../sql-reference/data-types/nullable.md/#finding-null) や [Array](../../sql-reference/data-types/array.md/#array-size) データ型のサブカラムです。

可能な値:

- 0 — サブカラムは `DESCRIBE` クエリには含まれません。
- 1 — サブカラムは `DESCRIBE` クエリに含まれます。

**例**

[DESCRIBE](../../sql-reference/statements/describe-table.md) ステートメントの例を参照してください。

## describe_include_virtual_columns {#describe_include_virtual_columns}

タイプ: Bool

デフォルト値: 0

真の場合、テーブルの仮想カラムは DESCRIBE クエリの結果に含まれます。

## dialect {#dialect}

タイプ: Dialect

デフォルト値: clickhouse

クエリを解析する際に使用されるダイアレクト。

## dictionary_validate_primary_key_type {#dictionary_validate_primary_key_type}

タイプ: Bool

デフォルト値: 0

辞書のために主キーの型を検証します。デフォルトでは、単純なレイアウトに対しては id 型は UInt64 に明示的に変換されます。

## distinct_overflow_mode {#distinct_overflow_mode}

タイプ: OverflowMode

デフォルト値: throw

制限が超過した場合に何をするか。

## distributed_aggregation_memory_efficient {#distributed_aggregation_memory_efficient}

タイプ: Bool

デフォルト値: 1

分散集約のメモリ節約モードが有効になっていますか。

## distributed_background_insert_batch {#distributed_background_insert_batch}

タイプ: Bool

デフォルト値: 0

挿入データをバッチで送信することを有効または無効にします。

バッチ送信が有効な場合、[Distributed](../../engines/table-engines/special/distributed.md) テーブルエンジンは、別々に送信するのではなく、1回の操作で挿入データの複数のファイルを送信しようとします。バッチ送信は、サーバーとネットワークリソースをより効率的に活用することにより、クラスタのパフォーマンスを向上させます。

可能な値:

- 1 — 有効。
- 0 — 無効。

## distributed_background_insert_max_sleep_time_ms {#distributed_background_insert_max_sleep_time_ms}

タイプ: ミリ秒

デフォルト値: 30000

[Distributed](../../engines/table-engines/special/distributed.md) テーブルエンジンがデータを送信するための最大間隔。 [distributed_background_insert_sleep_time_ms](#distributed_background_insert_sleep_time_ms) 設定で設定された間隔の指数関数的成長を制限します。

可能な値:

- 正の整数ミリ秒。

## distributed_background_insert_sleep_time_ms {#distributed_background_insert_sleep_time_ms}

タイプ: ミリ秒

デフォルト値: 100

[Distributed](../../engines/table-engines/special/distributed.md) テーブルエンジンがデータを送信するための基準間隔。エラーが発生した場合、実際の間隔は指数関数的に増加します。

可能な値:

- 正の整数ミリ秒。

## distributed_background_insert_split_batch_on_failure {#distributed_background_insert_split_batch_on_failure}

タイプ: Bool

デフォルト値: 0

失敗したバッチの分割を有効または無効にします。

特定のバッチをリモートシャードに送信する際に失敗することがあります。これは、`MATERIALIZED VIEW` と `GROUP BY` の後の複雑なパイプラインによるもので、例えば `Memory limit exceeded` や類似のエラーが原因です。この場合、再試行しても役に立たないことがありますが、そのバッチからファイルを1つずつ送信すれば `INSERT` が成功する可能性があります。

したがって、設定を `1` にすることで、失敗したバッチに対してバッチ処理を無効にします（すなわち、失敗したバッチに対しては `distributed_background_insert_batch` を一時的に無効にします）。

可能な値:

- 1 — 有効。
- 0 — 無効。

:::note
この設定は、異常なサーバー（マシン）の終了や、[Distributed](../../engines/table-engines/special/distributed.md) テーブルエンジンに対する `fsync_after_insert` / `fsync_directories` がない場合に発生する可能性のある壊れたバッチにも影響します。
:::

:::note
自動バッチ分割に依存しないようにしてください。これはパフォーマンスを損なう可能性があります。
:::

## distributed_background_insert_timeout {#distributed_background_insert_timeout}

タイプ: UInt64

デフォルト値: 0

分散インサートクエリのタイムアウト。この設定は、insert_distributed_sync が有効な場合にのみ使用されます。ゼロの値はタイムアウトなしを意味します。

## distributed_cache_bypass_connection_pool {#distributed_cache_bypass_connection_pool}

タイプ: Bool

デフォルト値: 0

ClickHouse Cloud のみ。 分散キャッシュ接続プールをバイパスすることを許可します。

## distributed_cache_connect_max_tries {#distributed_cache_connect_max_tries}

タイプ: UInt64

デフォルト値: 20

ClickHouse Cloud のみ。接続に失敗した場合の分散キャッシュへの接続試行回数。

## distributed_cache_data_packet_ack_window {#distributed_cache_data_packet_ack_window}

タイプ: UInt64

デフォルト値: 5

ClickHouse Cloud のみ。単一の分散キャッシュ読み取りリクエストにおける DataPacket シーケンスの ACK 送信のウィンドウ。

## distributed_cache_discard_connection_if_unread_data {#distributed_cache_discard_connection_if_unread_data}

タイプ: Bool

デフォルト値: 1

ClickHouse Cloud のみ。いくつかのデータが未読である場合、接続を破棄します。

## distributed_cache_fetch_metrics_only_from_current_az {#distributed_cache_fetch_metrics_only_from_current_az}

タイプ: Bool

デフォルト値: 1

ClickHouse Cloud のみ。system.distributed_cache_metrics、system.distributed_cache_events から現在のアベイラビリティゾーンのメトリクスのみを取得します。

## distributed_cache_log_mode {#distributed_cache_log_mode}

タイプ: DistributedCacheLogMode

デフォルト値: on_error

ClickHouse Cloud のみ。system.distributed_cache_log への書き込みモード。

## distributed_cache_max_unacked_inflight_packets {#distributed_cache_max_unacked_inflight_packets}

タイプ: UInt64

デフォルト値: 10

ClickHouse Cloud のみ。単一の分散キャッシュ読み取りリクエストにおける確認されていない最大数のインフライトパケット。

## distributed_cache_min_bytes_for_seek {#distributed_cache_min_bytes_for_seek}

タイプ: Bool

デフォルト値: 0

ClickHouse Cloud のみ。分散キャッシュでシークするための最小バイト数。

## distributed_cache_pool_behaviour_on_limit {#distributed_cache_pool_behaviour_on_limit}

タイプ: DistributedCachePoolBehaviourOnLimit

デフォルト値: wait

ClickHouse Cloud のみ。プール制限に達したときの分散キャッシュ接続の動作を識別します。

## distributed_cache_read_alignment {#distributed_cache_read_alignment}

タイプ: UInt64

デフォルト値: 0

ClickHouse Cloud のみ。テスト目的の設定、変更しないでください。

## distributed_cache_receive_response_wait_milliseconds {#distributed_cache_receive_response_wait_milliseconds}

タイプ: UInt64

デフォルト値: 60000

ClickHouse Cloud のみ。分散キャッシュからのリクエストのデータを受信するために待機する時間（ミリ秒）。

## distributed_cache_receive_timeout_milliseconds {#distributed_cache_receive_timeout_milliseconds}

タイプ: UInt64

デフォルト値: 10000

ClickHouse Cloud のみ。分散キャッシュからのあらゆる種類の応答を受け取るために待機する時間（ミリ秒）。

## distributed_cache_throw_on_error {#distributed_cache_throw_on_error}

タイプ: Bool

デフォルト値: 0

ClickHouse Cloud のみ。分散キャッシュとの通信中に発生した例外または、分散キャッシュから受け取った例外を再スローします。それ以外は、エラー時に分散キャッシュをスキップするようにフォールバックします。

## distributed_cache_wait_connection_from_pool_milliseconds {#distributed_cache_wait_connection_from_pool_milliseconds}

タイプ: UInt64

デフォルト値: 100

ClickHouse Cloud のみ。分散キャッシュプールの制限が待機する場合に、接続を受け取るために待機する時間（ミリ秒）。

## distributed_connections_pool_size {#distributed_connections_pool_size}

タイプ: UInt64

デフォルト値: 1024

単一の Distributed テーブルに対するすべてのクエリを分散処理のためにリモートサーバーと同時に接続できる最大数。クラスタ内のサーバー数以上の値を設定することをお勧めします。

## distributed_ddl_entry_format_version {#distributed_ddl_entry_format_version}

タイプ: UInt64

デフォルト値: 5

分散 DDL (ON CLUSTER) クエリの互換性バージョン。

## distributed_ddl_output_mode {#distributed_ddl_output_mode}

タイプ: DistributedDDLOutputMode

デフォルト値: throw

分散 DDL クエリの結果の形式を設定します。

可能な値:

- `throw` — クエリが完了したすべてのホストに対してクエリ実行ステータスを持つ結果セットを返します。クエリが一部のホストで失敗した場合、最初の例外が再スローされます。一部のホストでまだクエリが完了していない場合、及び [distributed_ddl_task_timeout](#distributed_ddl_task_timeout) を超えた場合、`TIMEOUT_EXCEEDED` 例外をスローします。
- `none` — throw と類似していますが、分散 DDL クエリは結果セットを返しません。
- `null_status_on_timeout` — 一部の結果セットの行において、`TIMEOUT_EXCEEDED` をスローするのではなく、実行ステータスを`NULL` として返します。
- `never_throw` — `TIMEOUT_EXCEEDED` をスローせず、クエリが一部のホストで失敗した場合に例外を再スローしません。
- `none_only_active` - `none ` に似ていますが、`Replicated` データベースの非アクティブなレプリカを待機しません。このモードでは、一部のレプリカでクエリが実行されなかったことを把握することができず、バックグラウンドで実行されることになります。
- `null_status_on_timeout_only_active` — `null_status_on_timeout` と似ていますが、`Replicated` データベースの非アクティブなレプリカを待機しません。
- `throw_only_active` — `throw` と似ていますが、`Replicated` データベースの非アクティブなレプリカを待機しません。

クラウドデフォルト値: `none`。

## distributed_ddl_task_timeout {#distributed_ddl_task_timeout}

タイプ: Int64

デフォルト値: 180

クラスタ内のすべてのホストからの DDL クエリ応答のタイムアウトを設定します。DDL リクエストがすべてのホストで行われていない場合、応答にはタイムアウトエラーが含まれ、リクエストは非同期モードで実行されます。負の値は無限を意味します。

可能な値:

- 正の整数。
- 0 — 非同期モード。
- 負の整数 — 無限のタイムアウト。

## distributed_foreground_insert {#distributed_foreground_insert}

タイプ: Bool

デフォルト値: 0

[Distributed](../../engines/table-engines/special/distributed.md/#distributed) テーブルへの同期データ挿入を有効または無効にします。

デフォルトでは、Distributed テーブルにデータを挿入すると、ClickHouse サーバーはデータをバックグラウンドモードでクラスタノードに送信します。`distributed_foreground_insert=1` の場合、データは同期的に処理され、`INSERT` 操作はすべてのシャードでデータが保存された後に成功します（`internal_replication` が true の場合は、各シャードの少なくとも1つのレプリカが必要です）。

可能な値:

- 0 — データはバックグラウンドモードで挿入されます。
- 1 — データは同期モードで挿入されます。

クラウドデフォルト値: `1`。

**関連情報**

- [Distributed Table Engine](../../engines/table-engines/special/distributed.md/#distributed)
- [Managing Distributed Tables](../../sql-reference/statements/system.md/#query-language-system-distributed)

## distributed_group_by_no_merge {#distributed_group_by_no_merge}

タイプ: UInt64

デフォルト値: 0

分散クエリ処理のために異なるサーバーからの集約状態をマージしないでください。異なるシャードに異なるキーがある場合、これを使用できます。

可能な値:

- `0` — 無効（最終クエリ処理は始動ノードで行われます）。
- `1` - 異なるサーバーからの集約状態を分散クエリ処理でマージしません（クエリはシャードで完全に処理され、始動者はデータをプロキシするだけです）。異なるシャードに異なるキーがある場合に使用できます。
- `2` - `1` と同様ですが、`ORDER BY` と `LIMIT` を適用します（`distributed_group_by_no_merge=1` の場合のように、リモートノード上でクエリが完全に処理されると不可能ですが）始動者に対して適用されます（`ORDER BY` または `LIMIT` を伴うクエリに使用できます）。

**例**

```sql
SELECT *
FROM remote('127.0.0.{2,3}', system.one)
GROUP BY dummy
LIMIT 1
SETTINGS distributed_group_by_no_merge = 1
FORMAT PrettyCompactMonoBlock

┌─dummy─┐
│     0 │
│     0 │
└───────┘
```

```sql
SELECT *
FROM remote('127.0.0.{2,3}', system.one)
GROUP BY dummy
LIMIT 1
SETTINGS distributed_group_by_no_merge = 2
FORMAT PrettyCompactMonoBlock

┌─dummy─┐
│     0 │
└───────┘
```

## distributed_insert_skip_read_only_replicas {#distributed_insert_skip_read_only_replicas}

タイプ: Bool

デフォルト値: 0

分散の INSERT クエリに対して読み取り専用レプリカをスキップすることを有効にします。

可能な値:

- 0 — 通常通り INSERT が行われ、読み取り専用レプリカに送信されると失敗します。
- 1 — 始動者はシャードにデータを送信する前に読み取り専用レプリカをスキップします。

## distributed_product_mode {#distributed_product_mode}

タイプ: DistributedProductMode

デフォルト値: deny

[分散サブクエリ](../../sql-reference/operators/in.md)の動作を変更します。

クエリに分散テーブルの生成物が含まれている場合（すなわち、分散テーブルに対するクエリが分散テーブルの非 GLOBAL サブクエリを含む場合）、ClickHouse はこの設定を適用します。

制限：

- IN および JOIN サブクエリに対してのみ適用されます。
- FROM セクションが 1 以上のシャードを持つ分散テーブルを使用している場合。
- サブクエリが 1 以上のシャードを持つ分散テーブルに関係している場合。
- テーブル値を持つ[remote](../../sql-reference/table-functions/remote.md)関数に対しては使用されません。

可能な値:

- `deny` — デフォルト値。一部のサブクエリタイプの使用を禁止します（“Double-distributed in/JOIN subqueries is denied” 例外を返します）。
- `local` — 目的のサーバー（シャード）に対して、サブクエリ内のデータベースとテーブルをローカルのものに置き換えます（通常の `IN` / `JOIN` を残します）。
- `global` — `IN` / `JOIN` クエリを `GLOBAL IN` / `GLOBAL JOIN` に置き換えます。
- `allow` — これらの種類のサブクエリの使用を許可します。

## distributed_push_down_limit {#distributed_push_down_limit}

タイプ: UInt64

デフォルト値: 1

各シャードに対して [LIMIT](#limit) を適用することを有効または無効にします。

これにより、次のことを回避できます:
- ネットワーク経由の余分な行の送信。
- 始動者に対して LIMIT の背後にある行の処理。

21.9 バージョン以降、次の条件のいずれかが満たされている場合にのみ、`distributed_push_down_limit` がクエリの実行を変更するため、不正確な結果を取得することはできなくなりました:
- [distributed_group_by_no_merge](#distributed_group_by_no_merge) > 0。
- クエリが `GROUP BY` / `DISTINCT` / `LIMIT BY` を含まないが、`ORDER BY` / `LIMIT` を含む。
- クエリが `GROUP BY` / `DISTINCT` / `LIMIT BY` を含むが、`ORDER BY` / `LIMIT` を持ち、さらに:
    - [optimize_skip_unused_shards](#optimize_skip_unused_shards) が有効。
    - [optimize_distributed_group_by_sharding_key](#optimize_distributed_group_by_sharding_key) が有効。

可能な値:

- 0 — 無効。
- 1 — 有効。

参照:

- [distributed_group_by_no_merge](#distributed_group_by_no_merge)
- [optimize_skip_unused_shards](#optimize_skip_unused_shards)
- [optimize_distributed_group_by_sharding_key](#optimize_distributed_group_by_sharding_key)

## distributed_replica_error_cap {#distributed_replica_error_cap}

タイプ: UInt64

デフォルト値: 1000

- タイプ: 符号なし整数
- デフォルト値: 1000

各レプリカのエラー数はこの値で制限され、単一のレプリカが過剰にエラーを蓄積するのを防ぎます。

参照:

- [load_balancing](#load_balancing-round_robin)
- [テーブルエンジン Distributed](../../engines/table-engines/special/distributed.md)
- [distributed_replica_error_half_life](#distributed_replica_error_half_life)
- [distributed_replica_max_ignored_errors](#distributed_replica_max_ignored_errors)

## distributed_replica_error_half_life {#distributed_replica_error_half_life}

タイプ: 秒

デフォルト値: 60

- タイプ: 秒
- デフォルト値: 60 秒。

分散テーブルでエラーがゼロになる速度を制御します。レプリカが一時的に利用できない場合、5つのエラーを蓄積し、distributed_replica_error_half_life が 1 秒に設定されている場合、最後のエラーから 3 秒後にレプリカは正常と見なされます。

参照:

- [load_balancing](#load_balancing-round_robin)
- [テーブルエンジン Distributed](../../engines/table-engines/special/distributed.md)
- [distributed_replica_error_cap](#distributed_replica_error_cap)
- [distributed_replica_max_ignored_errors](#distributed_replica_max_ignored_errors)

## distributed_replica_max_ignored_errors {#distributed_replica_max_ignored_errors}

タイプ: UInt64

デフォルト値: 0

- タイプ: 符号なし整数
- デフォルト値: 0

レプリカを選択する際に無視されるエラーの数（`load_balancing` アルゴリズムに従って）。

参照:

- [load_balancing](#load_balancing-round_robin)
- [テーブルエンジン Distributed](../../engines/table-engines/special/distributed.md)
- [distributed_replica_error_cap](#distributed_replica_error_cap)
- [distributed_replica_error_half_life](#distributed_replica_error_half_life)

## do_not_merge_across_partitions_select_final {#do_not_merge_across_partitions_select_final}

タイプ: Bool

デフォルト値: 0

最終選択時に一つのパーティション内だけのパーツをマージします。

## empty_result_for_aggregation_by_constant_keys_on_empty_set {#empty_result_for_aggregation_by_constant_keys_on_empty_set}

タイプ: Bool

デフォルト値: 1

空のセットで定数キーで集約したときに空の結果を返します。

## empty_result_for_aggregation_by_empty_set {#empty_result_for_aggregation_by_empty_set}

タイプ: Bool

デフォルト値: 0

空のセットでキーなしで集約したときに空の結果を返します。

## enable_adaptive_memory_spill_scheduler {#enable_adaptive_memory_spill_scheduler}
<ExperimentalBadge/>

タイプ: Bool

デフォルト値: 0

プロセッサがデータを外部ストレージに適応的にスピルするようにトリガーします。グレースジョインが現在サポートされています。

## enable_blob_storage_log {#enable_blob_storage_log}

タイプ: Bool

デフォルト値: 1

Blob ストレージ操作に関する情報を system.blob_storage_log テーブルに書き込みます。

## enable_deflate_qpl_codec {#enable_deflate_qpl_codec}

タイプ: Bool

デフォルト値: 0

有効になっている場合、DEFLATE_QPL コーデックを使用してカラムを圧縮できます。

## enable_early_constant_folding {#enable_early_constant_folding}

タイプ: Bool

デフォルト値: 1

式とサブクエリの結果を分析し、そこに定数がある場合はクエリを再構成するクエリ最適化を有効にします。

## enable_extended_results_for_datetime_functions {#enable_extended_results_for_datetime_functions}

タイプ: Bool

デフォルト値: 0

次のような拡張範囲の型を持つ結果の返却を有効または無効にします：
- `Date32` は、[toStartOfYear](../../sql-reference/functions/date-time-functions.md/#tostartofyear)、[toStartOfISOYear](../../sql-reference/functions/date-time-functions.md/#tostartofisoyear)、[toStartOfQuarter](../../sql-reference/functions/date-time-functions.md/#tostartofquarter)、[toStartOfMonth](../../sql-reference/functions/date-time-functions.md/#tostartofmonth)、[toLastDayOfMonth](../../sql-reference/functions/date-time-functions.md/#tolastdayofmonth)、[toStartOfWeek](../../sql-reference/functions/date-time-functions.md/#tostartofweek)、[toLastDayOfWeek](../../sql-reference/functions/date-time-functions.md/#tolastdayofweek)、および [toMonday](../../sql-reference/functions/date-time-functions.md/#tomonday) 関数に対して `Date` 型と比較して拡張範囲です。
- `DateTime64` は、[toStartOfDay](../../sql-reference/functions/date-time-functions.md/#tostartofday)、[toStartOfHour](../../sql-reference/functions/date-time-functions.md/#tostartofhour)、[toStartOfMinute](../../sql-reference/functions/date-time-functions.md/#tostartofminute)、[toStartOfFiveMinutes](../../sql-reference/functions/date-time-functions.md/#tostartoffiveminutes)、[toStartOfTenMinutes](../../sql-reference/functions/date-time-functions.md/#tostartoftenminutes)、[toStartOfFifteenMinutes](../../sql-reference/functions/date-time-functions.md/#tostartoffifteenminutes)、および [timeSlot](../../sql-reference/functions/date-time-functions.md/#timeslot) 関数に対して拡張範囲です。

可能な値:

- 0 — 関数はすべての引数タイプに対して `Date` または `DateTime` を返します。
- 1 — 関数は `Date32` または `DateTime64` の引数に対して `Date32` または `DateTime64` を返し、それ以外は `Date` または `DateTime` を返します。

## enable_filesystem_cache {#enable_filesystem_cache}

タイプ: Bool

デフォルト値: 1

リモートファイルシステム用のキャッシュを使用します。この設定はディスクのキャッシュをオン/オフにはしません（ディスク設定で行う必要があります）が、特定のクエリに対してキャッシュをバイパスすることを許可します。

## enable_filesystem_cache_log {#enable_filesystem_cache_log}

タイプ: Bool

デフォルト値: 0

各クエリのファイルシステムキャッシュログを記録することを許可します。

## enable_filesystem_cache_on_write_operations {#enable_filesystem_cache_on_write_operations}

タイプ: Bool

デフォルト値: 0

書き込み操作時にキャッシュに書き込みます。実際にこの設定が機能するには、ディスク設定にも追加する必要があります。

## enable_filesystem_read_prefetches_log {#enable_filesystem_read_prefetches_log}

タイプ: Bool

デフォルト値: 0

クエリ中に system.filesystem の prefetch_log に記録します。テストまたはデバッグのみで使用されるべきであり、デフォルトでは有効にすることは推奨されません。

## enable_global_with_statement {#enable_global_with_statement}

タイプ: Bool

デフォルト値: 1

WITH ステートメントを UNION クエリおよびすべてのサブクエリに伝播します。

## enable_http_compression {#enable_http_compression}

タイプ: Bool

デフォルト値: 0

HTTP リクエストへの応答におけるデータ圧縮を有効または無効にします。

詳細については、[HTTP インターフェースの説明](../../interfaces/http.md)をお読みください。

可能な値:

- 0 — 無効。
- 1 — 有効。

## enable_job_stack_trace {#enable_job_stack_trace}

タイプ: Bool

デフォルト値: 1

ジョブが例外を生じるとき、ジョブ作成者のスタックトレースを出力します。

## enable_lightweight_delete {#enable_lightweight_delete}

タイプ: Bool

デフォルト値: 1

MergeTree テーブルの軽量 DELETE 変更を有効にします。

## enable_memory_bound_merging_of_aggregation_results {#enable_memory_bound_merging_of_aggregation_results}

タイプ: Bool

デフォルト値: 1

集約に対してメモリ制限のあるマージ戦略を有効にします。

## enable_multiple_prewhere_read_steps {#enable_multiple_prewhere_read_steps}

タイプ: Bool

デフォルト値: 1

複数の条件が AND で結合されている場合、WHERE から PREWHERE により多くの条件を移動し、ディスクからの読み取りとフィルタリングを複数のステップで行います。

## enable_named_columns_in_function_tuple {#enable_named_columns_in_function_tuple}

タイプ: Bool

デフォルト値: 0

すべての名前が一意であり、引用されていない識別子として扱うことができる場合、function tuple() で名前付きタプルを生成します。

## enable_optimize_predicate_expression {#enable_optimize_predicate_expression}

タイプ: Bool

デフォルト値: 1

`SELECT` クエリにおける述語プッシュダウンを有効にします。

述語プッシュダウンは、分散クエリに対するネットワークトラフィックを大幅に削減する可能性があります。

可能な値:

- 0 — 無効。
- 1 — 有効。

使用方法

次のクエリを考えます。

1.  `SELECT count() FROM test_table WHERE date = '2018-10-10'`
2.  `SELECT count() FROM (SELECT * FROM test_table) WHERE date = '2018-10-10'`

`enable_optimize_predicate_expression = 1` の場合、これらのクエリの実行時間は等しいです。ClickHouseはサブクエリを処理する際に `WHERE` を適用します。

`enable_optimize_predicate_expression = 0` の場合、2番目のクエリの実行時間ははるかに長くなります。なぜなら、`WHERE` 句はサブクエリが終了した後のすべてのデータに適用されるためです。

## enable_optimize_predicate_expression_to_final_subquery {#enable_optimize_predicate_expression_to_final_subquery}

タイプ: Bool

デフォルト値: 1

述語を最終サブクエリにプッシュすることを許可します。

## enable_order_by_all {#enable_order_by_all}

タイプ: Bool

デフォルト値: 1

`ORDER BY ALL` 構文によるソートを有効または無効にします。詳しくは [ORDER BY](../../sql-reference/statements/select/order-by.md) を参照してください。

可能な値:

- 0 — ORDER BY ALL を無効にします。
- 1 — ORDER BY ALL を有効にします。

**例**

クエリ:

```sql
CREATE TABLE TAB(C1 Int, C2 Int, ALL Int) ENGINE=Memory();

INSERT INTO TAB VALUES (10, 20, 30), (20, 20, 10), (30, 10, 20);

SELECT * FROM TAB ORDER BY ALL; -- ALL が曖昧であるというエラーが返されます。

SELECT * FROM TAB ORDER BY ALL SETTINGS enable_order_by_all = 0;
```

結果:

```text
┌─C1─┬─C2─┬─ALL─┐
│ 20 │ 20 │  10 │
│ 30 │ 10 │  20 │
│ 10 │ 20 │  30 │
└────┴────┴─────┘
```

## enable_parsing_to_custom_serialization {#enable_parsing_to_custom_serialization}

タイプ: Bool

デフォルト値: 1

真の場合、データはテーブルから得られるシリアライゼーションに関するヒントに従って、カスタムシリアライゼーション（例えば、Sparse）を持つカラムに直接解析されます。

## enable_positional_arguments {#enable_positional_arguments}

タイプ: Bool

デフォルト値: 1

[GROUP BY](../../sql-reference/statements/select/group-by.md)、[LIMIT BY](../../sql-reference/statements/select/limit-by.md)、[ORDER BY](../../sql-reference/statements/select/order-by.md) ステートメントの位置引数のサポートを有効または無効にします。

可能な値:

- 0 — 位置引数はサポートされていません。
- 1 — 位置引数がサポートされます：カラム番号をカラム名の代わりに使用できます。

**例**

クエリ:

```sql
CREATE TABLE positional_arguments(one Int, two Int, three Int) ENGINE=Memory();

INSERT INTO positional_arguments VALUES (10, 20, 30), (20, 20, 10), (30, 10, 20);

SELECT * FROM positional_arguments ORDER BY 2,3;
```

結果:

```text
┌─one─┬─two─┬─three─┐
│  30 │  10 │   20  │
│  20 │  20 │   10  │
│  10 │  20 │   30  │
└─────┴─────┴───────┘
```

## enable_reads_from_query_cache {#enable_reads_from_query_cache}

タイプ: Bool

デフォルト値: 1

有効な場合、`SELECT` クエリの結果は [query cache](../query-cache.md) から取得されます。

可能な値:

- 0 - 無効
- 1 - 有効

## enable_s3_requests_logging {#enable_s3_requests_logging}

タイプ: Bool

デフォルト値: 0

S3 リクエストの非常に明示的なロギングを有効にします。デバッグ用のみ意味があります。

## enable_scalar_subquery_optimization {#enable_scalar_subquery_optimization}

タイプ: Bool

デフォルト値: 1

これが真に設定されている場合、大きなスカラー値の (デ)シリアライズを防ぎ、同じサブクエリを二度実行するのを避けます。

## enable_sharing_sets_for_mutations {#enable_sharing_sets_for_mutations}

タイプ: Bool

デフォルト値: 1

IN サブクエリ用に構築されたセットオブジェクトを同じ変更のさまざまなタスク間で共有します。これにより、メモリ使用量と CPU 消費が削減されます。

## enable_software_prefetch_in_aggregation {#enable_software_prefetch_in_aggregation}

タイプ: Bool

デフォルト値: 1

集約におけるソフトウェアプリフェッチの使用を有効にします。

## enable_unaligned_array_join {#enable_unaligned_array_join}

タイプ: Bool

デフォルト値: 0

異なるサイズの複数の配列で ARRAY JOIN を許可します。この設定が有効な場合、配列は最も長い配列にサイズを変更します。

## enable_url_encoding {#enable_url_encoding}

タイプ: Bool

デフォルト値: 1

[URL](../../engines/table-engines/special/url.md) エンジンテーブルのパスのデコード/エンコードを有効/無効にします。

デフォルトで有効です。

## enable_vertical_final {#enable_vertical_final}

タイプ: Bool

デフォルト値: 1

有効にすると、最終的に重複行をマージするのではなく、削除された行としてマークし、後でフィルタリングします。

## enable_writes_to_query_cache {#enable_writes_to_query_cache}

タイプ: Bool

デフォルト値: 1

有効な場合、`SELECT` クエリの結果は [query cache](../query-cache.md) に格納されます。

可能な値:

- 0 - 無効
- 1 - 有効

## enable_zstd_qat_codec {#enable_zstd_qat_codec}

タイプ: Bool

デフォルト値: 0

有効な場合、ZSTD_QAT コーデックを使用してカラムを圧縮できます。

## enforce_strict_identifier_format {#enforce_strict_identifier_format}

Type: Bool

Default value: 0

有効にすると、英数字とアンダースコアを含む識別子のみが許可されます。

## engine_file_allow_create_multiple_files {#engine_file_allow_create_multiple_files}

Type: Bool

Default value: 0

ファイルエンジンテーブルで、形式にサフィックス(`JSON`, `ORC`, `Parquet`など)がある場合、各挿入で新しいファイルの作成を有効または無効にします。有効にすると、各挿入時に次のパターンに従った名前の新しいファイルが作成されます：

`data.Parquet` -> `data.1.Parquet` -> `data.2.Parquet` など。

Possible values:
- 0 — `INSERT` クエリはファイルの末尾に新しいデータを追加します。
- 1 — `INSERT` クエリは新しいファイルを作成します。

## engine_file_empty_if_not_exists {#engine_file_empty_if_not_exists}

Type: Bool

Default value: 0

ファイルがなくてもファイルエンジンテーブルからデータを選択することを許可します。

Possible values:
- 0 — `SELECT` は例外をスローします。
- 1 — `SELECT` は空の結果を返します。

## engine_file_skip_empty_files {#engine_file_skip_empty_files}

Type: Bool

Default value: 0

[File](../../engines/table-engines/special/file.md) エンジンテーブルで空のファイルをスキップすることを有効または無効にします。

Possible values:
- 0 — 要求された形式と互換性のない空のファイルがあるとき、`SELECT`は例外をスローします。
- 1 — 空のファイルに対して、`SELECT`は空の結果を返します。

## engine_file_truncate_on_insert {#engine_file_truncate_on_insert}

Type: Bool

Default value: 0

[File](../../engines/table-engines/special/file.md) エンジンテーブルでの挿入前の切り捨てを有効または無効にします。

Possible values:
- 0 — `INSERT` クエリはファイルの末尾に新しいデータを追加します。
- 1 — `INSERT` クエリはファイルの既存のコンテンツを新しいデータで置き換えます。

## engine_url_skip_empty_files {#engine_url_skip_empty_files}

Type: Bool

Default value: 0

[URL](../../engines/table-engines/special/url.md) エンジンテーブルでの空のファイルをスキップすることを有効または無効にします。

Possible values:
- 0 — 要求された形式と互換性のない空のファイルがあるとき、`SELECT`は例外をスローします。
- 1 — 空のファイルに対して、`SELECT`は空の結果を返します。

## except_default_mode {#except_default_mode}

Type: SetOperationMode

Default value: ALL

EXCEPT クエリのデフォルトモードを設定します。可能な値: 空文字列、'ALL'、'DISTINCT'。空の場合、モードがないクエリは例外をスローします。

## external_storage_connect_timeout_sec {#external_storage_connect_timeout_sec}

Type: UInt64

Default value: 10

接続タイムアウト（秒）。現在、MySQLのみサポートされています。

## external_storage_max_read_bytes {#external_storage_max_read_bytes}

Type: UInt64

Default value: 0

外部エンジンでテーブルが履歴データをフラッシュする際の最大バイト数の制限。現在、MySQLテーブルエンジン、データベースエンジン、辞書のみサポートされています。0に等しい場合、この設定は無効になります。

## external_storage_max_read_rows {#external_storage_max_read_rows}

Type: UInt64

Default value: 0

外部エンジンでテーブルが履歴データをフラッシュする際の最大行数の制限。現在、MySQLテーブルエンジン、データベースエンジン、辞書のみサポートされています。0に等しい場合、この設定は無効になります。

## external_storage_rw_timeout_sec {#external_storage_rw_timeout_sec}

Type: UInt64

Default value: 300

読み込み/書き込みタイムアウト（秒）。現在、MySQLのみサポートされています。

## external_table_functions_use_nulls {#external_table_functions_use_nulls}

Type: Bool

Default value: 1

[mysql](../../sql-reference/table-functions/mysql.md)、[postgresql](../../sql-reference/table-functions/postgresql.md)、および [odbc](../../sql-reference/table-functions/odbc.md) テーブル関数が Nullable カラムをどのように使用するかを定義します。

Possible values:

- 0 — テーブル関数は明示的に Nullable カラムを使用します。
- 1 — テーブル関数は暗黙的に Nullable カラムを使用します。

**Usage**

設定が `0` に設定されている場合、テーブル関数は Nullable カラムを作成せず、NULL の代わりにデフォルト値を挿入します。これは、配列内の NULL 値にも適用されます。

## external_table_strict_query {#external_table_strict_query}

Type: Bool

Default value: 0

true に設定されている場合、外部テーブルへのクエリに対してローカルフィルタへの変換式が禁止されます。

## extract_key_value_pairs_max_pairs_per_row {#extract_key_value_pairs_max_pairs_per_row}

Type: UInt64

Default value: 1000

`extractKeyValuePairs` 関数によって生成されるペアの最大数。過剰なメモリ消費を防ぐための安全策として使用されます。

## extremes {#extremes}

Type: Bool

Default value: 0

クエリ結果のカラムの極値（最小値と最大値）をカウントするかどうか。0 または 1 が受け入れられます。デフォルトは 0（無効）。

より詳しい情報は「極値」のセクションを参照してください。

## fallback_to_stale_replicas_for_distributed_queries {#fallback_to_stale_replicas_for_distributed_queries}

Type: Bool

Default value: 1

更新されたデータが利用できない場合、古いレプリカへのクエリを強制します。[Replication](../../engines/table-engines/mergetree-family/replication.md)を参照してください。

ClickHouse は、古いレプリカの中から最も関連性の高いものを選択します。

レプリケートされたテーブルを指す分散テーブルから`SELECT`を実行する際に使用されます。

デフォルトでは 1（有効）。

## filesystem_cache_boundary_alignment {#filesystem_cache_boundary_alignment}

Type: UInt64

Default value: 0

ファイルシステムキャッシュの境界アラインメント。この設定は、非ディスク読み取りにのみ適用されます（例：リモートテーブルエンジン/テーブル関数のキャッシュ、ただし MergeTree テーブルのストレージ構成には適用されません）。値 0 はアラインメントなしを意味します。

## filesystem_cache_enable_background_download_during_fetch {#filesystem_cache_enable_background_download_during_fetch}

Type: Bool

Default value: 1

ClickHouse Cloud のみ。ファイルシステムキャッシュにおけるスペース予約のため、キャッシュをロックする待機時間です。

## filesystem_cache_enable_background_download_for_metadata_files_in_packed_storage {#filesystem_cache_enable_background_download_for_metadata_files_in_packed_storage}

Type: Bool

Default value: 1

ClickHouse Cloud のみ。ファイルシステムキャッシュにおけるスペース予約のため、キャッシュをロックする待機時間です。

## filesystem_cache_max_download_size {#filesystem_cache_max_download_size}

Type: UInt64

Default value: 137438953472

単一のクエリによってダウンロードできる最大のリモートファイルシステムキャッシュサイズ。

## filesystem_cache_name {#filesystem_cache_name}

Type: String

Default value:

ステートレステーブルエンジンまたはデータレイクに使用されるファイルシステムキャッシュ名。

## filesystem_cache_prefer_bigger_buffer_size {#filesystem_cache_prefer_bigger_buffer_size}

Type: Bool

Default value: 1

ファイルシステムキャッシュが有効な場合、小さなファイルセグメントの書き込みを避けるために、より大きなバッファサイズを優先します。一方で、この設定を有効にするとメモリ使用量が増加する可能性があります。

## filesystem_cache_reserve_space_wait_lock_timeout_milliseconds {#filesystem_cache_reserve_space_wait_lock_timeout_milliseconds}

Type: UInt64

Default value: 1000

ファイルシステムキャッシュにおけるスペース予約のため、キャッシュをロックする待機時間です。

## filesystem_cache_segments_batch_size {#filesystem_cache_segments_batch_size}

Type: UInt64

Default value: 20

読み取りバッファがキャッシュから要求できるファイルセグメントの単一バッチのサイズに制限を設けます。値が低すぎるとキャッシュへのリクエストが過剰になり、高すぎるとキャッシュからの追放が遅くなる可能性があります。

## filesystem_cache_skip_download_if_exceeds_per_query_cache_write_limit {#filesystem_cache_skip_download_if_exceeds_per_query_cache_write_limit}

Type: Bool

Default value: 1

クエリキャッシュサイズを超えた場合、リモートファイルシステムからのダウンロードをスキップします。

## filesystem_prefetch_max_memory_usage {#filesystem_prefetch_max_memory_usage}

Type: UInt64

Default value: 1073741824

プリフェッチ用の最大メモリ使用量。

## filesystem_prefetch_step_bytes {#filesystem_prefetch_step_bytes}

Type: UInt64

Default value: 0

バイト単位のプリフェッチステップ。ゼロは `auto` を意味し、最適なプリフェッチステップが自動的に推測されますが、100%最適とは限りません。実際の値は、`filesystem_prefetch_min_bytes_for_single_read_task` の設定によって異なる場合があります。

## filesystem_prefetch_step_marks {#filesystem_prefetch_step_marks}

Type: UInt64

Default value: 0

マークのプリフェッチステップ。ゼロは `auto` を意味し、最適なプリフェッチステップが自動的に推測されますが、100%最適とは限りません。実際の値は、`filesystem_prefetch_min_bytes_for_single_read_task` の設定によって異なる場合があります。

## filesystem_prefetches_limit {#filesystem_prefetches_limit}

Type: UInt64

Default value: 200

プリフェッチの最大数。ゼロは無制限を意味します。プリフェッチの数を制限したい場合は、`filesystem_prefetches_max_memory_usage` 設定が推奨されます。

## final {#final}

Type: Bool

Default value: 0

クエリ内のすべてのテーブルに自動的に [FINAL](../../sql-reference/statements/select/from.md/#final-modifier) 修飾子を適用します。[FINAL](../../sql-reference/statements/select/from.md/#final-modifier)が適用可能なテーブル、結合されたテーブル、サブクエリ内のテーブル、分散テーブルを含みます。

Possible values:

- 0 - 無効
- 1 - 有効

Example:

```sql
CREATE TABLE test
(
    key Int64,
    some String
)
ENGINE = ReplacingMergeTree
ORDER BY key;

INSERT INTO test FORMAT Values (1, 'first');
INSERT INTO test FORMAT Values (1, 'second');

SELECT * FROM test;
┌─key─┬─some───┐
│   1 │ second │
└─────┴────────┘
┌─key─┬─some──┐
│   1 │ first │
└─────┴───────┘

SELECT * FROM test SETTINGS final = 1;
┌─key─┬─some───┐
│   1 │ second │
└─────┴────────┘

SET final = 1;
SELECT * FROM test;
┌─key─┬─some───┐
│   1 │ second │
└─────┴────────┘
```

## flatten_nested {#flatten_nested}

Type: Bool

Default value: 1

[nested](../../sql-reference/data-types/nested-data-structures/index.md) カラムのデータ形式を設定します。

Possible values:

- 1 — ネストされたカラムが個別の配列にフラット化されます。
- 0 — ネストされたカラムはタプルの単一配列のままです。

**Usage**

設定が `0` に設定されている場合、任意のレベルのネストを使用することができます。

**Examples**

Query:

``` sql
SET flatten_nested = 1;
CREATE TABLE t_nest (`n` Nested(a UInt32, b UInt32)) ENGINE = MergeTree ORDER BY tuple();

SHOW CREATE TABLE t_nest;
```

Result:

``` text
┌─statement───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ CREATE TABLE default.t_nest
(
    `n.a` Array(UInt32),
    `n.b` Array(UInt32)
)
ENGINE = MergeTree
ORDER BY tuple()
SETTINGS index_granularity = 8192 │
└─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
```

Query:

``` sql
SET flatten_nested = 0;

CREATE TABLE t_nest (`n` Nested(a UInt32, b UInt32)) ENGINE = MergeTree ORDER BY tuple();

SHOW CREATE TABLE t_nest;
```

Result:

``` text
┌─statement──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐
│ CREATE TABLE default.t_nest
(
    `n` Nested(a UInt32, b UInt32)
)
ENGINE = MergeTree
ORDER BY tuple()
SETTINGS index_granularity = 8192 │
└────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘
```

## force_aggregate_partitions_independently {#force_aggregate_partitions_independently}

Type: Bool

Default value: 0

適用可能な場合に最適化の使用を強制しますが、ヒューリスティックはそれを使用しないことを決定しました。

## force_aggregation_in_order {#force_aggregation_in_order}

Type: Bool

Default value: 0

この設定は、サーバー自体が分散クエリをサポートするために使用します。手動で変更しないでください。正常な動作が破損します。（分散集計中にリモートノードでの順序の集約の使用を強制します）。

## force_data_skipping_indices {#force_data_skipping_indices}

Type: String

Default value:

指定されたデータスキッピングインデックスが使用されなかった場合、クエリの実行を無効にします。

次の例を考えてみましょう：

```sql
CREATE TABLE data
(
    key Int,
    d1 Int,
    d1_null Nullable(Int),
    INDEX d1_idx d1 TYPE minmax GRANULARITY 1,
    INDEX d1_null_idx assumeNotNull(d1_null) TYPE minmax GRANULARITY 1
)
Engine=MergeTree()
ORDER BY key;

SELECT * FROM data_01515;
SELECT * FROM data_01515 SETTINGS force_data_skipping_indices=''; -- クエリは CANNOT_PARSE_TEXT エラーを生成します。
SELECT * FROM data_01515 SETTINGS force_data_skipping_indices='d1_idx'; -- クエリは INDEX_NOT_USED エラーを生成します。
SELECT * FROM data_01515 WHERE d1 = 0 SETTINGS force_data_skipping_indices='d1_idx'; -- Ok。
SELECT * FROM data_01515 WHERE d1 = 0 SETTINGS force_data_skipping_indices='`d1_idx`'; -- Ok（フル機能のパーサーの例）。
SELECT * FROM data_01515 WHERE d1 = 0 AND assumeNotNull(d1_null) = 0 SETTINGS force_data_skipping_indices='`d1_idx`, d1_null_idx'; -- Ok。
```

## force_grouping_standard_compatibility {#force_grouping_standard_compatibility}

Type: Bool

Default value: 1

GROUPING 関数が引数が集約キーとして使用されていないときに 1 を返すようにします。

## force_index_by_date {#force_index_by_date}

Type: Bool

Default value: 0

インデックスが日付によって使用できない場合、クエリの実行を無効にします。

MergeTree ファミリーのテーブルで機能します。

`force_index_by_date=1` の場合、ClickHouse は、データ範囲を制限するために使用できる日付キー条件を持つかどうかをクエリで確認します。適切な条件がない場合、例外がスローされます。ただし、条件が読み取るデータ量を減少させるかどうかは確認しません。例えば、条件 `Date != ' 2000-01-01 '` は、テーブル内のすべてのデータに一致していても受け入れられます（すなわち、クエリの実行にはフルスキャンが必要です）。MergeTree テーブルでのデータ範囲についての詳細は、[MergeTree](../../engines/table-engines/mergetree-family/mergetree.md)を参照してください。

## force_optimize_projection {#force_optimize_projection}

Type: Bool

Default value: 0

`SELECT` クエリで[プロジェクション](../../engines/table-engines/mergetree-family/mergetree.md/#projections)の義務的な使用を有効または無効にします。プロジェクション最適化が有効な場合は、（`optimize_use_projections` 設定を参照）。

Possible values:

- 0 — プロジェクション最適化は義務的ではありません。
- 1 — プロジェクション最適化は義務的です。

## force_optimize_projection_name {#force_optimize_projection_name}

Type: String

Default value:

非空の文字列に設定されている場合、このプロジェクションがクエリで少なくとも一度使用されていることを確認します。

Possible values:

- string: クエリで使用されるプロジェクションの名前。

## force_optimize_skip_unused_shards {#force_optimize_skip_unused_shards}

Type: UInt64

Default value: 0

[optimize_skip_unused_shards](#force_optimize_skip_unused_shards)が有効であり、未使用のシャードのスキップが不可能な場合にクエリの実行を有効または無効にします。スキップが不可能で設定が有効な場合、例外がスローされます。

Possible values:

- 0 — 無効。ClickHouse は例外をスローしません。
- 1 — 有効。テーブルにシャーディングキーがある場合のみ、クエリの実行が無効になります。
- 2 — 有効。テーブルにシャーディングキーが定義されているかどうかにかかわらず、クエリの実行が無効になります。

## force_optimize_skip_unused_shards_nesting {#force_optimize_skip_unused_shards_nesting}

Type: UInt64

Default value: 0

[`force_optimize_skip_unused_shards`](#force_optimize_skip_unused_shards)（したがって`force_optimize_skip_unused_shards`がまだ必要）を制御し、分散クエリのネスティングレベルに依存します（Distributed テーブルが別の Distributed テーブルを参照する場合）。

Possible values:

- 0 - 無効、`force_optimize_skip_unused_shards` は常に機能します。
- 1 — `force_optimize_skip_unused_shards`を最初のレベルのみに有効にします。
- 2 — `force_optimize_skip_unused_shards`を2番目のレベルまで有効にします。

## force_primary_key {#force_primary_key}

Type: Bool

Default value: 0

プライマリキーによるインデックス設定が不可能な場合、クエリの実行を無効にします。

MergeTree ファミリーのテーブルで機能します。

`force_primary_key=1` の場合、ClickHouse はクエリにデータ範囲を制限するために使用できるプライマリキー条件があるかどうかを確認します。適切な条件がない場合、例外がスローされます。ただし、条件が読み取るデータ量を減少させるかどうかは確認しません。MergeTree テーブルでのデータ範囲についての詳細は、[MergeTree](../../engines/table-engines/mergetree-family/mergetree.md)を参照してください。

## force_remove_data_recursively_on_drop {#force_remove_data_recursively_on_drop}

Type: Bool

Default value: 0

DROP クエリ時にデータを再帰的に削除します。「ディレクトリは空ではありません」というエラーを回避しますが、切り離されたデータを静かに削除する可能性があります。

## formatdatetime_f_prints_scale_number_of_digits {#formatdatetime_f_prints_scale_number_of_digits}

Type: Bool

Default value: 0

関数 'formatDateTime' のフォーマッター '%f' は、固定の 6 桁の代わりに DateTime64 のスケールの桁数のみを出力します。

## formatdatetime_f_prints_single_zero {#formatdatetime_f_prints_single_zero}

Type: Bool

Default value: 0

関数 'formatDateTime' のフォーマッター '%f' は、フォーマットされた値に小数秒が存在しない場合、6 のゼロの代わりに単一のゼロを出力します。

## formatdatetime_format_without_leading_zeros {#formatdatetime_format_without_leading_zeros}

Type: Bool

Default value: 0

関数 'formatDateTime' のフォーマッター '%c', '%l' と '%k' は、先頭ゼロなしで月と時間を出力します。

## formatdatetime_parsedatetime_m_is_month_name {#formatdatetime_parsedatetime_m_is_month_name}

Type: Bool

Default value: 1

関数 'formatDateTime' と 'parseDateTime' のフォーマッター '%M' は、分の代わりに月の名前を出力/解析します。

## fsync_metadata {#fsync_metadata}

Type: Bool

Default value: 1

.sql ファイルを書き込む際に [fsync](http://pubs.opengroup.org/onlinepubs/9699919799/functions/fsync.html) を有効または無効にします。デフォルトでは有効です。

サーバーが常に作成および削除される数百万の小さなテーブルを持っている場合は、それを無効にすることが意味があります。

## function_implementation {#function_implementation}

Type: String

Default value:

特定のターゲットまたはバリアント（実験的）用の関数実装を選択します。空の場合はすべてが有効になります。

## function_json_value_return_type_allow_complex {#function_json_value_return_type_allow_complex}

Type: Bool

Default value: 0

json_value 関数で複雑な型（struct, array, map など）を返すことを許可するかどうかを制御します。

```sql
SELECT JSON_VALUE('{"hello":{"world":"!"}}', '$.hello') settings function_json_value_return_type_allow_complex=true

┌─JSON_VALUE('{"hello":{"world":"!"}}', '$.hello')─┐
│ {"world":"!"}                                    │
└──────────────────────────────────────────────────┘

1 row in set. Elapsed: 0.001 sec.
```

Possible values:

- true — 許可。
- false — 不許可。

## function_json_value_return_type_allow_nullable {#function_json_value_return_type_allow_nullable}

Type: Bool

Default value: 0

JSON_VALUE 関数の値が存在しない場合に `NULL` を返すことを許可するかどうかを制御します。

```sql
SELECT JSON_VALUE('{"hello":"world"}', '$.b') settings function_json_value_return_type_allow_nullable=true;

┌─JSON_VALUE('{"hello":"world"}', '$.b')─┐
│ ᴺᵁᴸᴸ                                   │
└────────────────────────────────────────┘

1 row in set. Elapsed: 0.001 sec.
```

Possible values:

- true — 許可。
- false — 不許可。

## function_locate_has_mysql_compatible_argument_order {#function_locate_has_mysql_compatible_argument_order}

Type: Bool

Default value: 1

関数 [locate](../../sql-reference/functions/string-search-functions.md/#locate) の引数の順序を制御します。

Possible values:

- 0 — 関数 `locate` は引数 `(haystack, needle[, start_pos])` を受け入れます。
- 1 — 関数 `locate` は引数 `(needle, haystack, [, start_pos])`（MySQL 互換の動作）を受け入れます。

## function_range_max_elements_in_block {#function_range_max_elements_in_block}

Type: UInt64

Default value: 500000000

関数 [range](../../sql-reference/functions/array-functions.md/#range) によって生成されるデータ量の安全閾値を設定します。データのブロックごとに生成される最大値の数（ブロック内の各行の配列サイズの合計）を定義します。

Possible values:

- 正の整数。

**See Also**

- [max_block_size](#max_block_size)
- [min_insert_block_size_rows](#min_insert_block_size_rows)

## function_sleep_max_microseconds_per_block {#function_sleep_max_microseconds_per_block}

Type: UInt64

Default value: 3000000

関数 `sleep` が各ブロックでスリープすることが許可される最大マイクロ秒数。ユーザーがそれより大きな値で呼び出した場合、例外がスローされます。これは安全閾値です。

## function_visible_width_behavior {#function_visible_width_behavior}

Type: UInt64

Default value: 1

`visibleWidth` の動作のバージョン。0 - コードポイントの数のみをカウントします。1 - ゼロ幅および結合文字を正しくカウントし、全角文字を2としてカウントし、タブ幅を見積もり、削除文字をカウントします。

## geo_distance_returns_float64_on_float64_arguments {#geo_distance_returns_float64_on_float64_arguments}

Type: Bool

Default value: 1

`geoDistance`、`greatCircleDistance`、`greatCircleAngle` 関数に対するすべての4つの引数が Float64 の場合、Float64 を返し、内部計算において倍精度を使用します。以前の ClickHouse バージョンでは、関数は常に Float32 を返しました。

## glob_expansion_max_elements {#glob_expansion_max_elements}

Type: UInt64

Default value: 1000

許可されるアドレスの最大数（外部ストレージ、テーブル関数など）。

## grace_hash_join_initial_buckets {#grace_hash_join_initial_buckets}
<ExperimentalBadge/>

Type: NonZeroUInt64

Default value: 1

グレースハッシュ結合バケットの初期数。

## grace_hash_join_max_buckets {#grace_hash_join_max_buckets}
<ExperimentalBadge/>

Type: NonZeroUInt64

Default value: 1024

グレースハッシュ結合バケットの数の制限。

## group_by_overflow_mode {#group_by_overflow_mode}

Type: OverflowModeGroupBy

Default value: throw

制限を超えた場合の動作。

## group_by_two_level_threshold {#group_by_two_level_threshold}

Type: UInt64

Default value: 100000

キーの数がこの値を超える場合、2レベル集約が始まります。0 - 制限が設定されていません。

## group_by_two_level_threshold_bytes {#group_by_two_level_threshold_bytes}

Type: UInt64

Default value: 50000000

集約状態のサイズがこのバイト数を超える場合、2レベル集約が使用され始めます。0 - 制限が設定されていません。少なくとも一つの閾値がトリガされるときに2レベル集約が使用されます。

## group_by_use_nulls {#group_by_use_nulls}

Type: Bool

Default value: 0

[GROUP BY句](/docs/sql-reference/statements/select/group-by.md) が集約キーの型を扱う方法を変更します。
`ROLLUP`、`CUBE`、または `GROUPING SETS` 指定子が使用されるとき、一部の集約キーは結果行を生成するために使用されないことがあります。
これらのキーに対する列は、この設定に応じて該当する行にデフォルト値または `NULL` で埋められます。

Possible values:

- 0 — 集約キータイプのデフォルト値が不足している値の生成に使用されます。
- 1 — ClickHouse は SQL 標準のように `GROUP BY` を実行します。集約キーの型は[Nullable](/docs/sql-reference/data-types/nullable.md/#data_type-nullable)に変換され、対応する集約キーの列は、使用しなかった行に対して[NULL](/docs/sql-reference/syntax.md)で埋められます。

さらに詳しい情報は:

- [GROUP BY 句](/docs/sql-reference/statements/select/group-by.md)を参照してください。

## h3togeo_lon_lat_result_order {#h3togeo_lon_lat_result_order}

Type: Bool

Default value: 0

関数 'h3ToGeo' は true の場合 (lon, lat) を返し、そうでない場合は (lat, lon) を返します。

## handshake_timeout_ms {#handshake_timeout_ms}

Type: Milliseconds

Default value: 10000

ハンドシェイク時にレプリカから Hello パケットを受信するためのタイムアウト（ミリ秒）。

## hdfs_create_new_file_on_insert {#hdfs_create_new_file_on_insert}

Type: Bool

Default value: 0

HDFSエンジンテーブルで、各挿入時に新しいファイルを作成することを有効または無効にします。有効にすると、各挿入時に次のパターンに従って新しいHDFSファイルが作成されます：

initial: `data.Parquet.gz` -> `data.1.Parquet.gz` -> `data.2.Parquet.gz` など。

Possible values:
- 0 — `INSERT` クエリはファイルの末尾に新しいデータを追加します。
- 1 — `INSERT` クエリは新しいファイルを作成します。

## hdfs_ignore_file_doesnt_exist {#hdfs_ignore_file_doesnt_exist}

Type: Bool

Default value: 0

特定のキーを読み取る際にファイルが存在しない場合、その存在を無視します。

Possible values:
- 1 — `SELECT` は空の結果を返します。
- 0 — `SELECT` は例外をスローします。

## hdfs_replication {#hdfs_replication}

Type: UInt64

Default value: 0

hdfsファイルを作成する際に指定できる実際の複製数。

## hdfs_skip_empty_files {#hdfs_skip_empty_files}

Type: Bool

Default value: 0

[HDFS](../../engines/table-engines/integrations/hdfs.md) エンジンテーブルで空のファイルをスキップすることを有効または無効にします。

Possible values:
- 0 — 要求された形式と互換性のない空のファイルがあるとき、`SELECT`は例外をスローします。
- 1 — 空のファイルに対して、`SELECT`は空の結果を返します。

## hdfs_throw_on_zero_files_match {#hdfs_throw_on_zero_files_match}

Type: Bool

Default value: 0

グローブ展開規則に従ってゼロファイルが一致した場合にエラーをスローします。

Possible values:
- 1 — `SELECT` は例外をスローします。
- 0 — `SELECT` は空の結果を返します。

## hdfs_truncate_on_insert {#hdfs_truncate_on_insert}

Type: Bool

Default value: 0

hdfsエンジンテーブルでの挿入前に切り捨てを実行することを有効または無効にします。無効の場合、HDFS内に既にファイルが存在する場合の挿入試行で例外がスローされます。

Possible values:
- 0 — `INSERT` クエリはファイルの末尾に新しいデータを追加します。
- 1 — `INSERT` クエリはファイルの既存のコンテンツを新しいデータで置き換えます。

## hedged_connection_timeout_ms {#hedged_connection_timeout_ms}

Type: Milliseconds

Default value: 50

ヘッジリクエストのためにレプリカとの接続を確立するための接続タイムアウト。

## hnsw_candidate_list_size_for_search {#hnsw_candidate_list_size_for_search}
<ExperimentalBadge/>

Type: UInt64

Default value: 256

ベクトル類似性インデックスを検索する際の動的候補リストのサイズ（ef_searchとしても知られます）。

## hsts_max_age {#hsts_max_age}

Type: UInt64

Default value: 0

HSTS の有効期限。0はHSTSを無効にします。

## http_connection_timeout {#http_connection_timeout}

Type: Seconds

Default value: 1

HTTP接続タイムアウト（秒）。

Possible values:

- 任意の正の整数。
- 0 - 無効（無限タイムアウト）。

## http_headers_progress_interval_ms {#http_headers_progress_interval_ms}

Type: UInt64

Default value: 100

指定された間隔ごとに、HTTPヘッダー X-ClickHouse-Progress をそれ以上頻繁に送信しない。

## http_make_head_request {#http_make_head_request}

Type: Bool

Default value: 1

`http_make_head_request` 設定は、HTTPからデータを読み取る際に `HEAD` リクエストを実行することを許可し、読み取るファイルに関する情報（サイズなど）を取得します。有効であるため、サーバーが `HEAD` リクエストをサポートしていない場合は、この設定を無効にすることが望ましいことがあります。

## http_max_field_name_size {#http_max_field_name_size}

Type: UInt64

Default value: 131072

HTTPヘッダー内のフィールド名の最大長。

## http_max_field_value_size {#http_max_field_value_size}

Type: UInt64

Default value: 131072

HTTPヘッダー内のフィールド値の最大長。

## http_max_fields {#http_max_fields}

Type: UInt64

Default value: 1000000

HTTPヘッダー内のフィールドの最大数。

## http_max_multipart_form_data_size {#http_max_multipart_form_data_size}

Type: UInt64

Default value: 1073741824

multipart/form-data コンテンツのサイズ制限。この設定はURLパラメータから解析できず、ユーザープロファイルで設定する必要があります。コンテンツは解析され、外部テーブルはクエリの実行開始前にメモリ内で作成されます。そして、これがその段階に影響を与える唯一の制限です（最大メモリ使用量および最大実行時間の制限は、HTTPフォームデータを読み取る際には影響を与えません）。

## http_max_request_param_data_size {#http_max_request_param_data_size}

Type: UInt64

Default value: 10485760

事前定義のHTTPリクエスト内のクエリパラメータとして使用されるリクエストデータのサイズ制限。

## http_max_tries {#http_max_tries}

Type: UInt64

Default value: 10

HTTP経由で読み取る最大試行回数。

## http_max_uri_size {#http_max_uri_size}

Type: UInt64

Default value: 1048576

HTTPリクエストの最大URI長を設定します。

Possible values:

- 正の整数。

## http_native_compression_disable_checksumming_on_decompress {#http_native_compression_disable_checksumming_on_decompress}

Type: Bool

Default value: 0

クライアントからのHTTP POSTデータを展開する際にチェックサムの検証を有効または無効にします。ClickHouse独自の圧縮形式にのみ使用されます（`gzip`や`deflate`では使用されません）。

より詳しい情報は、[HTTPインターフェースの説明](../../interfaces/http.md)を参照してください。

Possible values:

- 0 — 無効。
- 1 — 有効。

## http_receive_timeout {#http_receive_timeout}

Type: Seconds

Default value: 30

HTTP受信タイムアウト（秒）。

Possible values:

- 任意の正の整数。
- 0 - 無効（無限タイムアウト）。

## http_response_buffer_size {#http_response_buffer_size}

Type: UInt64

Default value: 0

HTTP応答をクライアントに送信する前にサーバーメモリ内でバッファリングするバイト数（または http_wait_end_of_query が有効な場合はディスクへのフラッシング）。
```
## http_response_headers {#http_response_headers}

Type: Map

Default value: {}

HTTPヘッダーを追加または上書きすることを許可します。これは、成功したクエリ結果に対してサーバーがレスポンスで返すものです。この設定はHTTPインターフェースにのみ影響します。

ヘッダーがデフォルトで既に設定されている場合、提供された値がそれを上書きします。デフォルトで設定されていない場合は、ヘッダーのリストに追加されます。この設定によって上書きされなかったデフォルトのサーバーによって設定されたヘッダーは、そのまま残ります。

この設定を使用すると、ヘッダーを定数値に設定することができます。現在、動的に計算された値を設定する方法はありません。

名前や値にASCII制御文字を含めることはできません。

設定を変更するユーザーインターフェースアプリケーションを実装する場合は、ユーザーがヘッダーを変更できるようにすることに加えて、返されたヘッダーに基づいて決定を行うときには、この設定を読み取り専用に制限することが推奨されます。

例: `SET http_response_headers = '{"Content-Type": "image/png"}'`

## http_retry_initial_backoff_ms {#http_retry_initial_backoff_ms}

Type: UInt64

Default value: 100

HTTPを介してリードを再試行する際のバックオフの最小ミリ秒。

## http_retry_max_backoff_ms {#http_retry_max_backoff_ms}

Type: UInt64

Default value: 10000

HTTPを介してリードを再試行する際のバックオフの最大ミリ秒。

## http_send_timeout {#http_send_timeout}

Type: Seconds

Default value: 30

HTTP送信タイムアウト（秒単位）。

可能な値:

- 任意の正の整数。
- 0 - 無効（無限タイムアウト）。

:::note
これはデフォルトプロファイルにのみ適用されます。変更を有効にするにはサーバーの再起動が必要です。
:::

## http_skip_not_found_url_for_globs {#http_skip_not_found_url_for_globs}

Type: Bool

Default value: 1

HTTP_NOT_FOUNDエラーのURLをグロブでスキップします。

## http_wait_end_of_query {#http_wait_end_of_query}

Type: Bool

Default value: 0

サーバー側でHTTPレスポンスバッファリングを有効にします。

## http_write_exception_in_output_format {#http_write_exception_in_output_format}

Type: Bool

Default value: 1

有効な出力を生成するために出力形式に例外を書き込みます。JSONおよびXMLフォーマットで動作します。

## http_zlib_compression_level {#http_zlib_compression_level}

Type: Int64

Default value: 3

[enable_http_compression = 1](#enable_http_compression) の場合、HTTPリクエストへのレスポンスのデータ圧縮レベルを設定します。

可能な値: 1から9の数値。

## idle_connection_timeout {#idle_connection_timeout}

Type: UInt64

Default value: 3600

指定された秒数後にアイドルTCP接続を閉じるタイムアウト。

可能な値:

- 正の整数（0 - 即時閉じる、0秒後）。

## ignore_cold_parts_seconds {#ignore_cold_parts_seconds}

Type: Int64

Default value: 0

ClickHouse Cloudでのみ利用可能。新しいデータパーツをSELECTクエリから除外します。これらのデータパーツは、事前にウォームアップされるか、この多くの秒が経過するまで選択されません。Replicated-/SharedMergeTreeのみに適用されます。

## ignore_data_skipping_indices {#ignore_data_skipping_indices}

Type: String

Default value:

クエリによって使用される場合、指定されたスキッピングインデックスを無視します。

次の例を考えてください。

```sql
CREATE TABLE data
(
    key Int,
    x Int,
    y Int,
    INDEX x_idx x TYPE minmax GRANULARITY 1,
    INDEX y_idx y TYPE minmax GRANULARITY 1,
    INDEX xy_idx (x,y) TYPE minmax GRANULARITY 1
)
Engine=MergeTree()
ORDER BY key;

INSERT INTO data VALUES (1, 2, 3);

SELECT * FROM data;
SELECT * FROM data SETTINGS ignore_data_skipping_indices=''; -- クエリは CANNOT_PARSE_TEXT エラーを生成します。
SELECT * FROM data SETTINGS ignore_data_skipping_indices='x_idx'; -- ok。
SELECT * FROM data SETTINGS ignore_data_skipping_indices='na_idx'; -- ok。

SELECT * FROM data WHERE x = 1 AND y = 1 SETTINGS ignore_data_skipping_indices='xy_idx',force_data_skipping_indices='xy_idx' ; -- クエリは INDEX_NOT_USED エラーを生成します。なぜなら、xy_idx は明示的に無視されているからです。
SELECT * FROM data WHERE x = 1 AND y = 2 SETTINGS ignore_data_skipping_indices='xy_idx';
```

インデックスを無視しないクエリ:
```sql
EXPLAIN indexes = 1 SELECT * FROM data WHERE x = 1 AND y = 2;

Expression ((Projection + Before ORDER BY))
  Filter (WHERE)
    ReadFromMergeTree (default.data)
    Indexes:
      PrimaryKey
        Condition: true
        Parts: 1/1
        Granules: 1/1
      Skip
        Name: x_idx
        Description: minmax GRANULARITY 1
        Parts: 0/1
        Granules: 0/1
      Skip
        Name: y_idx
        Description: minmax GRANULARITY 1
        Parts: 0/0
        Granules: 0/0
      Skip
        Name: xy_idx
        Description: minmax GRANULARITY 1
        Parts: 0/0
        Granules: 0/0
```

`xy_idx`インデックスを無視する:
```sql
EXPLAIN indexes = 1 SELECT * FROM data WHERE x = 1 AND y = 2 SETTINGS ignore_data_skipping_indices='xy_idx';

Expression ((Projection + Before ORDER BY))
  Filter (WHERE)
    ReadFromMergeTree (default.data)
    Indexes:
      PrimaryKey
        Condition: true
        Parts: 1/1
        Granules: 1/1
      Skip
        Name: x_idx
        Description: minmax GRANULARITY 1
        Parts: 0/1
        Granules: 0/1
      Skip
        Name: y_idx
        Description: minmax GRANULARITY 1
        Parts: 0/0
        Granules: 0/0
```

MergeTreeファミリーのテーブルで動作します。

## ignore_drop_queries_probability {#ignore_drop_queries_probability}

Type: Float

Default value: 0

有効な場合、指定された確率でDROPテーブルクエリをすべて無視します（MemoryおよびJOINエンジンの場合、DROPがTRUNCATEに置き換えられます）。テスト目的で使用されます。

## ignore_materialized_views_with_dropped_target_table {#ignore_materialized_views_with_dropped_target_table}

Type: Bool

Default value: 0

ビューへのプッシュ中に削除されたターゲットテーブルを持つMVを無視します。

## ignore_on_cluster_for_replicated_access_entities_queries {#ignore_on_cluster_for_replicated_access_entities_queries}

Type: Bool

Default value: 0

レプリケートされたアクセスエンティティ管理クエリのためにON CLUSTER句を無視します。

## ignore_on_cluster_for_replicated_named_collections_queries {#ignore_on_cluster_for_replicated_named_collections_queries}

Type: Bool

Default value: 0

レプリケートされた名前付きコレクション管理クエリのためにON CLUSTER句を無視します。

## ignore_on_cluster_for_replicated_udf_queries {#ignore_on_cluster_for_replicated_udf_queries}

Type: Bool

Default value: 0

レプリケートされたUDF管理クエリのためにON CLUSTER句を無視します。

## implicit_select {#implicit_select}

Type: Bool

Default value: 0

先頭にSELECTキーワードを付けずにシンプルなSELECTクエリを書くことを許可します。これにより、計算機スタイルの使用が簡単になります。例えば、`1 + 2`は有効なクエリになります。

`clickhouse-local`ではデフォルトで有効になっており、明示的に無効にできます。

## implicit_transaction {#implicit_transaction}
<ExperimentalBadge/>

Type: Bool

Default value: 0

有効な場合、すでにトランザクション内にないときに、クエリをフルトランザクション（開始 + コミットまたはロールバック）でラップします。

## input_format_parallel_parsing {#input_format_parallel_parsing}

Type: Bool

Default value: 1

データ形式の順序を保持する並列解析を有効または無効にします。サポートされているのは[TSV](../../interfaces/formats.md/#tabseparated)、[TSKV](../../interfaces/formats.md/#tskv)、[CSV](../../interfaces/formats.md/#csv)および[JSONEachRow](../../interfaces/formats.md/#jsoneachrow)形式のみです。

可能な値:

- 1 — 有効。
- 0 — 無効。

## insert_allow_materialized_columns {#insert_allow_materialized_columns}

Type: Bool

Default value: 0

この設定を有効にすると、INSERTでマテリアライズドカラムを許可します。

## insert_deduplicate {#insert_deduplicate}

Type: Bool

Default value: 1

`INSERT`のブロック重複を有効または無効にします（Replicated*テーブル用）。

可能な値:

- 0 — 無効。
- 1 — 有効。

デフォルトでは、`INSERT`ステートメントによってレプリケートテーブルに挿入されたブロックは重複削除されます（[データレプリケーション](../../engines/table-engines/mergetree-family/replication.md)を参照）。

レプリケートテーブルでは、デフォルトで各パーティションに対して最新の100ブロックのみが重複削除されます（[replicated_deduplication_window](merge-tree-settings.md/#replicated-deduplication-window)、[replicated_deduplication_window_seconds](merge-tree-settings.md/#replicated-deduplication-window-seconds)を参照）。

非レプリケートテーブルについては、[non_replicated_deduplication_window](merge-tree-settings.md/#non-replicated-deduplication-window)を参照してください。

## insert_deduplication_token {#insert_deduplication_token}

Type: String

Default value:

この設定により、ユーザーはMergeTree/ReplicatedMergeTreeで独自の重複削除セマンティクスを提供できます。たとえば、各INSERTステートメントでこの設定の一意の値を提供することにより、重複削除を避けることができます。

可能な値:

- 任意の文字列

`insert_deduplication_token`は、空でないときにのみ重複削除に使用されます。

レプリケートテーブルでは、デフォルトで各パーティションに対して最新の100挿入のみが重複削除されます（[replicated_deduplication_window](merge-tree-settings.md/#replicated-deduplication-window)、[replicated_deduplication_window_seconds](merge-tree-settings.md/#replicated-deduplication-window-seconds)を参照）。

非レプリケートテーブルについては、[non_replicated_deduplication_window](merge-tree-settings.md/#non_replicated_deduplication-window)を参照してください。

:::note
`insert_deduplication_token`はパーティションレベルで動作します（`insert_deduplication`チェックサムと同様）。複数のパーティションが同じ`insert_deduplication_token`を持つことができます。
:::

例:

```sql
CREATE TABLE test_table
( A Int64 )
ENGINE = MergeTree
ORDER BY A
SETTINGS non_replicated_deduplication_window = 100;

INSERT INTO test_table SETTINGS insert_deduplication_token = 'test' VALUES (1);

-- 次の挿入は重複削除されない。なぜなら、insert_deduplication_tokenが異なるからです。
INSERT INTO test_table SETTINGS insert_deduplication_token = 'test1' VALUES (1);

-- 次の挿入は重複削除される。なぜなら、insert_deduplication_tokenが以前のどれかと同じだからです。
INSERT INTO test_table SETTINGS insert_deduplication_token = 'test' VALUES (2);

SELECT * FROM test_table

┌─A─┐
│ 1 │
└───┘
┌─A─┐
│ 1 │
└───┘
```

## insert_keeper_fault_injection_probability {#insert_keeper_fault_injection_probability}

Type: Float

Default value: 0

挿入中のKeeperリクエストでの失敗の近似確率。有効な値は[0.0, 1.0]の範囲です。

## insert_keeper_fault_injection_seed {#insert_keeper_fault_injection_seed}

Type: UInt64

Default value: 0

0 - ランダムシード、そうでなければ設定値。

## insert_keeper_max_retries {#insert_keeper_max_retries}

Type: UInt64

Default value: 20

この設定は、レプリケートされたMergeTreeへの挿入中のClickHouse Keeper（またはZooKeeper）リクエストの最大再試行回数を設定します。ネットワークエラー、Keeperセッションタイムアウト、またはリクエストタイムアウトのために失敗したKeeperリクエストのみが再試行の対象となります。

可能な値:

- 正の整数。
- 0 — 再試行は無効。

Cloudデフォルト値: `20`。

Keeperリクエストの再試行は、一定のタイムアウトの後に行われます。タイムアウトは、次の設定によって制御されます: `insert_keeper_retry_initial_backoff_ms`, `insert_keeper_retry_max_backoff_ms`。
最初の再試行は`insert_keeper_retry_initial_backoff_ms`タイムアウト後に行われます。その後のタイムアウトは次のように計算されます:
```
timeout = min(insert_keeper_retry_max_backoff_ms, latest_timeout * 2)
```

たとえば、`insert_keeper_retry_initial_backoff_ms=100`、`insert_keeper_retry_max_backoff_ms=10000`、`insert_keeper_max_retries=8`の場合、タイムアウトは`100, 200, 400, 800, 1600, 3200, 6400, 10000`となります。

障害耐性のほかに、再試行はユーザーエクスペリエンスを向上させることを目的としています。たとえば、Keeperがアップグレードのために再起動された際に、INSERTの実行中にエラーを返さないようにします。

## insert_keeper_retry_initial_backoff_ms {#insert_keeper_retry_initial_backoff_ms}

Type: UInt64

Default value: 100

INSERTクエリの実行中に失敗したKeeperリクエストを再試行するための初期タイムアウト（ミリ秒単位）。

可能な値:

- 正の整数。
- 0 — タイムアウトなし。

## insert_keeper_retry_max_backoff_ms {#insert_keeper_retry_max_backoff_ms}

Type: UInt64

Default value: 10000

INSERTクエリの実行中に失敗したKeeperリクエストを再試行するための最大タイムアウト（ミリ秒単位）。

可能な値:

- 正の整数。
- 0 — 最大タイムアウトは制限されません。

## insert_null_as_default {#insert_null_as_default}

Type: Bool

Default value: 1

[nullable](../../sql-reference/data-types/nullable.md/#data_type-nullable)データ型のカラムに[NULL](../../sql-reference/syntax.md/#null-literal)の代わりに[デフォルト値](../../sql-reference/statements/create/table.md/#create-default-values)を挿入することを有効または無効にします。
カラム型がnullableでない場合、この設定が無効な場合に`NULL`を挿入すると例外が発生します。カラム型がnullableの場合、`NULL`値はこの設定に関係なくそのまま挿入されます。

この設定は[INSERT ... SELECT](../../sql-reference/statements/insert-into.md/#inserting-the-results-of-select)クエリに適用されます。SELECTサブクエリは、`UNION ALL`句で結合される可能性があります。

可能な値:

- 0 — 非nullableカラムに`NULL`を挿入すると例外が発生します。
- 1 — `NULL`の代わりにデフォルトカラム値が挿入されます。

## insert_quorum {#insert_quorum}

Type: UInt64Auto

Default value: 0

:::note
この設定はSharedMergeTreeには適用されません。詳細は[SharedMergeTreeの整合性](/docs/cloud/reference/shared-merge-tree/#consistency)を参照してください。
:::

クオーラム書き込みを有効にします。

- `insert_quorum < 2`の場合、クオーラム書き込みは無効です。
- `insert_quorum >= 2`の場合、クオーラム書き込みが有効です。
- `insert_quorum = 'auto'`の場合、過半数の数（`number_of_replicas / 2 + 1`）をクオーラム数として使用します。

クオーラム書き込みは、`INSERT`が成功するためにはClickHouseが`insert_quorum`のレプリカに正しくデータを書き込む必要があります。何らかの理由で成功した書き込みのレプリカの数が`insert_quorum`に達しない場合、その書き込みは失敗と見なされ、ClickHouseはデータがすでに書き込まれているすべてのレプリカから挿入されたブロックを削除します。

`insert_quorum_parallel`が無効な場合、クオーラム内のすべてのレプリカは一貫しており、すなわちすべての以前の`INSERT`クエリのデータを含みます（`INSERT`のシーケンスは直線化されます）。`insert_quorum`で書き込まれたデータを読み取るとき、`insert_quorum_parallel`が無効な場合は、[select_sequential_consistency](#select_sequential_consistency)を使用して`SELECT`クエリに対して逐次的一貫性をオンにできます。

ClickHouseは例外を生成します：

- クエリの実行時に利用可能なレプリカの数が`insert_quorum`未満の場合。
- `insert_quorum_parallel`が無効な状態で、以前のブロックが`insert_quorum`のレプリカに挿入されていないときにデータを書き込もうとした場合。この状況は、ユーザーが前の`INSERT`クエリが完了する前に同じテーブルに別の`INSERT`クエリを実行しようとした場合に発生する可能性があります。

以下も参照してください：

- [insert_quorum_timeout](#insert_quorum_timeout)
- [insert_quorum_parallel](#insert_quorum_parallel)
- [select_sequential_consistency](#select_sequential_consistency)

## insert_quorum_parallel {#insert_quorum_parallel}

Type: Bool

Default value: 1

:::note
この設定はSharedMergeTreeには適用されません。詳細は[SharedMergeTreeの整合性](/docs/cloud/reference/shared-merge-tree/#consistency)を参照してください。
:::

クオーラム`INSERT`クエリの並列性を有効または無効にします。有効な場合、以前のクエリがまだ完了していない間に追加の`INSERT`クエリを送信できます。無効な場合、同じテーブルへの追加の書き込みは拒否されます。

可能な値：

- 0 — 無効。
- 1 — 有効。

以下も参照してください：

- [insert_quorum](#insert_quorum)
- [insert_quorum_timeout](#insert_quorum_timeout)
- [select_sequential_consistency](#select_sequential_consistency)

## insert_quorum_timeout {#insert_quorum_timeout}

Type: Milliseconds

Default value: 600000

クオーラムへの書き込みタイムアウト（ミリ秒単位）。タイムアウトが過ぎても書き込みが行われていない場合、ClickHouseは例外を生成し、クライアントは同じブロックを同じレプリカまたは他のレプリカに書き込むためにクエリを繰り返す必要があります。

以下も参照してください：

- [insert_quorum](#insert_quorum)
- [insert_quorum_parallel](#insert_quorum_parallel)
- [select_sequential_consistency](#select_sequential_consistency)

## insert_shard_id {#insert_shard_id}

Type: UInt64

Default value: 0

0でない場合、データを同期的に挿入する[Distributed](../../engines/table-engines/special/distributed.md/#distributed)テーブルのシャードを指定します。

`insert_shard_id`の値が正しくない場合、サーバーは例外をスローします。

`requested_cluster`上のシャードの数を取得するには、サーバー構成を確認するか、次のクエリを使用できます：

``` sql
SELECT uniq(shard_num) FROM system.clusters WHERE cluster = 'requested_cluster';
```

可能な値：

- 0 — 無効。
- 対応する[Distributed](../../engines/table-engines/special/distributed.md/#distributed)テーブルの`1`から`shards_num`の範囲の任意の数。

**例**

クエリ：

```sql
CREATE TABLE x AS system.numbers ENGINE = MergeTree ORDER BY number;
CREATE TABLE x_dist AS x ENGINE = Distributed('test_cluster_two_shards_localhost', currentDatabase(), x);
INSERT INTO x_dist SELECT * FROM numbers(5) SETTINGS insert_shard_id = 1;
SELECT * FROM x_dist ORDER BY number ASC;
```

結果：

``` text
┌─number─┐
│      0 │
│      0 │
│      1 │
│      1 │
│      2 │
│      2 │
│      3 │
│      3 │
│      4 │
│      4 │
└────────┘
```

## interactive_delay {#interactive_delay}

Type: UInt64

Default value: 100000

リクエスト実行がキャンセルされたかどうかを確認し、進捗を送信するためのマイクロ秒単位の間隔。

## intersect_default_mode {#intersect_default_mode}

Type: SetOperationMode

Default value: ALL

INTERSECTクエリでのデフォルトモードを設定します。可能な値: 空の文字列、'ALL'、'DISTINCT'。空の場合、モードなしのクエリは例外をスローします。

## join_algorithm {#join_algorithm}

Type: JoinAlgorithm

Default value: direct,parallel_hash,hash

使用される[JOIN](../../sql-reference/statements/select/join.md)アルゴリズムを指定します。

いくつかのアルゴリズムを指定でき、利用可能なものが特定のクエリの種類/厳格さとテーブルエンジンに基づいて選択されます。

可能な値：

- grace_hash

 [Grace hash join](https://en.wikipedia.org/wiki/Hash_join#Grace_hash_join)が使用されます。 Grace hashは、メモリ使用量を制限しながら高性能の複雑な結合を提供するアルゴリズムオプションを提供します。

Grace joinの最初のフェーズは、右テーブルを読み込み、キー列のハッシュ値に応じてNバケットに分割します（最初は、Nは`grace_hash_join_initial_buckets`です）。これは、各バケットが独立して処理できることを確保する方法で行われます。最初のバケットからの行は、メモリ内のハッシュテーブルに追加され、他はディスクに保存されます。ハッシュテーブルがメモリ制限（例: [`max_bytes_in_join`](/docs/operations/settings/query-complexity.md/#max_bytes_in_join)によって設定された制限）を超えると、バケットの数が増加し、各行に割り当てられたバケットも変更されます。現在のバケットに属さない行はフラッシュされ、再割り当てされます。

 `INNER/LEFT/RIGHT/FULL ALL/ANY JOIN`をサポートします。

- hash

 [Hash join algorithm](https://en.wikipedia.org/wiki/Hash_join)が使用されます。最も一般的な実装で、すべての種類と厳格さ、`JOIN ON`セクションで`OR`で結合される複数の結合キーの組み合わせをサポートします。

 `hash`アルゴリズムを使用する場合、JOINの右部分はRAMにアップロードされます。

- parallel_hash

 データをバケットに分割し、並行して複数のハッシュテーブルを構築することでこのプロセスを迅速化する`hash`のバリエーションです。

 `parallel_hash`アルゴリズムを使用する場合、JOINの右部分はRAMにアップロードされます。

- partial_merge

 右テーブルのみが完全にソートされる[sort-merge algorithm](https://en.wikipedia.org/wiki/Sort-merge_join)のバリエーションです。

 `RIGHT JOIN`および`FULL JOIN`は、`ALL`の厳格さでのみサポートされます（`SEMI`、`ANTI`、`ANY`、`ASOF`はサポートされません）。

 `partial_merge`アルゴリズムを使用する場合、ClickHouseはデータをソートし、ディスクにダンプします。ClickHouseの`partial_merge`アルゴリズムは、クラシックな実装とは若干異なります。最初に、ClickHouseは結合キーに従って右テーブルをブロックに分けてソートし、ソートされたブロックのためにミン・マックスインデックスを作成します。その後、左テーブルの部分を`join key`に従ってソートし、それを右テーブルに結合します。ミン・マックスインデックスも不要な右テーブルブロックをスキップするために使用されます。

- direct

 右テーブルのストレージがキー値リクエストをサポートする場合に適用できるアルゴリズムです。

 `direct`アルゴリズムは、左テーブルの行をキーとして使用して右テーブルをルックアップします。特別なストレージ（[Dictionary](../../engines/table-engines/special/dictionary.md/#dictionary)または[EmbeddedRocksDB](../../engines/table-engines/integrations/embedded-rocksdb.md)など）によってのみサポートされ、その中で`LEFT`および`INNER` JOINのみがサポートされます。

- auto

 `auto`に設定された場合、最初に`hash` JOINが試行され、メモリ制限が違反された場合には別のアルゴリズムにスイッチされます。

- full_sorting_merge

 [Sort-merge algorithm](https://en.wikipedia.org/wiki/Sort-merge_join)を使用して結合前に結合テーブルを完全にソートします。

- prefer_partial_merge

 ClickHouseは、可能であれば常に`partial_merge` JOINを使用し、そうでなければ`hash`を使用します。*非推奨*、`partial_merge,hash`と同じ。

- default (非推奨)

 もはや使用しないでください。同じく`direct,hash`です。すなわち、まず直接結合を使用し、その後ハッシュ結合を試みます（この順序で）。

## join_any_take_last_row {#join_any_take_last_row}

Type: Bool

Default value: 0

`ANY`厳格さを持つ結合操作の動作を変更します。

:::note
この設定は[Join](../../engines/table-engines/special/join.md)エンジンテーブルの`JOIN`操作にのみ適用されます。
:::

可能な値:

- 0 — 右テーブルに一致する行が複数ある場合、見つかった最初の行のみが結合されます。
- 1 — 右テーブルに一致する行が複数ある場合、見つかった最後の行のみが結合されます。

以下も参照してください：

- [JOIN clause](../../sql-reference/statements/select/join.md/#select-join)
- [Join table engine](../../engines/table-engines/special/join.md)
- [join_default_strictness](#join_default_strictness)

## join_default_strictness {#join_default_strictness}

Type: JoinStrictness

Default value: ALL

[JOIN句](../../sql-reference/statements/select/join.md/#select-join)のデフォルトの厳格さを設定します。

可能な値：

- `ALL` — 右テーブルに一致する行が複数ある場合、ClickHouseは一致する行からの[Cartesian product](https://en.wikipedia.org/wiki/Cartesian_product)を作成します。これは、標準SQLからの通常の`JOIN`の動作です。
- `ANY` — 右テーブルに一致する行が複数ある場合、見つかった最初の行のみが結合されます。右テーブルに一致する行が1つしかない場合、ANYとALLの結果は同じです。
- `ASOF` — 一致が不確かなシーケンスの結合。
- `空の文字列` — クエリに`ALL`または`ANY`が指定されていない場合、ClickHouseは例外をスローします。

## join_on_disk_max_files_to_merge {#join_on_disk_max_files_to_merge}

Type: UInt64

Default value: 64

ディスク上で実行されるMergeJoin操作の並列ソートに許可されるファイルの数を制限します。

設定の値が大きいほど、より多くのRAMが使用され、ディスクI/Oが少なくて済みます。

可能な値：

- 2から始まる任意の正の整数。

## join_output_by_rowlist_perkey_rows_threshold {#join_output_by_rowlist_perkey_rows_threshold}

Type: UInt64

Default value: 5

ハッシュ結合において、右テーブルの毎キーの平均行数を出力するかどうかを決定する下限。

## join_overflow_mode {#join_overflow_mode}

Type: OverflowMode

Default value: throw

制限が超過した場合に実行するアクションを指定します。

## join_to_sort_maximum_table_rows {#join_to_sort_maximum_table_rows}
<ExperimentalBadge/>

Type: UInt64

Default value: 10000

左または内部結合において、右テーブルをキーで再範囲するかどうかを判断するための右テーブル内の最大行数。

## join_to_sort_minimum_perkey_rows {#join_to_sort_minimum_perkey_rows}
<ExperimentalBadge/>

Type: UInt64

Default value: 40

左または内部結合において、右テーブルをキーで再範囲するかどうかを判断するための右テーブル内の毎キーの平均行数の下限。この設定は、スパーステーブルキーのために最適化が適用されないことを確保します。

## join_use_nulls {#join_use_nulls}

Type: Bool

Default value: 0

[JOIN](../../sql-reference/statements/select/join.md)の動作を設定します。テーブルを結合する際、空のセルが現れる可能性があります。ClickHouseはこの設定に基づいて異なる方法でそれらを埋めます。

可能な値：

- 0 — 空のセルは、対応するフィールドタイプのデフォルト値で埋められます。
- 1 — `JOIN`は標準SQLと同様の動作をします。対応するフィールドの型は[Nullable](../../sql-reference/data-types/nullable.md/#data_type-nullable)に変換され、空のセルは[NULL](../../sql-reference/syntax.md)で埋められます。

## joined_subquery_requires_alias {#joined_subquery_requires_alias}

Type: Bool

Default value: 1

適切な名前の資格を確保するために、結合されたサブクエリおよびテーブル関数がエイリアスを持つように強制します。

## kafka_disable_num_consumers_limit {#kafka_disable_num_consumers_limit}

Type: Bool

Default value: 0

利用可能なCPUコア数に依存するkafka_num_consumersに対する制限を無効にします。

## kafka_max_wait_ms {#kafka_max_wait_ms}

Type: Milliseconds

Default value: 5000

[Kafka](../../engines/table-engines/integrations/kafka.md/#kafka)からメッセージを読み取るための待機時間（ミリ秒）。

可能な値：

- 正の整数。
- 0 — 無限タイムアウト。

以下も参照してください：

- [Apache Kafka](https://kafka.apache.org/)

## keeper_map_strict_mode {#keeper_map_strict_mode}

Type: Bool

Default value: 0

KeeperMapでの操作中に追加のチェックを強制します。例：既に存在するキーへの挿入時に例外をスローします。

## keeper_max_retries {#keeper_max_retries}

Type: UInt64

Default value: 10

一般的なKeeper操作の最大再試行回数。

## keeper_retry_initial_backoff_ms {#keeper_retry_initial_backoff_ms}

Type: UInt64

Default value: 100

一般的なKeeper操作の初期バックオフタイムアウト。

## keeper_retry_max_backoff_ms {#keeper_retry_max_backoff_ms}

Type: UInt64

Default value: 5000

一般的なKeeper操作の最大バックオフタイムアウト。

## least_greatest_legacy_null_behavior {#least_greatest_legacy_null_behavior}

Type: Bool

Default value: 0

有効な場合、'least'および'greatest'関数は、引数の1つがNULLの場合、NULLを返します。

## legacy_column_name_of_tuple_literal {#legacy_column_name_of_tuple_literal}

Type: Bool

Default value: 0

大きなタプルリテラルのすべての要素の名前をハッシュの代わりにそのカラム名ですべて表示します。この設定は互換性の理由でのみ存在します。バージョン21.7よりも低いクラスタを高いバージョンにロールアウトする際には、「true」に設定するのが適切です。

## lightweight_deletes_sync {#lightweight_deletes_sync}

Type: UInt64

Default value: 2

[`mutations_sync`](#mutations_sync)と同じですが、軽量削除の実行のみを制御します。

可能な値：

- 0 - 変更は非同期に実行されます。
- 1 - クエリは現在のサーバーでの軽量削除の完了を待ちます。
- 2 - クエリはすべてのレプリカでの軽量削除の完了を待ちます（存在する場合）。

**参照**

- [ALTERクエリの同期性](../../sql-reference/statements/alter/index.md/#synchronicity-of-alter-queries)
- [Mutations](../../sql-reference/statements/alter/index.md/#mutations)

## limit {#limit}

Type: UInt64

Default value: 0

クエリ結果から取得する最大行数を設定します。これは[LIMIT](../../sql-reference/statements/select/limit.md/#limit-clause)句で設定された値を調整し、クエリに指定された制限がこの設定で設定された制限を超えないようにします。

可能な値：

- 0 — 行数は制限されません。
- 正の整数。

## live_view_heartbeat_interval {#live_view_heartbeat_interval}
<ExperimentalBadge/>

Type: Seconds

Default value: 15

ライブクエリが生存していることを示すためのハートビート間隔（秒単位）。

## load_balancing {#load_balancing}

Type: LoadBalancing

Default value: random

分散クエリ処理に使用されるレプリカ選択のアルゴリズムを指定します。

ClickHouseは、レプリカを選択するための以下のアルゴリズムをサポートしています。

- [Random](#load_balancing-random)（デフォルト）
- [Nearest hostname](#load_balancing-nearest_hostname)
- [Hostname levenshtein distance](#load_balancing-hostname_levenshtein_distance)
- [In order](#load_balancing-in_order)
- [First or random](#load_balancing-first_or_random)
- [Round robin](#load_balancing-round_robin)

以下も参照してください：

- [distributed_replica_max_ignored_errors](#distributed_replica_max_ignored_errors)

### Random (によって デフォルト) {#load_balancing-random}

``` sql
load_balancing = random
```

エラーの数は、各レプリカに対してカウントされます。クエリは、最も少ないエラーのレプリカに送信され、もしそれが複数ある場合は、そのうちの任意の一つに送信されます。
欠点: サーバーの近接性は考慮されていない; もしレプリカが異なるデータを持っている場合、異なるデータを取得することになります。
### Nearest Hostname {#load_balancing-nearest_hostname}

``` sql
load_balancing = nearest_hostname
```

エラーの数は各レプリカごとにカウントされます。5分ごとに、エラーの数は整数で2で割られます。これにより、最近の時間に対して指数的スムージングを使ってエラーの数が計算されます。もし最小のエラー数を持つレプリカが1つだけであれば（つまり、他のレプリカで最近エラーが発生している場合）、クエリはそのレプリカに送信されます。同じ最小のエラー数を持つレプリカが複数あれば、設定ファイルのサーバーのホスト名に最も似たホスト名のレプリカにクエリが送信されます（同じ位置における異なる文字の数に基づき、いずれのホスト名の最小長さまで）。

例えば、example01-01-1とexample01-01-2は1つの位置で異なり、example01-01-1とexample01-02-2は2つの場所で異なります。
この方法は原始的に思えるかもしれませんが、ネットワークトポロジーに関する外部データを必要とせず、IPv6アドレスにとって複雑なIPアドレスの比較も行いません。

したがって、等しいレプリカが存在する場合、名前によって最も近いものが優先されます。
同じサーバーにクエリを送信する場合、障害がない限り、分散クエリも同じサーバーに送信されると仮定できます。したがって、レプリカに異なるデータが配置されていても、クエリは主に同じ結果を返します。

### ホスト名レーベンシュタイン距離 {#load_balancing-hostname_levenshtein_distance}

``` sql
load_balancing = hostname_levenshtein_distance
```

`nearest_hostname`と同様ですが、ホスト名を[レーベンシュタイン距離](https://en.wikipedia.org/wiki/Levenshtein_distance)の観点から比較します。例えば：

``` text
example-clickhouse-0-0 ample-clickhouse-0-0
1

example-clickhouse-0-0 example-clickhouse-1-10
2

example-clickhouse-0-0 example-clickhouse-12-0
3
```

### 順番に {#load_balancing-in_order}

``` sql
load_balancing = in_order
```

同じエラー数を持つレプリカは、設定で指定された順序でアクセスされます。
この方法は、どのレプリカが好ましいか正確にわかっている場合に適しています。

### 最初のものまたはランダム {#load_balancing-first_or_random}

``` sql
load_balancing = first_or_random
```

このアルゴリズムは、セット内の最初のレプリカを選択するか、最初が利用できない場合はランダムなレプリカを選択します。これは、クロスレプリケーショントポロジー構成で効果的ですが、他の構成では無用です。

`first_or_random`アルゴリズムは、`in_order`アルゴリズムの問題を解決します。`in_order`では、1つのレプリカがダウンすると、次のものが二重の負荷を受け、他のレプリカは通常のトラフィックを処理します。`first_or_random`アルゴリズムを使用する場合、負荷はまだ利用可能なレプリカ間で均等に分散されます。

最初のレプリカを明示的に定義するには、設定 `load_balancing_first_offset` を使用できます。これにより、レプリカ間でクエリのワークロードを再バランスするためのより多くの制御が提供されます。

### ラウンドロビン {#load_balancing-round_robin}

``` sql
load_balancing = round_robin
```

このアルゴリズムは、同じエラー数を持つレプリカ間でラウンドロビンポリシーを使用します（`round_robin`ポリシーのクエリのみがカウントされます）。

## load_balancing_first_offset {#load_balancing_first_offset}

タイプ: UInt64

デフォルト値: 0

FIRST_OR_RANDOMロードバランシング戦略が使用されている場合に、クエリを優先的に送信するレプリカ。

## load_marks_asynchronously {#load_marks_asynchronously}

タイプ: Bool

デフォルト値: 0

MergeTreeマークを非同期でロードします。

## local_filesystem_read_method {#local_filesystem_read_method}

タイプ: String

デフォルト値: pread_threadpool

ローカルファイルシステムからデータを読み取る方法。選択肢は: read, pread, mmap, io_uring, pread_threadpool。 'io_uring'メソッドは実験的であり、Log, TinyLog, StripeLog, File, Set, Joinや、同時読み取りと書き込みの存在下での追加可能なファイルを持つ他のテーブルでは機能しません。

## local_filesystem_read_prefetch {#local_filesystem_read_prefetch}

タイプ: Bool

デフォルト値: 0

ローカルファイルシステムからデータを読み取る際に、プリフェッチを使用すべきかどうか。

## lock_acquire_timeout {#lock_acquire_timeout}

タイプ: Seconds

デフォルト値: 120

ロッキングリクエストが失敗するまでの待機時間（秒数）を定義します。

ロックタイムアウトは、テーブルでの読み取り/書き込み操作中のデッドロックを防ぐために使用されます。タイムアウトが切れると、ロッキングリクエストは失敗し、ClickHouseサーバーは「ロックの試みがタイムアウトしました！ デッドロックを回避しました。クライアントは再試行すべきです。」という例外をスローします。エラーコードは`DEADLOCK_AVOIDED`です。

可能な値：

- 正の整数（秒単位）。
- 0 — ロックタイムアウトなし。

## log_comment {#log_comment}

タイプ: String

デフォルト値:

`system.query_log`テーブルの`log_comment`フィールドの値およびサーバーログのコメントテキストを指定します。

サーバーログの可読性を向上させるために使用できます。加えて、[clickhouse-test](../../development/tests.md)を実行した後、`system.query_log`からテストに関連するクエリを選択するのにも役立ちます。

可能な値：

- [max_query_size](#max_query_size)を超えない任意の文字列。もしmax_query_sizeを超えると、サーバーは例外をスローします。

**例**

クエリ：

``` sql
SET log_comment = 'log_comment test', log_queries = 1;
SELECT 1;
SYSTEM FLUSH LOGS;
SELECT type, query FROM system.query_log WHERE log_comment = 'log_comment test' AND event_date >= yesterday() ORDER BY event_time DESC LIMIT 2;
```

結果：

``` text
┌─type────────┬─query─────┐
│ QueryStart  │ SELECT 1; │
│ QueryFinish │ SELECT 1; │
└─────────────┴───────────┘
```

## log_formatted_queries {#log_formatted_queries}

タイプ: Bool

デフォルト値: 0

フォーマットされたクエリを[system.query_log](../../operations/system-tables/query_log.md)システムテーブルにログすることを許可します（`formatted_query`カラムにデータを格納）。

可能な値：

- 0 — フォーマットされたクエリはシステムテーブルにログされません。
- 1 — フォーマットされたクエリはシステムテーブルにログされます。

## log_processors_profiles {#log_processors_profiles}

タイプ: Bool

デフォルト値: 1

実行中のプロセッサがデータの取得を待つ間にかけた時間を`system.processors_profile_log`テーブルに書き込みます。

参照：

- [`system.processors_profile_log`](../../operations/system-tables/processors_profile_log.md)
- [`EXPLAIN PIPELINE`](../../sql-reference/statements/explain.md/#explain-pipeline)

## log_profile_events {#log_profile_events}

タイプ: Bool

デフォルト値: 1

クエリのパフォーマンス統計をquery_log、query_thread_log、およびquery_views_logにログします。

## log_queries {#log_queries}

タイプ: Bool

デフォルト値: 1

クエリログの設定。

この設定でClickHouseに送信されたクエリは、[query_log](../../operations/server-configuration-parameters/settings.md/#query-log)サーバー設定パラメーターの規則に従ってログされます。

例：

``` text
log_queries=1
```

## log_queries_cut_to_length {#log_queries_cut_to_length}

タイプ: UInt64

デフォルト値: 100000

クエリの長さが指定された閾値を超える場合（バイト数）、クエリをクエリログに書き込む際に切り捨てます。通常のテキストログに印刷されるクエリの長さも制限します。

## log_queries_min_query_duration_ms {#log_queries_min_query_duration_ms}

タイプ: ミリ秒

デフォルト値: 0

これが有効（非ゼロ）である場合、この設定の値よりも速いクエリはログに記録されません（これは[MySQL Slow Query Log](https://dev.mysql.com/doc/refman/5.7/en/slow-query-log.html)の`long_query_time`として考えることができます）。これにより、以下のテーブルには見つかりません：

- `system.query_log`
- `system.query_thread_log`

ログに記録されるのは以下のタイプのクエリのみです：

- `QUERY_FINISH`
- `EXCEPTION_WHILE_PROCESSING`

- タイプ: ミリ秒
- デフォルト値: 0（任意のクエリ）

## log_queries_min_type {#log_queries_min_type}

タイプ: LogQueriesType

デフォルト値: QUERY_START

記録する`query_log`の最小タイプ。

可能な値：
- `QUERY_START` (`=1`)
- `QUERY_FINISH` (`=2`)
- `EXCEPTION_BEFORE_START` (`=3`)
- `EXCEPTION_WHILE_PROCESSING` (`=4`)

どのエンティティが`query_log`に記録されるかを制限するために使用できます。例えば、エラーのみに興味がある場合は、`EXCEPTION_WHILE_PROCESSING`を使用できます：

``` text
log_queries_min_type='EXCEPTION_WHILE_PROCESSING'
```

## log_queries_probability {#log_queries_probability}

タイプ: Float

デフォルト値: 1

ユーザーは、指定された確率でランダムに選択されたクエリのみを[query_log](../../operations/system-tables/query_log.md)、[query_thread_log](../../operations/system-tables/query_thread_log.md)、および[query_views_log](../../operations/system-tables/query_views_log.md)システムテーブルに書き込むことができます。これは、毎秒多数のクエリを交換する負荷を減らすのに役立ちます。

可能な値：

- 0 — クエリはシステムテーブルにログされません。
- 0..1の範囲の正の浮動小数点数。例えば、設定値が`0.5`の場合、おおよそクエリの半分がシステムテーブルに記録されます。
- 1 — すべてのクエリがシステムテーブルに記録されます。

## log_query_settings {#log_query_settings}

タイプ: Bool

デフォルト値: 1

クエリの設定をquery_logとOpenTelemetryスパンログに記録します。

## log_query_threads {#log_query_threads}

タイプ: Bool

デフォルト値: 0

クエリスレッドのログを設定します。

クエリスレッドは、[system.query_thread_log](../../operations/system-tables/query_thread_log.md)テーブルにログされます。この設定は、[log_queries](#log_queries)がtrueの場合のみ影響します。この設定で実行されるClickHouseのクエリスレッドは、[query_thread_log](../../operations/server-configuration-parameters/settings.md/#query_thread_log)サーバー設定パラメーターの規則に従ってログされます。

可能な値：

- 0 — 無効。
- 1 — 有効。

**例**

``` text
log_query_threads=1
```

## log_query_views {#log_query_views}

タイプ: Bool

デフォルト値: 1

クエリビューのログを設定します。

この設定を有効にしてClickHouseで実行されるクエリに関連するビュー（マテリアライズドビューまたはライブビュー）がある場合、これらは[query_views_log](../../operations/server-configuration-parameters/settings.md/#query_views_log)サーバー設定パラメーターにログされます。

例：

``` text
log_query_views=1
```

## low_cardinality_allow_in_native_format {#low_cardinality_allow_in_native_format}

タイプ: Bool

デフォルト値: 1

[LowCardinality](../../sql-reference/data-types/lowcardinality.md)データ型を[Native](../../interfaces/formats.md/#native)形式で使用することを許可または制限します。

`LowCardinality`の使用が制限されている場合、ClickHouseサーバーは`SELECT`クエリのために`LowCardinality`カラムを通常のカラムに変換し、`INSERT`クエリのために通常のカラムを`LowCardinality`カラムに変換します。

この設定は、主に`LowCardinality`データ型をサポートしていないサードパーティクライアントのために必要です。

可能な値：

- 1 — `LowCardinality`の使用が制限されていない。
- 0 — `LowCardinality`の使用が制限されている。

## low_cardinality_max_dictionary_size {#low_cardinality_max_dictionary_size}

タイプ: UInt64

デフォルト値: 8192

[LowCardinality](../../sql-reference/data-types/lowcardinality.md)データ型のために、ストレージファイルシステムに書き込むことができる共有グローバル辞書の最大サイズ（行数）を設定します。この設定は、辞書の無限成長によるRAMの問題を防ぐためのものです。最大辞書サイズの制限によりエンコードできないデータは、ClickHouseが通常の方法で書き込むことになります。

可能な値：

- 任意の正の整数。

## low_cardinality_use_single_dictionary_for_part {#low_cardinality_use_single_dictionary_for_part}

タイプ: Bool

デフォルト値: 0

データ部分のために単一の辞書を使用するかどうかをオンまたはオフにします。

デフォルトでは、ClickHouseサーバーは辞書のサイズを監視し、辞書がオーバーフローすると次の辞書の書き込みを開始します。複数の辞書の作成を禁止するには、`low_cardinality_use_single_dictionary_for_part = 1`を設定します。

可能な値：

- 1 — データ部分のための複数の辞書の作成が禁止される。
- 0 — データ部分のための複数の辞書の作成が禁止されない。

## materialize_skip_indexes_on_insert {#materialize_skip_indexes_on_insert}

タイプ: Bool

デフォルト値: 1

INSERTがスキップインデックスを構築して保存します。無効にすると、スキップインデックスはマージ中または明示的なMATERIALIZE INDEXで構築および保存されます。

## materialize_statistics_on_insert {#materialize_statistics_on_insert}

タイプ: Bool

デフォルト値: 1

INSERTが統計を構築して挿入します。無効にすると、統計はマージ中または明示的なMATERIALIZE STATISTICSで構築および保存されます。

## materialize_ttl_after_modify {#materialize_ttl_after_modify}

タイプ: Bool

デフォルト値: 1

ALTER MODIFY TTLクエリの後、古いデータにTTLを適用します。

## materialized_views_ignore_errors {#materialized_views_ignore_errors}

タイプ: Bool

デフォルト値: 0

MATERIALIZED VIEWに対するエラーを無視し、MVに関係なく元のブロックをテーブルに提供します。

## max_analyze_depth {#max_analyze_depth}

タイプ: UInt64

デフォルト値: 5000

インタープリターによって実行される分析の最大数。

## max_ast_depth {#max_ast_depth}

タイプ: UInt64

デフォルト値: 1000

クエリ構文ツリーの最大深度。パース後にチェックされます。

## max_ast_elements {#max_ast_elements}

タイプ: UInt64

デフォルト値: 50000

クエリ構文ツリーの最大サイズ（ノードの数）。パース後にチェックされます。

## max_autoincrement_series {#max_autoincrement_series}

タイプ: UInt64

デフォルト値: 1000

`generateSeriesID`関数によって作成される列の最大数。

各系列はKeeperのノードを表すため、数百万を超えない方が推奨されます。

## max_backup_bandwidth {#max_backup_bandwidth}

タイプ: UInt64

デフォルト値: 0

サーバー上の特定のバックアップの最大読み取り速度（バイト/秒）。ゼロは無制限を意味します。

## max_block_size {#max_block_size}

タイプ: UInt64

デフォルト値: 65409

ClickHouseでは、データはブロック（カラムパーツのセット）で処理されます。単一のブロックに対する内部処理サイクルは効率的ですが、各ブロックを処理する際には顕著なコストがかかります。

`max_block_size`設定は、テーブルからデータをロードする際に単一ブロックに含まれることが推奨される最大行数を示します。`max_block_size`のサイズのブロックは、必ずしもテーブルからロードされるわけではありません。ClickHouseが取得する必要があるデータが少ないと判断した場合は、より小さいブロックが処理されます。

ブロックサイズは小さすぎないようにすべきです。そうでないと、各ブロックを処理する際に顕著なコストがかかります。また、LIMIT句を持つクエリが最初のブロックを処理した後に迅速に実行されるよう、大きすぎることも避けるべきです。`max_block_size`を設定する際は、大量の列を複数のスレッドで抽出する際にメモリを消費しすぎないようにし、ある程度のキャッシュローカリティを保つことが目標です。

## max_bytes_before_external_group_by {#max_bytes_before_external_group_by}

タイプ: UInt64

デフォルト値: 0

GROUP BY操作中のメモリ使用量がこのバイト数を超えた場合、「外部集計」モードを有効にします（ディスクにデータをスピルします）。推奨値は、使用可能なシステムメモリの半分です。

## max_bytes_before_external_sort {#max_bytes_before_external_sort}

タイプ: UInt64

デフォルト値: 0

ORDER BY操作中のメモリ使用量がこのバイト数を超えた場合、「外部ソート」モードを有効にします（ディスクにデータをスピルします）。推奨値は、使用可能なシステムメモリの半分です。

## max_bytes_before_remerge_sort {#max_bytes_before_remerge_sort}

タイプ: UInt64

デフォルト値: 1000000000

LIMITを持つORDER BYの場合、メモリ使用量が指定された閾値を超えた場合は、最終マージの前にブロックをマージして上位LIMIT行のみを保持します。

## max_bytes_in_distinct {#max_bytes_in_distinct}

タイプ: UInt64

デフォルト値: 0

DISTINCTの実行のために、メモリ内での状態の最大合計サイズ（非圧縮バイト）。

## max_bytes_in_join {#max_bytes_in_join}

タイプ: UInt64

デフォルト値: 0

JOINのためのハッシュテーブルの最大サイズ（メモリ内のバイト数）。

## max_bytes_in_set {#max_bytes_in_set}

タイプ: UInt64

デフォルト値: 0

INセクションの実行から生じるセットの最大サイズ（メモリ内のバイト数）。

## max_bytes_ratio_before_external_group_by {#max_bytes_ratio_before_external_group_by}

タイプ: Double

デフォルト値: 0.5

外部GROUP BYを有効にする前の使用メモリの比率。0.6に設定すると、メモリ使用量がクエリに対して許可されたメモリの60%に達すると外部GROUP BYが使用されます。

## max_bytes_ratio_before_external_sort {#max_bytes_ratio_before_external_sort}

タイプ: Double

デフォルト値: 0.5

外部ORDER BYを有効にする前の使用メモリの比率。0.6に設定すると、メモリ使用量がクエリに対して許可されたメモリの60%に達すると外部ORDER BYが使用されます。

## max_bytes_to_read {#max_bytes_to_read}

タイプ: UInt64

デフォルト値: 0

最も「深い」ソースから読み取るバイト数への制限（デコンプレッション後）。つまり、最も深いサブクエリ内でのみ適用されます。リモートサーバーから読み取る際には、リモートサーバーでのみチェックされます。

## max_bytes_to_read_leaf {#max_bytes_to_read_leaf}

タイプ: UInt64

デフォルト値: 0

分散クエリのリーフノードでの読み取りバイト数への制限（デコンプレッション後）。制限は、ルートノードの最終的なマージ段階を除くローカル読み取りに適用されます。設定は、prefer_localhost_replica=1の際は不安定です。

## max_bytes_to_sort {#max_bytes_to_sort}

タイプ: UInt64

デフォルト値: 0

ORDER BY操作のために処理しなければならない（非圧縮）バイト数が指定された量を超えた場合、`sort_overflow_mode`によって動作が決定されます。デフォルトではこれは例外をスローします。

## max_bytes_to_transfer {#max_bytes_to_transfer}

タイプ: UInt64

デフォルト値: 0

GLOBAL IN/JOINセクションが実行されるときに伝達される外部テーブルの最大サイズ（非圧縮バイト）。

## max_columns_to_read {#max_columns_to_read}

タイプ: UInt64

デフォルト値: 0

クエリが指定された数の列を超えて読み取ることを要求する場合、例外がスローされます。ゼロの値は無制限を意味します。この設定は、あまりにも複雑なクエリを防ぐために役立ちます。

## max_compress_block_size {#max_compress_block_size}

タイプ: UInt64

デフォルト値: 1048576

テーブルに書き込むために圧縮前の非圧縮データの最大サイズのブロック。デフォルト値は1,048,576（1 MiB）です。一般的に、小さいブロックサイズを指定すると、圧縮比率が若干低下し、圧縮およびデコンプレッション速度がわずかに向上し、メモリ消費量が減少します。

:::note
これはエキスパートレベルの設定であり、ClickHouseを始めたばかりの方は変更するべきではありません。
:::

圧縮のためのブロック（バイトで構成されるメモリのチャンク）をクエリ処理のためのブロック（テーブルの行のセット）と混同しないでください。

## max_concurrent_queries_for_all_users {#max_concurrent_queries_for_all_users}

タイプ: UInt64

デフォルト値: 0

この設定の値が同時に処理されたクエリの現在の数以下である場合、例外をスローします。

例： `max_concurrent_queries_for_all_users`を99に設定すると、データベース管理者は100に設定して、サーバーが過負荷の状態で調査のためのクエリを実行できます。

1つのクエリまたはユーザーのために設定を変更しても、他のクエリには影響しません。

可能な値：

- 正の整数。
- 0 — 制限なし。

**例**

```xml
<max_concurrent_queries_for_all_users>99</max_concurrent_queries_for_all_users>
```

**参照**

- [max_concurrent_queries](/docs/operations/server-configuration-parameters/settings.md/#max_concurrent_queries)

## max_concurrent_queries_for_user {#max_concurrent_queries_for_user}

タイプ: UInt64

デフォルト値: 0

ユーザーごとに同時に処理されるクエリの最大数です。

可能な値：

- 正の整数。
- 0 — 制限なし。

**例**

```xml
<max_concurrent_queries_for_user>5</max_concurrent_queries_for_user>
```

## max_distributed_connections {#max_distributed_connections}

タイプ: UInt64

デフォルト値: 1024

単一の分散テーブルへの単一クエリの処理のためのリモートサーバーとの同時接続の最大数。クラスタ内のサーバー数よりも少なくともそれ以下に設定することをお勧めします。

これらのパラメータは、分散テーブルを作成する際（およびサーバーを起動する際）にのみ使用されるため、実行時に変更する理由はありません。

## max_distributed_depth {#max_distributed_depth}

タイプ: UInt64

デフォルト値: 5

[Distributed](../../engines/table-engines/special/distributed.md)テーブルの再帰クエリの最大深度を制限します。

値が超過した場合、サーバーは例外をスローします。

可能な値：

- 正の整数。
- 0 — 無制限の深さ。

## max_download_buffer_size {#max_download_buffer_size}

タイプ: UInt64

デフォルト値: 10485760

各スレッドの平行ダウンロードのためのバッファの最大サイズ（例：URLエンジン用）。

## max_download_threads {#max_download_threads}

タイプ: MaxThreads

デフォルト値: 4

データをダウンロードするためのスレッドの最大数（例：URLエンジン用）。

## max_estimated_execution_time {#max_estimated_execution_time}

タイプ: 秒数

デフォルト値: 0

最大クエリ推定実行時間（秒数）。

## max_execution_speed {#max_execution_speed}

タイプ: UInt64

デフォルト値: 0

1秒あたりの最大実行行数。

## max_execution_speed_bytes {#max_execution_speed_bytes}

タイプ: UInt64

デフォルト値: 0

1秒あたりの最大実行バイト数。

## max_execution_time {#max_execution_time}

タイプ: 秒数

デフォルト値: 0

クエリの実行時間が指定された秒数を超えた場合、動作は`timeout_overflow_mode`によって決定され、デフォルトでは例外をスローします。タイムアウトはチェックされ、クエリはデータ処理中に指定された場所でのみ停止できます。現在、集約状態のマージ中やクエリ分析中に停止することはできず、実際の実行時間はこの設定の値を超えることになります。

## max_execution_time_leaf {#max_execution_time_leaf}

タイプ: 秒数

デフォルト値: 0

max_execution_timeと同様の意味ですが、分散クエリのリーフノードのみに適用され、タイムアウトの動作は`timeout_overflow_mode_leaf`によって決定されます。デフォルトでは例外をスローします。

## max_expanded_ast_elements {#max_expanded_ast_elements}

タイプ: UInt64

デフォルト値: 500000

エイリアスとアスタリスクの展開後のクエリ構文ツリーのノード数の最大サイズ。

## max_fetch_partition_retries_count {#max_fetch_partition_retries_count}

タイプ: UInt64

デフォルト値: 5

別のホストからパーティションを取得する際のリトライ回数。

## max_final_threads {#max_final_threads}

タイプ: MaxThreads

デフォルト値: 'auto(12)'

[FINAL](../../sql-reference/statements/select/from.md/#select-from-final)修飾子を持つ`SELECT`クエリのデータ読み取りフェーズのための最大並列スレッド数を設定します。

可能な値：

- 正の整数。
- 0または1 — 無効。`SELECT`クエリは単一スレッドで実行されます。

## max_http_get_redirects {#max_http_get_redirects}

タイプ: UInt64

デフォルト値: 0

許可されるHTTP GETリダイレクトホップの最大数。悪意のあるサーバーがリクエストを予期しないサービスにリダイレクトするのを防ぐ追加のセキュリティ対策が講じられます。\n\nこれは、外部サーバーが別のアドレスにリダイレクトする場合に該当しますが、そのアドレスは企業インフラ内に見えるときであり、内部サーバーにHTTPリクエストを送信することで、認証をバイパスして内部ネットワークから内部APIをリクエストしたり、RedisやMemcachedなどの他のサービスを照会したりすることができます。内部インフラストラクチャがない場合（localhostで実行されているものを含む）やサーバーを信頼する場合は、リダイレクトを許可することが安全です。ただし、URLがHTTPであり、リモートサーバーだけでなく、ISPや中間のすべてのネットワークも信頼する必要があります。

## max_hyperscan_regexp_length {#max_hyperscan_regexp_length}

タイプ: UInt64

デフォルト値: 0

[hyperscanマルチマッチ関数](../../sql-reference/functions/string-search-functions.md/#multimatchanyhaystack-pattern1-pattern2-patternn)における各正規表現の最大長を定義します。

可能な値：

- 正の整数。
- 0 - 長さは制限されません。

**例**

クエリ：

```sql
SELECT multiMatchAny('abcd', ['ab','bcd','c','d']) SETTINGS max_hyperscan_regexp_length = 3;
```

結果：

```text
┌─multiMatchAny('abcd', ['ab', 'bcd', 'c', 'd'])─┐
│                                              1 │
└────────────────────────────────────────────────┘
```

クエリ：

```sql
SELECT multiMatchAny('abcd', ['ab','bcd','c','d']) SETTINGS max_hyperscan_regexp_length = 2;
```

結果：

```text
Exception: Regexp length too large.
```

**参照**

- [max_hyperscan_regexp_total_length](#max_hyperscan_regexp_total_length)

## max_hyperscan_regexp_total_length {#max_hyperscan_regexp_total_length}

タイプ: UInt64

デフォルト値: 0

各[hyperscanマルチマッチ関数](../../sql-reference/functions/string-search-functions.md/#multimatchanyhaystack-pattern1-pattern2-patternn)におけるすべての正規表現の長さの最大合計を設定します。

可能な値：

- 正の整数。
- 0 - 長さは制限されません。

**例**

クエリ：

```sql
SELECT multiMatchAny('abcd', ['a','b','c','d']) SETTINGS max_hyperscan_regexp_total_length = 5;
```

結果：

```text
┌─multiMatchAny('abcd', ['a', 'b', 'c', 'd'])─┐
│                                           1 │
└─────────────────────────────────────────────┘
```

クエリ：

```sql
SELECT multiMatchAny('abcd', ['ab','bc','c','d']) SETTINGS max_hyperscan_regexp_total_length = 5;
```

結果：

```text
Exception: Total regexp lengths too large.
```

**参照**

- [max_hyperscan_regexp_length](#max_hyperscan_regexp_length)

## max_insert_block_size {#max_insert_block_size}

タイプ: UInt64

デフォルト値: 1048449

テーブルに挿入するために形成するブロック（行数）のサイズ。
この設定は、サーバーがブロックを形成する場合にのみ適用されます。
例えば、HTTPインターフェイス経由でのINSERTの場合、サーバーはデータ形式を解析し、指定されたサイズのブロックを形成します。
しかし、clickhouse-clientを使用する場合、クライアントが自分でデータを解析するため、サーバーでの`max_insert_block_size`設定は挿入されたブロックのサイズに影響を与えません。
また、INSERT SELECTを使用する場合にも目的はありません。なぜなら、データはSELECT後に形成された同じブロックを使用して挿入されるためです。

デフォルトは`max_block_size`を少し超えています。これは、特定のテーブルエンジン（`*MergeTree`）が挿入された各ブロックのためにディスク上にデータ部分を形成するため、大きなエンティティです。さらに、`*MergeTree`テーブルは挿入時にデータをソートしますので、十分に大きなブロックサイズはRAM内でより多くのデータをソートすることを可能にします。

## max_insert_delayed_streams_for_parallel_write {#max_insert_delayed_streams_for_parallel_write}

タイプ: UInt64

デフォルト値: 0

最終部分フラッシュを遅延する最大数のストリーム（カラム）。デフォルト - 自動（基盤となるストレージが並列書き込みをサポートする場合は1000、それ以外は無効）。

## max_insert_threads {#max_insert_threads}

タイプ: UInt64

デフォルト値: 0

`INSERT SELECT`クエリを実行するための最大スレッド数です。

可能な値：

- 0（または1） — `INSERT SELECT`が並列実行されません。
- 正の整数。1より大きい。

クラウドのデフォルト値は、サービスのサイズに応じて`2`から`4`までです。

並列の`INSERT SELECT`は、`SELECT`部分が並列で実行される場合にのみ効果があります。[max_threads](#max_threads)設定を参照してください。
高い値は高いメモリ使用を引き起こします。

## max_joined_block_size_rows {#max_joined_block_size_rows}

タイプ: UInt64

デフォルト値: 65409

JOIN結果の最大ブロックサイズ（JOINアルゴリズムがサポートしている場合）。0は無制限を意味します。

## max_limit_for_ann_queries {#max_limit_for_ann_queries}
<ExperimentalBadge/>

タイプ: UInt64

デフォルト値: 1000000

この設定を超えるLIMITのあるSELECTクエリは、ベクトル類似インデックスを使用できません。ベクトル類似インデックスのメモリオーバーフローを防ぐのに役立ちます。

## max_live_view_insert_blocks_before_refresh {#max_live_view_insert_blocks_before_refresh}
<ExperimentalBadge/>

タイプ: UInt64

デフォルト値: 64

マージ可能なブロックが削除され、クエリが再実行されるまでに挿入された最大ブロック数を制限します。

## max_local_read_bandwidth {#max_local_read_bandwidth}

タイプ: UInt64

デフォルト値: 0

ローカル読み取りの最大速度（バイト/秒）。

## max_local_write_bandwidth {#max_local_write_bandwidth}

タイプ: UInt64

デフォルト値: 0

ローカル書き込みの最大速度（バイト/秒）。

## max_memory_usage {#max_memory_usage}

タイプ: UInt64

デフォルト値: 0

単一のクエリ処理のための最大メモリ使用量。ゼロは無制限を意味します。

## max_memory_usage_for_user {#max_memory_usage_for_user}

タイプ: UInt64

デフォルト値: 0

ユーザーに対して同時に実行されるすべてのクエリの処理のための最大メモリ使用量。ゼロは無制限を意味します。

## max_network_bandwidth {#max_network_bandwidth}

タイプ: UInt64

デフォルト値: 0

ネットワーク上のデータ交換の速度（バイト/秒）を制限します。この設定はすべてのクエリに適用されます。

可能な値：

- 正の整数。
- 0 — 帯域幅の制御が無効。

## max_network_bandwidth_for_user {#max_network_bandwidth_for_user}

Type: UInt64

Default value: 0

ネットワーク上でのデータ交換の速度を制限します（バイト/秒）。この設定は、単一のユーザーによって同時に実行されるすべてのクエリに適用されます。

Possible values:

- 正の整数。
- 0 — データ速度の制御が無効。

## max_network_bytes {#max_network_bytes}

Type: UInt64

Default value: 0

クエリを実行するときにネットワーク経由で受信または送信されるデータ量（バイト単位）を制限します。この設定は、各個別のクエリに適用されます。

Possible values:

- 正の整数。
- 0 — データ量の制御が無効。

## max_number_of_partitions_for_independent_aggregation {#max_number_of_partitions_for_independent_aggregation}

Type: UInt64

Default value: 128

最適化を適用するためのテーブル内のパーティションの最大数。

## max_parallel_replicas {#max_parallel_replicas}

Type: NonZeroUInt64

Default value: 1000

クエリを実行する際の各シャードのレプリカの最大数です。

Possible values:

- 正の整数。

**Additional Info**

このオプションは、使用される設定に応じて異なる結果を生成します。

:::note
この設定は、結合やサブクエリが含まれている場合に正しくない結果を生じることがあり、すべてのテーブルが特定の要件を満たしていない場合に発生します。詳細については、[Distributed Subqueries and max_parallel_replicas](../../sql-reference/operators/in.md/#max_parallel_replica-subqueries)を参照してください。
:::

### Parallel processing using `SAMPLE` key

クエリは、複数のサーバーで並行して実行されると、より早く処理されることがあります。ただし、クエリのパフォーマンスは次のような場合に低下する可能性があります。

- サンプリングキーの位置がパーティショニングキー内にあり、効率的な範囲スキャンを許可しない。
- テーブルにサンプリングキーを追加すると、他のカラムによるフィルタリングが非効率になる。
- サンプリングキーが計算が高コストな式である。
- クラスタのレイテンシ分布に長い尾があり、より多くのサーバーをクエリすることで全体のクエリレイテンシが増加します。

### Parallel processing using [parallel_replicas_custom_key](#parallel_replicas_custom_key)

この設定は、任意のレプリケーションテーブルに便利です。

## max_parser_backtracks {#max_parser_backtracks}

Type: UInt64

Default value: 1000000

パーサーのバックトラックの最大数（再帰的な降下パースプロセスで異なる選択肢を試す回数）。

## max_parser_depth {#max_parser_depth}

Type: UInt64

Default value: 1000

再帰的降下パーサーの最大再帰深度を制限します。スタックサイズを制御することができます。

Possible values:

- 正の整数。
- 0 — 再帰深度は無制限。

## max_parsing_threads {#max_parsing_threads}

Type: MaxThreads

Default value: 'auto(12)'

並行パースをサポートする入力形式でデータを解析するためのスレッドの最大数です。デフォルトでは、自動的に決定されます。

## max_partition_size_to_drop {#max_partition_size_to_drop}

Type: UInt64

Default value: 50000000000

クエリ時間中にパーティションを削除する制限。値が 0 の場合は、制限なしにパーティションを削除できます。

Cloud default value: 1 TB。

:::note
このクエリ設定は、サーバー設定の同等の設定を上書きします。詳しくは、[max_partition_size_to_drop](/docs/operations/server-configuration-parameters/settings.md/#max-partition-size-to-drop)を参照してください。
:::

## max_partitions_per_insert_block {#max_partitions_per_insert_block}

Type: UInt64

Default value: 100

単一のINSERTブロック内の最大パーティション数を制限します。ゼロは無制限を意味します。ブロックにパーティションが多すぎる場合は例外がスローされます。この設定は、安全のための閾値です。パーティションの数が多いことは一般的な誤解です。

## max_partitions_to_read {#max_partitions_to_read}

Type: Int64

Default value: -1

1つのクエリでアクセスできる最大パーティション数を制限します。 &lt;= 0 は無制限を意味します。

## max_parts_to_move {#max_parts_to_move}

Type: UInt64

Default value: 1000

1つのクエリで移動できるパーツの数を制限します。ゼロは無制限を意味します。

## max_query_size {#max_query_size}

Type: UInt64

Default value: 262144

SQLパーサーによって解析されるクエリ文字列の最大バイト数。
INSERTクエリのVALUES句内のデータは、別のストリームパーサーによって処理され（O(1) RAMを消費）、この制限の影響を受けません。

:::note
`max_query_size`はSQLクエリ内に設定できません（例：`SELECT now() SETTINGS max_query_size=10000`）。なぜなら、ClickHouseがクエリを解析するためのバッファを割り当てる必要があり、このバッファサイズは`max_query_size`設定によって決定され、その設定はクエリが実行される前に構成されなければならないからです。
:::

## max_read_buffer_size {#max_read_buffer_size}

Type: UInt64

Default value: 1048576

ファイルシステムから読み取るためのバッファの最大サイズ。

## max_read_buffer_size_local_fs {#max_read_buffer_size_local_fs}

Type: UInt64

Default value: 131072

ローカルファイルシステムから読み取るためのバッファの最大サイズ。0に設定すると、max_read_buffer_sizeが使用されます。

## max_read_buffer_size_remote_fs {#max_read_buffer_size_remote_fs}

Type: UInt64

Default value: 0

リモートファイルシステムから読み取るためのバッファの最大サイズ。0に設定すると、max_read_buffer_sizeが使用されます。

## max_recursive_cte_evaluation_depth {#max_recursive_cte_evaluation_depth}

Type: UInt64

Default value: 1000

再帰的CTE評価の深度に対する最大制限。

## max_remote_read_network_bandwidth {#max_remote_read_network_bandwidth}

Type: UInt64

Default value: 0

読み取り用のネットワーク上でのデータ交換の最大速度（バイト/秒）。

## max_remote_write_network_bandwidth {#max_remote_write_network_bandwidth}

Type: UInt64

Default value: 0

書き込み用のネットワーク上でのデータ交換の最大速度（バイト/秒）。

## max_replica_delay_for_distributed_queries {#max_replica_delay_for_distributed_queries}

Type: UInt64

Default value: 300

分散クエリ用の遅延レプリカを無効にします。詳しくは、[Replication](../../engines/table-engines/mergetree-family/replication.md)を参照してください。

秒数で設定します。レプリカの遅延が設定値以上の場合、そのレプリカは使用されません。

Possible values:

- 正の整数。
- 0 — レプリカの遅延はチェックされません。

非ゼロ遅延のレプリカを使用しないようにするには、このパラメータを1に設定してください。

レプリケートされたテーブルを指す分散テーブルから`SELECT`を実行する際に使用されます。

## max_result_bytes {#max_result_bytes}

Type: UInt64

Default value: 0

結果サイズの制限（バイト単位、圧縮されていない）。閾値に達した場合、クエリはデータブロックの処理を停止しますが、結果の最後のブロックをカットすることはありません。したがって、結果サイズは閾値を超える可能性があります。注意点：この閾値に対してメモリ内の結果サイズが考慮されます。結果サイズが小さい場合でも、LowCardinalityカラムの辞書やAggregateFunctionカラムのArenaなど、メモリ内のより大きなデータ構造を参照する場合があり、小さな結果サイズにもかかわらず閾値を超える可能性があります。この設定はかなり低レベルであり、注意して使用する必要があります。

## max_result_rows {#max_result_rows}

Type: UInt64

Default value: 0

結果サイズの制限（行数単位）。閾値に達した場合、クエリはデータブロックの処理を停止しますが、結果の最後のブロックをカットすることはありません。したがって、結果サイズは閾値を超える可能性があります。

## max_rows_in_distinct {#max_rows_in_distinct}

Type: UInt64

Default value: 0

DISTINCTの実行中の最大要素数。

## max_rows_in_join {#max_rows_in_join}

Type: UInt64

Default value: 0

JOINのハッシュテーブルの最大サイズ（行数単位）。

## max_rows_in_set {#max_rows_in_set}

Type: UInt64

Default value: 0

INセクションの実行結果として得られるセットの最大サイズ（要素数単位）。

## max_rows_in_set_to_optimize_join {#max_rows_in_set_to_optimize_join}

Type: UInt64

Default value: 0

結合前に互いの行セットでフィルタリングするためのセットの最大サイズ。

Possible values:

- 0 — 無効化。
- 正の整数。

## max_rows_to_group_by {#max_rows_to_group_by}

Type: UInt64

Default value: 0

GROUP BY中に生成される行数が指定された数を超える場合、動作は`group_by_overflow_mode`によって決定されます。デフォルトでは例外をスローしますが、おおよそのGROUP BYモードに切り替えることもできます。

## max_rows_to_read {#max_rows_to_read}

Type: UInt64

Default value: 0

最も「深い」ソースから読み取る行数の制限。つまり、最も深いサブクエリ内でのみです。リモートサーバーから読み取るときは、リモートサーバーでのみ確認されます。

## max_rows_to_read_leaf {#max_rows_to_read_leaf}

Type: UInt64

Default value: 0

分散クエリに対するリーフノードでの読み取り行の制限。この制限はローカル読み取りのみに適用され、ルートノードの最終マージ段階は除外されます。この設定は、prefer_localhost_replica=1 で不安定になることに注意してください。

## max_rows_to_sort {#max_rows_to_sort}

Type: UInt64

Default value: 0

ORDER BY操作のために処理されなければならないレコードが指定された量を超える場合、動作は`sort_overflow_mode`によって決定されます。デフォルトでは例外をスローします。

## max_rows_to_transfer {#max_rows_to_transfer}

Type: UInt64

Default value: 0

GLOBAL IN/JOINセクションが実行される際に取得された外部テーブルの最大サイズ（行単位）。

## max_sessions_for_user {#max_sessions_for_user}

Type: UInt64

Default value: 0

ユーザーの同時セッションの最大数。

## max_size_to_preallocate_for_aggregation {#max_size_to_preallocate_for_aggregation}

Type: UInt64

Default value: 1000000000000

集約前に全てのハッシュテーブルに合計で事前に確保を許可される要素数。

## max_size_to_preallocate_for_joins {#max_size_to_preallocate_for_joins}

Type: UInt64

Default value: 1000000000000

結合前に全てのハッシュテーブルに合計で事前に確保を許可される要素数。

## max_streams_for_merge_tree_reading {#max_streams_for_merge_tree_reading}

Type: UInt64

Default value: 0

0でない場合、MergeTreeテーブルの読み取りストリームの数を制限します。

## max_streams_multiplier_for_merge_tables {#max_streams_multiplier_for_merge_tables}

Type: Float

Default value: 5

Mergeテーブルから読み取るときに、より多くのストリームを要求します。ストリームはMergeテーブルが使用するテーブルにわたって広がります。これにより、スレッド間での作業の分配がより均等になり、マージされたテーブルのサイズが異なる場合に特に役立ちます。

## max_streams_to_max_threads_ratio {#max_streams_to_max_threads_ratio}

Type: Float

Default value: 1

スレッドの数よりも多くのソースを使用することを許可します - スレッド間での作業の分配をより均等にします。これは一時的な解決策であることを前提としています。将来的には、各ソースが自分自身のために動的に利用可能な作業を選択できるようにするために、ソースの数をスレッドの数と同じにすることが可能になると想定されています。

## max_subquery_depth {#max_subquery_depth}

Type: UInt64

Default value: 100

クエリが指定された数のネストされたサブクエリを超える場合、例外をスローします。これにより、クラスターのユーザーがクエリで精神的におかしくなるのを防ぐためのサニティチェックが可能になります。

## max_table_size_to_drop {#max_table_size_to_drop}

Type: UInt64

Default value: 50000000000

クエリ時間中にテーブルを削除する制限。値が 0 の場合は、制限なしに全てのテーブルを削除できます。

Cloud default value: 1 TB。

:::note
このクエリ設定は、サーバー設定の同等の設定を上書きします。詳しくは、[max_table_size_to_drop](/docs/operations/server-configuration-parameters/settings.md/#max-table-size-to-drop)を参照してください。
:::

## max_temporary_columns {#max_temporary_columns}

Type: UInt64

Default value: 0

クエリが中間計算の結果としてメモリ内に指定された数の一時カラムを生成する場合、例外がスローされます。ゼロ値は無制限を意味します。この設定は、あまりにも複雑なクエリを防ぐために便利です。

## max_temporary_data_on_disk_size_for_query {#max_temporary_data_on_disk_size_for_query}

Type: UInt64

Default value: 0

すべての同時実行クエリのメモリ内で使用される一時ファイルによって消費される最大データ量（バイト単位）。ゼロは無制限を意味します。

## max_temporary_data_on_disk_size_for_user {#max_temporary_data_on_disk_size_for_user}

Type: UInt64

Default value: 0

すべての同時実行ユーザークエリのメモリ内で使用される一時ファイルによって消費される最大データ量（バイト単位）。ゼロは無制限を意味します。

## max_temporary_non_const_columns {#max_temporary_non_const_columns}

Type: UInt64

Default value: 0

'max_temporary_columns'設定と似ていますが、定数でないカラムにのみ適用されます。定数カラムは安価ですので、それらをより多く許可するのは合理的です。

## max_threads {#max_threads}

Type: MaxThreads

Default value: 'auto(12)'

クエリ処理スレッドの最大数（リモートサーバーからデータを取得するためのスレッドを除く、[max_distributed_connections]パラメータ参照）。

このパラメータは、クエリ処理パイプラインの同じ段階を並行して実行するスレッドに適用されます。
例えば、テーブルから読み取るとき、関数を含む式を評価したり、WHEREでフィルタリングを行い、GROUP BYのためにあらかじめ集約を並行して実行できる場合、‘max_threads’数のスレッドが使用されます。

LIMITのために迅速に完了するクエリに対して、‘max_threads’の数を低く設定することができます。例えば、必要な数のエントリが各ブロックに配置され、max_threads = 8の場合、8ブロックが取得されますが、1つだけで十分でした。

`max_threads`の値が小さいほど、消費されるメモリは少なくなります。

## max_threads_for_indexes {#max_threads_for_indexes}

Type: UInt64

Default value: 0

インデックス処理のための最大スレッド数。

## max_untracked_memory {#max_untracked_memory}

Type: UInt64

Default value: 4194304

小さな割り当てと解放は、スレッドローカル変数にグループ化され、量が指定された値よりも大きくなるまでトラッキングまたはプロファイリングされません。値が'memory_profiler_step'よりも大きい場合は、効果的に'memory_profiler_step'に低下します。

## memory_overcommit_ratio_denominator {#memory_overcommit_ratio_denominator}

Type: UInt64

Default value: 1073741824

これは、グローバルレベルでハード制限に達したときのソフトメモリ制限を表します。この値は、クエリのオーバーコミット比率を計算するために使用されます。ゼロはクエリをスキップします。 [memory overcommit](memory-overcommit.md)についての詳細はここを参照してください。

## memory_overcommit_ratio_denominator_for_user {#memory_overcommit_ratio_denominator_for_user}

Type: UInt64

Default value: 1073741824

これは、ユーザーレベルでハード制限に達したときのソフトメモリ制限を表します。この値は、クエリのオーバーコミット比率を計算するために使用されます。ゼロはクエリをスキップします。 [memory overcommit](memory-overcommit.md)についての詳細はここを参照してください。

## memory_profiler_sample_max_allocation_size {#memory_profiler_sample_max_allocation_size}

Type: UInt64

Default value: 0

指定された値以下のサイズのランダムな割り当てを、`memory_profiler_sample_probability`に等しい確率で収集します。0は無効を意味します。期待通りにこの閾値が機能するために、'max_untracked_memory'を0に設定することをお勧めします。

## memory_profiler_sample_min_allocation_size {#memory_profiler_sample_min_allocation_size}

Type: UInt64

Default value: 0

指定された値以上のサイズのランダムな割り当てを、`memory_profiler_sample_probability`に等しい確率で収集します。0は無効を意味します。期待通りにこの閾値が機能するために、'max_untracked_memory'を0に設定することをお勧めします。

## memory_profiler_sample_probability {#memory_profiler_sample_probability}

Type: Float

Default value: 0

ランダムな割り当てと解放を収集し、それらを'system.trace_log'に'MemorySample'トレースタイプで書き込みます。この確率は、割り当てのサイズに関係なく、すべての割り当て/解放に適用されます（`memory_profiler_sample_min_allocation_size`および`memory_profiler_sample_max_allocation_size`で変更できます）。注意：サンプリングは、未トラッキングメモリの量が'max_untracked_memory'を超えたときだけ発生します。追加の詳細なサンプリングのために、'max_untracked_memory'を0に設定することをお勧めします。

## memory_profiler_step {#memory_profiler_step}

Type: UInt64

Default value: 4194304

メモリプロファイラのステップを設定します。クエリのメモリ使用量が、バイト数の各次のステップを超えるたびに、メモリプロファイラは割り当てスタックトレースを収集し、それを[trace_log](../../operations/system-tables/trace_log.md/#system_tables-trace_log)に書き込みます。

Possible values:

- Positive integer number of bytes.
- 0 for turning off the memory profiler.

## memory_tracker_fault_probability {#memory_tracker_fault_probability}

Type: Float

Default value: 0

`exception safety`のテスト用 - 指定された確率でメモリを割り当てるたびに例外をスローします。

## memory_usage_overcommit_max_wait_microseconds {#memory_usage_overcommit_max_wait_microseconds}

Type: UInt64

Default value: 5000000

ユーザーレベルでメモリのオーバーコミットが発生した場合、スレッドがメモリが解放されるのを待つ最大時間。このタイムアウトに達し、メモリが解放されない場合、例外がスローされます。 [memory overcommit](memory-overcommit.md)についての詳細はここを参照してください。

## merge_table_max_tables_to_look_for_schema_inference {#merge_table_max_tables_to_look_for_schema_inference}

Type: UInt64

Default value: 1000

明示的スキーマなしで`Merge`テーブルを作成する場合や、`merge`テーブル関数を使用する場合、一致するテーブルの指定された数を超えないようにスキーマを推定します。より多くのテーブルがある場合、スキーマは指定された最初の数のテーブルから推定されます。

## merge_tree_coarse_index_granularity {#merge_tree_coarse_index_granularity}

Type: UInt64

Default value: 8

データを検索する際、ClickHouseはインデックスファイル内のデータマークをチェックします。ClickHouseが必要なキーがある範囲を見つけた場合、その範囲を`merge_tree_coarse_index_granularity`サブレンジに分割し、そこで必要なキーを再帰的に検索します。

Possible values:

- 正の偶数。

## merge_tree_compact_parts_min_granules_to_multibuffer_read {#merge_tree_compact_parts_min_granules_to_multibuffer_read}

Type: UInt64

Default value: 16

ClickHouse Cloudでのみ利用可能。MergeTreeテーブルのコンパクトパートのストライプ内でマルチバッファリーダーを使用するためのグラニュールの数。これにより、並行読み取りとプリフェッチがサポートされます。リモート fs から読み取る場合、マルチバッファリーダーの使用は、読み取りリクエストの数を増加させます。

## merge_tree_determine_task_size_by_prewhere_columns {#merge_tree_determine_task_size_by_prewhere_columns}

Type: Bool

Default value: 1

読み取りタスクサイズを決定するために、先行するカラムのサイズのみを使用するかどうか。

## merge_tree_max_bytes_to_use_cache {#merge_tree_max_bytes_to_use_cache}

Type: UInt64

Default value: 2013265920

ClickHouseが1つのクエリで`merge_tree_max_bytes_to_use_cache`バイトを超えて読み込む必要がある場合、圧縮されていないブロックのキャッシュは使用されません。

圧縮されていないブロックのキャッシュは、クエリのために抽出されたデータを保存します。ClickHouseは、このキャッシュを使用して小さなクエリへの反応を迅速化します。この設定は、大量のデータを読み取るクエリによってキャッシュが壊れないように保護します。[uncompressed_cache_size](../../operations/server-configuration-parameters/settings.md/#server-settings-uncompressed_cache_size)サーバー設定は、圧縮されていないブロックのキャッシュのサイズを定義します。

Possible values:

- 正の整数。

## merge_tree_max_rows_to_use_cache {#merge_tree_max_rows_to_use_cache}

Type: UInt64

Default value: 1048576

ClickHouseが1つのクエリで`merge_tree_max_rows_to_use_cache`行を超えて読み込む必要がある場合、圧縮されていないブロックのキャッシュは使用されません。

圧縮されていないブロックのキャッシュは、クエリのために抽出されたデータを保存します。ClickHouseは、このキャッシュを使用して小さなクエリへの反応を迅速化します。この設定は、大量のデータを読み取るクエリによってキャッシュが壊れないように保護します。[uncompressed_cache_size](../../operations/server-configuration-parameters/settings.md/#server-settings-uncompressed_cache_size)サーバー設定は、圧縮されていないブロックのキャッシュのサイズを定義します。

Possible values:

- 正の整数。

## merge_tree_min_bytes_for_concurrent_read {#merge_tree_min_bytes_for_concurrent_read}

Type: UInt64

Default value: 251658240

[MergeTree](../../engines/table-engines/mergetree-family/mergetree.md)エンジンテーブルの1つのファイルから読み取るバイト数が`merge_tree_min_bytes_for_concurrent_read`を超える場合、ClickHouseはこのファイルから並行して読み取ろうとします。

Possible value:

- 正の整数。

## merge_tree_min_bytes_for_concurrent_read_for_remote_filesystem {#merge_tree_min_bytes_for_concurrent_read_for_remote_filesystem}

Type: UInt64

Default value: 0

リモートファイルシステムからの読み取り時に、[MergeTree](../../engines/table-engines/mergetree-family/mergetree.md)エンジンが読み取りを並行化できるようになる前に、1つのファイルから読み取る最低バイト数。この設定の使用は推奨されません。

Possible values:

- 正の整数。

## merge_tree_min_bytes_for_seek {#merge_tree_min_bytes_for_seek}

Type: UInt64

Default value: 0

1つのファイル内で読み取るデータブロック間の距離が`merge_tree_min_bytes_for_seek`バイト未満の場合、ClickHouseは両方のブロックを含むファイルレンジを順次読み取り、追加のシークを回避します。

Possible values:

- 正の整数。

## merge_tree_min_bytes_per_task_for_remote_reading {#merge_tree_min_bytes_per_task_for_remote_reading}

Type: UInt64

Default value: 2097152

タスクごとの最小バイト数。

## merge_tree_min_read_task_size {#merge_tree_min_read_task_size}

Type: UInt64

Default value: 8

タスクサイズのハード下限（グラニュールの数が少なく、利用可能なスレッドの数が高い場合でも、より小さいタスクを割り当てません）。

## merge_tree_min_rows_for_concurrent_read {#merge_tree_min_rows_for_concurrent_read}

Type: UInt64

Default value: 163840

[MergeTree](../../engines/table-engines/mergetree-family/mergetree.md)テーブルの1つのファイルから読み取る行の数が`merge_tree_min_rows_for_concurrent_read`を超える場合、ClickHouseはこのファイルからの並行読み取りを試みます。

Possible values:

- 正の整数。

## merge_tree_min_rows_for_concurrent_read_for_remote_filesystem {#merge_tree_min_rows_for_concurrent_read_for_remote_filesystem}

Type: UInt64

Default value: 0

リモートファイルシステムからの読み取り時に、[MergeTree](../../engines/table-engines/mergetree-family/mergetree.md)エンジンが読み取りを並行化できるようになる前に、1つのファイルから読み取る最低行数。この設定の使用は推奨されません。

Possible values:

- 正の整数。

## merge_tree_min_rows_for_seek {#merge_tree_min_rows_for_seek}

Type: UInt64

Default value: 0

1つのファイル内で読み取るデータブロック間の距離が`merge_tree_min_rows_for_seek`行未満の場合、ClickHouseはファイルをシークせず、データを順次読み取ります。

Possible values:

- 正の整数。

## merge_tree_read_split_ranges_into_intersecting_and_non_intersecting_injection_probability {#merge_tree_read_split_ranges_into_intersecting_and_non_intersecting_injection_probability}

Type: Float

Default value: 0

`PartsSplitter`のテスト用 - MergeTreeから読み取る際に、指定された確率で読み取り範囲を交差きまない部分に分割します。

## merge_tree_use_const_size_tasks_for_remote_reading {#merge_tree_use_const_size_tasks_for_remote_reading}

Type: Bool

Default value: 1

リモートテーブルから読み取る際に定数サイズのタスクを使用するかどうか。

## merge_tree_use_deserialization_prefixes_cache {#merge_tree_use_deserialization_prefixes_cache}

Type: Bool

Default value: 1

MergeTreeのワイドパーツからの読み取り中にファイルプレフィックスからのカラムメタデータキャッシュを有効にします。

## merge_tree_use_prefixes_deserialization_thread_pool {#merge_tree_use_prefixes_deserialization_thread_pool}

Type: Bool

Default value: 1

MergeTreeのワイドパーツで並行してプレフィックスを読み取るためのスレッドプールの使用を有効にします。このスレッドプールのサイズは、サーバー設定`max_prefixes_deserialization_thread_pool_size`によって制御されます。

## merge_tree_use_v1_object_and_dynamic_serialization {#merge_tree_use_v1_object_and_dynamic_serialization}

Type: Bool

Default value: 0

有効にすると、MergeTreeではJSONとDynamic型のV1シリアライゼーションバージョンが使用され、V2では使用されません。この設定を変更するには、サーバーを再起動する必要があります。

## metrics_perf_events_enabled {#metrics_perf_events_enabled}

Type: Bool

Default value: 0

有効にすると、クエリの実行中にいくつかのパフォーマンスイベントが測定されます。

## metrics_perf_events_list {#metrics_perf_events_list}

Type: String

Default value:

コンマ区切りのパフォーマンスメトリックリストで、クエリの実行中に測定されます。空はすべてのイベントを意味します。利用可能なイベントのためにソース内のPerfEventInfoを参照してください。

## min_bytes_to_use_direct_io {#min_bytes_to_use_direct_io}

Type: UInt64

Default value: 0

ストレージディスクに直接I/Oアクセスを使用するために必要な最小データ量。

ClickHouseは、テーブルからデータを読み取る際にこの設定を使用します。読み取るすべてのデータの合計ストレージ容量が`min_bytes_to_use_direct_io`バイトを超える場合、ClickHouseは`O_DIRECT`オプションを使用してストレージディスクからデータを読み取ります。

Possible values:

- 0 — Direct I/Oは無効です。
- 正の整数。

## min_bytes_to_use_mmap_io {#min_bytes_to_use_mmap_io}

Type: UInt64

Default value: 0

これは実験的な設定です。カーネルからユーザースペースにデータをコピーせずに大きなファイルを読み取るために必要なメモリの最小量を設定します。推奨される閾値は約64MBです。なぜなら、[mmap/munmap](https://en.wikipedia.org/wiki/Mmap)は遅いためです。これは大きなファイルに対してのみ意味があります。

Possible values:

- 正の整数。
- 0 — 大きなファイルはカーネルからユーザースペースにデータをコピーするだけで読み取ります。

## min_chunk_bytes_for_parallel_parsing {#min_chunk_bytes_for_parallel_parsing}

Type: NonZeroUInt64

Default value: 10485760

- Type: unsigned int
- Default value: 1 MiB

各スレッドが並行して解析する最小チャンクサイズ（バイト単位）。

## min_compress_block_size {#min_compress_block_size}

Type: UInt64

Default value: 65536

[MergeTree](../../engines/table-engines/mergetree-family/mergetree.md)テーブル用。クエリ処理の遅延を減らすために、次のマークを書き込む際にブロックがそのサイズが`min_compress_block_size`以上である場合、圧縮されます。デフォルトは65,536です。

圧縮されていないデータのサイズが`max_compress_block_size`未満の場合、ブロックの実際のサイズはこの値以上でなければならず、マークのデータ量以上でなければなりません。

例を見てみましょう。テーブル作成時に`index_granularity`が8192に設定されていたとしましょう。

UInt32型のカラム（各値4バイト）を書き込むと、8192行で合計32KBのデータになります。`min_compress_block_size`が65,536のため、2つのマークごとに圧縮ブロックが形成されます。

平均サイズが60バイトのString型のURLカラムを書き込むと、8192行で平均して約500KBのデータになります。65,536を超えているため、各マークごとに圧縮ブロックが形成されます。この場合は、単一のマークの範囲でディスクからデータを読み取るとき、余分なデータは解凍されません。

:::note
これは専門家レベルの設定であり、ClickHouseを始めたばかりの場合は変更しないでください。
:::

## min_count_to_compile_aggregate_expression {#min_count_to_compile_aggregate_expression}

Type: UInt64

Default value: 3

JITコンパイルを開始するための同一の集約式の最小数。これは、[compile_aggregate_expressions](#compile_aggregate_expressions)設定が有効な場合のみ機能します。

Possible values:

- 正の整数。
- 0 — 同一の集約式は常にJITコンパイルされます。

## min_count_to_compile_expression {#min_count_to_compile_expression}

Type: UInt64

Default value: 3

同じ式が実行される最小回数。これをコンパイルする前に実行されます。

## min_count_to_compile_sort_description {#min_count_to_compile_sort_description}

Type: UInt64

Default value: 3

JITコンパイルされる同一のソート説明の数。

## min_execution_speed {#min_execution_speed}

Type: UInt64

Default value: 0

1秒あたりの最小実行行数。

## min_execution_speed_bytes {#min_execution_speed_bytes}

Type: UInt64

Default value: 0

1秒あたりの最小実行バイト数。

## min_external_sort_block_bytes {#min_external_sort_block_bytes}

Type: UInt64

Default value: 104857600

ディスクにダンプされる外部ソート用の最小ブロックサイズ（バイト単位）。ファイルの数が多すぎるのを回避します。

## min_external_table_block_size_bytes {#min_external_table_block_size_bytes}

Type: UInt64

Default value: 268402944

ブロックが十分大きくない場合、外部テーブルに渡されたブロックを指定されたサイズ（バイト単位）に圧縮します。

## min_external_table_block_size_rows {#min_external_table_block_size_rows}

Type: UInt64

Default value: 1048449

ブロックが十分大きくない場合、外部テーブルに渡されたブロックを指定されたサイズ（行単位）に圧縮します。

## min_free_disk_bytes_to_perform_insert {#min_free_disk_bytes_to_perform_insert}

Type: UInt64

Default value: 0

挿入を行うために必要な最小の空きディスクスペース（バイト単位）。

## min_free_disk_ratio_to_perform_insert {#min_free_disk_ratio_to_perform_insert}

Type: Float

Default value: 0

挿入を行うために必要な最小の空きディスクスペースの比率。

## min_free_disk_space_for_temporary_data {#min_free_disk_space_for_temporary_data}

Type: UInt64

Default value: 0

外部ソートや集約に使用する一時データを書き込むために保持すべき最小ディスクスペース。
```

## min_hit_rate_to_use_consecutive_keys_optimization {#min_hit_rate_to_use_consecutive_keys_optimization}

タイプ: Float

デフォルト値: 0.5

集約における連続キー最適化のために使用されるキャッシュの最小ヒット率で、これを有効のままに保つために必要です。

## min_insert_block_size_bytes {#min_insert_block_size_bytes}

タイプ: UInt64

デフォルト値: 268402944

`INSERT` クエリによってテーブルに挿入できるブロックの最小バイト数を設定します。サイズの小さいブロックは、大きなブロックに圧縮されます。

可能な値:

- 正の整数。
- 0 — 圧縮無効。

## min_insert_block_size_bytes_for_materialized_views {#min_insert_block_size_bytes_for_materialized_views}

タイプ: UInt64

デフォルト値: 0

`INSERT` クエリによってテーブルに挿入できるブロックの最小バイト数を設定します。サイズの小さいブロックは、大きなブロックに圧縮されます。この設定は [materialized view](../../sql-reference/statements/create/view.md) に挿入されたブロックのみに適用されます。この設定を調整することで、マテリアライズドビューにプッシュする際のブロックの圧縮を制御し、過剰なメモリ使用を避けることができます。

可能な値:

- 任意の正の整数。
- 0 — 圧縮無効。

**他に見るもの**

- [min_insert_block_size_bytes](#min_insert_block_size_bytes)

## min_insert_block_size_rows {#min_insert_block_size_rows}

タイプ: UInt64

デフォルト値: 1048449

`INSERT` クエリによってテーブルに挿入できるブロックの最小行数を設定します。サイズの小さいブロックは、大きなブロックに圧縮されます。

可能な値:

- 正の整数。
- 0 — 圧縮無効。

## min_insert_block_size_rows_for_materialized_views {#min_insert_block_size_rows_for_materialized_views}

タイプ: UInt64

デフォルト値: 0

`INSERT` クエリによってテーブルに挿入できるブロックの最小行数を設定します。サイズの小さいブロックは、大きなブロックに圧縮されます。この設定は [materialized view](../../sql-reference/statements/create/view.md) に挿入されたブロックのみに適用されます。この設定を調整することで、マテリアライズドビューにプッシュする際のブロックの圧縮を制御し、過剰なメモリ使用を避けることができます。

可能な値:

- 任意の正の整数。
- 0 — 圧縮無効。

**他に見るもの**

- [min_insert_block_size_rows](#min_insert_block_size_rows)

## min_joined_block_size_bytes {#min_joined_block_size_bytes}

タイプ: UInt64

デフォルト値: 524288

JOIN 結果の最小ブロックサイズ（JOIN アルゴリズムがこれをサポートしている場合）。0 は無制限を意味します。

## mongodb_throw_on_unsupported_query {#mongodb_throw_on_unsupported_query}

タイプ: Bool

デフォルト値: 1

有効にすると、MongoDB テーブルは、MongoDB クエリを構築できない場合にエラーを返します。それ以外の場合、ClickHouse はテーブル全体を読み取り、ローカルで処理します。このオプションは、レガシー実装や 'allow_experimental_analyzer=0' の場合には適用されません。

## move_all_conditions_to_prewhere {#move_all_conditions_to_prewhere}

タイプ: Bool

デフォルト値: 1

すべての有効な条件を WHERE から PREWHERE に移動します。

## move_primary_key_columns_to_end_of_prewhere {#move_primary_key_columns_to_end_of_prewhere}

タイプ: Bool

デフォルト値: 1

主キー列を含む PREWHERE 条件を AND チェーンの末尾に移動します。これらの条件は主キーの分析中に考慮される可能性が高く、そのため PREWHERE フィルタリングに多くは寄与しません。

## multiple_joins_try_to_keep_original_names {#multiple_joins_try_to_keep_original_names}

タイプ: Bool

デフォルト値: 0

複数のテーブルを結合したときのトップレベルの式リストにエイリアスを追加しない。

## mutations_execute_nondeterministic_on_initiator {#mutations_execute_nondeterministic_on_initiator}

タイプ: Bool

デフォルト値: 0

true の場合、定数の非決定論的関数（例: `now()` 関数）はイニシエータ上で実行され、`UPDATE` および `DELETE` クエリ内のリテラルに置き換えられます。これにより、定数の非決定論的関数を使用して変異を実行する際に、レプリカ間でデータの整合性を維持できます。デフォルト値: `false`。

## mutations_execute_subqueries_on_initiator {#mutations_execute_subqueries_on_initiator}

タイプ: Bool

デフォルト値: 0

true の場合、スカラのサブクエリはイニシエータ上で実行され、`UPDATE` および `DELETE` クエリ内のリテラルに置き換えられます。デフォルト値: `false`。

## mutations_max_literal_size_to_replace {#mutations_max_literal_size_to_replace}

タイプ: UInt64

デフォルト値: 16384

`UPDATE` および `DELETE` クエリで置き換えるためのシリアライズされたリテラルの最大サイズ（バイト単位）。上記の設定のうちの少なくとも一つが有効な場合にのみ効力を発揮します。デフォルト値: 16384 (16 KiB)。

## mutations_sync {#mutations_sync}

タイプ: UInt64

デフォルト値: 0

`ALTER TABLE ... UPDATE|DELETE|MATERIALIZE INDEX|MATERIALIZE PROJECTION|MATERIALIZE COLUMN` クエリを（[mutations](../../sql-reference/statements/alter/index.md/#mutations)）同期的に実行します。

可能な値:

- 0 - 変異が非同期で実行されます。
- 1 - クエリは現在のサーバー上のすべての変異が完了するのを待ちます。
- 2 - クエリはすべてのレプリカ（存在する場合）でのすべての変異が完了するのを待ちます。

## mysql_datatypes_support_level {#mysql_datatypes_support_level}

タイプ: MySQLDataTypesSupport

デフォルト値:

MySQL タイプが対応する ClickHouse タイプにどのように変換されるかを定義します。 `decimal`, `datetime64`, `date2Date32` または `date2String` の任意の組み合わせのカンマ区切りリスト。

- `decimal`: 精度が許す場合、`NUMERIC` および `DECIMAL` タイプを `Decimal` に変換します。
- `datetime64`: 精度が `0` でない場合、`DATETIME` および `TIMESTAMP` タイプを `DateTime64` に変換します（`DateTime` ではなく）。
- `date2Date32`: `DATE` を `Date32` に変換します（`Date` の代わりに）。`date2String` より優先されます。
- `date2String`: `DATE` を `String` に変換します（`Date` の代わりに）。`datetime64` によって上書きされます。

## mysql_map_fixed_string_to_text_in_show_columns {#mysql_map_fixed_string_to_text_in_show_columns}

タイプ: Bool

デフォルト値: 1

有効にすると、[FixedString](../../sql-reference/data-types/fixedstring.md) ClickHouse データ型は [SHOW COLUMNS](../../sql-reference/statements/show.md/#show_columns) で `TEXT` として表示されます。

MySQL ワイヤプロトコルを介して接続された場合にのみ効果があります。

- 0 - `BLOB` を使用。
- 1 - `TEXT` を使用。

## mysql_map_string_to_text_in_show_columns {#mysql_map_string_to_text_in_show_columns}

タイプ: Bool

デフォルト値: 1

有効にすると、[String](../../sql-reference/data-types/string.md) ClickHouse データ型は [SHOW COLUMNS](../../sql-reference/statements/show.md/#show_columns) で `TEXT` として表示されます。

MySQL ワイヤプロトコルを介して接続された場合にのみ効果があります。

- 0 - `BLOB` を使用。
- 1 - `TEXT` を使用。

## mysql_max_rows_to_insert {#mysql_max_rows_to_insert}

タイプ: UInt64

デフォルト値: 65536

MySQL ストレージエンジンによる MySQL バッチ挿入の最大行数。

## network_compression_method {#network_compression_method}

タイプ: String

デフォルト値: LZ4

サーバー間、およびサーバーと [clickhouse-client](../../interfaces/cli.md) 間での通信に使用されるデータ圧縮メソッドを設定します。

可能な値:

- `LZ4` — LZ4 圧縮方式を設定します。
- `ZSTD` — ZSTD 圧縮方式を設定します。

**他に見るもの**

- [network_zstd_compression_level](#network_zstd_compression_level)

## network_zstd_compression_level {#network_zstd_compression_level}

タイプ: Int64

デフォルト値: 1

ZSTD 圧縮のレベルを調整します。これは、[network_compression_method](#network_compression_method) が `ZSTD` に設定されている場合にのみ使用されます。

可能な値:

- 1 から 15 までの正の整数。

## normalize_function_names {#normalize_function_names}

タイプ: Bool

デフォルト値: 1

関数名を標準的な名前に正規化します。

## number_of_mutations_to_delay {#number_of_mutations_to_delay}

タイプ: UInt64

デフォルト値: 0

変更されたテーブルが未完了の変異を少なくともこの数だけ含む場合、テーブルの変異を人工的に遅延させます。0 - 無効。

## number_of_mutations_to_throw {#number_of_mutations_to_throw}

タイプ: UInt64

デフォルト値: 0

変更されたテーブルが未完了の変異を少なくともこの数だけ含む場合、'Too many mutations ...' 例外を投げます。0 - 無効。

## odbc_bridge_connection_pool_size {#odbc_bridge_connection_pool_size}

タイプ: UInt64

デフォルト値: 16

ODBC ブリッジにおける各接続設定文字列の接続プールサイズ。

## odbc_bridge_use_connection_pooling {#odbc_bridge_use_connection_pooling}

タイプ: Bool

デフォルト値: 1

ODBC ブリッジでの接続プーリングを使用します。false に設定すると、毎回新しい接続が作成されます。

## offset {#offset}

タイプ: UInt64

デフォルト値: 0

クエリから行を返し始める前にスキップする行数を設定します。これは、[OFFSET](../../sql-reference/statements/select/offset.md/#offset-fetch) 句によって設定されたオフセットを調整し、これらの二つの値を合算します。

可能な値:

- 0 — 行はスキップされません。
- 正の整数。

**例**

入力テーブル:

``` sql
CREATE TABLE test (i UInt64) ENGINE = MergeTree() ORDER BY i;
INSERT INTO test SELECT number FROM numbers(500);
```

クエリ:

``` sql
SET limit = 5;
SET offset = 7;
SELECT * FROM test LIMIT 10 OFFSET 100;
```

結果:

``` text
┌───i─┐
│ 107 │
│ 108 │
│ 109 │
└─────┘
```

## opentelemetry_start_trace_probability {#opentelemetry_start_trace_probability}

タイプ: Float

デフォルト値: 0

ClickHouse が実行されたクエリのトレースを開始できる確率を設定します（親の [trace context](https://www.w3.org/TR/trace-context/) が供給されていない場合）。

可能な値:

- 0 — すべての実行されたクエリのトレースが無効です（親のトレースコンテキストが供給されていない場合）。
- [0..1] の範囲の正の浮動小数点数。たとえば、設定値が `0.5` の場合、ClickHouse は平均して半分のクエリでトレースを開始できます。
- 1 — すべての実行されたクエリのトレースが有効です。

## opentelemetry_trace_processors {#opentelemetry_trace_processors}

タイプ: Bool

デフォルト値: 0

プロセッサのために OpenTelemetry スパンを収集します。

## optimize_aggregation_in_order {#optimize_aggregation_in_order}

タイプ: Bool

デフォルト値: 0

[MergeTree](../../engines/table-engines/mergetree-family/mergetree.md) テーブルのデータを対応する順序で集約するための [GROUP BY](../../sql-reference/statements/select/group-by.md) 最適化を [SELECT](../../sql-reference/statements/select/index.md) クエリ内で有効にします。

可能な値:

- 0 — `GROUP BY` 最適化無効。
- 1 — `GROUP BY` 最適化有効。

**他に見るもの**

- [GROUP BY optimization](../../sql-reference/statements/select/group-by.md/#aggregation-in-order)

## optimize_aggregators_of_group_by_keys {#optimize_aggregators_of_group_by_keys}

タイプ: Bool

デフォルト値: 1

SELECT セクションの GROUP BY キーの最小/最大/任意/最後の集計子を排除します。

## optimize_and_compare_chain {#optimize_and_compare_chain}

タイプ: Bool

デフォルト値: 1

フィルタリング能力を向上させるために AND チェーン内の定数比較を充填します。 `<`, `<=`, `>`, `>=`, `=` およびそれらの混合の演算子をサポートします。たとえば、 `(a < b) AND (b < c) AND (c < 5)` は ` (a < b) AND (b < c) AND (c < 5) AND (b < 5) AND (a < 5)` に書き換えられます。

## optimize_append_index {#optimize_append_index}

タイプ: Bool

デフォルト値: 0

インデックス条件を追加するために [constraints](../../sql-reference/statements/create/table.md/#constraints) を使用します。デフォルトは `false` です。

可能な値:

- true, false

## optimize_arithmetic_operations_in_aggregate_functions {#optimize_arithmetic_operations_in_aggregate_functions}

タイプ: Bool

デフォルト値: 1

集約関数の外に算術演算を移動します。

## optimize_count_from_files {#optimize_count_from_files}

タイプ: Bool

デフォルト値: 1

異なる入力形式のファイルからの行数をカウントする最適化を有効または無効にします。これは、テーブル関数/エンジン `file`/`s3`/`url`/`hdfs`/`azureBlobStorage` に適用されます。

可能な値:

- 0 — 最適化無効。
- 1 — 最適化有効。

## optimize_distinct_in_order {#optimize_distinct_in_order}

タイプ: Bool

デフォルト値: 1

DISTINCT の一部のカラムがソートのプレフィックスを形成する場合、DISTINCT 最適化を有効にします。たとえば、マージツリーや ORDER BY 文のソートキーのプレフィックスを形成します。

## optimize_distributed_group_by_sharding_key {#optimize_distributed_group_by_sharding_key}

タイプ: Bool

デフォルト値: 1

イニシエータサーバー上でのコストのかかる集約を回避することにより、`GROUP BY sharding_key` クエリを最適化します（これにより、イニシエータサーバーのクエリのメモリ使用量が削減されます）。

サポートされるクエリのタイプは次のとおりです（それらのすべての組み合わせ）：

- `SELECT DISTINCT [..., ]sharding_key[, ...] FROM dist`
- `SELECT ... FROM dist GROUP BY sharding_key[, ...]`
- `SELECT ... FROM dist GROUP BY sharding_key[, ...] ORDER BY x`
- `SELECT ... FROM dist GROUP BY sharding_key[, ...] LIMIT 1`
- `SELECT ... FROM dist GROUP BY sharding_key[, ...] LIMIT 1 BY x`

次のクエリのタイプはサポートされていません（今後のサポートが追加される可能性があります）：

- `SELECT ... GROUP BY sharding_key[, ...] WITH TOTALS`
- `SELECT ... GROUP BY sharding_key[, ...] WITH ROLLUP`
- `SELECT ... GROUP BY sharding_key[, ...] WITH CUBE`
- `SELECT ... GROUP BY sharding_key[, ...] SETTINGS extremes=1`

可能な値：

- 0 — 無効。
- 1 — 有効。

他に見るもの：

- [distributed_group_by_no_merge](#distributed_group_by_no_merge)
- [distributed_push_down_limit](#distributed_push_down_limit)
- [optimize_skip_unused_shards](#optimize_skip_unused_shards)

:::note
現在、この設定は `optimize_skip_unused_shards` を必要とします（これは、いつかデフォルトで有効にすることになり、データが分散テーブルを介して挿入されている場合にのみ、つまりデータが sharding_key に従って分散されている場合にのみ正しく機能するためです）。
:::

## optimize_extract_common_expressions {#optimize_extract_common_expressions}

タイプ: Bool

デフォルト値: 1

WHERE、PREWHERE、ON、HAVING、および QUALIFY の式から共通の式を抽出できるようにします。`(A AND B) OR (A AND C)` のような論理式は `A AND (B OR C)` に書き換えられ、これは次のことに役立ちます：
- 単純フィルタリング式のインデックスを利用すること
- クロスから内部結合の最適化

## optimize_functions_to_subcolumns {#optimize_functions_to_subcolumns}

タイプ: Bool

デフォルト値: 1

一部の関数をサブカラムの読み込みに変換することによる最適化を有効または無効にします。これにより、読み込むデータ量が減ります。

変換できる関数：

- [length](../../sql-reference/functions/array-functions.md/#array_functions-length) を [size0](../../sql-reference/data-types/array.md/#array-size) サブカラムを読み取るように。
- [empty](../../sql-reference/functions/array-functions.md/#function-empty) を [size0](../../sql-reference/data-types/array.md/#array-size) サブカラムを読み取るように。
- [notEmpty](../../sql-reference/functions/array-functions.md/#function-notempty) を [size0](../../sql-reference/data-types/array.md/#array-size) サブカラムを読み取るように。
- [isNull](../../sql-reference/operators/index.md/#operator-is-null) を [null](../../sql-reference/data-types/nullable.md/#finding-null) サブカラムを読み取るように。
- [isNotNull](../../sql-reference/operators/index.md/#is-not-null) を [null](../../sql-reference/data-types/nullable.md/#finding-null) サブカラムを読み取るように。
- [count](../../sql-reference/aggregate-functions/reference/count.md) を [null](../../sql-reference/data-types/nullable.md/#finding-null) サブカラムを読み取るように。
- [mapKeys](../../sql-reference/functions/tuple-map-functions.md/#mapkeys) を [keys](../../sql-reference/data-types/map.md/#map-subcolumns) サブカラムを読み取るように。
- [mapValues](../../sql-reference/functions/tuple-map-functions.md/#mapvalues) を [values](../../sql-reference/data-types/map.md/#map-subcolumns) サブカラムを読み取るように。

可能な値:

- 0 — 最適化無効。
- 1 — 最適化有効。

## optimize_group_by_constant_keys {#optimize_group_by_constant_keys}

タイプ: Bool

デフォルト値: 1

ブロック内のすべてのキーが定数である場合、GROUP BY を最適化します。

## optimize_group_by_function_keys {#optimize_group_by_function_keys}

タイプ: Bool

デフォルト値: 1

GROUP BY セクションに他のキーの関数を排除します。

## optimize_if_chain_to_multiif {#optimize_if_chain_to_multiif}

タイプ: Bool

デフォルト値: 0

`if(cond1, then1, if(cond2, ...))` チェーンを multiIf に置き換えます。現在、数値タイプに対しては有益ではありません。

## optimize_if_transform_strings_to_enum {#optimize_if_transform_strings_to_enum}

タイプ: Bool

デフォルト値: 0

If および Transform の文字列タイプの引数を列挙型に置き換えます。これは、分散クエリで一貫性のない変更を引き起こし失敗につながるため、デフォルトで無効にされています。

## optimize_injective_functions_in_group_by {#optimize_injective_functions_in_group_by}

タイプ: Bool

デフォルト値: 1

GROUP BY セクション内の引数によって決定的な関数をその引数に置き換えます。

## optimize_injective_functions_inside_uniq {#optimize_injective_functions_inside_uniq}

タイプ: Bool

デフォルト値: 1

uniq*() 関数内の一引数の決定的関数を削除します。

## optimize_min_equality_disjunction_chain_length {#optimize_min_equality_disjunction_chain_length}

タイプ: UInt64

デフォルト値: 3

最適化のための式 `expr = x1 OR ... expr = xN` の最小長。

## optimize_min_inequality_conjunction_chain_length {#optimize_min_inequality_conjunction_chain_length}

タイプ: UInt64

デフォルト値: 3

最適化のための式 `expr <> x1 AND ... expr <> xN` の最小長。

## optimize_move_to_prewhere {#optimize_move_to_prewhere}

タイプ: Bool

デフォルト値: 1

[SELECT](../../sql-reference/statements/select/index.md) クエリでの自動 [PREWHERE](../../sql-reference/statements/select/prewhere.md) 最適化を有効または無効にします。

これは [*MergeTree](../../engines/table-engines/mergetree-family/index.md) テーブルにのみ適用されます。

可能な値:

- 0 — 自動 `PREWHERE` 最適化無効。
- 1 — 自動 `PREWHERE` 最適化有効。

## optimize_move_to_prewhere_if_final {#optimize_move_to_prewhere_if_final}

タイプ: Bool

デフォルト値: 0

[FINAL](../../sql-reference/statements/select/from.md/#select-from-final) 修飾子を持つ [SELECT](../../sql-reference/statements/select/index.md) クエリでの自動 [PREWHERE](../../sql-reference/statements/select/prewhere.md) 最適化を有効または無効にします。

これは [*MergeTree](../../engines/table-engines/mergetree-family/index.md) テーブルにのみ適用されます。

可能な値:

- 0 — `FINAL` 修飾子を持つ `SELECT` クエリでの自動 `PREWHERE` 最適化無効。
- 1 — `FINAL` 修飾子を持つ `SELECT` クエリでの自動 `PREWHERE` 最適化有効。

**他に見るもの**

- [optimize_move_to_prewhere](#optimize_move_to_prewhere) 設定。

## optimize_multiif_to_if {#optimize_multiif_to_if}

タイプ: Bool

デフォルト値: 1

条件が1つだけの `multiIf` を `if` に置き換えます。

## optimize_normalize_count_variants {#optimize_normalize_count_variants}

タイプ: Bool

デフォルト値: 1

`count()` と意味的に等しい集約関数を `count()` として書き換えます。

## optimize_on_insert {#optimize_on_insert}

タイプ: Bool

デフォルト値: 1

挿入の前にデータ変換を有効または無効にします。これは、テーブルエンジンに応じて、このブロックでマージが実行されたかのように行われます。

可能な値:

- 0 — 無効。
- 1 — 有効。

**例**

有効と無効の違い：

クエリ:

```sql
SET optimize_on_insert = 1;

CREATE TABLE test1 (`FirstTable` UInt32) ENGINE = ReplacingMergeTree ORDER BY FirstTable;

INSERT INTO test1 SELECT number % 2 FROM numbers(5);

SELECT * FROM test1;

SET optimize_on_insert = 0;

CREATE TABLE test2 (`SecondTable` UInt32) ENGINE = ReplacingMergeTree ORDER BY SecondTable;

INSERT INTO test2 SELECT number % 2 FROM numbers(5);

SELECT * FROM test2;
```

結果:

``` text
┌─FirstTable─┐
│          0 │
│          1 │
└────────────┘

┌─SecondTable─┐
│           0 │
│           0 │
│           0 │
│           1 │
│           1 │
└─────────────┘
```

この設定が [Materialized view](../../sql-reference/statements/create/view.md/#materialized) の動作に影響することに注意してください。

## optimize_or_like_chain {#optimize_or_like_chain}

タイプ: Bool

デフォルト値: 0

複数の OR LIKE を multiMatchAny に最適化します。この最適化は、いくつかの場合にインデックス分析を妨げるため、デフォルトで有効にすべきではありません。

## optimize_read_in_order {#optimize_read_in_order}

タイプ: Bool

デフォルト値: 1

[MergeTree](../../engines/table-engines/mergetree-family/mergetree.md) テーブルからデータを読み取るための [ORDER BY](../../sql-reference/statements/select/order-by.md/#optimize_read_in_order) 最適化を有効にします。

可能な値:

- 0 — `ORDER BY` 最適化無効。
- 1 — `ORDER BY` 最適化有効。

**他に見るもの**

- [ORDER BY Clause](../../sql-reference/statements/select/order-by.md/#optimize_read_in_order)

## optimize_read_in_window_order {#optimize_read_in_window_order}

タイプ: Bool

デフォルト値: 1

MergeTree テーブルで対応する順序でデータを読み取るためのウィンドウ句における ORDER BY 最適化を有効にします。

## optimize_redundant_functions_in_order_by {#optimize_redundant_functions_in_order_by}

タイプ: Bool

デフォルト値: 1

ORDER BY で、その引数が同じく ORDER BY にも含まれている場合、ORDER BY から関数を削除します。

## optimize_respect_aliases {#optimize_respect_aliases}

タイプ: Bool

デフォルト値: 1

true に設定されている場合、WHERE/GROUP BY/ORDER BY におけるエイリアスを尊重し、これによりパーティションのプルーニング/二次インデックス/optimize_aggregation_in_order/optimize_read_in_order/optimize_trivial_count に役立ちます。

## optimize_rewrite_aggregate_function_with_if {#optimize_rewrite_aggregate_function_with_if}

タイプ: Bool

デフォルト値: 1

論理的に等しい場合、if 式を引数に持つ集約関数を再書き換えます。
たとえば、`avg(if(cond, col, null))` は `avgOrNullIf(cond, col)` に書き換えられ、パフォーマンスを向上させる可能性があります。

:::note
これはアナライザー (`enable_analyzer = 1`) でのみサポートされています。
:::

## optimize_rewrite_array_exists_to_has {#optimize_rewrite_array_exists_to_has}

タイプ: Bool

デフォルト値: 0

論理的に等しい場合、arrayExists() 関数を has() に書き換えます。たとえば、`arrayExists(x -> x = 1, arr)` は `has(arr, 1)` に書き換えられます。

## optimize_rewrite_sum_if_to_count_if {#optimize_rewrite_sum_if_to_count_if}

タイプ: Bool

デフォルト値: 1

論理的に等しい場合、sumIf() および sum(if()) 関数を countIf() 関数に書き換えます。

## optimize_skip_merged_partitions {#optimize_skip_merged_partitions}

タイプ: Bool

デフォルト値: 0

TTL が期限切れでない、レベルが 0 より大きいパートが 1 つだけである場合、[OPTIMIZE TABLE ... FINAL](../../sql-reference/statements/optimize.md) クエリの最適化を有効または無効にします。

- `OPTIMIZE TABLE ... FINAL SETTINGS optimize_skip_merged_partitions=1`

デフォルトでは、`OPTIMIZE TABLE ... FINAL` クエリは、単一のパートしかない場合でも、そのパートを再書き込みます。

可能な値:

- 1 - 最適化有効。
- 0 - 最適化無効。

## optimize_skip_unused_shards {#optimize_skip_unused_shards}

タイプ: Bool

デフォルト値: 0

`WHERE/PREWHERE` に sharding key 条件がある [SELECT](../../sql-reference/statements/select/index.md) クエリに対する未使用のシャードをスキップすることを有効または無効にします（データが sharding key によって分散されていると仮定します。そうでない場合、クエリは不正確な結果を返します）。

可能な値:

- 0 — 無効。
- 1 — 有効。

## optimize_skip_unused_shards_limit {#optimize_skip_unused_shards_limit}

タイプ: UInt64

デフォルト値: 1000

sharding key 値の数の制限、制限に達した場合は `optimize_skip_unused_shards` をオフにします。

値が多すぎると、処理にかなりの量が必要になる可能性がありますが、利点は疑わしいです。なぜなら、`IN (...)` に多数の値がある場合、ほとんどのケースでクエリは全シャードに送信されるからです。

## optimize_skip_unused_shards_nesting {#optimize_skip_unused_shards_nesting}

タイプ: UInt64

デフォルト値: 0

[`optimize_skip_unused_shards`](#optimize_skip_unused_shards) を制御します（したがって、[`optimize_skip_unused_shards`](#optimize_skip_unused_shards) を必要とします）。分散クエリのネスティングレベルに依存します（分散テーブルが別の分散テーブルを参照している場合のケース）。

可能な値:

- 0 — 無効、`optimize_skip_unused_shards` は常に機能します。
- 1 — 第一レベルのみに `optimize_skip_unused_shards` を有効にします。
- 2 — 第二レベルまで `optimize_skip_unused_shards` を有効にします。

## optimize_skip_unused_shards_rewrite_in {#optimize_skip_unused_shards_rewrite_in}

タイプ: Bool

デフォルト値: 1

シャードに属さない値を除外するためにリモートシャードのクエリで IN を再書き換えます（`optimize_skip_unused_shards` を要求します）。

可能な値:

- 0 — 無効。
- 1 — 有効。

## optimize_sorting_by_input_stream_properties {#optimize_sorting_by_input_stream_properties}

タイプ: Bool

デフォルト値: 1

入力ストリームのソートプロパティによるソートを最適化します。

## optimize_substitute_columns {#optimize_substitute_columns}

タイプ: Bool

デフォルト値: 0

カラムの置換のために [constraints](../../sql-reference/statements/create/table.md/#constraints) を使用します。デフォルトは `false` です。

可能な値:

- true, false

## optimize_syntax_fuse_functions {#optimize_syntax_fuse_functions}

タイプ: Bool

デフォルト値: 0

同一の引数を持つ集計関数を融合させることを有効にします。これは、同一の引数を持つ少なくとも 2 つの集計関数（[sum](../../sql-reference/aggregate-functions/reference/sum.md/#agg_function-sum)、[count](../../sql-reference/aggregate-functions/reference/count.md/#agg_function-count)、または [avg](../../sql-reference/aggregate-functions/reference/avg.md/#agg_function-avg)）を含むクエリを書き換え、[sumCount](../../sql-reference/aggregate-functions/reference/sumcount.md/#agg_function-sumCount) へと変換します。

可能な値:

- 0 — 同一の引数を持つ関数は融合しない。
- 1 — 同一の引数を持つ関数は融合する。

**例**

クエリ:

``` sql
CREATE TABLE fuse_tbl(a Int8, b Int8) Engine = Log;
SET optimize_syntax_fuse_functions = 1;
EXPLAIN SYNTAX SELECT sum(a), sum(b), count(b), avg(b) from fuse_tbl FORMAT TSV;
```

結果:

``` text
SELECT
    sum(a),
    sumCount(b).1,
    sumCount(b).2,
    (sumCount(b).1) / (sumCount(b).2)
FROM fuse_tbl
```

## optimize_throw_if_noop {#optimize_throw_if_noop}

タイプ: Bool

デフォルト値: 0

[OPTIMIZE](../../sql-reference/statements/optimize.md) クエリがマージを実行しなかった場合に例外を投げるかどうかを有効または無効にします。

デフォルトでは、`OPTIMIZE` は何も行わなかった場合でも正常に戻ります。この設定を使用すると、これらの状況を区別し、例外メッセージで理由を得ることができます。

可能な値:

- 1 — 例外を投げることが有効です。
- 0 — 例外を投げないことが無効です。

## optimize_time_filter_with_preimage {#optimize_time_filter_with_preimage}

タイプ: Bool

デフォルト値: 1

関数を変換することによって Date および DateTime の述語を最適化します。変換された比較（例：`toYear(col) = 2023 -> col >= '2023-01-01' AND col <= '2023-12-31'`）。

## optimize_trivial_approximate_count_query {#optimize_trivial_approximate_count_query}

タイプ: Bool

デフォルト値: 0

そのような推定をサポートするストレージ（たとえば、EmbeddedRocksDB）の単純カウント最適化のために近似値を使用します。

可能な値:

   - 0 — 最適化無効。
   - 1 — 最適化有効。

## optimize_trivial_count_query {#optimize_trivial_count_query}

タイプ: Bool

デフォルト値: 1

MergeTree のメタデータを使用して、単純なクエリ `SELECT count() FROM table` の最適化を有効または無効にします。行レベルのセキュリティを使用する必要がある場合は、この設定を無効にしてください。

可能な値:

   - 0 — 最適化無効。
   - 1 — 最適化有効です。

他に見るもの：

- [optimize_functions_to_subcolumns](#optimize_functions_to_subcolumns)

## optimize_trivial_insert_select {#optimize_trivial_insert_select}

タイプ: Bool

デフォルト値: 0

単純な 'INSERT INTO table SELECT ... FROM TABLES' クエリの最適化。

## optimize_uniq_to_count {#optimize_uniq_to_count}

タイプ: Bool

デフォルト値: 1

uniq およびそのバリアント（uniqUpTo を除く）をサブクエリに distinct または group by 句がある場合の count に書き換えます。

## optimize_use_implicit_projections {#optimize_use_implicit_projections}

タイプ: Bool

デフォルト値: 1

SELECT クエリを実行するために暗黙的なプロジェクションを自動的に選択します。

## optimize_use_projections {#optimize_use_projections}

タイプ: Bool

デフォルト値: 1

`SELECT` クエリを処理する際の [projection](../../engines/table-engines/mergetree-family/mergetree.md/#projections) 最適化を有効または無効にします。

可能な値:

- 0 — プロジェクション最適化無効。
- 1 — プロジェクション最適化有効。

## optimize_using_constraints {#optimize_using_constraints}

タイプ: Bool

デフォルト値: 0

クエリ最適化のために [constraints](../../sql-reference/statements/create/table.md/#constraints) を使用します。デフォルトは `false` です。

可能な値:

- true, false

## os_thread_priority {#os_thread_priority}

タイプ: Int64

デフォルト値: 0

クエリを実行するスレッドの優先度（[nice](https://en.wikipedia.org/wiki/Nice_(Unix))）を設定します。OS スケジューラは、各利用可能な CPU コア上で次のスレッドを実行する際にこの優先度を考慮します。

:::note
この設定を使用するには、`CAP_SYS_NICE` 権限を設定する必要があります。`clickhouse-server` パッケージは、インストール時にこれを設定します。一部の仮想環境では、`CAP_SYS_NICE` 権限を設定することができません。この場合、`clickhouse-server` は起動時にそのメッセージを表示します。
:::

可能な値:

- 値は `[-20, 19]` の範囲に設定できます。

値が低いほど優先度が高くなります。優先度が低い `nice` 値を持つスレッドは、高い値を持つスレッドより頻繁に実行されます。高い値は長時間実行される非対話型クエリに好ましく、それにより短い対話型クエリが到着したときにリソースを迅速に放棄できます。

## output_format_compression_level {#output_format_compression_level}

タイプ: UInt64

デフォルト値: 3

クエリの出力が圧縮される場合のデフォルトの圧縮レベル。この設定は、`SELECT` クエリが `INTO OUTFILE` を持っている場合や、テーブル関数 `file`、`url`、`hdfs`、`s3`、または `azureBlobStorage` に書き込む場合に適用されます。

可能な値: `1` から `22` まで
```
## output_format_compression_zstd_window_log {#output_format_compression_zstd_window_log}

タイプ: UInt64

デフォルト値: 0

出力圧縮方法が `zstd` の場合に使用可能です。0より大きい場合、この設定は圧縮ウィンドウのサイズ（2の累乗）を明示的に設定し、zstd圧縮の長距離モードを有効にします。これにより、より良い圧縮比を達成できる可能性があります。

可能な値: 非負の数。値が小さすぎたり大きすぎたりすると、`zstdlib` は例外をスローします。典型的な値は `20`（ウィンドウサイズ = `1MB`）から `30`（ウィンドウサイズ = `1GB`）です。

## output_format_parallel_formatting {#output_format_parallel_formatting}

タイプ: Bool

デフォルト値: 1

データフォーマットの並行フォーマットを有効または無効にします。[TSV](../../interfaces/formats.md/#tabseparated)、[TSKV](../../interfaces/formats.md/#tskv)、[CSV](../../interfaces/formats.md/#csv)および[JSONEachRow](../../interfaces/formats.md/#jsoneachrow)フォーマットのみにサポートされています。

可能な値:

- 1 — 有効。
- 0 — 無効。

## page_cache_inject_eviction {#page_cache_inject_eviction}

タイプ: Bool

デフォルト値: 0

ユーザースペースのページキャッシュは、時々ランダムにページを無効にすることがあります。テスト用に意図されたものです。

## parallel_distributed_insert_select {#parallel_distributed_insert_select}

タイプ: UInt64

デフォルト値: 0

並行分散 `INSERT ... SELECT` クエリを有効にします。

`INSERT INTO distributed_table_a SELECT ... FROM distributed_table_b` のクエリを実行し、両方のテーブルが同じクラスタを使用しており、両方のテーブルが [レプリケートされた](../../engines/table-engines/mergetree-family/replication.md) または非レプリケートされた場合、このクエリは各シャードでローカルに処理されます。

可能な値:

- 0 — 無効。
- 1 — `SELECT` が分散エンジンの基になるテーブルから各シャードで実行されます。
- 2 — `SELECT` と `INSERT` が分散エンジンの基になるテーブルから各シャードで実行されます。

## parallel_replica_offset {#parallel_replica_offset}
<BetaBadge/>

タイプ: UInt64

デフォルト値: 0

これは内部設定であり、直接使用すべきではなく、'parallel replicas'モードの実装詳細を表します。この設定は、クエリ処理に参加しているレプリカのインデックスに対する分散クエリのために、イニシエータサーバーによって自動的に設定されます。

## parallel_replicas_allow_in_with_subquery {#parallel_replicas_allow_in_with_subquery}
<BetaBadge/>

タイプ: Bool

デフォルト値: 1

trueの場合、INに対するサブクエリがすべてのフォロワーレプリカで実行されます。

## parallel_replicas_count {#parallel_replicas_count}
<BetaBadge/>

タイプ: UInt64

デフォルト値: 0

これは内部設定であり、直接使用すべきではなく、'parallel replicas'モードの実装詳細を表します。この設定は、クエリ処理に参加しているパラレルレプリカの数のためにイニシエータサーバーによって自動的に設定されます。

## parallel_replicas_custom_key {#parallel_replicas_custom_key}
<BetaBadge/>

タイプ: String

デフォルト値:

特定のテーブル間で作業を分割するために使用できる任意の整数式。
値は任意の整数式とすることができます。

主キーを使用したシンプルな式が推奨されます。

この設定が複数のレプリカを持つ単一のシャードからなるクラスタで使用されている場合、それらのレプリカは仮想シャードに変換されます。
そうでなければ、`SAMPLE`キーの場合と同じように動作し、各シャードの複数のレプリカを使用します。

## parallel_replicas_custom_key_range_lower {#parallel_replicas_custom_key_range_lower}
<BetaBadge/>

タイプ: UInt64

デフォルト値: 0

フィルタータイプ `range` によって、カスタム範囲 `[parallel_replicas_custom_key_range_lower, INT_MAX]` に基づいて作業を均等に分割します。

[parallel_replicas_custom_key_range_upper](#parallel_replicas_custom_key_range_upper) と組み合わせて使用することで、カスタム範囲 `[parallel_replicas_custom_key_range_lower, parallel_replicas_custom_key_range_upper]` に対して作業が均等に分割されます。

注意: この設定は、クエリ処理中に追加のデータがフィルタリングされる原因にはなりませんが、パラレル処理のために範囲 `[0, INT_MAX]` の範囲フィルターが分割されるポイントを変更します。

## parallel_replicas_custom_key_range_upper {#parallel_replicas_custom_key_range_upper}
<BetaBadge/>

タイプ: UInt64

デフォルト値: 0

フィルタータイプ `range` により、カスタム範囲 `[0, parallel_replicas_custom_key_range_upper]` に基づいて作業を均等に分割します。値が0の場合、上限を無効にし、カスタムキー式の最大値を設定します。

[parallel_replicas_custom_key_range_lower](#parallel_replicas_custom_key_range_lower) と組み合わせて使用することで、カスタム範囲 `[parallel_replicas_custom_key_range_lower, parallel_replicas_custom_key_range_upper]` に対して作業を均等に分割します。

注意: この設定はクエリ処理中に追加のデータがフィルタリングされる原因にはなりませんが、パラレル処理のために範囲 `[0, INT_MAX]` の範囲フィルターが分割されるポイントを変更します。

## parallel_replicas_for_non_replicated_merge_tree {#parallel_replicas_for_non_replicated_merge_tree}
<BetaBadge/>

タイプ: Bool

デフォルト値: 0

trueの場合、ClickHouseは非レプリケートされたMergeTreeテーブルに対しても並行レプリカアルゴリズムを使用します。

## parallel_replicas_index_analysis_only_on_coordinator {#parallel_replicas_index_analysis_only_on_coordinator}
<BetaBadge/>

タイプ: Bool

デフォルト値: 1

インデックス分析はレプリカコーディネーターでのみ行われ、他のレプリカではスキップされます。`parallel_replicas_local_plan` が有効な場合のみ有効です。

## parallel_replicas_local_plan {#parallel_replicas_local_plan}
<BetaBadge/>

タイプ: Bool

デフォルト値: 1

ローカルレプリカ用のローカルプランを構築します。

## parallel_replicas_mark_segment_size {#parallel_replicas_mark_segment_size}
<BetaBadge/>

タイプ: UInt64

デフォルト値: 0

パーツは仮想的にセグメントに分割され、並行読み取りのためにレプリカ間で分配されます。この設定はこれらのセグメントのサイズを制御します。あなたが自分のしていることに確信を持っている場合を除き、変更することはお勧めしません。値は [128; 16384] の範囲であるべきです。

## parallel_replicas_min_number_of_rows_per_replica {#parallel_replicas_min_number_of_rows_per_replica}
<BetaBadge/>

タイプ: UInt64

デフォルト値: 0

クエリで使用されるレプリカの数を制限します（推定読み取り行数 / min_number_of_rows_per_replica）。最大は依然として 'max_parallel_replicas' に制限されています。

## parallel_replicas_mode {#parallel_replicas_mode}
<BetaBadge/>

タイプ: ParallelReplicasMode

デフォルト値: read_tasks

カスタムキーでパラレルレプリカに使用するフィルターの種類。デフォルト - カスタムキーのモジュロ演算を使用、範囲 - カスタムキーの値のタイプに対して可能なすべての値を使用してカスタムキーに範囲フィルターを使用します。

## parallel_replicas_prefer_local_join {#parallel_replicas_prefer_local_join}
<BetaBadge/>

タイプ: Bool

デフォルト値: 1

trueの場合、JOINが並行レプリカアルゴリズムで実行可能であり、すべての右JOIN部分のストレージが *MergeTree であれば、ローカルJOINが使用され、GLOBAL JOINの代わりになります。

## parallel_view_processing {#parallel_view_processing}

タイプ: Bool

デフォルト値: 0

添付視点へのプッシュを逐次的ではなく並行して有効にします。

## parallelize_output_from_storages {#parallelize_output_from_storages}

タイプ: Bool

デフォルト値: 1

ストレージからの読み取りステップの出力を並行化します。これは、可能な場合、ストレージからの読み取り直後にクエリ処理を並行化することを許可します。

## parsedatetime_parse_without_leading_zeros {#parsedatetime_parse_without_leading_zeros}

タイプ: Bool

デフォルト値: 1

関数 'parseDateTime' におけるフォーマッタ '%c'、'%l' および '%k' は、月と時間を先頭ゼロなしで解析します。

## partial_merge_join_left_table_buffer_bytes {#partial_merge_join_left_table_buffer_bytes}

タイプ: UInt64

デフォルト値: 0

0でない場合、部分マージJOINにおける左側テーブルのために、左側テーブルのブロックをより大きいものにグループ化します。結合スレッドごとに最大2倍の指定されたメモリを使用します。

## partial_merge_join_rows_in_right_blocks {#partial_merge_join_rows_in_right_blocks}

タイプ: UInt64

デフォルト値: 65536

[JOIN](../../sql-reference/statements/select/join.md)クエリに対する部分マージJOINアルゴリズムにおける右側結合データブロックのサイズを制限します。

ClickHouseサーバー:

1. 右側の結合データを指定した数の行までのブロックに分割します。
2. 各ブロックをその最小値と最大値でインデックス付けします。
3. 可能であれば、準備したブロックをディスクにアンロードします。

可能な値:

- 任意の正の整数。推奨範囲: \[1000, 100000\]。

## partial_result_on_first_cancel {#partial_result_on_first_cancel}

タイプ: Bool

デフォルト値: 0

キャンセル後に部分結果を返すことを許可します。

## parts_to_delay_insert {#parts_to_delay_insert}

タイプ: UInt64

デフォルト値: 0

宛先テーブルが単一のパーティションにその数以上のアクティブなパーツを含む場合、テーブルへの挿入を人工的に遅くします。

## parts_to_throw_insert {#parts_to_throw_insert}

タイプ: UInt64

デフォルト値: 0

宛先テーブルの単一のパーティションにこの数以上のアクティブなパーツがある場合、'Too many parts ...' 例外をスローします。

## periodic_live_view_refresh {#periodic_live_view_refresh}

タイプ: Seconds

デフォルト値: 60

定期的にリフレッシュされるライブビューを強制的にリフレッシュするまでの間隔。

## poll_interval {#poll_interval}

タイプ: UInt64

デフォルト値: 10

サーバーでのクエリ待機ループを指定された秒数だけブロックします。

## postgresql_connection_attempt_timeout {#postgresql_connection_attempt_timeout}

タイプ: UInt64

デフォルト値: 2

単一のPostgreSQLエンドポイントに接続するための試行の接続タイムアウト（秒）。

この値は接続URLの `connect_timeout` パラメータとして渡されます。

## postgresql_connection_pool_auto_close_connection {#postgresql_connection_pool_auto_close_connection}

タイプ: Bool

デフォルト値: 0

接続をプールに戻す前に接続を閉じます。

## postgresql_connection_pool_retries {#postgresql_connection_pool_retries}

タイプ: UInt64

デフォルト値: 2

PostgreSQLテーブルエンジンおよびデータベースエンジンのための接続プールのプッシュ/ポップのリトライ数。

## postgresql_connection_pool_size {#postgresql_connection_pool_size}

タイプ: UInt64

デフォルト値: 16

PostgreSQLテーブルエンジンおよびデータベースエンジンのための接続プールのサイズ。

## postgresql_connection_pool_wait_timeout {#postgresql_connection_pool_wait_timeout}

タイプ: UInt64

デフォルト値: 5000

PostgreSQLテーブルエンジンおよびデータベースエンジンの空のプールに対する接続プールのプッシュ/ポップタイムアウト。デフォルトでは空のプールでブロックされます。

## postgresql_fault_injection_probability {#postgresql_fault_injection_probability}

タイプ: Float

デフォルト値: 0

内部（レプリケーション用の）PostgreSQLクエリが失敗するおおよその確率。正当な値は [0.0f, 1.0f] の範囲内である必要があります。

## prefer_column_name_to_alias {#prefer_column_name_to_alias}

タイプ: Bool

デフォルト値: 0

クエリの式や句でエイリアスの代わりに元のカラム名を使用することを有効または無効にします。特に、エイリアスがカラム名と同じである場合に重要です。ClickHouseにおけるエイリアスの構文規則を他のほとんどのデータベースエンジンとより互換性のあるものにするために、この設定を有効にしてください。

可能な値:

- 0 — カラム名はエイリアスで置き換えられます。
- 1 — カラム名はエイリアスで置き換えられません。

**例**

有効と無効の違い:

クエリ:

```sql
SET prefer_column_name_to_alias = 0;
SELECT avg(number) AS number, max(number) FROM numbers(10);
```

結果:

```text
Received exception from server (version 21.5.1):
Code: 184. DB::Exception: Received from localhost:9000. DB::Exception: Aggregate function avg(number) is found inside another aggregate function in query: While processing avg(number) AS number.
```

クエリ:

```sql
SET prefer_column_name_to_alias = 1;
SELECT avg(number) AS number, max(number) FROM numbers(10);
```

結果:

```text
┌─number─┬─max(number)─┐
│    4.5 │           9 │
└────────┴─────────────┘
```

## prefer_external_sort_block_bytes {#prefer_external_sort_block_bytes}

タイプ: UInt64

デフォルト値: 16744704

外部ソートの最大ブロックサイズを優先し、マージ中のメモリ使用量を削減します。

## prefer_global_in_and_join {#prefer_global_in_and_join}

タイプ: Bool

デフォルト値: 0

`IN` / `JOIN` 演算子を `GLOBAL IN` / `GLOBAL JOIN` 演算子に置き換えることを有効にします。

可能な値:

- 0 — 無効。`IN` / `JOIN` 演算子は `GLOBAL IN` / `GLOBAL JOIN` に置き換えられません。
- 1 — 有効。`IN` / `JOIN` 演算子は `GLOBAL IN` / `GLOBAL JOIN` に置き換えられます。

**使用方法**

`SET distributed_product_mode=global`が分散テーブルのクエリの動作を変更することができますが、ローカルテーブルや外部リソースのテーブルには適していません。ここで `prefer_global_in_and_join` 設定が活躍します。

例えば、ローカルテーブルを持つクエリサービスノードがあり、これは分散処理に適していません。`GLOBAL`キーワード（`GLOBAL IN` / `GLOBAL JOIN`）でデータを動的に散らす必要があります。

`prefer_global_in_and_join` の別の使用例は、外部エンジンによって作成されたテーブルにアクセスすることです。この設定は、こうしたテーブルを結合する際の外部ソースへの呼び出し回数を減らすのに役立ちます: 1クエリにつき1呼び出しだけです。

**参照:**

- より詳しい情報は、[Distributed subqueries](../../sql-reference/operators/in.md/#select-distributed-subqueries)を参照してください。

## prefer_localhost_replica {#prefer_localhost_replica}

タイプ: Bool

デフォルト値: 1

分散クエリを処理する際にlocalhostレプリカを優先的に使用することを有効または無効にします。

可能な値:

- 1 — ClickHouseは常にローカルホストレプリカにクエリを送信します（存在する場合）。
- 0 — ClickHouseは [load_balancing](#load_balancing) 設定で指定されたバランス戦略を使用します。

:::note
[parallel_replicas_custom_key](#parallel_replicas_custom_key) を使用しない場合は、この設定を無効にしてください。
[parallel_replicas_custom_key](#parallel_replicas_custom_key) が設定されている場合、複数のレプリカを含む複数のシャードを持つクラスタで使用する場合を除いて、この設定は無効にしてください。
単一のシャードと複数のレプリカを持つクラスタで使用される場合、この設定を無効にすると悪影響を与えることになります。
:::

## prefer_warmed_unmerged_parts_seconds {#prefer_warmed_unmerged_parts_seconds}

タイプ: Int64

デフォルト値: 0

ClickHouse Cloudでのみ利用可能です。マージされていないパーツがこの秒数未満の年齢であり、事前にウォームアップされていない場合（[cache_populated_by_fetch](merge-tree-settings.md/#cache_populated_by_fetch) を参照）、すべてのソースパーツが利用可能で事前にウォームアップされている場合、SELECTクエリはそれらのパーツから読み取ります。これは Replicated-/SharedMergeTree のみです。この設定は、CacheWarmer がそのパーツを処理したかどうかのみをチェックします。他の何かがキャッシュにパーツを取得した場合、それはまだ冷たいと見なされます。潜熱があった場合、キャッシュから追い出されると、まだ温かいと見なされます。

## preferred_block_size_bytes {#preferred_block_size_bytes}

タイプ: UInt64

デフォルト値: 1000000

この設定は、クエリ処理のデータブロックサイズを調整し、より粗い 'max_block_size' 設定への追加の微調整を表します。カラムが大きく、'max_block_size' 行のブロックのサイズが指定されたバイト数よりも大きい場合、そのサイズはより良いCPUキャッシュのローカリティのために低下します。

## preferred_max_column_in_block_size_bytes {#preferred_max_column_in_block_size_bytes}

タイプ: UInt64

デフォルト値: 0

読み取り中のブロック内の最大カラムサイズの制限。キャッシュミスの数を減らすのに役立ちます。L2キャッシュサイズの近くであるべきです。

## preferred_optimize_projection_name {#preferred_optimize_projection_name}

タイプ: String

デフォルト値:

空でない文字列に設定されている場合、ClickHouseはクエリ内に指定された投影を適用しようとします。

可能な値:

- 文字列: 推奨される投影の名前。

## prefetch_buffer_size {#prefetch_buffer_size}

タイプ: UInt64

デフォルト値: 1048576

ファイルシステムから読み取るためのプレフェッチバッファの最大サイズ。

## print_pretty_type_names {#print_pretty_type_names}

タイプ: Bool

デフォルト値: 1

`DESCRIBE`クエリおよび`toTypeName()`関数で、深くネストされた型名をきれいにインデントして印刷できるようにします。

例:

```sql
CREATE TABLE test (a Tuple(b String, c Tuple(d Nullable(UInt64), e Array(UInt32), f Array(Tuple(g String, h Map(String, Array(Tuple(i String, j UInt64))))), k Date), l Nullable(String))) ENGINE=Memory;
DESCRIBE TABLE test FORMAT TSVRaw SETTINGS print_pretty_type_names=1;
```

```
a   Tuple(
    b String,
    c Tuple(
        d Nullable(UInt64),
        e Array(UInt32),
        f Array(Tuple(
            g String,
            h Map(
                String,
                Array(Tuple(
                    i String,
                    j UInt64
                ))
            )
        )),
        k Date
    ),
    l Nullable(String)
)
```

## priority {#priority}

タイプ: UInt64

デフォルト値: 0

クエリの優先度。1 - 最高、より高い値 - より低い優先度; 0 - 優先度を使用しない。

## push_external_roles_in_interserver_queries {#push_external_roles_in_interserver_queries}

タイプ: Bool

デフォルト値: 1

クエリを実行する際に、発信者から他のノードにユーザーロールをプッシュすることを有効にします。

## query_cache_compress_entries {#query_cache_compress_entries}

タイプ: Bool

デフォルト値: 1

[クエリキャッシュ](../query-cache.md)内のエントリを圧縮します。クエリキャッシュのメモリ消費を抑え、挿入や読み取り速度を遅くします。

可能な値:

- 0 - 無効
- 1 - 有効

## query_cache_max_entries {#query_cache_max_entries}

タイプ: UInt64

デフォルト値: 0

現在のユーザーが[クエリキャッシュ](../query-cache.md)に保存できるクエリ結果の最大数。0は無制限を意味します。

可能な値:

- 正の整数 >= 0。

## query_cache_max_size_in_bytes {#query_cache_max_size_in_bytes}

タイプ: UInt64

デフォルト値: 0

現在のユーザーが[クエリキャッシュ](../query-cache.md)に割り当てることができる最大メモリ量（バイト）。0は無制限を意味します。

可能な値:

- 正の整数 >= 0。

## query_cache_min_query_duration {#query_cache_min_query_duration}

タイプ: ミリ秒

デフォルト値: 0

クエリの結果が[クエリキャッシュ](../query-cache.md)に保存されるために必要な最小期間（ミリ秒）。

可能な値:

- 正の整数 >= 0。

## query_cache_min_query_runs {#query_cache_min_query_runs}

タイプ: UInt64

デフォルト値: 0

その結果が[クエリキャッシュ](../query-cache.md)に保存されるために、`SELECT` クエリが実行する必要がある最小回数。

可能な値:

- 正の整数 >= 0。

## query_cache_nondeterministic_function_handling {#query_cache_nondeterministic_function_handling}

タイプ: QueryCacheNondeterministicFunctionHandling

デフォルト値: throw

[クエリキャッシュ](../query-cache.md)が `rand()` や `now()` のような非決定的関数を持つ `SELECT` クエリを処理する方法を制御します。

可能な値:

- `'throw'` - 例外をスローし、クエリ結果をキャッシュしない。
- `'save'` - クエリ結果をキャッシュする。
- `'ignore'` - クエリ結果をキャッシュせず、例外をスローしない。

## query_cache_share_between_users {#query_cache_share_between_users}

タイプ: Bool

デフォルト値: 0

オンになっている場合、[クエリキャッシュ](../query-cache.md)でキャッシュされた `SELECT` クエリの結果を他のユーザーが読み取ることができるようになります。
セキュリティ上の理由から、この設定を有効にすることは推奨されません。

可能な値:

- 0 - 無効
- 1 - 有効

## query_cache_squash_partial_results {#query_cache_squash_partial_results}

タイプ: Bool

デフォルト値: 1

部分結果ブロックを[Max_block_size](#max_block_size)のサイズのブロックに圧潰します。これにより、[クエリキャッシュ](../query-cache.md)への挿入のパフォーマンスが低下しますが、キャッシュエントリの圧縮可能性が向上します（[query_cache_compress_entries](#query_cache_compress_entries)参照）。

可能な値:

- 0 - 無効
- 1 - 有効

## query_cache_system_table_handling {#query_cache_system_table_handling}

タイプ: QueryCacheSystemTableHandling

デフォルト値: throw

[クエリキャッシュ](../query-cache.md)が `system.*` および `information_schema.*` のデータベースに対する `SELECT` クエリを処理する方法を制御します。

可能な値:

- `'throw'` - 例外をスローし、クエリ結果をキャッシュしない。
- `'save'` - クエリ結果をキャッシュする。
- `'ignore'` - クエリ結果をキャッシュせず、例外をスローしない。

## query_cache_tag {#query_cache_tag}

タイプ: String

デフォルト値:

[クエリキャッシュ](../query-cache.md)エントリのラベルとして機能する文字列。
異なるタグを持つ同じクエリは、クエリキャッシュによって異なるものと見なされます。

可能な値:

- 任意の文字列

## query_cache_ttl {#query_cache_ttl}

タイプ: Seconds

デフォルト値: 60

この時間（秒）が経過すると、[クエリキャッシュ](../query-cache.md)内のエントリは古くなります。

可能な値:

- 正の整数 >= 0。

## query_metric_log_interval {#query_metric_log_interval}

タイプ: Int64

デフォルト値: -1

個々のクエリに対する[query_metric_log](../../operations/system-tables/query_metric_log.md)が収集されるミリ秒単位のインターバル。

任意の負の値に設定すると、[query_metric_log](../../operations/server-configuration-parameters/settings.md/#query_metric_log)設定の `collect_interval_milliseconds` の値を取るか、存在しない場合は1000になります。

単一のクエリの収集を無効にするには、`query_metric_log_interval` を0に設定します。

デフォルト値: -1

## query_plan_aggregation_in_order {#query_plan_aggregation_in_order}

タイプ: Bool

デフォルト値: 1

順序での集約クエリプラン最適化の切り替え。
設定 [query_plan_enable_optimizations](#query_plan_enable_optimizations) が1の場合にのみ効果があります。

:::note
これはデバッグ用の開発者向けの専門設定です。この設定は将来的に後方互換性のない方法で変更されるか、削除される可能性があります。
:::

可能な値:

- 0 - 無効
- 1 - 有効

## query_plan_convert_outer_join_to_inner_join {#query_plan_convert_outer_join_to_inner_join}

タイプ: Bool

デフォルト値: 1

JOIN後のフィルターが常にデフォルト値をフィルタリングする場合、OUTER JOINをINNER JOINに変換することを許可します。

## query_plan_enable_multithreading_after_window_functions {#query_plan_enable_multithreading_after_window_functions}

タイプ: Bool

デフォルト値: 1

ウィンドウ関数の評価後にマルチスレッドを有効にし、並行ストリーム処理を許可します。

## query_plan_enable_optimizations {#query_plan_enable_optimizations}

タイプ: Bool

デフォルト値: 1

クエリプランレベルでのクエリ最適化の切り替え。

:::note
これはデバッグ用の開発者向けの専門設定です。この設定は将来的に後方互換性のない方法で変更されるか、削除される可能性があります。
:::

可能な値:

- 0 - クエリプランレベルのすべての最適化を無効にする。
- 1 - クエリプランレベルの最適化を有効にする（ただし、個々の最適化はそれぞれの設定で無効にされる場合があります）。

## query_plan_execute_functions_after_sorting {#query_plan_execute_functions_after_sorting}

タイプ: Bool

デフォルト値: 1

ソート処理の後に式を移動するクエリプランレベルの最適化を切り替えます。
設定 [query_plan_enable_optimizations](#query_plan_enable_optimizations) が1の場合にのみ効果があります。

:::note
これはデバッグ用の開発者向けの専門設定です。この設定は将来的に後方互換性のない方法で変更されるか、削除される可能性があります。
:::

可能な値:

- 0 - 無効
- 1 - 有効

## query_plan_filter_push_down {#query_plan_filter_push_down}

タイプ: Bool

デフォルト値: 1

クエリプラン内のフィルターを実行プランの下に移動させる最適化を切り替えます。
設定 [query_plan_enable_optimizations](#query_plan_enable_optimizations) が1の場合にのみ効果があります。

:::note
これはデバッグ用の開発者向けの専門設定です。この設定は将来的に後方互換性のない方法で変更されるか、削除される可能性があります。
:::

可能な値:

- 0 - 無効
- 1 - 有効

## query_plan_join_swap_table {#query_plan_join_swap_table}

タイプ: BoolAuto

デフォルト値: auto

    JOINのクエリプラン内で、どの側のテーブルがビルドテーブル（ハッシュJOINのハッシュテーブルに挿入されるテーブルとも呼ばれる）であるべきかを決定します。この設定は、`JOIN ON` 節を使用した `ALL` ジョインの厳密さのためのみにサポートされています。可能な値は次のとおりです。
    - 'auto': プランナーがビルドテーブルとして使用するテーブルを決定します。
    - 'false': テーブルを決して交換しない（右テーブルがビルドテーブル）。
    - 'true': テーブルを常に交換する（左テーブルがビルドテーブル）。

## query_plan_lift_up_array_join {#query_plan_lift_up_array_join}

タイプ: Bool

デフォルト値: 1

クエリプラン内でARRAY JOINを実行プランの上に移動させる最適化を切り替えます。
設定 [query_plan_enable_optimizations](#query_plan_enable_optimizations) が1の場合にのみ効果があります。

:::note
これはデバッグ用の開発者向けの専門設定です。この設定は将来的に後方互換性のない方法で変更されるか、削除される可能性があります。
:::

可能な値:

- 0 - 無効
- 1 - 有効

## query_plan_lift_up_union {#query_plan_lift_up_union}

タイプ: Bool

デフォルト値: 1

クエリプランの大きなサブツリーをユニオンに移動させ、さらなる最適化を可能にする最適化を切り替えます。
設定 [query_plan_enable_optimizations](#query_plan_enable_optimizations) が1の場合にのみ効果があります。

:::note
これはデバッグ用の開発者向けの専門設定です。この設定は将来的に後方互換性のない方法で変更されるか、削除される可能性があります。
:::

可能な値:

- 0 - 無効
- 1 - 有効

## query_plan_max_optimizations_to_apply {#query_plan_max_optimizations_to_apply}

タイプ: UInt64

デフォルト値: 10000

クエリプランに適用される最適化の総数を制限します。設定 [query_plan_enable_optimizations](#query_plan_enable_optimizations) を参照してください。
複雑なクエリの長い最適化時間を回避するのに役立ちます。
実際の最適化の数がこの設定を超えた場合、例外がスローされます。

:::note
これはデバッグ用の開発者向けの専門設定です。この設定は将来的に後方互換性のない方法で変更されるか、削除される可能性があります。
:::

## query_plan_merge_expressions {#query_plan_merge_expressions}

タイプ: Bool

デフォルト値: 1

連続するフィルターをマージするクエリプランレベルの最適化を切り替えます。
設定 [query_plan_enable_optimizations](#query_plan_enable_optimizations) が1の場合にのみ効果があります。

:::note
これはデバッグ用の開発者向けの専門設定です。この設定は将来的に後方互換性のない方法で変更されるか、削除される可能性があります。
:::

可能な値:

- 0 - 無効
- 1 - 有効

## query_plan_merge_filters {#query_plan_merge_filters}

タイプ: Bool

デフォルト値: 1

クエリプラン内のフィルターをマージすることを許可します。

## query_plan_optimize_prewhere {#query_plan_optimize_prewhere}

タイプ: Bool

デフォルト値: 1

サポートされているストレージのためにフィルターをPREWHERE式にプッシュダウンすることを許可します。

## query_plan_push_down_limit {#query_plan_push_down_limit}

タイプ: Bool

デフォルト値: 1

LIMITを実行プランの下に移動させる最適化を切り替えます。
設定 [query_plan_enable_optimizations](#query_plan_enable_optimizations) が1の場合にのみ効果があります。

:::note
これはデバッグ用の開発者向けの専門設定です。この設定は将来的に後方互換性のない方法で変更されるか、削除される可能性があります。
:::

可能な値:

- 0 - 無効
- 1 - 有効

## query_plan_read_in_order {#query_plan_read_in_order}

タイプ: Bool

デフォルト値: 1

順序での読み取り最適化のクエリプランレベルの最適化を切り替えます。
設定 [query_plan_enable_optimizations](#query_plan_enable_optimizations) が1の場合にのみ効果があります。

:::note
これはデバッグ用の開発者向けの専門設定です。この設定は将来的に後方互換性のない方法で変更されるか、削除される可能性があります。
:::

可能な値:

- 0 - 無効
- 1 - 有効

## query_plan_remove_redundant_distinct {#query_plan_remove_redundant_distinct}

タイプ: Bool

デフォルト値: 1

冗長なDISTINCTステップを削除するクエリプランレベルの最適化を切り替えます。
設定 [query_plan_enable_optimizations](#query_plan_enable_optimizations) が1の場合にのみ効果があります。

:::note
これはデバッグ用の開発者向けの専門設定です。この設定は将来的に後方互換性のない方法で変更されるか、削除される可能性があります。
:::

可能な値:

- 0 - 無効
- 1 - 有効

## query_plan_remove_redundant_sorting {#query_plan_remove_redundant_sorting}

タイプ: Bool

デフォルト値: 1

冗長なソートステップを削除するクエリプランレベルの最適化を切り替えます。例: サブクエリ内。
設定 [query_plan_enable_optimizations](#query_plan_enable_optimizations) が1の場合にのみ効果があります。

:::note
これはデバッグ用の開発者向けの専門設定です。この設定は将来的に後方互換性のない方法で変更されるか、削除される可能性があります。
:::

可能な値:

- 0 - 無効
- 1 - 有効

## query_plan_reuse_storage_ordering_for_window_functions {#query_plan_reuse_storage_ordering_for_window_functions}

タイプ: Bool

デフォルト値: 1

ウィンドウ関数のためにソートストレージを使用するクエリプランレベルの最適化を切り替えます。
設定 [query_plan_enable_optimizations](#query_plan_enable_optimizations) が1の場合にのみ効果があります。

:::note
これはデバッグ用の開発者向けの専門設定です。この設定は将来的に後方互換性のない方法で変更されるか、削除される可能性があります。
:::

## query_plan_split_filter {#query_plan_split_filter}

Type: Bool

Default value: 1

:::note
この設定は専門的なものであり、開発者によるデバッグのためだけに使用すべきです。この設定は将来、後方互換性のない方法で変更されるか削除される可能性があります。
:::

クエリ計画レベルの最適化を切り替え、フィルターを式に分割します。
設定 [query_plan_enable_optimizations](#query_plan_enable_optimizations) が 1 の場合のみ効果があります。

可能な値:

- 0 - 無効
- 1 - 有効

## query_plan_try_use_vector_search {#query_plan_try_use_vector_search}

Type: Bool

Default value: 1

ベクトル類似性インデックスを使用しようとするクエリ計画レベルの最適化を切り替えます。
設定 [query_plan_enable_optimizations](#query_plan_enable_optimizations) が 1 の場合のみ効果があります。

:::note
この設定は専門的なものであり、開発者によるデバッグのためだけに使用すべきです。この設定は将来、後方互換性のない方法で変更されるか削除される可能性があります。
:::

可能な値:

- 0 - 無効
- 1 - 有効

## query_plan_use_new_logical_join_step {#query_plan_use_new_logical_join_step}

Type: Bool

Default value: 1

クエリ計画で新しい論理結合ステップを使用します。

## query_profiler_cpu_time_period_ns {#query_profiler_cpu_time_period_ns}

Type: UInt64

Default value: 1000000000

[クエリプロファイラー](../../operations/optimizing-performance/sampling-query-profiler.md)のためのCPUクロックタイマーの期間を設定します。このタイマーはCPU時間のみをカウントします。

可能な値:

- 正の整数ナノ秒数。

    推奨値:

            - 単一のクエリの場合は10000000（1秒間に100回）ナノ秒以上。
            - クラスター全体のプロファイリングには1000000000（1秒間に1回）。


- タイマーをオフにするための0。

**ClickHouse Cloudでは一時的に無効になっています。**

関連情報:

- システムテーブル [trace_log](../../operations/system-tables/trace_log.md/#system_tables-trace_log)

## query_profiler_real_time_period_ns {#query_profiler_real_time_period_ns}

Type: UInt64

Default value: 1000000000

[クエリプロファイラー](../../operations/optimizing-performance/sampling-query-profiler.md)のためのリアルクロックタイマーの期間を設定します。リアルクロックタイマーは、時計の時間をカウントします。

可能な値:

- 正の整数ナノ秒数。

    推奨値:

            - 単一のクエリの場合は10000000（1秒間に100回）ナノ秒未満。
            - クラスター全体のプロファイリングには1000000000（1秒間に1回）。

- タイマーをオフにするための0。

**ClickHouse Cloudでは一時的に無効になっています。**

関連情報:

- システムテーブル [trace_log](../../operations/system-tables/trace_log.md/#system_tables-trace_log)

## queue_max_wait_ms {#queue_max_wait_ms}

Type: Milliseconds

Default value: 0

同時リクエストの数が最大を超えた場合の、リクエストキューでの待機時間。

## rabbitmq_max_wait_ms {#rabbitmq_max_wait_ms}

Type: Milliseconds

Default value: 5000

リトライする前にRabbitMQから読み取るための待機時間。

## read_backoff_max_throughput {#read_backoff_max_throughput}

Type: UInt64

Default value: 1048576

遅い読み取りの際にスレッド数を減らすための設定です。読み取り帯域幅が一定のバイト数/秒未満のときにイベントをカウントします。

## read_backoff_min_concurrency {#read_backoff_min_concurrency}

Type: UInt64

Default value: 1

遅い読み取りの際に、最小限のスレッド数を維持しようとする設定です。

## read_backoff_min_events {#read_backoff_min_events}

Type: UInt64

Default value: 2

遅い読み取りの際にスレッド数を減らすための設定です。スレッド数が減少する前のイベント数です。

## read_backoff_min_interval_between_events_ms {#read_backoff_min_interval_between_events_ms}

Type: Milliseconds

Default value: 1000

遅い読み取りの際にスレッド数を減らすための設定です。前のイベントが特定の時間未満の場合、イベントを無視します。

## read_backoff_min_latency_ms {#read_backoff_min_latency_ms}

Type: Milliseconds

Default value: 1000

遅い読み取りの際にスレッド数を減らすための設定です。この時間以上かかった読み取りのみに注意します。

## read_from_filesystem_cache_if_exists_otherwise_bypass_cache {#read_from_filesystem_cache_if_exists_otherwise_bypass_cache}

Type: Bool

Default value: 0

ファイルシステムキャッシュをパッシブモードで使用できるようにします - 既存のキャッシュエントリを利用しますが、さらにエントリをキャッシュに追加しません。この設定を重いアドホッククエリ用に設定し、短いリアルタイムクエリには無効にすることで、重いクエリによるキャッシュのスレッシングを回避し、全体的なシステム効率を改善します。

## read_from_page_cache_if_exists_otherwise_bypass_cache {#read_from_page_cache_if_exists_otherwise_bypass_cache}

Type: Bool

Default value: 0

`read_from_filesystem_cache_if_exists_otherwise_bypass_cache` のように、ユーザースペースページキャッシュをパッシブモードで使用します。

## read_in_order_two_level_merge_threshold {#read_in_order_two_level_merge_threshold}

Type: UInt64

Default value: 100

プライマリキーの順序でマルチスレッド読み取り中に予備マージステップを実行するために読み取る最小パーツ数です。

## read_in_order_use_buffering {#read_in_order_use_buffering}

Type: Bool

Default value: 1

プライマリキーの順序で読み取る際にマージ前にバッファリングを使用します。これはクエリ実行の並列性を増加させます。

## read_in_order_use_virtual_row {#read_in_order_use_virtual_row}

Type: Bool

Default value: 0

プライマリキーまたはその単調関数の順序で読み取る際に仮想行を使用します。これは、関連するパーツのみをタッチするため、複数のパーツを検索する際に有用です。

## read_overflow_mode {#read_overflow_mode}

Type: OverflowMode

Default value: throw

制限が超えた場合の処理方法です。

## read_overflow_mode_leaf {#read_overflow_mode_leaf}

Type: OverflowMode

Default value: throw

リーフ制限が超えた場合の処理方法です。

## read_priority {#read_priority}

Type: Int64

Default value: 0

ローカルファイルシステムまたはリモートファイルシステムからデータを読む際の優先度です。これは、ローカルファイルシステムの `pread_threadpool` メソッドと、リモートファイルシステムの `threadpool` メソッドにのみ対応しています。

## read_through_distributed_cache {#read_through_distributed_cache}

Type: Bool

Default value: 0

ClickHouse Cloud のみ。分散キャッシュからの読み取りを許可します。

## readonly {#readonly}

Type: UInt64

Default value: 0

0 - 読み取り専用制限なし。 1 - 読み取りリクエストのみで、明示的に許可された設定を変更可能。 2 - 読み取りリクエストのみで、'readonly' 設定を除く設定を変更可能。

## receive_data_timeout_ms {#receive_data_timeout_ms}

Type: Milliseconds

Default value: 2000

最初のデータパケットまたはレプリカからの進捗が正のパケットを受信するための接続タイムアウト。

## receive_timeout {#receive_timeout}

Type: Seconds

Default value: 300

ネットワークからデータを受信するためのタイムアウト（秒）。この間にバイトが受信されなかった場合、例外がスローされます。この設定をクライアントで設定すると、ソケットの 'send_timeout' もサーバーの対応する接続端末に設定されます。

## regexp_max_matches_per_row {#regexp_max_matches_per_row}

Type: UInt64

Default value: 1000

単一の行あたりの正規表現の最大一致数を設定します。これは、[extractAllGroupsHorizontal](../../sql-reference/functions/string-search-functions.md/#extractallgroups-horizontal) 関数で貪欲な正規表現を使用する際のメモリオーバーロードを防ぐために使用されます。

可能な値:

- 正の整数。

## reject_expensive_hyperscan_regexps {#reject_expensive_hyperscan_regexps}

Type: Bool

Default value: 1

ハイパースキャンで評価するのに高コストになる可能性のあるパターンを拒否します（NFA状態の爆発により）。

## remerge_sort_lowered_memory_bytes_ratio {#remerge_sort_lowered_memory_bytes_ratio}

Type: Float

Default value: 2

再マージ後のメモリ使用量がこの比率で減少しなかった場合、再マージは無効になります。

## remote_filesystem_read_method {#remote_filesystem_read_method}

Type: String

Default value: threadpool

リモートファイルシステムからデータを読み取る方法。`read` または `threadpool` のいずれかです。

## remote_filesystem_read_prefetch {#remote_filesystem_read_prefetch}

Type: Bool

Default value: 1

リモートファイルシステムからデータを読み取る際にプリフェッチを使用する必要があります。

## remote_fs_read_backoff_max_tries {#remote_fs_read_backoff_max_tries}

Type: UInt64

Default value: 5

バックオフを伴う読み取りの最大試行回数。

## remote_fs_read_max_backoff_ms {#remote_fs_read_max_backoff_ms}

Type: UInt64

Default value: 10000

リモートディスクからデータを読み取る際の最大待機時間。

## remote_read_min_bytes_for_seek {#remote_read_min_bytes_for_seek}

Type: UInt64

Default value: 4194304

リモート読み取り（url, s3）でシークを行うために必要な最小バイト数です。それ以外は無視して読み取ります。

## rename_files_after_processing {#rename_files_after_processing}

Type: String

Default value:

- **Type:** String

- **Default value:** 空の文字列

この設定は、`file` テーブル関数によって処理されたファイルのリネームパターンを指定できるようにします。オプションが設定されると、`file` テーブル関数によって読み取られたすべてのファイルは、指定されたパターンに従ってプレースホルダーを使用してリネームされます。ただし、ファイル処理が成功した場合のみです。

### プレースホルダー

- `%a` — 完全な元のファイル名（例: "sample.csv"）。
- `%f` — 拡張子なしの元のファイル名（例: "sample"）。
- `%e` — ドット付きの元のファイル拡張子（例: ".csv"）。
- `%t` — タイムスタンプ（マイクロ秒単位）。
- `%%` — パーセント記号 ("%")。

### 例

- オプション: `--rename_files_after_processing="processed_%f_%t%e"`

- クエリ: `SELECT * FROM file('sample.csv')`

`sample.csv` の読み取りが成功した場合は、ファイル名が `processed_sample_1683473210851438.csv` にリネームされます。

## replace_running_query {#replace_running_query}

Type: Bool

Default value: 0

HTTPインターフェースを使用する場合、`query_id` パラメータを渡すことができます。これは、クエリ識別子として機能する任意の文字列です。同じユーザーから同じ `query_id` のクエリが現在実行中の場合、この動作は `replace_running_query` パラメータに依存します。

`0` （デフォルト） - 例外をスローします（同じ `query_id` で実行中のクエリがある場合、そのクエリの実行を許可しません）。

`1` - 古いクエリをキャンセルし、新しいクエリの実行を開始します。

セグメンテーション条件の提案を実装するには、このパラメータを1に設定します。次の文字を入力したときに、古いクエリがまだ終了していなければ、それはキャンセルされるべきです。

## replace_running_query_max_wait_ms {#replace_running_query_max_wait_ms}

Type: Milliseconds

Default value: 5000

[replace_running_query](#replace_running_query) 設定がアクティブなときに、同じ `query_id` を持つクエリの実行が終了するまでの待機時間です。

可能な値:

- 正の整数。
- 0 — 同じ `query_id` を持つクエリがサーバーで実行中の場合、新しいクエリを実行できない例外をスローします。

## replication_wait_for_inactive_replica_timeout {#replication_wait_for_inactive_replica_timeout}

Type: Int64

Default value: 120

非アクティブなレプリカが [ALTER](../../sql-reference/statements/alter/index.md)、[OPTIMIZE](../../sql-reference/statements/optimize.md)、または [TRUNCATE](../../sql-reference/statements/truncate.md) クエリを実行するまで待機する時間（秒）を指定します。

可能な値:

- 0 — 待機しない。
- 負の整数 — 無制限の時間待機します。
- 正の整数 — 待機する秒数。

## restore_replace_external_dictionary_source_to_null {#restore_replace_external_dictionary_source_to_null}

Type: Bool

Default value: 0

復元時に外部辞書のソースを Null に置き換えます。テスト目的に有用です。

## restore_replace_external_engines_to_null {#restore_replace_external_engines_to_null}

Type: Bool

Default value: 0

テスト目的です。すべての外部エンジンを Null に置き換えて、外部接続を開始しないようにします。

## restore_replace_external_table_functions_to_null {#restore_replace_external_table_functions_to_null}

Type: Bool

Default value: 0

テスト目的です。すべての外部テーブル関数を Null に置き換えて、外部接続を開始しないようにします。

## result_overflow_mode {#result_overflow_mode}

Type: OverflowMode

Default value: throw

制限が超えた場合の処理方法です。

## rewrite_count_distinct_if_with_count_distinct_implementation {#rewrite_count_distinct_if_with_count_distinct_implementation}

Type: Bool

Default value: 0

`countDistcintIf` を[ count_distinct_implementation](#count_distinct_implementation) 設定で置き換えることを許可します。

可能な値:

- true — 許可。
- false — 不許可。

## s3_allow_parallel_part_upload {#s3_allow_parallel_part_upload}

Type: Bool

Default value: 1

S3のマルチパートアップロードに複数のスレッドを使用します。これにより、メモリ使用量がわずかに増加する可能性があります。

## s3_check_objects_after_upload {#s3_check_objects_after_upload}

Type: Bool

Default value: 0

アップロードした各オブジェクトを s3 に対してヘッドリクエストでチェックし、アップロードが成功したことを確認します。

## s3_connect_timeout_ms {#s3_connect_timeout_ms}

Type: UInt64

Default value: 1000

s3 ディスクからホストへの接続タイムアウトです。

## s3_create_new_file_on_insert {#s3_create_new_file_on_insert}

Type: Bool

Default value: 0

S3エンジンテーブルに挿入するたびに新しいファイルの作成を有効または無効にします。これが有効な場合、各挿入時に次のパターンに似たキーで新しいS3オブジェクトが作成されます:

初期: `data.Parquet.gz` -> `data.1.Parquet.gz` -> `data.2.Parquet.gz`など。

可能な値:
- 0 — `INSERT` クエリはファイルの末尾に新しいデータを追加します。
- 1 — `INSERT` クエリは新しいファイルを作成します。

## s3_disable_checksum {#s3_disable_checksum}

Type: Bool

Default value: 0

ファイルをS3に送信する際にチェックサムを計算しないようにします。これにより、ファイル上の過剰な処理パスを避けることで書き込みが高速化されます。MergeTree テーブルのデータは ClickHouse によってチェックサムされるため、ほぼ安全です。また、S3が HTTPS経由でアクセスされる際には、TLS 層がネットワークを介して転送中の整合性を提供します。S3上の追加のチェックサムは深い防御を提供します。

## s3_ignore_file_doesnt_exist {#s3_ignore_file_doesnt_exist}

Type: Bool

Default value: 0

特定のキーを読み取る際にファイルが存在しない場合の無視の設定です。

可能な値:
- 1 — `SELECT` は空の結果を返します。
- 0 — `SELECT` は例外をスローします。

## s3_list_object_keys_size {#s3_list_object_keys_size}

Type: UInt64

Default value: 1000

ListObject リクエストでバッチとして返すことができるファイルの最大数です。

## s3_max_connections {#s3_max_connections}

Type: UInt64

Default value: 1024

サーバーごとの最大接続数です。

## s3_max_get_burst {#s3_max_get_burst}

Type: UInt64

Default value: 0

リクエストが毎秒制限に達する前に同時に発行できる最大リクエスト数です。デフォルト (0) は `s3_max_get_rps` と等しいです。

## s3_max_get_rps {#s3_max_get_rps}

Type: UInt64

Default value: 0

スロットルがかかる前のS3 GETリクエストの制限です。ゼロは無制限を意味します。

## s3_max_inflight_parts_for_one_file {#s3_max_inflight_parts_for_one_file}

Type: UInt64

Default value: 20

マルチパートアップロードリクエストで同時に読み込まれるパーツの最大数です。0 は無制限を意味します。

## s3_max_part_number {#s3_max_part_number}

Type: UInt64

Default value: 10000

S3アップロードパートの最大パート番号です。

## s3_max_put_burst {#s3_max_put_burst}

Type: UInt64

Default value: 0

リクエストが毎秒制限に達する前に同時に発行できる最大リクエスト数です。デフォルト (0) は `s3_max_put_rps` と等しいです。

## s3_max_put_rps {#s3_max_put_rps}

Type: UInt64

Default value: 0

スロットルがかかる前のS3 PUTリクエストの制限です。ゼロは無制限を意味します。

## s3_max_redirects {#s3_max_redirects}

Type: UInt64

Default value: 10

許可される最大のS3リダイレクトホップ数です。

## s3_max_single_operation_copy_size {#s3_max_single_operation_copy_size}

Type: UInt64

Default value: 33554432

S3での単一のコピー操作の最大サイズです。

## s3_max_single_part_upload_size {#s3_max_single_part_upload_size}

Type: UInt64

Default value: 33554432

単一パートアップロードでアップロードするオブジェクトの最大サイズです。

## s3_max_single_read_retries {#s3_max_single_read_retries}

Type: UInt64

Default value: 4

単一のS3読み取り中の最大再試行回数です。

## s3_max_unexpected_write_error_retries {#s3_max_unexpected_write_error_retries}

Type: UInt64

Default value: 4

S3書き込み中の予期しないエラー発生時の最大再試行回数です。

## s3_max_upload_part_size {#s3_max_upload_part_size}

Type: UInt64

Default value: 5368709120

マルチパートアップロードをS3にする際の最大パートサイズです。

## s3_min_upload_part_size {#s3_min_upload_part_size}

Type: UInt64

Default value: 16777216

マルチパートアップロードをS3にする際の最小パートサイズです。

## s3_request_timeout_ms {#s3_request_timeout_ms}

Type: UInt64

Default value: 30000

S3へのデータの送受信時のアイドルタイムアウト。この長さの時間、単一のTCP読み取りまたは書き込み呼び出しがブロックされると失敗します。

## s3_retry_attempts {#s3_retry_attempts}

Type: UInt64

Default value: 100

Aws::Client::RetryStrategy のための設定。Aws::Clientは自動的に再試行を行うため、0は再試行なしを意味します。

## s3_skip_empty_files {#s3_skip_empty_files}

Type: Bool

Default value: 1

[S3](../../engines/table-engines/integrations/s3.md) エンジンテーブルで空のファイルをスキップするかどうかを有効または無効にします。

可能な値:
- 0 — 空のファイルが要求されたフォーマットと互換性がない場合、`SELECT` は例外をスローします。
- 1 — 空のファイルに対して `SELECT` は空の結果を返します。

## s3_strict_upload_part_size {#s3_strict_upload_part_size}

Type: UInt64

Default value: 0

マルチパートアップロードをS3にする際のパートの正確なサイズ（いくつかの実装は可変サイズのパーツをサポートしていません）。

## s3_throw_on_zero_files_match {#s3_throw_on_zero_files_match}

Type: Bool

Default value: 0

ListObjectsリクエストがファイルと一致しない場合にエラーをスローします。

## s3_truncate_on_insert {#s3_truncate_on_insert}

Type: Bool

Default value: 0

S3エンジンテーブルへの挿入前にトランケートを有効または無効にします。無効にすると、既に存在するS3オブジェクトがある場合、挿入時に例外がスローされます。

可能な値:
- 0 — `INSERT` クエリはファイルの末尾に新しいデータを追加します。
- 1 — `INSERT` クエリは新しいデータで既存のファイルの内容を置き換えます。

## s3_upload_part_size_multiply_factor {#s3_upload_part_size_multiply_factor}

Type: UInt64

Default value: 2

このファクターで `s3_min_upload_part_size` を増やします。これは、s3_multiply_parts_count_threshold パーツが S3 にアップロードされた場合ごとに行われます。

## s3_upload_part_size_multiply_parts_count_threshold {#s3_upload_part_size_multiply_parts_count_threshold}

Type: UInt64

Default value: 500

この数のパーツがS3にアップロードされるたびに、`s3_min_upload_part_size`が `s3_upload_part_size_multiply_factor` 通りに増やされます。

## s3_use_adaptive_timeouts {#s3_use_adaptive_timeouts}

Type: Bool

Default value: 1

`true` に設定されている場合、すべてのS3リクエストで最初の2回の試行が低い送信および受信のタイムアウトで行われます。
`false` に設定されている場合、すべての試行が同じタイムアウトで行われます。

## s3_validate_request_settings {#s3_validate_request_settings}

Type: Bool

Default value: 1

S3リクエスト設定の検証を有効にします。

可能な値:
- 1 — 設定を検証します。
- 0 — 設定を検証しません。

## s3queue_default_zookeeper_path {#s3queue_default_zookeeper_path}

Type: String

Default value: /clickhouse/s3queue/

S3Queueエンジンのデフォルトのzookeeperパスプレフィックスです。

## s3queue_enable_logging_to_s3queue_log {#s3queue_enable_logging_to_s3queue_log}

Type: Bool

Default value: 0

system.s3queue_log への書き込みを有効にします。この値は、テーブル設定によって上書き可能です。

## s3queue_migrate_old_metadata_to_buckets {#s3queue_migrate_old_metadata_to_buckets}

Type: Bool

Default value: 0

S3Queueテーブルの古いメタデータ構造を新しいものに移行します。

## schema_inference_cache_require_modification_time_for_url {#schema_inference_cache_require_modification_time_for_url}

Type: Bool

Default value: 1

最終変更時刻の検証にキャッシュからのスキーマを使用します（Last-Modifiedヘッダーを持つURL用）。

## schema_inference_use_cache_for_azure {#schema_inference_use_cache_for_azure}

Type: Bool

Default value: 1

Azureテーブル関数を使用する際にスキーマ推論の割り当てにキャッシュを使用します。

## schema_inference_use_cache_for_file {#schema_inference_use_cache_for_file}

Type: Bool

Default value: 1

ファイルテーブル関数を使用する際にスキーマ推論にキャッシュを使用します。

## schema_inference_use_cache_for_hdfs {#schema_inference_use_cache_for_hdfs}

Type: Bool

Default value: 1

HDFS テーブル関数を使用する際にスキーマ推論にキャッシュを使用します。

## schema_inference_use_cache_for_s3 {#schema_inference_use_cache_for_s3}

Type: Bool

Default value: 1

S3 テーブル関数を使用する際にスキーマ推論にキャッシュを使用します。

## schema_inference_use_cache_for_url {#schema_inference_use_cache_for_url}

Type: Bool

Default value: 1

URL テーブル関数を使用する際にスキーマ推論にキャッシュを使用します。

## select_sequential_consistency {#select_sequential_consistency}

Type: UInt64

Default value: 0

:::note
この設定は SharedMergeTree と ReplicatedMergeTree で動作が異なるため、[SharedMergeTree 一貫性](/docs/cloud/reference/shared-merge-tree/#consistency)を参照して、SharedMergeTree における `select_sequential_consistency` の動作についての詳細を確認してください。
:::

`SELECT` クエリのための選択的一貫性を有効または無効にします。`insert_quorum_parallel` が無効である必要があります（デフォルトでは有効です）。

可能な値:

- 0 — 無効。
- 1 — 有効。

使用法

選択的一貫性が有効になっている場合、ClickHouse はクライアントが `insert_quorum` で実行されたすべての以前の `INSERT` クエリのデータを含むレプリカに対してのみ `SELECT` クエリを実行することを許可します。クライアントが部分的なレプリカを参照する場合、ClickHouse は例外を生成します。SELECT クエリは、まだクオーラムのレプリカに書き込まれていないデータを含みません。

`insert_quorum_parallel` が有効な場合（デフォルト）、`select_sequential_consistency` は機能しません。これは、並行 `INSERT` クエリが異なるクオーラムレプリカのセットに書き込まれる可能性があるため、単一のレプリカがすべての書き込みを受信することが保証されないからです。

関連情報:

- [insert_quorum](#insert_quorum)
- [insert_quorum_timeout](#insert_quorum_timeout)
- [insert_quorum_parallel](#insert_quorum_parallel)

## send_logs_level {#send_logs_level}

Type: LogsLevel

Default value: fatal

指定された最小レベルでサーバーテキストログをクライアントに送信します。有効な値: 'trace', 'debug', 'information', 'warning', 'error', 'fatal', 'none'

## send_logs_source_regexp {#send_logs_source_regexp}

Type: String

Default value:

指定された正規表現に合致するログソース名を持つサーバーテキストログを送信します。空である場合、すべてのソースが対象です。

## send_progress_in_http_headers {#send_progress_in_http_headers}

Type: Bool

Default value: 0

`clickhouse-server` の応答に `X-ClickHouse-Progress` HTTP応答ヘッダを有効または無効にします。

詳細については、[HTTPインターフェースの説明](../../interfaces/http.md)を参照してください。

可能な値:

- 0 — 無効。
- 1 — 有効。

## send_timeout {#send_timeout}

Type: Seconds

Default value: 300

ネットワークにデータを送信するためのタイムアウト（秒）。クライアントがデータを送信する必要があるが、この間にバイトを送信できなかった場合、例外がスローされます。この設定をクライアントで設定すると、ソケットの 'receive_timeout' もサーバーの対応する接続端末に設定されます。

## session_timezone {#session_timezone}
<BetaBadge/>

Type: Timezone

Default value:

現在のセッションまたはクエリの暗黙のタイムゾーンを設定します。
暗黙のタイムゾーンは、明示的に指定されたタイムゾーンがない DateTime/DateTime64 型の値に適用されます。
この設定は、全体的に設定された（サーバーレベルの）暗黙のタイムゾーンよりも優先されます。
''（空文字列）の値は、現在のセッションまたはクエリの暗黙のタイムゾーンが [サーバーのタイムゾーン](../server-configuration-parameters/settings.md/#timezone) と等しいことを意味します。

`timeZone()` および `serverTimeZone()` 関数を使用してセッションタイムゾーンおよびサーバータイムゾーンを取得できます。

可能な値:

- `system.time_zones` からの任意のタイムゾーン名、例: `Europe/Berlin`, `UTC` または `Zulu`

例:

```sql
SELECT timeZone(), serverTimeZone() FORMAT CSV

"Europe/Berlin","Europe/Berlin"
```

```sql
SELECT timeZone(), serverTimeZone() SETTINGS session_timezone = 'Asia/Novosibirsk' FORMAT CSV

"Asia/Novosibirsk","Europe/Berlin"
```

暗黙のタイムゾーン 'America/Denver' を日付時刻に明示的にタイムゾーンを指定せずに割り当てます:

```sql
SELECT toDateTime64(toDateTime64('1999-12-12 23:23:23.123', 3), 3, 'Europe/Zurich') SETTINGS session_timezone = 'America/Denver' FORMAT TSV

1999-12-13 07:23:23.123
```

:::warning
すべての DateTime/DateTime64 を解析する関数が `session_timezone` を尊重するわけではありません。これにより微妙なエラーが発生する可能性があります。
次の例と説明を参照してください。
:::

```sql
CREATE TABLE test_tz (`d` DateTime('UTC')) ENGINE = Memory AS SELECT toDateTime('2000-01-01 00:00:00', 'UTC');

SELECT *, timeZone() FROM test_tz WHERE d = toDateTime('2000-01-01 00:00:00') SETTINGS session_timezone = 'Asia/Novosibirsk'
0 rows in set.

SELECT *, timeZone() FROM test_tz WHERE d = '2000-01-01 00:00:00' SETTINGS session_timezone = 'Asia/Novosibirsk'
┌───────────────────d─┬─timeZone()───────┐
│ 2000-01-01 00:00:00 │ Asia/Novosibirsk │
└─────────────────────┴──────────────────┘
```

これは、異なるパースパイプラインによるものです:

- 明示的なタイムゾーンなしの `toDateTime()` は、最初の `SELECT` クエリで `session_timezone` およびグローバルタイムゾーンの設定を尊重します。
- 第二のクエリでは、文字列から DateTime が解析され、既存の列 `d` の型とタイムゾーンを継承します。したがって、`session_timezone` およびグローバルタイムゾーンの設定は尊重されません。

**詳細は**

- [timezone](../server-configuration-parameters/settings.md/#timezone)

## set_overflow_mode {#set_overflow_mode}

Type: OverflowMode

Default value: throw

制限が超えた場合の処理方法です。

## shared_merge_tree_sync_parts_on_partition_operations {#shared_merge_tree_sync_parts_on_partition_operations}

Type: Bool

Default value: 1

SMTテーブルでのMOVE|REPLACE|ATTACHパーティション操作の後にデータパーツのセットを自動的に同期します。クラウドのみ。

## short_circuit_function_evaluation {#short_circuit_function_evaluation}

Type: ShortCircuitFunctionEvaluation

Default value: enable

[if](../../sql-reference/functions/conditional-functions.md/#if)、[multiIf](../../sql-reference/functions/conditional-functions.md/#multiif)、[and](../../sql-reference/functions/logical-functions.md/#logical-and-function)、および [or](../../sql-reference/functions/logical-functions.md/#logical-or-function) 関数を短絡方式で計算できます。これにより、これらの関数内の複雑な式の実行が最適化され、予期しない例外（ゼロ除算など）を防ぐのに役立ちます。

可能な値:

- `enable` — 適切な関数に対して短絡関数評価を有効にします（例外を投げる可能性があるまたは計算コストが高い）。
- `force_enable` — すべての関数に対して短絡関数評価を有効にします。
- `disable` — 短絡関数評価を無効にします。

## short_circuit_function_evaluation_for_nulls {#short_circuit_function_evaluation_for_nulls}

Type: Bool

Default value: 1

引数にNULLでない値が含まれる行のみに対して、NULL引数を持つ関数を実行できるようにします。NULL 引数の比率が `short_circuit_function_evaluation_for_nulls_threshold` を超えた場合に適用されます。これは、少なくとも1つの引数にNULL値がある行に対してNULL値を返す関数にのみ適用されます。

## short_circuit_function_evaluation_for_nulls_threshold {#short_circuit_function_evaluation_for_nulls_threshold}

Type: Double

Default value: 1

全ての引数が非NULL値である行に対して、NULL引数を持つ関数を実行するためのNULL値の比率の閾値です。 `short_circuit_function_evaluation_for_nulls` 設定が有効になっている場合に適用されます。
NULL値を含む行の比率がこの閾値を超えると、NULL値を含むこれらの行は評価されなくなります。

## show_table_uuid_in_table_create_query_if_not_nil {#show_table_uuid_in_table_create_query_if_not_nil}

Type: Bool

Default value: 0

`SHOW TABLE` クエリの表示設定です。

可能な値:

- 0 — クエリはテーブル UUID なしで表示されます。
- 1 — クエリはテーブル UUID と共に表示されます。

## single_join_prefer_left_table {#single_join_prefer_left_table}

Type: Bool

Default value: 1

識別子の曖昧さがある場合、単一のJOINで左側のテーブルを優先します。

## skip_redundant_aliases_in_udf {#skip_redundant_aliases_in_udf}

Type: Bool

Default value: 0

ユーザー定義関数内では冗長なエイリアスが使用されず（置き換えられず）、その使用が簡素化されます。

可能な値:

- 1 — エイリアスは UDF でスキップされます（置き換えられます）。
- 0 — エイリアスは UDF でスキップされません（置き換えられません）。

**例**

有効と無効の違い:

クエリ:

```sql
SET skip_redundant_aliases_in_udf = 0;
CREATE FUNCTION IF NOT EXISTS test_03274 AS ( x ) -> ((x + 1 as y, y + 2));

EXPLAIN SYNTAX SELECT test_03274(4 + 2);
```

結果:

```text
SELECT ((4 + 2) + 1 AS y, y + 2)
```

クエリ:

```sql
SET skip_redundant_aliases_in_udf = 1;
CREATE FUNCTION IF NOT EXISTS test_03274 AS ( x ) -> ((x + 1 as y, y + 2));

EXPLAIN SYNTAX SELECT test_03274(4 + 2);
```

結果:

```text
SELECT ((4 + 2) + 1, ((4 + 2) + 1) + 2)
```
```

## skip_unavailable_shards {#skip_unavailable_shards}

Type: Bool

Default value: 0

利用できないシャードを静かにスキップするかどうかを有効または無効にします。

シャードは、その全てのレプリカが利用できない場合、利用できないと見なされます。レプリカが利用できないのは以下のケースです：

- ClickHouseが何らかの理由でレプリカに接続できない。

    レプリカに接続する際、ClickHouseは複数回の試行を行います。これらの試行が全て失敗した場合、そのレプリカは利用できないと見なされます。

- レプリカがDNSを通じて解決できない。

    レプリカのホスト名がDNSを通じて解決できない場合、以下の状況を示唆するかもしれません：

    - レプリカのホストにDNSレコードが存在しない。この状況は、ダイナミックDNSを持つシステムでは発生する可能性があり、例えば、[Kubernetes](https://kubernetes.io)のように、ノードがダウンタイム中に解決できないことがありますが、これはエラーではありません。

    - 設定エラー。ClickHouseの設定ファイルに誤ったホスト名が含まれています。

可能な値：

- 1 — スキップが有効です。

    シャードが利用できない場合、ClickHouseは部分データに基づいて結果を返し、ノードの利用可能性の問題を報告しません。

- 0 — スキップが無効です。

    シャードが利用できない場合、ClickHouseは例外をスローします。

## sleep_after_receiving_query_ms {#sleep_after_receiving_query_ms}

Type: Milliseconds

Default value: 0

TCPHandlerでクエリを受信した後のスリープ時間

## sleep_in_send_data_ms {#sleep_in_send_data_ms}

Type: Milliseconds

Default value: 0

TCPHandlerでデータ送信時のスリープ時間

## sleep_in_send_tables_status_ms {#sleep_in_send_tables_status_ms}

Type: Milliseconds

Default value: 0

TCPHandlerでテーブルのステータス応答を送信する際のスリープ時間

## sort_overflow_mode {#sort_overflow_mode}

Type: OverflowMode

Default value: throw

制限を超えた場合の対処方法。

## split_intersecting_parts_ranges_into_layers_final {#split_intersecting_parts_ranges_into_layers_final}

Type: Bool

Default value: 1

最終最適化中に交差するパーツレンジをレイヤーに分割します。

## split_parts_ranges_into_intersecting_and_non_intersecting_final {#split_parts_ranges_into_intersecting_and_non_intersecting_final}

Type: Bool

Default value: 1

最終最適化中にパーツレンジを交差し、交差しないものに分割します。

## splitby_max_substrings_includes_remaining_string {#splitby_max_substrings_includes_remaining_string}

Type: Bool

Default value: 0

`max_substrings` > 0の引数を持つ関数 [splitBy*()](../../sql-reference/functions/splitting-merging-functions.md) が、結果配列の最終要素に残りの文字列を含むかどうかを制御します。

可能な値：

- `0` - 残りの文字列は結果配列の最終要素に含まれません。
- `1` - 残りの文字列は結果配列の最終要素に含まれます。これはSparkの[`split()`](https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.functions.split.html)関数とPythonの['string.split()'](https://docs.python.org/3/library/stdtypes.html#str.split)メソッドの動作です。

## stop_refreshable_materialized_views_on_startup {#stop_refreshable_materialized_views_on_startup}
<ExperimentalBadge/>

Type: Bool

Default value: 0

サーバー起動時に、まるで SYSTEM STOP VIEWS のように更新可能なマテリアライズドビューのスケジューリングを防ぎます。その後、手動で `SYSTEM START VIEWS` や `SYSTEM START VIEW <name>` で再開できます。新たに作成されたビューにも適用されます。非更新可能なマテリアライズドビューには影響ありません。

## storage_file_read_method {#storage_file_read_method}

Type: LocalFSReadMethod

Default value: pread

ストレージファイルからデータを読み込む方法。`read`, `pread`, `mmap`のいずれかです。mmapメソッドはclickhouse-serverには適用されません（clickhouse-local用です）。

## storage_system_stack_trace_pipe_read_timeout_ms {#storage_system_stack_trace_pipe_read_timeout_ms}

Type: Milliseconds

Default value: 100

`system.stack_trace`テーブルからスレッドからの情報を取得するときにパイプから読む最大時間。この設定はテスト目的で使用され、ユーザーによって変更されることは意図されていません。

## stream_flush_interval_ms {#stream_flush_interval_ms}

Type: Milliseconds

Default value: 7500

ストリーミングを行うテーブルにおいて、タイムアウトが発生した場合や、スレッドが [max_insert_block_size](#max_insert_block_size) 行を生成した場合に機能します。

デフォルト値は7500です。

値が小さくなるほど、データがテーブルにフラッシュされる頻度が高くなります。値を低く設定しすぎると、パフォーマンスが低下します。

## stream_like_engine_allow_direct_select {#stream_like_engine_allow_direct_select}

Type: Bool

Default value: 0

Kafka、RabbitMQ、FileLog、Redis Streams、およびNATSエンジンに対して直接SELECTクエリを許可します。マテリアライズドビューが接続されている場合、この設定が有効でもSELECTクエリは許可されません。

## stream_like_engine_insert_queue {#stream_like_engine_insert_queue}

Type: String

Default value:

ストリームライクエンジンが複数のキューからデータを読み取る場合、書き込み時に挿入するキューを選択する必要があります。Redis StreamsとNATSによって使用されます。

## stream_poll_timeout_ms {#stream_poll_timeout_ms}

Type: Milliseconds

Default value: 500

ストリーミングストレージからのデータのポーリングに対するタイムアウト。

## system_events_show_zero_values {#system_events_show_zero_values}

Type: Bool

Default value: 0

[`system.events`](../../operations/system-tables/events.md)からゼロ値のイベントを選択することを許可します。

一部の監視システムは、メトリクスの値がゼロであっても、各チェックポイントに対してすべてのメトリクスの値を渡すことを要求します。

可能な値：

- 0 — 無効。
- 1 — 有効。

**例**

クエリ

```sql
SELECT * FROM system.events WHERE event='QueryMemoryLimitExceeded';
```

結果

```text
Ok.
```

クエリ
```sql
SET system_events_show_zero_values = 1;
SELECT * FROM system.events WHERE event='QueryMemoryLimitExceeded';
```

結果

```text
┌─event────────────────────┬─value─┬─description───────────────────────────────────────────┐
│ QueryMemoryLimitExceeded │     0 │ クエリのためにメモリ制限を超えた回数。                           │
└──────────────────────────┴───────┴───────────────────────────────────────────────────────┘
```

## table_function_remote_max_addresses {#table_function_remote_max_addresses}

Type: UInt64

Default value: 1000

[remote](../../sql-reference/table-functions/remote.md)関数のために生成されるパターンからの最大アドレス数を設定します。

可能な値：

- 正の整数。

## tcp_keep_alive_timeout {#tcp_keep_alive_timeout}

Type: Seconds

Default value: 290

TCPがキープアライブプローブを送信する前に接続がアイドルの状態を保つ必要がある秒数。

## temporary_data_in_cache_reserve_space_wait_lock_timeout_milliseconds {#temporary_data_in_cache_reserve_space_wait_lock_timeout_milliseconds}

Type: UInt64

Default value: 600000

ファイルシステムキャッシュのために、一時データのスペース予約のためにキャッシュをロックするまでの待機時間。

## temporary_files_codec {#temporary_files_codec}

Type: String

Default value: LZ4

ディスク上のソートおよび結合操作に使用される一時ファイルの圧縮コーデックを設定します。

可能な値：

- LZ4 — [LZ4](https://en.wikipedia.org/wiki/LZ4_(compression_algorithm))圧縮が適用されます。
- NONE — 圧縮は適用されません。

## throw_if_deduplication_in_dependent_materialized_views_enabled_with_async_insert {#throw_if_deduplication_in_dependent_materialized_views_enabled_with_async_insert}

Type: Bool

Default value: 1

`async_insert`と共に`deduplicate_blocks_in_dependent_materialized_views`が有効な場合、INSERTクエリで例外をスローします。これにより正確性が保証されます；なぜなら、これらの機能は共存できないためです。

## throw_if_no_data_to_insert {#throw_if_no_data_to_insert}

Type: Bool

Default value: 1

空のINSERTを許可するかどうかを制御します。デフォルトでは有効（空の挿入でエラーをスローします）。これは、[`clickhouse-client`](/docs/interfaces/cli)または[gRPCインターフェース](/docs/interfaces/grpc)を使用するINSERTにのみ適用されます。

## throw_on_error_from_cache_on_write_operations {#throw_on_error_from_cache_on_write_operations}

Type: Bool

Default value: 0

書き込み操作（INSERT、マージ）時にキャッシュからのエラーを無視します。

## throw_on_max_partitions_per_insert_block {#throw_on_max_partitions_per_insert_block}

Type: Bool

Default value: 1

max_partitions_per_insert_blockと共に使用されます。true（デフォルト）の場合、max_partitions_per_insert_blockに達した際に例外がスローされます。falseの場合は、この制限に達したINSERTクエリの詳細とパーティション数がログに記録されます。これは、max_partitions_per_insert_blockを変更する際のユーザーへの影響を理解するのに役立ちます。

## throw_on_unsupported_query_inside_transaction {#throw_on_unsupported_query_inside_transaction}
<ExperimentalBadge/>

Type: Bool

Default value: 1

トランザクション内でサポートされていないクエリが使用された場合に例外をスローします。

## timeout_before_checking_execution_speed {#timeout_before_checking_execution_speed}

Type: Seconds

Default value: 10

指定された時間が経過した後、速度が遅すぎないことを確認します。

## timeout_overflow_mode {#timeout_overflow_mode}

Type: OverflowMode

Default value: throw

制限を超えた場合の対処方法。

## timeout_overflow_mode_leaf {#timeout_overflow_mode_leaf}

Type: OverflowMode

Default value: throw

リーフ制限を超えた場合の対処方法。

## totals_auto_threshold {#totals_auto_threshold}

Type: Float

Default value: 0.5

`totals_mode = 'auto'`のためのしきい値です。
「WITH TOTALS修飾子」セクションを参照してください。

## totals_mode {#totals_mode}

Type: TotalsMode

Default value: after_having_exclusive

HAVINGが存在する場合のTOTALSの計算方法、及びmax_rows_to_group_byとgroup_by_overflow_mode = ‘any’が存在する場合の計算方法。
「WITH TOTALS修飾子」セクションを参照してください。

## trace_profile_events {#trace_profile_events}

Type: Bool

Default value: 0

プロファイルイベントの各更新時に、プロファイルイベント名とインクリメント値と共にスタックトレースを収集することを有効または無効にします。そして、収集したデータを[trace_log](../../operations/system-tables/trace_log.md/#system_tables-trace_log)に送信します。

可能な値：

- 1 — プロファイルイベントのトレースが有効です。
- 0 — プロファイルイベントのトレースが無効です。

## transfer_overflow_mode {#transfer_overflow_mode}

Type: OverflowMode

Default value: throw

制限を超えた場合の対処方法。

## transform_null_in {#transform_null_in}

Type: Bool

Default value: 0

[IN](../../sql-reference/operators/in.md)演算子での[NULL](../../sql-reference/syntax.md/#null-literal)値の等価性を有効にします。

デフォルトでは、`NULL`値は比較できません。なぜなら`NULL`は未定義の値を意味するからです。したがって、比較 `expr = NULL` は常に`false`を返すべきです。この設定を有効にすると `NULL = NULL` が`IN`演算子で`true`を返します。

可能な値：

- 0 — `IN`演算子での`NULL`値の比較は`false`を返します。
- 1 — `IN`演算子での`NULL`値の比較は`true`を返します。

**例**

`null_in`テーブルを考えます：

``` text
┌──idx─┬─────i─┐
│    1 │     1 │
│    2 │  NULL │
│    3 │     3 │
└──────┴───────┘
```

クエリ：

``` sql
SELECT idx, i FROM null_in WHERE i IN (1, NULL) SETTINGS transform_null_in = 0;
```

結果：

``` text
┌──idx─┬────i─┐
│    1 │    1 │
└──────┴──────┘
```

クエリ：

``` sql
SELECT idx, i FROM null_in WHERE i IN (1, NULL) SETTINGS transform_null_in = 1;
```

結果：

``` text
┌──idx─┬─────i─┐
│    1 │     1 │
│    2 │  NULL │
└──────┴───────┘
```

**参照**

- [IN演算子におけるNULL処理](../../sql-reference/operators/in.md/#in-null-processing)

## traverse_shadow_remote_data_paths {#traverse_shadow_remote_data_paths}

Type: Bool

Default value: 0

クエリsystem.remote_data_pathsの際に、実際のテーブルデータに加えてフリーズしたデータ（シャドウディレクトリ）をトラバースします。

## union_default_mode {#union_default_mode}

Type: SetOperationMode

Default value:

`SELECT`クエリ結果を結合するためのモードを設定します。この設定は、`UNION ALL`または`UNION DISTINCT`を明示的に指定せずに[`UNION`](../../sql-reference/statements/select/union.md)で使用される時のみ使用されます。

可能な値：

- `'DISTINCT'` — ClickHouseは重複行を除外してクエリの結合結果を出力します。
- `'ALL'` — ClickHouseは重複行を含めてクエリの結合結果を出力します。
- `''` — ClickHouseは`UNION`と共に使用される場合に例外を生成します。

[UNION](../../sql-reference/statements/select/union.md)の例を参照してください。

## unknown_packet_in_send_data {#unknown_packet_in_send_data}

Type: UInt64

Default value: 0

データのNthデータパケットの代わりに不明なパケットを送信します。

## use_async_executor_for_materialized_views {#use_async_executor_for_materialized_views}

Type: Bool

Default value: 0

マテリアライズドビュークエリの非同期および潜在的にマルチスレッド実行を使用します。これにより、INSERT時のビュー処理が高速化されますが、メモリの使用量も増加する可能性があります。

## use_cache_for_count_from_files {#use_cache_for_count_from_files}

Type: Bool

Default value: 1

テーブル関数`file`/`s3`/`url`/`hdfs`/`azureBlobStorage`におけるファイルからのカウント時に行数のキャッシュを有効にします。

デフォルトで有効です。

## use_client_time_zone {#use_client_time_zone}

Type: Bool

Default value: 0

サーバーのタイムゾーンを採用する代わりに、DateTime文字列値を解釈するためにクライアントタイムゾーンを使用します。

## use_compact_format_in_distributed_parts_names {#use_compact_format_in_distributed_parts_names}

Type: Bool

Default value: 1

`Distributed`エンジンを持つテーブルへのバックグラウンド挿入（`distributed_foreground_insert`）のためにブロックを保存するコンパクトフォーマットを使用します。

可能な値：

- 0 — `user[:password]@host:port#default_database`ディレクトリフォーマットを使用します。
- 1 — `[shard{shard_index}[_replica{replica_index}]]`ディレクトリフォーマットを使用します。

:::note
- `use_compact_format_in_distributed_parts_names=0`の場合、クラスタ定義の変更はバックグラウンドINSERTには適用されません。
- `use_compact_format_in_distributed_parts_names=1`の場合、クラスタ定義内のノードの順序を変更すると、`shard_index`/`replica_index`が変更されるため、注意が必要です。
:::

## use_concurrency_control {#use_concurrency_control}

Type: Bool

Default value: 1

サーバーの同時実行制御を尊重します（`concurrent_threads_soft_limit_num`および`concurrent_threads_soft_limit_ratio_to_cores`のグローバルサーバー設定を参照）。無効にすると、サーバーが過負荷の場合でも、より多くのスレッドを使用できます（通常の使用では推奨されません。主にテストのために必要です）。

## use_hedged_requests {#use_hedged_requests}

Type: Bool

Default value: 1

リモートクエリのためのヘッジリクエストロジックを有効にします。これにより、クエリのために異なるレプリカとの多くの接続を確立できます。
既存の接続が`hedged_connection_timeout`内に確立されていない場合や、受信データタイムアウト内にデータが受信されなかった場合に新しい接続が有効になります。クエリは、非空のプログレスパケット（またはデータパケットが`allow_changing_replica_until_first_data_packet`かつ空でない場合）を送信した最初の接続を使用します。他の接続はキャンセルされます。`max_parallel_replicas > 1`のクエリがサポートされます。

デフォルトで有効です。

Cloudではデフォルトで無効です。

## use_hive_partitioning {#use_hive_partitioning}

Type: Bool

Default value: 1

有効にすると、ClickHouseはファイルライクテーブルエンジン [File](../../engines/table-engines/special/file.md/#hive-style-partitioning)/[S3](../../engines/table-engines/integrations/s3.md/#hive-style-partitioning)/[URL](../../engines/table-engines/special/url.md/#hive-style-partitioning)/[HDFS](../../engines/table-engines/integrations/hdfs.md/#hive-style-partitioning)/[AzureBlobStorage](../../engines/table-engines/integrations/azureBlobStorage.md/#hive-style-partitioning)においてパス（`/name=value/`）中のHiveスタイルのパーティショニングを検出し、クエリ内でパーティションカラムを仮想カラムとして使用できるようにします。これらの仮想カラムは、パーティションパス内と同じ名前を持ちますが、`_`で始まります。

## use_iceberg_partition_pruning {#use_iceberg_partition_pruning}

Type: Bool

Default value: 0

Icebergテーブルに対するIcebergパーティショニングのプルーニングを使用します。

## use_index_for_in_with_subqueries {#use_index_for_in_with_subqueries}

Type: Bool

Default value: 1

IN演算子の右側にサブクエリやテーブル式がある場合、インデックスを使用しようとします。

## use_index_for_in_with_subqueries_max_values {#use_index_for_in_with_subqueries_max_values}

Type: UInt64

Default value: 0

IN演算子の右側のセットの最大サイズで、フィルタリングのためにテーブルインデックスを使用するためのものです。これにより、大きなクエリのために追加のデータ構造を準備することによるパフォーマンスの低下やメモリの使用量の増加を回避できます。ゼロは制限なしを意味します。

## use_json_alias_for_old_object_type {#use_json_alias_for_old_object_type}

Type: Bool

Default value: 0

有効にすると、`JSON`データ型のエイリアスが、新しい[JSON](../../sql-reference/data-types/newjson.md)型の代わりに古い[Object('json')](../../sql-reference/data-types/json.md)型を作成するために使用されます。

## use_local_cache_for_remote_storage {#use_local_cache_for_remote_storage}

Type: Bool

Default value: 1

HDFSやS3のようなリモートストレージのためにローカルキャッシュを使用します。これはリモートテーブルエンジンのみに使用されます。

## use_page_cache_for_disks_without_file_cache {#use_page_cache_for_disks_without_file_cache}

Type: Bool

Default value: 0

ファイルシステムキャッシュが有効でないリモートディスクのためにユーザースペースページキャッシュを使用します。

## use_query_cache {#use_query_cache}

Type: Bool

Default value: 0

有効にすると、`SELECT`クエリは[クエリキャッシュ](../query-cache.md)を利用できるようになります。パラメータ[enable_reads_from_query_cache](#enable_reads_from_query_cache)と[enable_writes_to_query_cache](#enable_writes_to_query_cache)が、キャッシュの使用方法をより詳細に制御します。

可能な値：

- 0 - 無効
- 1 - 有効

## use_skip_indexes {#use_skip_indexes}

Type: Bool

Default value: 1

クエリ実行時にデータスキッピングインデックスを使用します。

可能な値：

- 0 — 無効。
- 1 — 有効。

## use_skip_indexes_if_final {#use_skip_indexes_if_final}

Type: Bool

Default value: 0

FINAL修飾子付きのクエリ実行時にスキップインデックスが使用されるかどうかを制御します。

デフォルトでは、この設定は無効です。なぜならスキップインデックスは、最新のデータを含む行（グラニュール）を除外する可能性があり、不正確な結果を引き起こす可能性があるからです。有効にすると、FINAL修飾子があってもスキップインデックスが適用され、パフォーマンスが向上する可能性がありますが、最近の更新が見逃されるリスクがあります。

可能な値：

- 0 — 無効。
- 1 — 有効。

## use_structure_from_insertion_table_in_table_functions {#use_structure_from_insertion_table_in_table_functions}

Type: UInt64

Default value: 2

データからのスキーマ推論の代わりに、挿入テーブルの構造を使用します。可能な値：0 - 無効、1 - 有効、2 - 自動

## use_uncompressed_cache {#use_uncompressed_cache}

Type: Bool

Default value: 0

非圧縮ブロックのキャッシュを使用するかどうかを示します。0または1を受け入れます。デフォルトでは、0（無効）です。
非圧縮キャッシュを使用することで（MergeTreeファミリーのテーブルのみ）、多くの短いクエリを処理する際の待機時間を大幅に削減し、スループットを向上させることができます。この設定は、頻繁に短いリクエストを送信するユーザー向けに有効にします。また、[uncompressed_cache_size](../../operations/server-configuration-parameters/settings.md/#server-settings-uncompressed_cache_size)設定パラメータにも注意してください（設定ファイルのみで設定） - 非圧縮キャッシュブロックのサイズ。デフォルトでは8GiBです。非圧縮キャッシュは必要に応じて充填され、使用頻度の低いデータは自動的に削除されます。

データの少なくともかなりの量を読み取るクエリ（百万行以上）は、自動的に非圧縮キャッシュを無効化して、実際に小さなクエリのためのスペースを節約します。つまり、`use_uncompressed_cache`設定を常に1に設定しておくことができます。

## use_variant_as_common_type {#use_variant_as_common_type}

Type: Bool

Default value: 0

引数の型に共通の型がない場合、[if](../../sql-reference/functions/conditional-functions.md/#if)/[multiIf](../../sql-reference/functions/conditional-functions.md/#multiif)/[array](../../sql-reference/functions/array-functions.md)/[map](../../sql-reference/functions/tuple-map-functions.md)関数の結果型として`Variant`型を使用できるようにします。

例：

```sql
SET use_variant_as_common_type = 1;
SELECT toTypeName(if(number % 2, number, range(number))) as variant_type FROM numbers(1);
SELECT if(number % 2, number, range(number)) as variant FROM numbers(5);
```

```text
┌─variant_type───────────────────┐
│ Variant(Array(UInt64), UInt64) │
└────────────────────────────────┘
┌─variant───┐
│ []        │
│ 1         │
│ [0,1]     │
│ 3         │
│ [0,1,2,3] │
└───────────┘
```

```sql
SET use_variant_as_common_type = 1;
SELECT toTypeName(multiIf((number % 4) = 0, 42, (number % 4) = 1, [1, 2, 3], (number % 4) = 2, 'Hello, World!', NULL)) AS variant_type FROM numbers(1);
SELECT multiIf((number % 4) = 0, 42, (number % 4) = 1, [1, 2, 3], (number % 4) = 2, 'Hello, World!', NULL) AS variant FROM numbers(4);
```

```text
─variant_type─────────────────────────┐
│ Variant(Array(UInt8), String, UInt8) │
└──────────────────────────────────────┘

┌─variant───────┐
│ 42            │
│ [1,2,3]       │
│ Hello, World! │
│ ᴺᵁᴸᴸ          │
└───────────────┘
```

```sql
SET use_variant_as_common_type = 1;
SELECT toTypeName(array(range(number), number, 'str_' || toString(number))) as array_of_variants_type from numbers(1);
SELECT array(range(number), number, 'str_' || toString(number)) as array_of_variants FROM numbers(3);
```

```text
┌─array_of_variants_type────────────────────────┐
│ Array(Variant(Array(UInt64), String, UInt64)) │
└───────────────────────────────────────────────┘

┌─array_of_variants─┐
│ [[],0,'str_0']    │
│ [[0],1,'str_1']   │
│ [[0,1],2,'str_2'] │
└───────────────────┘
```

```sql
SET use_variant_as_common_type = 1;
SELECT toTypeName(map('a', range(number), 'b', number, 'c', 'str_' || toString(number))) as map_of_variants_type from numbers(1);
SELECT map('a', range(number), 'b', number, 'c', 'str_' || toString(number)) as map_of_variants FROM numbers(3);
```

```text
┌─map_of_variants_type────────────────────────────────┐
│ Map(String, Variant(Array(UInt64), String, UInt64)) │
└─────────────────────────────────────────────────────┘

┌─map_of_variants───────────────┐
│ {'a':[],'b':0,'c':'str_0'}    │
│ {'a':[0],'b':1,'c':'str_1'}   │
│ {'a':[0,1],'b':2,'c':'str_2'} │
└───────────────────────────────┘
```

## use_with_fill_by_sorting_prefix {#use_with_fill_by_sorting_prefix}

Type: Bool

Default value: 1

ORDER BY句のWITH FILLの前にカラムが並ぶ場合、ソートプレフィックスを形成します。ソートプレフィックス内の異なる値を持つ行は独立して埋められます。

## validate_enum_literals_in_operators {#validate_enum_literals_in_operators}

Type: Bool

Default value: 0

有効にすると、`IN`、`NOT IN`、`==`、`!=`のような演算子で列挙型リテラルを列挙型と照らし合わせて検証し、リテラルが有効な列挙型値でない場合に例外をスローします。

## validate_mutation_query {#validate_mutation_query}

Type: Bool

Default value: 1

ミューテーションクエリを受け入れる前に検証します。ミューテーションはバックグラウンドで実行されており、無効なクエリを実行するとミューテーションがスタックし、手動介入が必要になる場合があります。

後方互換性のないバグに遭遇した場合以外は、この設定を変更しないでください。

## validate_polygons {#validate_polygons}

Type: Bool

Default value: 1

多角形が自己交差または自己接触している場合、[pointInPolygon](../../sql-reference/functions/geo/index.md/#pointinpolygon)関数で例外をスローするかどうかを有効または無効にします。

可能な値：

- 0 — 例外のスローが無効です。`pointInPolygon`は無効な多角形を受け入れ、それに対して不正確な結果を返す可能性があります。
- 1 — 例外のスローが有効です。

## wait_changes_become_visible_after_commit_mode {#wait_changes_become_visible_after_commit_mode}
<ExperimentalBadge/>

Type: TransactionsWaitCSNMode

Default value: wait_unknown

コミットされた変更が最新のスナップショットで実際に表示されるのを待ちます。

## wait_for_async_insert {#wait_for_async_insert}

Type: Bool

Default value: 1

trueの場合、非同期挿入の処理を待ちます。

## wait_for_async_insert_timeout {#wait_for_async_insert_timeout}

Type: Seconds

Default value: 120

非同期挿入処理を待つためのタイムアウト。

## wait_for_window_view_fire_signal_timeout {#wait_for_window_view_fire_signal_timeout}
<ExperimentalBadge/>

Type: Seconds

Default value: 10

イベント時間処理におけるウィンドウビューの発火信号を待つためのタイムアウト。

## window_view_clean_interval {#window_view_clean_interval}
<ExperimentalBadge/>

Type: Seconds

Default value: 60

古いデータを解放するためのウィンドウビューのクリーニング間隔（秒）。

## window_view_heartbeat_interval {#window_view_heartbeat_interval}
<ExperimentalBadge/>

Type: Seconds

Default value: 15

クエリが生存していることを示すハートビート間隔（秒）。

## workload {#workload}

Type: String

Default value: default

リソースにアクセスするために使用されるワークロードの名前。

## write_through_distributed_cache {#write_through_distributed_cache}

Type: Bool

Default value: 0

ClickHouse Cloudのみに適用されます。分散キャッシュへの書き込みを許可します（S3への書き込みも分散キャッシュを介して行われます）。

## zstd_window_log_max {#zstd_window_log_max}

Type: Int64

Default value: 0

ZSTDの最大ウィンドウログを選択することを許可します（MergeTreeファミリーには使用されません）。
```
