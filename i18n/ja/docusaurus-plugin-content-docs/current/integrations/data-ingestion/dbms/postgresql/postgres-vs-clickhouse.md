---
slug: /integrations/postgresql/postgres-vs-clickhouse
title: PostgreSQLとClickHouseの比較
keywords: [postgres, postgresql, comparison]
---

## Postgres vs ClickHouse: 同等の概念と異なる概念 {#postgres-vs-clickhouse-equivalent-and-different-concepts}

OLTPシステムから来たユーザーで、ACIDトランザクションに慣れている人は、ClickHouseがパフォーマンスと引き換えにこれらを完全に提供しない意図的な妥協をしていることを理解しておくべきです。ClickHouseのセマンティクスは、良く理解されれば高い耐久性の保証と高い書き込みスループットを提供できます。以下に、PostgresからClickHouseで作業する前にユーザーが熟知しておくべきいくつかの重要な概念を示します。

### シャードとレプリカ {#shards-vs-replicas}

シャーディングとレプリケーションは、ストレージおよび/またはコンピュートがパフォーマンスのボトルネックになるときに、1つのPostgresインスタンスを超えてスケーリングするために使用される2つの戦略です。Postgresにおけるシャーディングは、大規模なデータベースを複数のノードにわたって小さく、より管理しやすい部分に分割することを含みます。しかし、Postgresはネイティブでシャーディングをサポートしていません。代わりに、[Citus](https://www.citusdata.com/)などの拡張機能を使用して、Postgresを水平スケーリング可能な分散データベースにすることができます。このアプローチにより、Postgresは複数のマシンに負荷を分散させることによって、より高いトランザクションレートと大規模なデータセットを処理できます。シャードは、トランザクション型や分析型などのワークロードタイプに柔軟性を提供するために、行ベースまたはスキーマベースで定義できます。シャーディングは、複数のマシンにわたる調整や整合性の保証が必要なため、データ管理とクエリ実行の観点から重大な複雑さをもたらすことがあります。

シャードとは異なり、レプリカはプライマリノードからのすべてまたは一部のデータを含む追加のPostgresインスタンスです。レプリカは、読み取りパフォーマンスの向上やHA（高可用性）シナリオなど、さまざまな理由で使用されます。物理レプリケーションはPostgresのネイティブ機能で、データベース全体または重要な部分を別のサーバーにコピーすることを含み、すべてのデータベース、テーブル、インデックスを含みます。これには、TCP/IPを介してプライマリノードからレプリカにWALセグメントをストリーミングすることが含まれます。対照的に、論理レプリケーションは、`INSERT`、`UPDATE`、および`DELETE`操作に基づいて変更をストリーミングする高レベルの抽象化です。物理レプリケーションと同じ結果が得られる場合もありますが、特定のテーブルや操作をターゲットにしたり、データ変換をサポートしたり、異なるPostgresバージョンをサポートしたりするための柔軟性が向上します。

**これに対して、ClickHouseのシャードとレプリカは、データの分配と冗長性に関連する2つの重要な概念です**。ClickHouseのレプリカはPostgresのレプリカと類似的に考えることができますが、レプリケーションは最終的に一貫性があり、プライマリの概念はありません。シャーディングは、Postgresとは異なり、ネイティブでサポートされています。

シャードは、テーブルデータの一部です。常に少なくとも1つのシャードがあります。単一のサーバーの容量を超える場合、複数のサーバーにシャーディングデータを分散することで負荷を分散できます。すべてのシャードを使用してクエリを並列で実行することができます。ユーザーは異なるサーバーに対して手動でシャードを作成し、データを直接挿入できます。また、データがどのシャードにルーティングされるかを定義するシャーディングキーを持つ分散テーブルを使用することもできます。シャーディングキーはランダムであったり、ハッシュ関数の出力であったりすることがあります。重要なことに、シャードは複数のレプリカで構成されることがあります。

レプリカはデータのコピーです。ClickHouseには常にデータの最小コピーが1つ存在するため、レプリカの最小数は1です。データの2番目のレプリカを追加することで、フォールトトレランスが提供され、より多くのクエリ処理のために潜在的に追加のコンピュートが提供されます（[並列レプリカ](https://clickhouse.com/blog/clickhouse-release-23-03#parallel-replicas-for-utilizing-the-full-power-of-your-replicas-nikita-mikhailov)を使用して、単一のクエリの計算を分散させ、レイテンシを低下させることも可能です）。レプリケーションは、[ReplicatedMergeTreeテーブルエンジン](/engines/table-engines/mergetree-family/replication)によって実現され、ClickHouseは異なるサーバー間でデータの複数のコピーを同期させることができます。レプリケーションは物理的なものであり、ノード間でクエリではなく圧縮されたパーツのみが転送されます。

要約すると、レプリカは冗長性と信頼性（および潜在的な分散処理）を提供するデータのコピーであり、シャードは分散処理とロードバランシングを可能にするデータのサブセットです。

> ClickHouse Cloudは、S3にバックアップされたデータの単一コピーと複数のコンピュートレプリカを使用します。データは各レプリカノードで利用可能で、各ノードにはローカルSSDキャッシュがあります。これは、ClickHouse Keeperを通じてのみメタデータのレプリケーションに依存します。

## 最終的一貫性 {#eventual-consistency}

ClickHouseは、内部レプリケーションメカニズムの管理にClickHouse Keeper（C++ ZooKeeperの実装、ZooKeeperも使用可能）を使用し、主にメタデータストレージと最終的一貫性の確保に重点を置いています。Keeperは、分散環境内で各挿入のためにユニークな順序番号を割り当てるために使用されます。これは、操作間の順序と整合性を維持するために重要です。このフレームワークは、マージや変更などのバックグラウンド操作も処理し、これらの作業が分散されることを保証し、すべてのレプリカ間で同じ順序で実行されることを保証します。メタデータに加えて、Keeperは保存されたデータパーツのチェックサムを追跡し、レプリカ間で分散通知システムとして機能する包括的な制御センターとしても機能します。

ClickHouseのレプリケーションプロセスは、(1) データが任意のレプリカに挿入されると開始されます。このデータは、生の挿入形式のままで(2) ディスクに書き込まれ、チェックサムとともに保存されます。書き込みが完了したら、レプリカは(3) Keeperに新しいデータパーツを登録し、ユニークなブロック番号を割り当て、新しいパーツの詳細をログに記録しようとします。他のレプリカは、(4) レプリケーションログに新しいエントリを検出すると、(5) 内部HTTPプロトコルを介して対応するデータパーツをダウンロードし、ZooKeeperにリストされているチェックサムと照合します。この方法により、すべてのレプリカが、処理速度や遅延が異なっても、最終的に整合性があり、最新のデータを保持することが保証されます。さらに、システムは複数の操作を同時に処理することができ、データ管理プロセスを最適化し、システムのスケーラビリティとハードウェアの不整合に対する堅牢性を確保します。

<br />

<img src={require('../images/postgres-replicas.png').default}    
  class="image"
  alt="NEEDS ALT"
  style={{width: '500px'}} />

<br />

ClickHouse Cloudが、ストレージとコンピュートアーキテクチャの分離に適応した[クラウド最適化レプリケーションメカニズム](https://clickhouse.com/blog/clickhouse-cloud-boosts-performance-with-sharedmergetree-and-lightweight-updates)を使用している点に注意してください。データを共有オブジェクトストレージに保存することにより、データはノード間で物理的に複製する必要なく、すべてのコンピュートノードで自動的に利用可能になります。その代わりに、Keeperはメタデータ（共有オブジェクトストレージ内にどのデータが存在するか）をコンピュートノード間で共有するためにのみ使用されます。

PostgreSQLは、ClickHouseとは異なるレプリケーション戦略を採用しており、主にストリーミングレプリケーションを使用します。これは、データがプライマリから1つまたは複数のレプリカノードに継続的にストリーミングされるプライマリレプリカモデルを含みます。このタイプのレプリケーションにより、ほぼリアルタイムの一貫性が保証され、同期または非同期で行われます。これにより、管理者は可用性と一貫性のバランスを制御できます。ClickHouseとは異なり、PostgreSQLはWAL（Write-Ahead Logging）を論理レプリケーションおよびデコーディングとともに使用し、データオブジェクトや変更をノード間でストリームします。このアプローチはPostgreSQLではより簡潔ですが、ClickHouseがその複雑な使用を通じて達成している高度な分散環境における同じスケーラビリティとフォールトトレランスを提供できない可能性があります。

## ユーザーへの影響 {#user-implications}

ClickHouseでは、ユーザーが1つのレプリカにデータを書き込んでから、別のレプリカから潜在的に未複製のデータを読み取ることができる「ダーティリード」の可能性が生じます。これは、Keeperによって管理される最終的一貫性のあるレプリケーションモデルによるものです。このモデルは、分散システム全体でパフォーマンスとスケーラビリティを強調し、レプリカが独立して動作し、非同期で同期することを可能にします。その結果、新しく挿入されたデータは、レプリケーション遅延や変更がシステム内を伝播するのにかかる時間に応じて、すべてのレプリカに即座には表示されない場合があります。

これに対して、PostgreSQLのストリーミングレプリケーションモデルは、プライマリがレプリカの1つ以上からデータの受信を確認するのを待ってからトランザクションをコミットする同期レプリケーションオプションを採用することで通常はダーティリードを防ぎます。これにより、トランザクションがコミットされると、そのデータが別のレプリカに存在することが保証されます。プライマリの障害が発生した場合、レプリカはクエリがコミットされたデータを表示できるようにし、より厳密な一貫性レベルを維持します。

## 推奨事項 {#recommendations}

ClickHouseに初めて触れるユーザーは、これらの違いを認識しておくべきであり、これはレプリケートされた環境で明確に現れます。通常、最終的一貫性は、数十億、場合によっては数兆のデータポイントに対する分析には十分であり—メトリクスがより安定しているか、見積もりが新しいデータが高率で連続して挿入されるために十分です。

読み取りの一貫性を高めるためのいくつかのオプションがありますが、いずれもクエリパフォーマンスを低下させ、ClickHouseのスケーリングをさらに難しくする可能性があるため、**絶対に必要な場合にのみこれらのアプローチを推奨します。**

## 一貫したルーティング {#consistent-routing}

最終的一貫性のいくつかの制限を克服するために、ユーザーはクライアントが同じレプリカにルーティングされることを保証できます。これは、複数のユーザーがClickHouseをクエリしていて、結果がリクエスト間で決定論的である必要がある場合に便利です。新しいデータが挿入されると結果は異なるかもしれませんが、同じレプリカをクエリすることで一貫したビューが保証されます。

これは、アーキテクチャやClickHouse OSSまたはClickHouse Cloudを使用しているかどうかに応じていくつかのアプローチを通じて達成できます。

## ClickHouse Cloud {#clickhouse-cloud}

ClickHouse Cloudは、S3にバックアップされたデータの単一コピーと複数のコンピュートレプリカを使用します。データは各レプリカノードで利用可能で、各ノードにはローカルSSDキャッシュがあります。このため、結果を一貫して得るためには、ユーザーは同じノードへの一貫したルーティングを保証する必要があります。

ClickHouse Cloudサービスのノードとの通信は、プロキシを介して行われます。HTTPおよびネイティブプロトコル接続は、接続がオープンな期間中に同じノードにルーティングされます。ほとんどのクライアントからのHTTP 1.1接続の場合、これはKeep-Aliveウィンドウに依存します。これは、Node Jsなどのほとんどのクライアントで構成できます。また、これはサーバー側の設定を必要とし、クライアントよりも高く、ClickHouse Cloudでは10秒に設定されています。

接続間で一貫したルーティングを保証するために、例えば接続プールを使用している場合や接続が期限切れになる場合、ユーザーは同じ接続が使用されることを確認する（ネイティブの場合は簡単）か、スティッキーエンドポイントの公開をリクエストすることができます。これにより、クラスタ内の各ノードのためのエンドポイントのセットが提供され、クライアントがクエリを決定論的にルーティングできるようになります。

> スティッキーエンドポイントへのアクセスについてはサポートにお問い合わせください。

## ClickHouse OSS {#clickhouse-oss}

OSSでこの動作を実現するには、シャードとレプリカのトポロジーと、クエリに[分散テーブル](/engines/table-engines/special/distributed)を使用しているかどうかに依存します。

シャードとレプリカが1つだけある場合（ClickHouseが垂直方向にスケーリングされることが一般的なので）、ユーザーはクライアントレイヤーでノードを選び、レプリカを直接クエリして、決定論的に選択されるようにします。

分散テーブルなしで複数のシャードとレプリカのトポロジーは可能ですが、これらの高度なデプロイメントでは通常は独自のルーティングインフラを持っています。そのため、ユーザーは1つ以上のシャードを持つデプロイメントは分散テーブルを使用していると仮定します（分散テーブルは単一のシャードデプロイメントでも使用可能ですが、通常は不要です）。

この場合、ユーザーは、`session_id`や`user_id`などのプロパティに基づいて一貫したノードルーティングが実行されることを保証すべきです。設定は[`prefer_localhost_replica=0`](/operations/settings/settings#prefer-localhost-replica)、[`load_balancing=in_order`](/operations/settings/settings#load_balancing)を[クエリで設定する](/operations/settings/query-level)必要があります。これにより、シャードのローカルレプリカが優先され、他のレプリカは設定されている順に優先されます—エラーの数が同じであれば、より多くのエラーがあった場合はランダム選択でフェイルオーバーが発生します。[`load_balancing=nearest_hostname`](/operations/settings/settings#load_balancing)を、決定論的なシャード選択の代替手段として使用することもできます。

> 分散テーブルを作成する際、ユーザーはクラスタを指定します。このクラスタ定義はconfig.xmlで指定され、シャード（およびそのレプリカ）をリスト化します—これにより、各ノードから使用される順序を制御できます。これを使用することで、ユーザーは選択を決定論的に行うことができます。

## 逐次的な一貫性 {#sequential-consistency}

例外的な場合、ユーザーは逐次的な一貫性を必要とすることがあります。

データベースにおける逐次的な一貫性とは、データベース上の操作がある逐次的な順序で実行されているように見え、その順序がデータベースとやり取りするすべてのプロセス間で一貫していることを意味します。つまり、すべての操作は、その呼び出しと完了の間に瞬時に効果を発揮するように見え、すべての操作が任意のプロセスによって観測される単一の合意された順序があります。

ユーザーの視点から見ると、これは通常、ClickHouseにデータを書き込み、データを読み取る際に、最新の挿入された行が返されることを保証する必要があるという形で現れます。
これは、優先順位を付けた方法で以下のように達成できます：

1. **同じノードに読み書きする** - ネイティブプロトコルを使用する場合や、HTTPを介して書き込み/読み取りを行う[セッションを利用する](/interfaces/http#default-database)場合、同じレプリカに接続されるべきです：このシナリオでは、読み取りは書き込まれたノードから直接行いますので、読み取りは常に一貫しています。
2. **レプリカを手動で同期する** - 1つのレプリカに書き込み、別のレプリカから読み取る場合、読み取る前に`SYSTEM SYNC REPLICA LIGHTWEIGHT`を発行することができます。
3. **逐次的な一貫性の有効化** - クエリ設定[`select_sequential_consistency = 1`](/operations/settings/settings#select_sequential_consistency)を通じて。有効なOSSでは、`insert_quorum = 'auto'`も指定する必要があります。

<br />

これらの設定を有効にする詳細については[ここ](/cloud/reference/shared-merge-tree#consistency)を参照してください。

> 逐次的な一貫性の使用はClickHouse Keeperに負荷をかけるため、その結果、挿入や読み取りが遅くなる可能性があります。ClickHouse Cloudで主要なテーブルエンジンとして使用されるSharedMergeTreeは、逐次的な一貫性の[オーバーヘッドが少なく、よりスケールしやすいです](/cloud/reference/shared-merge-tree#consistency)。OSSユーザーはこのアプローチを慎重に使用し、Keeper負荷を測定する必要があります。

## トランザクション（ACID）サポート {#transactional-acid-support}

PostgreSQLから移行するユーザーは、ACID（原子性、一貫性、独立性、耐久性）の特性に対するその強力なサポートに慣れているかもしれません。これにより、信頼性の高いトランザクションデータベースとして選ばれます。PostgreSQLの原子性は、各トランザクションが単一のユニットとして扱われることを保証し、完全に成功するか完全にロールバックされることを防ぎ、部分的な更新を防ぎます。一貫性は、すべてのデータベーストランザクションが有効な状態に導くことを保証する制約、トリガー、およびルールを強制することによって維持されます。PostgreSQLは、Read CommittedからSerializableまでのアイソレーションレベルをサポートしており、同時トランザクションによって行われた変更の可視性を細かく制御できます。最後に、耐久性は書き込み前ログ（WAL）を通じて達成され、トランザクションがコミットされると、システム障害が発生した場合でもそのまま保持されます。

これらの特性は、真実の源となるOLTPデータベースに共通しています。

強力ではあるものの、これは固有の制限があり、PBスケールの課題をもたらします。ClickHouseは、高速な分析クエリを大規模に提供し、同時に高い書き込みスループットを維持するために、これらの特性に妥協します。

ClickHouseは、[限られた構成](https://guides.developer.transactional)の下でACID特性を提供します—最もシンプルなケースは、パーティションが1つのMergeTreeテーブルエンジンの非レプリケートインスタンスを使用するときです。ユーザーはこれらの特性を他のケースでは期待せず、要件でないことを確認するべきです。

## ClickPipes（PeerDBによる）を使用してPostgresデータをレプリケートまたは移行する {#replicating-or-migrating-postgres-data-with-clickpipes-powered-by-peerdb}

:::info
PeerDBは現在、ClickHouse Cloudにネイティブで利用可能です—[新しいClickPipeコネクタ](/integrations/clickpipes/postgres)を使用してBlazing-fastなPostgresからClickHouseへのCDCが可能—現在パブリックベータ版です。
:::

[PeerDB](https://www.peerdb.io/)を使用すると、PostgresからClickHouseにデータをシームレスにレプリケートできます。このツールを次の目的で使用できます。
1. CDCを使用した継続的レプリケーションにより、PostgresとClickHouseが共存できるようにします—PostgresはOLTP、ClickHouseはOLAPです。
2. PostgresからClickHouseへの移行。
