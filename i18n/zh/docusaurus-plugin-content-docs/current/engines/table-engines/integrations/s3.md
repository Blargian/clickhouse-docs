---
'description': '该引擎与Amazon S3生态系统集成。类似于HDFS引擎，但提供了特定于S3的功能。'
'sidebar_label': 'S3'
'sidebar_position': 180
'slug': '/engines/table-engines/integrations/s3'
'title': 'S3 表引擎'
---




# S3 表引擎

该引擎提供与 [Amazon S3](https://aws.amazon.com/s3/) 生态系统的集成。该引擎与 [HDFS](/engines/table-engines/integrations/hdfs) 引擎类似，但提供特定于 S3 的功能。

## 示例 {#example}

```sql
CREATE TABLE s3_engine_table (name String, value UInt32)
    ENGINE=S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/test-data.csv.gz', 'CSV', 'gzip')
    SETTINGS input_format_with_names_use_header = 0;

INSERT INTO s3_engine_table VALUES ('one', 1), ('two', 2), ('three', 3);

SELECT * FROM s3_engine_table LIMIT 2;
```

```text
┌─name─┬─value─┐
│ one  │     1 │
│ two  │     2 │
└──────┴───────┘
```

## 创建表 {#creating-a-table}

```sql
CREATE TABLE s3_engine_table (name String, value UInt32)
    ENGINE = S3(path [, NOSIGN | aws_access_key_id, aws_secret_access_key,] format, [compression])
    [PARTITION BY expr]
    [SETTINGS ...]
```

### 引擎参数 {#parameters}

- `path` — 存储桶 URL 和文件路径。支持在只读模式下使用以下通配符：`*`，`**`，`?`，`{abc,def}` 和 `{N..M}`，其中 `N` 和 `M` 为数字，`'abc'` 和 `'def'` 为字符串。有关更多信息，请参见 [下面](#wildcards-in-path)。
- `NOSIGN` - 如果在凭据的位置提供此关键字，则所有请求将不被签名。
- `format` — 文件的 [格式](/sql-reference/formats#formats-overview)。
- `aws_access_key_id`，`aws_secret_access_key` - AWS [AWS](https://aws.amazon.com/) 账户用户的长期凭据。您可以使用这些凭据来验证您的请求。此参数是可选的。如果未指定凭据，则将从配置文件中使用。有关更多信息，请参见 [使用 S3 进行数据存储](../mergetree-family/mergetree.md#table_engine-mergetree-s3)。
- `compression` — 压缩类型。支持的值：`none`，`gzip/gz`，`brotli/br`，`xz/LZMA`，`zstd/zst`。此参数是可选的。默认情况下，它将根据文件扩展名自动检测压缩。

### 数据缓存 {#data-cache}

`S3` 表引擎支持在本地磁盘上缓存数据。有关文件系统缓存配置选项和使用方法，请参见本 [部分](/operations/storing-data.md/#using-local-cache)。缓存是根据存储对象的路径和 ETag 进行的，因此 clickhouse 将不会读取过期的缓存版本。

要启用缓存，请使用设置 `filesystem_cache_name = '<name>'` 和 `enable_filesystem_cache = 1`。

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_3.csv', 'minioadmin', 'minioadminpassword', 'CSV')
SETTINGS filesystem_cache_name = 'cache_for_s3', enable_filesystem_cache = 1;
```

在配置文件中定义缓存有两种方法。

1. 将以下部分添加到 ClickHouse 配置文件中：

```xml
<clickhouse>
    <filesystem_caches>
        <cache_for_s3>
            <path>path to cache directory</path>
            <max_size>10Gi</max_size>
        </cache_for_s3>
    </filesystem_caches>
</clickhouse>
```

2. 从 ClickHouse 的 `storage_configuration` 部分重用缓存配置（因此缓存存储），[在此处描述](/operations/storing-data.md/#using-local-cache)

### 按分区 {#partition-by}

`PARTITION BY` — 可选。在大多数情况下，您不需要分区键，如果需要，通常不需要比按月更细的分区键。分区并不能加速查询（与 ORDER BY 表达式形成对比）。您永远不应使用过细的分区。不应按客户端标识符或名称对数据进行分区（反之，应使客户端标识符或名称成为 ORDER BY 表达式中的第一列）。

要按月分区，请使用 `toYYYYMM(date_column)` 表达式，其中 `date_column` 为一个类型为 [Date](/sql-reference/data-types/date.md) 的列。这里的分区名称具有 `"YYYYMM"` 格式。

### 查询分区数据 {#querying-partitioned-data}

该示例使用 [docker compose 配方](https://github.com/ClickHouse/examples/tree/5fdc6ff72f4e5137e23ea075c88d3f44b0202490/docker-compose-recipes/recipes/ch-and-minio-S3)，集成了 ClickHouse 和 MinIO。您应该能够通过替换端点和身份验证值来重现相同的查询。

请注意，`ENGINE` 配置中的 S3 端点使用参数 token `{_partition_id}` 作为 S3 对象（文件名）的一部分，并且 SELECT 查询是对这些结果对象名称进行选择 (例如，`test_3.csv`)。

:::note
如示例所示，当前不支持直接从分区的 S3 表中查询，但可以通过使用 S3 表函数查询各个分区来完成。

在 S3 中写入分区数据的主要用例是使该数据能够转移到另一个 ClickHouse 系统（例如，从本地系统迁移到 ClickHouse Cloud）。由于 ClickHouse 数据集通常非常大，并且网络可靠性有时不完美，因此分批传输数据集是合理的，因此采用了分区写入。
:::

#### 创建表 {#create-the-table}
```sql
CREATE TABLE p
(
    `column1` UInt32,
    `column2` UInt32,
    `column3` UInt32
)
ENGINE = S3(
-- highlight-next-line
           'http://minio:10000/clickhouse//test_{_partition_id}.csv',
           'minioadmin',
           'minioadminpassword',
           'CSV')
PARTITION BY column3
```

#### 插入数据 {#insert-data}
```sql
insert into p values (1, 2, 3), (3, 2, 1), (78, 43, 45)
```

#### 从分区 3 选择 {#select-from-partition-3}

:::tip
该查询使用 s3 表函数
:::

```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_3.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```
```response
┌─c1─┬─c2─┬─c3─┐
│  1 │  2 │  3 │
└────┴────┴────┘
```

#### 从分区 1 选择 {#select-from-partition-1}
```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_1.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```
```response
┌─c1─┬─c2─┬─c3─┐
│  3 │  2 │  1 │
└────┴────┴────┘
```

#### 从分区 45 选择 {#select-from-partition-45}
```sql
SELECT *
FROM s3('http://minio:10000/clickhouse//test_45.csv', 'minioadmin', 'minioadminpassword', 'CSV')
```
```response
┌─c1─┬─c2─┬─c3─┐
│ 78 │ 43 │ 45 │
└────┴────┴────┘
```

#### 限制 {#limitation}

您可能自然会尝试 `Select * from p`，但正如上面所述，该查询将失败；请使用前面的查询。

```sql
SELECT * FROM p
```
```response
Received exception from server (version 23.4.1):
Code: 48. DB::Exception: Received from localhost:9000. DB::Exception: Reading from a partitioned S3 storage is not implemented yet. (NOT_IMPLEMENTED)
```

## 插入数据 {#inserting-data}

请注意，只能将行插入到新文件中。没有合并周期或文件拆分操作。一旦文件写入，后续插入将失败。为避免此情况，您可以使用 `s3_truncate_on_insert` 和 `s3_create_new_file_on_insert` 设置。有关更多细节，请参见 [此处](/integrations/s3#inserting-data)。

## 虚拟列 {#virtual-columns}

- `_path` — 文件的路径。类型：`LowCardinality(String)`。
- `_file` — 文件名。类型：`LowCardinality(String)`。
- `_size` — 文件大小（以字节为单位）。类型：`Nullable(UInt64)`。如果大小未知，值为 `NULL`。
- `_time` — 文件的最后修改时间。类型：`Nullable(DateTime)`。如果时间未知，值为 `NULL`。
- `_etag` — 文件的 ETag。类型：`LowCardinality(String)`。如果 etag 未知，值为 `NULL`。

有关虚拟列的更多信息，请参见 [此处](../../../engines/table-engines/index.md#table_engines-virtual_columns)。

## 实现细节 {#implementation-details}

- 读取和写入可以并行
- 不支持：
    - `ALTER` 和 `SELECT...SAMPLE` 操作。
    - 索引。
    - [零拷贝](../../../operations/storing-data.md#zero-copy) 复制可能是可行的，但不支持。

  :::note
零拷贝复制在生产环境中尚未准备好。
在 ClickHouse 版本 22.8 及更高版本中，默认情况下禁用零拷贝复制。此功能不建议在生产环境中使用。
  :::

## 路径中的通配符 {#wildcards-in-path}

`path` 参数可以使用类似 bash 的通配符指定多个文件。要被处理，文件必须存在并与整个路径模式匹配。文件的列出是在 `SELECT` 时确定的（而不是在 `CREATE` 时）。

- `*` — 替换任意数量的任意字符（但不包括 `/`），包括空字符串。
- `**` — 替换任意数量的任意字符（包括 `/`），包括空字符串。
- `?` — 替换任意单个字符。
- `{some_string,another_string,yet_another_one}` — 替换任意一个字符串 `'some_string'`, `'another_string'`, `'yet_another_one'`。
- `{N..M}` — 替换从 N 到 M 的任意数字，包括两个边界。N 和 M 可以有前导零，例如 `000..078`。

带有 `{}` 的构造与 [remote](../../../sql-reference/table-functions/remote.md) 表函数类似。

:::note
如果文件的列出包含有前导零的数字范围，则应分别为每个数字使用大括号构造，或者使用 `?`。
:::

**使用通配符的示例 1**

创建文件名为 `file-000.csv`，`file-001.csv`，...，`file-999.csv` 的表：

```sql
CREATE TABLE big_table (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/my_folder/file-{000..999}.csv', 'CSV');
```

**使用通配符的示例 2**

假设我们在 S3 上有多个 CSV 文件，URI 如下：

- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_1.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_2.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_3.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_1.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_2.csv'
- 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_3.csv'

有几种方法可以创建一个由所有六个文件组成的表：

1. 指定文件后缀的范围：

```sql
CREATE TABLE table_with_range (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/some_file_{1..3}', 'CSV');
```

2. 获取所有以 `some_file_` 为前缀的文件（这两个文件夹中不应有其他带有此前缀的额外文件）：

```sql
CREATE TABLE table_with_question_mark (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/some_file_?', 'CSV');
```

3. 获取两个文件夹中的所有文件（所有文件应满足查询中描述的格式和模式）：

```sql
CREATE TABLE table_with_asterisk (name String, value UInt32)
    ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/*', 'CSV');
```

## 存储设置 {#storage-settings}

- [s3_truncate_on_insert](/operations/settings/settings.md#s3_truncate_on_insert) - 允许在插入之前截断文件。默认情况下禁用。
- [s3_create_new_file_on_insert](/operations/settings/settings.md#s3_create_new_file_on_insert) - 允许在每次插入时创建新文件（如果格式附有后缀）。默认情况下禁用。
- [s3_skip_empty_files](/operations/settings/settings.md#s3_skip_empty_files) - 允许在读取时跳过空文件。默认情况下启用。

## 与 S3 相关的设置 {#settings}

以下设置可以在查询执行前设置或放入配置文件中。

- `s3_max_single_part_upload_size` — 使用单部分上传到 S3 的对象的最大大小。默认值是 `32Mb`。
- `s3_min_upload_part_size` — 在多部分上传到 [S3 Multipart upload](https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html) 时，上传部分的最小大小。默认值是 `16Mb`。
- `s3_max_redirects` — 允许的 S3 重定向跳数的最大值。默认值是 `10`。
- `s3_single_read_retries` — 单次读取的最大尝试次数。默认值是 `4`。
- `s3_max_put_rps` — 在限速之前每秒最大 PUT 请求数。默认值是 `0`（无限制）。
- `s3_max_put_burst` — 可以同时发出的请求的最大数量，达到每秒请求限制之前。默认为 `0`（值）等于 `s3_max_put_rps`。
- `s3_max_get_rps` — 在限速之前每秒最大 GET 请求数。默认值是 `0`（无限制）。
- `s3_max_get_burst` — 可以同时发出的请求的最大数量，达到每秒请求限制之前。默认为 `0`（值）等于 `s3_max_get_rps`。
- `s3_upload_part_size_multiply_factor` - 每次从单个写入 S3 上传 `s3_multiply_parts_count_threshold` 部分时，将 `s3_min_upload_part_size` 乘以该因子。默认值为 `2`。
- `s3_upload_part_size_multiply_parts_count_threshold` - 每当上传到 S3 的部分数达到此数字时，将 `s3_min_upload_part_size` 乘以 `s3_upload_part_size_multiply_factor`。默认值为 `500`。
- `s3_max_inflight_parts_for_one_file` - 限制可以同时为一个对象运行的 PUT 请求数量。该数量应受限。值为 `0` 表示无限制。默认值为 `20`。每个在途中部分的缓冲区大小为 `s3_min_upload_part_size`，供前 `s3_upload_part_size_multiply_factor` 个部分使用，并且当文件足够大时更多，请参见 `upload_part_size_multiply_factor`。使用默认设置时，上传的文件的消耗不超过 `320Mb`（对于小于 `8G` 的文件）。较大的文件将消耗更多。

安全考虑：如果恶意用户可以指定任意 S3 URL，则必须将 `s3_max_redirects` 设置为零，以避免 [SSRF](https://en.wikipedia.org/wiki/Server-side_request_forgery) 攻击；或者，必须在服务器配置中指定 `remote_host_filter`。

## 基于端点的设置 {#endpoint-settings}

以下设置可以在配置文件中为给定端点（将通过 URL 的精确前缀进行匹配）进行指定：

- `endpoint` — 指定端点的前缀。强制。
- `access_key_id` 和 `secret_access_key` — 指定与给定端点使用的凭据。可选。
- `use_environment_credentials` — 如果设置为 `true`，S3 客户端将尝试从环境变量和 [Amazon EC2](https://en.wikipedia.org/wiki/Amazon_Elastic_Compute_Cloud) 元数据获取凭据。可选，默认值为 `false`。
- `region` — 指定 S3 区域名称。可选。
- `use_insecure_imds_request` — 如果设置为 `true`，S3 客户端将在从 Amazon EC2 元数据获取凭据时使用不安全的 IMDS 请求。可选，默认值为 `false`。
- `expiration_window_seconds` — 检查基于过期的凭据是否已过期的宽限期。可选，默认值为 `120`。
- `no_sign_request` - 忽略所有凭据，因此请求未被签名。对于访问公共存储桶很有用。
- `header` — 向请求中添加指定的 HTTP 头至给定端点。可选，可以多次指定。
- `access_header` - 向请求中添加指定的 HTTP 头至给定端点，以防没有来自其他来源的其他凭据。
- `server_side_encryption_customer_key_base64` — 如果指定，将设置访问带有 SSE-C 加密的 S3 对象所需的头。可选。
- `server_side_encryption_kms_key_id` - 如果指定，将设置访问带有 [SSE-KMS 加密](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html) 的 S3 对象所需的头。如果指定空字符串，将使用 AWS 管理的 S3 密钥。可选。
- `server_side_encryption_kms_encryption_context` - 如果与 `server_side_encryption_kms_key_id` 一起指定，将为 SSE-KMS 设置给定的加密上下文头。可选。
- `server_side_encryption_kms_bucket_key_enabled` - 如果与 `server_side_encryption_kms_key_id` 一起指定，将设置启用 S3 存储桶密钥的头。可选，可以是 `true` 或 `false`，默认值为无（与存储桶级设置匹配）。
- `max_single_read_retries` — 单次读取的最大尝试次数。默认值为 `4`。可选。
- `max_put_rps`，`max_put_burst`，`max_get_rps` 和 `max_get_burst` - 限速设置（有关描述，请参见上述内容）用于特定端点，而不是每个查询。可选。

**示例：**

```xml
<s3>
    <endpoint-name>
        <endpoint>https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/</endpoint>
        <!-- <access_key_id>ACCESS_KEY_ID</access_key_id> -->
        <!-- <secret_access_key>SECRET_ACCESS_KEY</secret_access_key> -->
        <!-- <region>us-west-1</region> -->
        <!-- <use_environment_credentials>false</use_environment_credentials> -->
        <!-- <use_insecure_imds_request>false</use_insecure_imds_request> -->
        <!-- <expiration_window_seconds>120</expiration_window_seconds> -->
        <!-- <no_sign_request>false</no_sign_request> -->
        <!-- <header>Authorization: Bearer SOME-TOKEN</header> -->
        <!-- <server_side_encryption_customer_key_base64>BASE64-ENCODED-KEY</server_side_encryption_customer_key_base64> -->
        <!-- <server_side_encryption_kms_key_id>KMS_KEY_ID</server_side_encryption_kms_key_id> -->
        <!-- <server_side_encryption_kms_encryption_context>KMS_ENCRYPTION_CONTEXT</server_side_encryption_kms_encryption_context> -->
        <!-- <server_side_encryption_kms_bucket_key_enabled>true</server_side_encryption_kms_bucket_key_enabled> -->
        <!-- <max_single_read_retries>4</max_single_read_retries> -->
    </endpoint-name>
</s3>
```

## 使用归档 {#working-with-archives}

假设我们在 S3 上有几个归档文件，URI 如下：

- 'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-10.csv.zip'
- 'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-11.csv.zip'
- 'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-12.csv.zip'

从这些归档中提取数据可以使用 ::。通配符可以在 URL 部分和 :: 之后的部分（负责归档内文件的名称）中使用。

```sql
SELECT *
FROM s3(
   'https://s3-us-west-1.amazonaws.com/umbrella-static/top-1m-2018-01-1{0..2}.csv.zip :: *.csv'
);
```

:::note
ClickHouse 支持三种归档格式：
ZIP
TAR
7Z
虽然 ZIP 和 TAR 归档可以从任何支持的存储位置访问，但 7Z 归档只能从安装 ClickHouse 的本地文件系统读取。
:::

## 访问公共存储桶 {#accessing-public-buckets}

ClickHouse 尝试从多种不同类型的来源获取凭据。有时，当访问一些公共存储桶时，可能会导致客户端返回 `403` 错误代码。可以通过使用 `NOSIGN` 关键字来避免此问题，强制客户端忽略所有凭据，不签名请求。

```sql
CREATE TABLE big_table (name String, value UInt32)
    ENGINE = S3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/aapl_stock.csv', NOSIGN, 'CSVWithNames');
```

## 优化性能 {#optimizing-performance}

有关优化 s3 功能性能的详细信息，请参见 [我们的详细指南](/integrations/s3/performance)。

## 另请参阅 {#see-also}

- [s3 表函数](../../../sql-reference/table-functions/s3.md)
- [将 S3 与 ClickHouse 集成](/integrations/s3)
