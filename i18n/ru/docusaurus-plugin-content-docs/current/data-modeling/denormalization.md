---
slug: /data-modeling/denormalization
title: 'Денормализация данных'
description: 'Как использовать денормализацию для улучшения производительности запросов'
keywords: ['денормализация данных', 'денормализовать', 'оптимизация запросов']
---

import denormalizationDiagram from '@site/static/images/data-modeling/denormalization-diagram.png';
import denormalizationSchema from '@site/static/images/data-modeling/denormalization-schema.png';
import Image from '@theme/IdealImage';


# Денормализация данных

Денормализация данных — это техника в ClickHouse, позволяющая использовать сплюснутые таблицы для минимизации задержек при выполнении запросов путем избегания соединений (joins).

## Сравнение нормализованных и денормализованных схем {#comparing-normalized-vs-denormalized-schemas}

Денормализация данных включает в себя преднамеренное обращение процесса нормализации с целью оптимизации производительности базы данных для определенных паттернов запросов. В нормализованных базах данных данные разбиваются на несколько связанных таблиц, чтобы минимизировать избыточность и обеспечить целостность данных. Денормализация вновь вводит избыточность, комбинируя таблицы, дублируя данные и включая вычисляемые поля в одну таблицу или в меньшее количество таблиц, эффективно перемещая любые соединения с времени запроса на время вставки.

Этот процесс снижает потребность в сложных соединениях во время выполнения запроса и может значительно ускорить операции чтения, что делает его идеальным для приложений с тяжелыми требованиями к чтению и сложными запросами. Однако это может увеличить сложность операций записи и обслуживания, поскольку любые изменения в дублированных данных должны быть распространены по всем инстанциям для поддержания согласованности.

<Image img={denormalizationDiagram} size="lg" alt="Денормализация в ClickHouse"/>

<br />

Распространенной техникой, популяризированной решениями NoSQL, является денормализация данных в отсутствие поддержки `JOIN`, эффективно сохраняя все статистики или связанные строки на родительской строке в виде колонок и вложенных объектов. Например, в схеме блога мы можем хранить все `Комментарии` в виде `Array` объектов на их соответствующих постах.

## Когда использовать денормализацию {#when-to-use-denormalization}

В общем, мы рекомендуем денормализовать в следующих случаях:

- Денормализовать таблицы, которые изменяются редко или для которых можно допустить задержку перед тем, как данные будут доступны для аналитических запросов, т.е. данные могут быть полностью перезагружены партиями.
- Избегать денормализации связей многие-ко-многим. Это может потребовать обновления множества строк в случае изменения одной исходной строки.
- Избегать денормализации связей с высокой кардинальностью. Если каждая строка в таблице имеет тысячи связанных записей в другой таблице, эти записи должны быть представлены как `Array` - либо примитивного типа, либо кортежи. Обычно не рекомендуется использовать массивы с более чем 1000 кортежами.
- Вместо денормализации всех колонок как вложенных объектов, рассмотрите возможность денормализации только одной статистики с помощью материализованных представлений (см. ниже).

Не все данные должны быть денормализованы - только ключевая информация, к которой необходимо часто обращаться.

Работа по денормализации может быть выполнена как в ClickHouse, так и upstream, например, с использованием Apache Flink.

## Избегайте денормализации часто обновляемых данных {#avoid-denormalization-on-frequently-updated-data}

Для ClickHouse денормализация является одним из нескольких вариантов, которые пользователи могут использовать для оптимизации производительности запросов, но ее следует использовать с осторожностью. Если данные часто обновляются и их нужно обновлять в почти реальном времени, следует избегать этого подхода. Используйте это, если основная таблица в основном только добавляется или может периодически перезагружаться партиями, например, ежедневно.

Как подход он сталкивается с одной основной проблемой - производительность записи и обновления данных. Более конкретно, денормализация фактически переносит ответственность за соединение данных с времени запроса на время приема данных. Хотя это может значительно улучшить производительность запросов, это усложняет прием данных и означает, что конвейеры данных должны вновь вставлять строку в ClickHouse, если изменяется любая из строк, использованных для ее составления. Это может означать, что изменение одной исходной строки потенциально требует обновления многих строк в ClickHouse. В сложных схемах, где строки были составлены из сложных соединений, изменение одной строки в вложенном компоненте соединения потенциально может означать, что необходимо обновить миллионы строк.

Достигнуть этого в реальном времени часто нереалистично и требует значительных усилий в инженерии из-за двух проблем:

1. Инициация правильных операторов соединения, когда изменяется строка таблицы. Это не должно вызывать обновление всех объектов для соединения - только тех, которые были затронуты. Модификация соединений, чтобы фильтровать на правильные строки эффективно, и достижение этого при высокой пропускной способности требует внешних инструментов или инженерии.
2. Обновление строк в ClickHouse должно быть тщательно управляемым, что добавляет дополнительную сложность.

<br />

Процесс пакетного обновления более распространен, когда все денормализованные объекты периодически перезагружаются.

## Практические случаи для денормализации {#practical-cases-for-denormalization}

Рассмотрим несколько практических примеров, где денормализация может иметь смысл, и других, где альтернативные подходы более желательны.

Рассмотрим таблицу `Posts`, которая уже была денормализована со статистиками, такими как `AnswerCount` и `CommentCount` - исходные данные предоставляются в этой форме. На самом деле мы можем захотеть нормализовать эту информацию, так как она вероятно подлежит частым изменениям. Многие из этих колонок также доступны через другие таблицы, например, комментарии для поста доступны через колонку `PostId` и таблицу `Comments`. Для целей примера мы предполагаем, что посты перезагружаются в пакетном процессе.

Мы также рассматриваем только денормализацию других таблиц к `Posts`, поскольку считаем это нашей основной таблицей для аналитики. Денормализация в другую сторону также была бы уместна для некоторых запросов, при этом те же самые вышеуказанные соображения применимы.

*Для каждого из следующих примеров предположим, что существует запрос, который требует использования обеих таблиц в соединении.*

### Посты и Голоса {#posts-and-votes}

Голоса за посты представлены как отдельные таблицы. Оптимизированная схема для этого показана ниже, а также команда вставки для загрузки данных:

```sql
CREATE TABLE votes
(
        `Id` UInt32,
        `PostId` Int32,
        `VoteTypeId` UInt8,
        `CreationDate` DateTime64(3, 'UTC'),
        `UserId` Int32,
        `BountyAmount` UInt8
)
ENGINE = MergeTree
ORDER BY (VoteTypeId, CreationDate, PostId)

INSERT INTO votes SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/stackoverflow/parquet/votes/*.parquet')

0 rows in set. Elapsed: 26.272 sec. Processed 238.98 million rows, 2.13 GB (9.10 million rows/s., 80.97 MB/s.)
```

На первый взгляд, это могут быть кандидаты для денормализации в таблицу постов. Однако здесь есть несколько проблем с этим подходом.

Голоса часто добавляются к постам. Хотя это может сократиться по времени на пост, следующий запрос показывает, что у нас имеется около 40k голосов в час на 30k постов.

```sql
SELECT round(avg(c)) AS avg_votes_per_hr, round(avg(posts)) AS avg_posts_per_hr
FROM
(
        SELECT
        toStartOfHour(CreationDate) AS hr,
        count() AS c,
        uniq(PostId) AS posts
        FROM votes
        GROUP BY hr
)

┌─avg_votes_per_hr─┬─avg_posts_per_hr─┐
│               41759 │         33322 │
└──────────────────┴──────────────────┘
```

Это может быть решено путем пакетной обработки, если задержка может быть допущена, но все равно требует обновления данных, если мы периодически не перезагружаем все посты (что маловероятно).

Более проблемно то, что некоторые посты имеют крайне большое количество голосов:

```sql
SELECT PostId, concat('https://stackoverflow.com/questions/', PostId) AS url, count() AS c
FROM votes
GROUP BY PostId
ORDER BY c DESC
LIMIT 5

┌───PostId─┬─url──────────────────────────────────────────┬─────c─┐
│ 11227902 │ https://stackoverflow.com/questions/11227902 │ 35123 │
│   927386 │ https://stackoverflow.com/questions/927386   │ 29090 │
│ 11227809 │ https://stackoverflow.com/questions/11227809 │ 27475 │
│   927358 │ https://stackoverflow.com/questions/927358   │ 26409 │
│  2003515 │ https://stackoverflow.com/questions/2003515  │ 25899 │
└──────────┴──────────────────────────────────────────────┴───────┘
```

Основное наблюдение заключается в том, что агрегированные статистики голосования для каждого поста будут достаточными для большинства анализа - нам не нужно денормализовать всю информацию о голосах. Например, текущая колонка `Score` представляет собой такую статистику, т.е. общее количество голосов "за" минус "против". В идеале мы могли бы просто извлекать эту статистику во время выполнения запроса с помощью простого поиска (см. [словарей](/dictionary)).

### Пользователи и Значки {#users-and-badges}

Теперь давайте рассмотрим наших `Users` и `Badges`:

<Image img={denormalizationSchema} size="lg" alt="Схема Пользователи и Значки"/>

<p></p>
Сначала мы вставляем данные с помощью следующей команды:
<p></p>

```sql
CREATE TABLE users
(
    `Id` Int32,
    `Reputation` LowCardinality(String),
    `CreationDate` DateTime64(3, 'UTC') CODEC(Delta(8), ZSTD(1)),
    `DisplayName` String,
    `LastAccessDate` DateTime64(3, 'UTC'),
    `AboutMe` String,
    `Views` UInt32,
    `UpVotes` UInt32,
    `DownVotes` UInt32,
    `WebsiteUrl` String,
    `Location` LowCardinality(String),
    `AccountId` Int32
)
ENGINE = MergeTree
ORDER BY (Id, CreationDate)
```

```sql
CREATE TABLE badges
(
    `Id` UInt32,
    `UserId` Int32,
    `Name` LowCardinality(String),
    `Date` DateTime64(3, 'UTC'),
    `Class` Enum8('Gold' = 1, 'Silver' = 2, 'Bronze' = 3),
    `TagBased` Bool
)
ENGINE = MergeTree
ORDER BY UserId

INSERT INTO users SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/stackoverflow/parquet/users.parquet')

0 rows in set. Elapsed: 26.229 sec. Processed 22.48 million rows, 1.36 GB (857.21 thousand rows/s., 51.99 MB/s.)

INSERT INTO badges SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/stackoverflow/parquet/badges.parquet')

0 rows in set. Elapsed: 18.126 sec. Processed 51.29 million rows, 797.05 MB (2.83 миллиона строк/с., 43.97 MB/с.)
```

Хотя пользователи могут часто получать значки, это маловероятно, что это набор данных, который нужно обновлять более одного раза в день. Связь между значками и пользователями является "один-ко-многим". Возможно, мы можем просто денормализовать значки на пользователей в виде списка кортежей? Хотя это возможно, быстрая проверка, чтобы подтвердить максимальное количество значков на пользователя, указывает на то, что это не является идеальным решением:

```sql
SELECT UserId, count() AS c FROM badges GROUP BY UserId ORDER BY c DESC LIMIT 5

┌─UserId─┬─────c─┐
│  22656 │ 19334 │
│   6309 │ 10516 │
│ 100297 │  7848 │
│ 157882 │  7574 │
│  29407 │  6512 │
└────────┴───────┘
```

Вероятно, нереалистично денормализовать 19k объектов на одну строку. Эта связь, вероятно, лучше оставить как отдельные таблицы или добавить статистику.

> Мы можем пожелать денормализовать статистику по значкам на пользователей, например, количество значков. Мы рассматриваем такой пример при использовании словарей для этого набора данных во время вставки.

### Посты и PostLinks {#posts-and-postlinks}

`PostLinks` соединяют `Posts`, которые пользователи считают связанными или дублированными. Следующий запрос показывает схему и команду загрузки:

```sql
CREATE TABLE postlinks
(
  `Id` UInt64,
  `CreationDate` DateTime64(3, 'UTC'),
  `PostId` Int32,
  `RelatedPostId` Int32,
  `LinkTypeId` Enum('Linked' = 1, 'Duplicate' = 3)
)
ENGINE = MergeTree
ORDER BY (PostId, RelatedPostId)

INSERT INTO postlinks SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/stackoverflow/parquet/postlinks.parquet')

0 rows in set. Elapsed: 4.726 sec. Processed 6.55 million rows, 129.70 MB (1.39 million rows/s., 27.44 MB/s.)
```

Мы можем подтвердить, что ни один пост не имеет чрезмерного количества ссылок, что препятствует денормализации:

```sql
SELECT PostId, count() AS c
FROM postlinks
GROUP BY PostId
ORDER BY c DESC LIMIT 5

┌───PostId─┬───c─┐
│ 22937618 │ 125 │
│  9549780 │ 120 │
│  3737139 │ 109 │
│ 18050071 │ 103 │
│ 25889234 │  82 │
└──────────┴─────┘
```

Аналогично, эти ссылки не являются событиями, которые происходят слишком часто:

```sql
SELECT
  round(avg(c)) AS avg_votes_per_hr,
  round(avg(posts)) AS avg_posts_per_hr
FROM
(
  SELECT
  toStartOfHour(CreationDate) AS hr,
  count() AS c,
  uniq(PostId) AS posts
  FROM postlinks
  GROUP BY hr
)

┌─avg_votes_per_hr─┬─avg_posts_per_hr─┐
│                54 │                    44     │
└──────────────────┴──────────────────┘
```

Мы используем это в нашем примере денормализации ниже.

### Пример простой статистики {#simple-statistic-example}

В большинстве случаев денормализация требует добавления одной колонки или статистики к родительской строке. Например, мы можем просто захотеть обогатить наши посты количеством дублирующих постов, и нам нужно просто добавить колонку.

```sql
CREATE TABLE posts_with_duplicate_count
(
  `Id` Int32 CODEC(Delta(4), ZSTD(1)),
   ... -other columns
   `DuplicatePosts` UInt16
) ENGINE = MergeTree
ORDER BY (PostTypeId, toDate(CreationDate), CommentCount)
```

Чтобы заполнить эту таблицу, мы используем `INSERT INTO SELECT`, объединяя нашу статистику дубликатов с постами.

```sql
INSERT INTO posts_with_duplicate_count SELECT
    posts.*,
    DuplicatePosts
FROM posts AS posts
LEFT JOIN
(
    SELECT PostId, countIf(LinkTypeId = 'Duplicate') AS DuplicatePosts
    FROM postlinks
    GROUP BY PostId
) AS postlinks ON posts.Id = postlinks.PostId
```

### Использование комплексных типов для связей один-ко-многим {#exploiting-complex-types-for-one-to-many-relationships}

Для выполнения денормализации нам часто нужно использовать комплексные типы. Если денормализуется связь один-один с небольшим количеством колонок, пользователи могут просто добавить их как строки с их исходными типами, как показано выше. Однако это часто нежелательно для больших объектов и невозможно для отношений один-ко-многим.

В случаях сложных объектов или отношений один-ко-многим пользователи могут использовать:

- Именованные кортежи - Эти позволяют связанной структуре быть представленной как набор колонок.
- Array(Tuple) или Nested - Массив именованных кортежей, также известных как вложенные, где каждая запись представляет собой объект. Подходит для отношений один-ко-многим.

В качестве примера мы демонстрируем денормализацию `PostLinks` к `Posts` ниже.

Каждый пост может содержать несколько ссылок на другие посты, как показано в схеме `PostLinks` ранее. Как вложенный тип, мы можем представить эти связанные и дублирующие посты следующим образом:

```sql
SET flatten_nested=0
CREATE TABLE posts_with_links
(
  `Id` Int32 CODEC(Delta(4), ZSTD(1)),
   ... -other columns
   `LinkedPosts` Nested(CreationDate DateTime64(3, 'UTC'), PostId Int32),
   `DuplicatePosts` Nested(CreationDate DateTime64(3, 'UTC'), PostId Int32),
) ENGINE = MergeTree
ORDER BY (PostTypeId, toDate(CreationDate), CommentCount)
```

> Обратите внимание на настройку `flatten_nested=0`. Мы рекомендуем отключать упрощение вложенных данных.

Мы можем выполнить эту денормализацию с помощью `INSERT INTO SELECT` с запросом `OUTER JOIN`:

```sql
INSERT INTO posts_with_links
SELECT
    posts.*,
    arrayMap(p -> (p.1, p.2), arrayFilter(p -> p.3 = 'Linked' AND p.2 != 0, Related)) AS LinkedPosts,
    arrayMap(p -> (p.1, p.2), arrayFilter(p -> p.3 = 'Duplicate' AND p.2 != 0, Related)) AS DuplicatePosts
FROM posts
LEFT JOIN (
    SELECT
         PostId,
         groupArray((CreationDate, RelatedPostId, LinkTypeId)) AS Related
    FROM postlinks
    GROUP BY PostId
) AS postlinks ON posts.Id = postlinks.PostId

0 rows in set. Elapsed: 155.372 sec. Processed 66.37 million rows, 76.33 GB (427.18 thousand rows/s., 491.25 MB/s.)
Peak memory usage: 6.98 GiB.
```

> Обратите внимание на время выполнения. Мы смогли денормализовать 66m строк за около 2 минут. Как мы увидим позже, это операция, которую мы можем запланировать.

Обратите внимание на использование функции `groupArray`, чтобы свести `PostLinks` в массив для каждого `PostId` перед объединением. Этот массив затем фильтруется на два подсписка: `LinkedPosts` и `DuplicatePosts`, которые также исключают любые пустые результаты из внешнего соединения.

Мы можем выбрать некоторые строки, чтобы увидеть нашу новую денормализованную структуру:

```sql
SELECT LinkedPosts, DuplicatePosts
FROM posts_with_links
WHERE (length(LinkedPosts) > 2) AND (length(DuplicatePosts) > 0)
LIMIT 1
FORMAT Vertical

Row 1:
──────
LinkedPosts:    [('2017-04-11 11:53:09.583',3404508),('2017-04-11 11:49:07.680',3922739),('2017-04-11 11:48:33.353',33058004)]
DuplicatePosts: [('2017-04-11 12:18:37.260',3922739),('2017-04-11 12:18:37.260',33058004)]
```

## Оркестрация и планирование денормализации {#orchestrating-and-scheduling-denormalization}

### Пакет {#batch}

Использование денормализации требует процесса трансформации, в рамках которого она может быть выполнена и организована.

Мы показали ранее, как ClickHouse может использоваться для выполнения этой трансформации после того, как данные были загружены через `INSERT INTO SELECT`. Это подходит для периодических пакетных трансформаций.

Пользователи имеют несколько вариантов для оркестрации этого в ClickHouse, если периодический пакетный процесс загрузки является приемлемым:

- **[Обновляемые материализованные представления](/materialized-view/refreshable-materialized-view)** - Обновляемые материализованные представления могут использоваться для периодического планирования запроса с результатами, отправляемыми в целевую таблицу. При выполнении запроса представление гарантирует, что целевая таблица атомарно обновляется. Это предоставляет нативное средство ClickHouse для планирования этой работы.
- **Внешние инструменты** - Использование таких инструментов, как [dbt](https://www.getdbt.com/) и [Airflow](https://airflow.apache.org/), для периодического планирования преобразования. [Интеграция ClickHouse для dbt](/integrations/dbt) гарантирует, что это выполняется атомарно с новой версией целевой таблицы, созданной и затем атомарно замененной с версией, получающей запросы (через команду [EXCHANGE](/sql-reference/statements/exchange)).


### Потоковая обработка {#streaming}

Пользователи могут также захотеть выполнить это вне ClickHouse, до вставки, используя потоковые технологии, такие как [Apache Flink](https://flink.apache.org/). В качестве альтернативы можно использовать инкрементные [материализованные представления](/guides/developer/cascading-materialized-views) для выполнения этого процесса по мере вставки данных.
