---
'slug': '/academic_overview'
'title': '架构概览'
'description': '我们2024年VLDB论文的文档版本'
'keywords':
- 'architecture'
---

import useBrokenLinks from "@docusaurus/useBrokenLinks";
import image_01 from '@site/static/images/managing-data/core-concepts/_vldb2024_1_Figure_0.png'
import image_02 from '@site/static/images/managing-data/core-concepts/_vldb2024_2_Figure_0.png'
import image_03 from '@site/static/images/managing-data/core-concepts/_vldb2024_2_Figure_5.png'
import image_04 from '@site/static/images/managing-data/core-concepts/_vldb2024_3_Figure_7.png'
import image_05 from '@site/static/images/managing-data/core-concepts/_vldb2024_4_Figure_6.png'
import image_06 from '@site/static/images/managing-data/core-concepts/_vldb2024_5_Figure_8.png'
import image_07 from '@site/static/images/managing-data/core-concepts/_vldb2024_6_Figure_0.png'
import image_08 from '@site/static/images/managing-data/core-concepts/_vldb2024_7_Figure_1.png'
import image_09 from '@site/static/images/managing-data/core-concepts/_vldb2024_8_Figure_7.png'
import image_10 from '@site/static/images/managing-data/core-concepts/_vldb2024_10_Figure_14.png'
import image_11 from '@site/static/images/managing-data/core-concepts/_vldb2024_10_Figure_0.png'
import image_12 from '@site/static/images/managing-data/core-concepts/_vldb2024_10_Figure_12.png'
import image_13 from '@site/static/images/managing-data/core-concepts/_vldb2024_10_Figure_13.png'
import Image from '@theme/IdealImage';

<!-- needed as docusaurus cannot resolve links to span ids, we need a custom span -->
export function Anchor(props) {
    useBrokenLinks().collectAnchor(props.id);
    return <span style={{scrollMarginTop: "var(--ifm-navbar-height)"}} {...props}/>;
}

这是我们 [VLDB 2024 科学论文](https://www.vldb.org/pvldb/vol17/p3731-schulze.pdf) 的网络版本。我们还 [撰写了博客](https://clickhouse.com/blog/first-clickhouse-research-paper-vldb-lightning-fast-analytics-for-everyone)，介绍了其背景和历程，并推荐观看 ClickHouse CTO 和创始人 Alexey Milovidov 的 VLDB 2024 演讲：

<iframe width="1024" height="576" src="https://www.youtube.com/embed/7QXKBKDOkJE?si=5uFerjqPSXQWqDkF" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
## 摘要 {#abstract}

在过去的几十年中，被存储和分析的数据量呈指数级增长。各行业和领域的企业开始依赖这些数据来改善产品、评估绩效，并做出业务关键决策。然而，随着数据量越来越大，企业需要以经济有效和可扩展的方式管理历史和新数据，同时通过高并发查询进行分析，并期望实时的延迟（例如，根据用例不同，低于一秒）。

本文介绍了 ClickHouse 的概述，ClickHouse 是一个流行的开源 OLAP 数据库，旨在高性能分析 PB 级数据集，具有高摄取率。其存储层结合了基于传统日志结构合并（LSM）树的数据格式，以及在后台对历史数据进行持续转换（例如聚合、归档）的新技术。查询使用方便的 SQL 方言编写，并由最先进的向量化查询执行引擎处理，支持可选的代码编译。ClickHouse 积极使用修剪技术，以避免在查询中评估无关数据。其他数据管理系统可以在表函数、表引擎或数据库引擎级别集成。现实世界的基准测试表明，ClickHouse 是市场上最快的分析数据库之一。
## 1 引言 {#1-introduction}

本文描述了 ClickHouse，一个针对高性能分析查询设计的列式 OLAP 数据库，适用于具有数万亿行和数百列的表。ClickHouse 于 2009 年 [启动](https://clickhou.se/evolution)，作为用于 web 规模日志文件数据的过滤和聚合操作符，并于 2016 年开源。 [图 1](#page-1-0) 说明了本文中描述的主要功能何时引入 ClickHouse。

ClickHouse 旨在解决现代分析数据管理的五个关键挑战：

1. **海量数据集，具有高摄取率**。许多数据驱动的应用程序，例如网络分析、金融和电子商务，特点是数据量巨大且持续增长。为了处理庞大的数据集，分析数据库不仅必须提供有效的索引和压缩策略，还必须允许数据分布在多个节点间（横向扩展），因为单个服务器的存储容量限制在几十 TB。此外，最近的数据通常比历史数据更具实时洞察的相关性。因此，分析数据库必须能够以持续高速度或突发方式摄取新数据，并能够不断“降级”历史数据（例如聚合、归档），而不减慢并行报告查询的速度。

2. **许多同时查询，并期望低延迟**。查询一般可以分为临时（例如探索性数据分析）或定期（例如周期性仪表盘查询）。用例越互动，期望的查询延迟越低，这对查询优化和执行带来了挑战。定期查询还提供了根据工作负载调整物理数据库布局的机会。因此，数据库应提供修剪技术，允许优化频繁查询。根据查询优先级，数据库必须进一步为共享系统资源（如 CPU、内存、磁盘和网络 I/O）提供平等或优先的访问，即使在大量查询同时运行时。

3. **多样化的数据存储、存储位置和格式**。现代分析数据库应表现出高程度的开放性，以便在任何系统、位置或格式中读取和写入外部数据，从而与现有数据架构集成。

4. **便捷的查询语言，支持性能自省**。现实世界中使用 OLAP 数据库会带来额外的“软”需求。例如，用户通常希望使用一种表达力强的 SQL 方言与数据库接口，而不是一种小众编程语言，该语言支持嵌套数据类型和广泛的常规、聚合和窗口函数。分析数据库还应提供复杂的工具，用于自省系统或单个查询的性能。

5. **工业级的鲁棒性和多功能的部署**。由于商品硬件不可靠，数据库必须提供数据复制，以应对节点故障的鲁棒性。此外，数据库应能够运行在任何硬件上，从旧笔记本到强大的服务器。最后，为了避免 JVM 基础程序中的垃圾回收开销并实现裸机性能（例如 SIMD），数据库理想情况下作为目标平台的本地二进制文件进行部署。

<Anchor id="page-1-0"/><Image img={image_01} size="lg" alt="Image 01"/>

图 1: ClickHouse 时间线。
## 2 架构 {#2-architecture}

<Anchor id="page-2-0"/><Image img={image_02} size="lg" alt="Image 02"/>

图 2: ClickHouse 数据库引擎的高层架构。

如 [图 2](#page-2-0) 所示，ClickHouse 引擎分为三个主要层次：查询处理层（在第 [4 节](#page-6-0) 中描述）、存储层（第 [3 节](#page-1-1)）和集成层（第 [5 节](#page-9-0)）。此外，访问层通过不同协议管理用户会话和与应用程序的通信。还存在正交组件用于线程处理、缓存、基于角色的访问控制、备份和持续监控。ClickHouse 使用 C++ 构建为单个、静态链接的二进制文件，没有依赖项。

查询处理遵循传统模式：解析传入的查询、构建和优化逻辑和物理查询计划、执行。ClickHouse 使用与 MonetDB/X100 类似的向量化执行模型 [\[11\]](#page-12-0)，结合机会主义的代码编译 [\[53\]](#page-13-0)。查询可以使用功能丰富的 SQL 方言、PRQL [\[76\]](#page-13-1) 或 Kusto 的 KQL [\[50\]](#page-13-2) 编写。

存储层由不同的表引擎组成，这些引擎封装了表数据的格式和位置。表引擎分为三类：第一类是 MergeTree* 家族的表引擎，它代表了 ClickHouse 中的主要持久性格式。基于 LSM 树的概念 [\[60\]](#page-13-3)，表被拆分为水平的、已排序的部分，这些部分由后台进程持续合并。各个 MergeTree* 表引擎在合并其输入部分的行的方式上有所不同。例如，行可以被聚合或替换，如果数据过时。

第二类是特定用途的表引擎，用于加速或分布查询执行。这一类别包括称为字典的内存键值表引擎。一个 [字典](https://clickhou.se/dictionaries) 定期缓存针对内部或外部数据源执行的查询的结果。这在数据不需要完全新鲜的场景中显著减少了查询延迟。其他示例包括用于临时表的纯内存引擎以及用于透明数据分片的分布式表引擎（见下文）。

第三类表引擎是虚拟表引擎，用于与外部系统（如关系数据库（例如 PostgreSQL、MySQL）、发布/订阅系统（例如 Kafka、RabbitMQ [\[24\]](#page-12-1)）或键值存储（例如 Redis））进行双向数据交换。虚拟引擎还可以与数据湖（例如 Iceberg、DeltaLake、Hudi [\[36\]](#page-12-2)）或对象存储中的文件（例如 AWS S3、Google GCP）进行交互。

ClickHouse 支持在多个集群节点间的表分片和复制，以实现可扩展性和可用性。分片将一个表根据分片表达式划分为一组表分片。各个分片是相互独立的表，通常位于不同的节点上。客户端可以直接读取和写入分片，即将它们视为独立的表，或使用分布式特殊表引擎，提供所有表分片的全局视图。分片的主要目的是处理超出单个节点容量的数据集（通常为几十 TB 的数据）。分片的另一个用途是将表的读写负载分布到多个节点，从而实现负载均衡。与此正交的是，分片可以在多个节点间进行复制，以容忍节点故障。为此，每个 Merge-Tree* 表引擎有一个相应的 ReplicatedMergeTree* 引擎，使用基于 Raft 共识 [\[59\]](#page-13-4) 的多主协调方案（由 [Keeper](https://clickhou.se/keeper) 实现，后者是一个用 C++ 编写的 Apache Zookeeper 的可替代品），以确保每个分片始终具有可配置数量的副本。第 [3.6 节](#page-5-0) 详细讨论了复制机制。作为示例， [图 2](#page-2-0) 显示了一个具有两个分片的表，每个分片被复制到两个节点。

最后，ClickHouse 数据库引擎可以在本地、云、独立或进程内模式下运行。在本地模式下，用户将 ClickHouse 设置为作为单个服务器或带有分片和/或复制的多节点集群进行本地运行。客户端通过原生、MySQL 和 PostgreSQL 的二进制协议或 HTTP REST API 与数据库通信。云模式由 ClickHouse Cloud 表示，提供完全托管和自动扩展的 DBaaS 服务。虽然本文集中于本地模式，但我们计划在后续出版物中描述 ClickHouse Cloud 的架构。[独立模式](https://clickhou.se/local-fastest-tool) 使 ClickHouse 成为一个用于分析和转换文件的命令行工具，使其成为 Unix 工具（例如 cat 和 grep）的 SQL 基替代方案。虽然这不需要先前配置，但该模式限制为单个服务器。最近开发了一种称为 chDB [\[15\]](#page-12-3) 的进程内模式，适用于交互式数据分析用例，如带有 Pandas 数据框的 Jupyter notebooks [\[37\]](#page-12-4)。受 DuckDB [\[67\]](#page-13-6) 的启发，[chDB](https://clickhou.se/chdb-rocket-engine) 将 ClickHouse 作为高性能 OLAP 引擎嵌入到主机进程中。与其他模式相比，这允许在数据库引擎和应用程序之间高效地传递源数据和结果数据，而无需复制，因为它们在同一地址空间中运行。
## <Anchor id="page-1-1"/>3 存储层 {#3-storage-layer}

本节讨论 MergeTree* 表引擎作为 ClickHouse 的本地存储格式。我们描述它们的磁盘表示，并讨论 ClickHouse 中的三种数据修剪技术。随后，我们介绍无影响并发插入的合并策略。最后，我们解释更新和删除的实现，以及数据去重、数据复制和 ACID 合规性。
### <Anchor id="page-2-2"/>3.1 磁盘格式 {#3-1-on-disk-format}

MergeTree* 表引擎中的每个表都组织为不变的表部分的集合。每当将一组行插入表中时，就会创建一个部分。部分是自包含的，因为它们包含了解释其内容所需的所有元数据，而无需额外查找中央目录。为了保持每个表的部分数量较低，后台合并作业定期将多个较小的部分合并为更大的部分，直到达到可配置的部分大小（默认150 GB）。由于部分按表的主键列排序（见第 [3.2 节](#page-3-0)），因此使用有效的 k 方式合并排序 [\[40\]](#page-12-5) 进行合并。源部分标记为非活动状态，并在其引用计数降为零时最终删除，即没有进一步的查询从中读取。

行可以以两种模式插入：在同步插入模式下，每个 INSERT 语句创建一个新部分并将其附加到表中。为了最小化合并的开销，数据库客户端鼓励批量插入元组，例如一次插入 20,000 行。然而，如果数据需要实时分析，客户端批处理引起的延迟通常是不可接受的。例如，观察用例通常涉及数千个监控代理不断发送少量事件和指标数据。这种场景可以利用异步插入模式，在此模式下，ClickHouse 将来自多个传入 INSERT 的行缓冲到同一表中，并仅在缓冲区大小超过可配置阈值或超时到期后创建一个新部分。

<Anchor id="page-2-1"/><Image img={image_03} size="lg" alt="Image 03"/>

图 3: MergeTree*-引擎表的插入和合并。

[图 3](#page-2-1) 说明了对一个 MergeTree*-引擎表的四个同步和两个异步插入。两次合并将活动部分的数量从最初的五个减少到两个。

与 LSM 树 [\[58\]](#page-13-7) 及其在各种数据库中的实现 [\[13,](#page-12-6) [26,](#page-12-7) [56\]](#page-13-8) 相比，ClickHouse 将所有部分视为平等的，而不是将它们排列在层次结构中。因此，合并不再局限于同一层级的部分。由于这也放弃了部分的隐含时间顺序，因此需要不基于墓碑的更新和删除的替代机制（见第 [3.4 节](#page-4-0)）。ClickHouse 直接将插入写入磁盘，而其他 LSM 树基存储通常使用写前日志（见第 [3.7 节](#page-5-1)）。

一个部分对应于磁盘上的一个目录，包含每列一个文件。作为优化，小部分（默认小于 10 MB）的列被连续存储在一个文件中，以增加读取和写入的空间局部性。部分的行进一步逻辑上被划分为 8192 条记录的组，称为 granules。一个 granule 代表 ClickHouse 中扫描和索引查找运算符处理的最小不可分割数据单元。然而，在磁盘数据的读取和写入不是在 granule 级别进行，而是在块的粒度下进行，块将多个相邻 granules 组合在一起。新块的形成基于每个块的可配置字节大小（默认 1 MB），即块中的 granule 数量是可变的，取决于列的数据类型和分布。块还经过压缩以减少其大小和 I/O 成本。默认情况下，ClickHouse 使用 LZ4 [\[75\]](#page-13-9) 作为通用压缩算法，但用户也可以为浮点数据指定专用的编码器，如 Gorilla [\[63\]](#page-13-10) 或 FPC [\[12\]](#page-12-8)。可以将压缩算法链接在一起。例如，首先使用增量编码 [\[23\]](#page-12-9) 减少数值中的逻辑冗余，然后执行重型压缩，最后使用 AES 编码器对数据进行加密。块在从磁盘加载到内存时实时解压缩。为了使 ClickHouse 能够快速随机访问单个 granules，尽管进行了压缩，ClickHouse 还为每个列存储了一个映射，该映射将每个 granule ID 与其所包含压缩块在列文件中的偏移量以及该 granule 在未压缩块中的偏移量相关联。

列还可以使用两种特殊的包装数据类型进行字典编码 [\[2,](#page-12-10) [77,](#page-13-11) [81\]](#page-13-12) 或设置为 Nullable：LowCardinality(T) 用整数 ID 替换原始列值，从而显著减少具有少量唯一值的数据的存储开销。Nullable(T) 为列 T 添加一个内部位图，表示列值是否为 NULL。

最后，表可以使用任意分区表达式进行范围、哈希或轮询分区。为了启用分区裁剪，ClickHouse 还存储每个分区的分区表达式的最小值和最大值。用户可以选择创建更高级的列统计信息（例如，HyperLogLog [\[30\]](#page-12-11) 或 t-digest [\[28\]](#page-12-12) 统计信息），也提供基数估算。
### <Anchor id="page-3-0"/>3.2 数据修剪 {#3-2-data-pruning}

在大多数用例中，仅扫描 PB 级数据以回答单个查询是太慢和昂贵的。ClickHouse 支持三种数据修剪技术，可以在搜索时跳过大多数行，从而显著加快查询速度。

首先，用户可以为表定义 **主键索引**。主键列决定了每个部分内行的排序顺序，即索引是局部聚集的。ClickHouse 还为每个部分存储从每个 granule 的第一行的主键列值到 granule ID 的映射，即索引是稀疏的 [\[31\]](#page-12-13)。生成的数据结构通常足够小，可以完全保留在内存中，例如，仅需 1000 个条目即可索引 810 万条行。主键的主要目的是使用二分查找评估频繁过滤列的相等性和范围谓词，而不是顺序扫描（见第 [4.4 节](#page-7-0)）。局部排序还可用于部分合并和查询优化，例如基于排序的聚合，或在主键列形成排序列的前缀时，从物理执行计划中删除排序运算符。

[图 4](#page-3-1) 显示了针对页面印象统计表的事件时间列的主键索引。可以通过对主键索引进行二分搜索找到与查询中的范围谓词匹配的 granules，而不是顺序扫描事件时间。

<Anchor id="page-3-1"/><Image img={image_04} size="lg" alt="Image 04"/>

图 4: 使用主键索引评估过滤条件。

其次，用户可以创建 **表投影**，即包含按不同主键排序的相同行的表的替代版本 [\[71\]](#page-13-13)。投影可以加速在主要表的主键以外的列上进行过滤的查询，但会增加插入、合并和空间消耗的开销。默认情况下，投影仅从新插入主表的部分延迟填充，而不从现有部分填充，除非用户完全物化该投影。查询优化器在从主表加载或从投影加载之间做出选择，基于估计的 I/O 成本。如果某部分没有投影，则查询执行会回退到相应的主表部分。

第三，**跳过索引** 提供了一种轻量级的替代方案。跳过索引的想法是在多个连续的 granules 级别存储少量元数据，以避免扫描无关行。可以为任意索引表达式创建跳过索引，并使用可配置的粒度，即跳过索引块中的 granules 数量。可用的跳过索引类型包括：1. 最小-最大索引 [\[51\]](#page-13-14)，为每个索引块存储索引表达式的最小值和最大值。此索引类型适用于具有小绝对范围的局部聚集数据，例如松散排序的数据。 2. 集合索引，存储可配置数量的唯一索引块值。这些索引最适合与具有小局部基数的数据一起使用，即“聚集在一起”的值。 3. Bloom 过滤器索引 [\[9\]](#page-12-14)，为行、标记或 n-gram 值构建，具有可配置的假阳性率。这些索引支持文本搜索 [\[73\]](#page-13-15)，但与最小-最大和集合索引不同，它们不能用于范围或负谓词。
### <Anchor id="page-4-3"/>3.3 合并时数据转换 {#3-3-merge-time-data-transformation}

商业智能和可观察性用例通常需要处理以不断的高速或突发形式生成的数据。此外，最近生成的数据通常比历史数据在有意义的实时洞察中更具相关性。这类用例要求数据库在持续降低历史数据的体积（通过聚合或数据老化等技术）的同时，保持高数据摄取率。ClickHouse 允许通过不同的合并策略对现有数据进行连续增量转换。合并时的数据转换不会影响 INSERT 语句的性能，但无法保证表中从不包含不需要的（例如过时或未聚合）值。如果需要，可以通过在 SELECT 语句中指定关键字 FINAL 在查询时间应用所有合并时转换。

**替换合并** 保留基于其所包含部分的创建时间戳的元组的最新插入版本，旧版本被删除。如果元组的主键列值相同，则视为等效。为了对保留哪个元组进行明确控制，还可以指定一个特殊的版本列进行比较。替换合并通常用作合并时更新机制（通常在更新频繁的用例中），或作为插入时数据去重的替代方案（见第 [3.5 节](#page-5-2)）。

**聚合合并** 将具有相同主键列值的行聚合为一行。非主键列必须处于部分聚合状态，以保持摘要值。两种部分聚合状态，例如 avg() 的和与计数，将合并为新的部分聚合状态。聚合合并通常在物化视图中使用，而不是普通表。物化视图是基于针对源表的转换查询填充的。与其他数据库不同，ClickHouse 不会定期通过整个源表的内容刷新物化视图。相反，物化视图是通过在新部分插入源表时更新的。

[图 5](#page-4-1) 显示了定义在页面印象统计表上的物化视图。对于插入到源表的新部分，转换查询计算最大和平均延迟，按区域分组，并将结果插入物化视图中。聚合函数 avg() 和 max() 使用扩展 -State 返回部分聚合状态，而不是实际结果。为物化视图定义的聚合合并持续组合不同部分中的部分聚合状态。为了获得最终结果，用户使用 avg() 和 max() 结合 -Merge 扩展来整合物化视图中的部分聚合状态。

<Anchor id="page-4-1"/><Image img={image_05} size="lg" alt="Image 05"/>

图 5: 物化视图中的聚合合并。

**生存时间 (TTL) 合并** 为历史数据提供老化。与删除和聚合合并不同，TTL 合并一次只处理一个部分。TTL 合并的定义以触发器和操作的形式进行。触发器是计算每行时间戳的表达式，与执行 TTL 合并时的时间进行比较。虽然这允许用户在行粒度上控制操作，但我们发现只需检查所有行是否满足给定条件，并对整个部分运行操作就足够了。可能的操作包括 1. 将部分移动到另一个卷（例如，便宜而较慢的存储）， 2. 重新压缩部分（例如，使用更重的编码器）， 3. 删除部分，以及 4. 汇总，即使用分组键和聚合函数聚合行。

作为示例，请考虑 [列表 1](#page-4-2) 中的日志记录表定义。ClickHouse 将把时间戳列值超过一周的部分移至缓慢但便宜的 S3 对象存储。
<Anchor id="page-4-2"/>
```
1 CREATE TABLE tab ( ts DateTime , msg String )
2 ENGINE MergeTree PRIMARY KEY ts
3 TTL ( ts + INTERVAL 1 WEEK ) TO VOLUME 's3 '
```
列表 1: 一周后将部分移动到对象存储。
### <Anchor id="page-4-0"/>3.4 更新和删除 {#3-4-updates-and-deletes}

MergeTree* 表引擎的设计更倾向于追加工作负载，然而，有些用例偶尔需要修改现有数据，例如为了遵守法规。更新或删除数据有两种方法，两者都不阻塞并行插入。

**变更** 在原位重写表的所有部分。为了防止在操作期间表（删除）或列（更新）暂时膨胀，这个操作是非原子的，即并行的 SELECT 语句可能会读取变更与不变更的部分。变更保证在操作结束时数据会物理变化。删除变更仍然昂贵，因为它们会在所有部分中重写所有列。

作为替代方案，**轻量级删除** 仅更新一个内部位图列，指示某行是否被删除。ClickHouse 把带有附加过滤器的 SELECT 查询与位图列结合，以排除结果中的删除行。已删除的行仅在未来某个不确定的时间通过常规合并物理移除。根据列的数量，轻量级删除可能比变更快，但代价是 SELECT 操作的速度较慢。

对同一表执行的更新和删除操作预计将是稀少的，并且是串行化的，以避免逻辑冲突。
### <Anchor id="page-5-2"/>3.5 幂等性插入 {#3-5-idempotent-inserts}

在实践中，客户端应该如何处理发送数据到服务器进行表格插入后连接超时的问题是一个常见问题。在这种情况下，客户端很难区分数据是否成功插入。这个问题传统上通过从客户端重新发送数据到服务器来解决，并依赖主键或唯一约束来拒绝重复插入。数据库使用基于二叉树 [\[39,](#page-12-15) [68\]](#page-13-16)、基数树 [\[45\]](#page-13-17) 或哈希表 [\[29\]](#page-12-16) 的索引结构迅速执行所需的点查找。由于这些数据结构给每个元组建立索引，其空间和更新开销对于大型数据集和高摄取率来说变得过于昂贵。

ClickHouse 提供了一种更轻量级的替代方案，基于以下事实：每个插入最终会创建一个部分。更具体地说，服务器会保留最近插入的 N 个部分的哈希（例如 N=100），并忽略已知哈希的部分的再插入。非复制和复制表的哈希分别存储在 Keeper 中。因此，插入变得幂等，即客户端可以在超时后简单地重新发送相同的行批次，并假定服务器会处理去重。为了对去重过程有更好的控制，客户端可以选择提供一个插入令牌作为部分哈希。尽管基于哈希的去重会产生与哈希新行相关的开销，但存储和比较哈希的成本是微不足道的。

### <Anchor id="page-5-0"/>3.6 数据复制 {#3-6-data-replication}

复制是高可用性的前提（对节点故障的容错），同时也用于负载均衡和零停机升级 [\[14\]](#page-12-17)。在 ClickHouse 中，复制基于表状态的概念，这些状态由一组表片段（第 [3.1 章](#page-2-2)）和表元数据（例如列名和类型）组成。节点通过三种操作来推进表的状态：1. 插入操作向状态中添加一个新片段，2. 合并操作将新片段添加到状态中，并删除状态中的现有片段，3. 变更和 DDL 语句根据具体操作添加片段和/或删除片段和/或更改表元数据。操作在单个节点本地执行，并记录为全局复制日志中的状态转换序列。

复制日志由通常三个 ClickHouse Keeper 进程的集群维护，这些进程使用 Raft 共识算法 [\[59\]](#page-13-4) 提供集中式和容错的协调层，适用于 ClickHouse 节点的集群。所有集群节点最初指向复制日志中的同一位置。当节点执行本地插入、合并、变更和 DDL 语句时，复制日志会在所有其他节点上异步重放。因此，复制的表只是最终一致的，即节点在趋向最新状态时可能暂时读取旧的表状态。大多数前述操作可以选择同步执行，直到达到法定数量的节点（例如大多数节点或所有节点）接受新状态。

例如，图 [6](#page-5-3) 显示了一个在三个 ClickHouse 节点集群中的初始空复制表。节点 1 首先接收两个插入语句，并在 Keeper 集群中记录它们（ 1 2 ）。接下来，节点 2 重放第一个日志条目，通过获取它（ 3 ）并从节点 1 下载新片段（ 4 ），而节点 3 重放两个日志条目（ 3 4 5 6 ）。最后，节点 3 将两个部分合并为一个新部分，删除输入部分，并在复制日志中记录合并条目（ 7 ）。

<Anchor id="page-5-3"/><Image img={image_06} size="lg" alt="Image 06"/>

图 6: 三个节点的集群中的复制。

为了加速同步，有三种优化措施：首先，添加到集群的新节点确实是从头重放复制日志，而是简单地复制写入最后一个复制日志条目的节点的状态。其次，合并操作可以通过在本地重复它们或者从其他节点获取结果片段来重放。确切的行为是可配置的，并允许在 CPU 消耗和网络 I/O 之间进行平衡。例如，跨数据中心的复制通常倾向于本地合并，以最小化运营成本。第三，节点以并行方式重放互不依赖的复制日志条目。这包括从同一表中连续插入的新片段的获取，或对不同表的操作。

### <Anchor id="page-5-1"/>3.7 ACID 合规性 {#3-7-acid-compliance}

为了最大化并发读写操作的性能，ClickHouse 尽可能避免了锁定。查询是在查询开始时创建的所有相关表中所有片段的快照上执行的。这确保了并行 INSERT 或合并（第 [3.1 章](#page-2-2)）插入的新片段不会参与执行。为了防止片段在同一时间被修改或删除（第 [3.4 章](#page-4-0)），处理的片段的引用计数在查询期间增加。形式上，这对应于快照隔离，由基于版本化片段的 MVCC 变体 [\[6\]](#page-12-18) 实现。因此，语句通常不符合 ACID，除非在快照被捕获时，并发写入只影响单个片段的罕见情况。

在实践中，ClickHouse 处理写重的决策使用场景甚至容忍在断电时丢失新数据的小风险。数据库通过默认情况下不强制将新插入的片段提交（fsync）到磁盘来利用这一点，允许内核批量写入，以换取放弃原子性。

## <Anchor id="page-6-0"/>4 查询处理层 {#4-query-processing-layer}

<Anchor id="page-6-1"/><Image img={image_07} size="lg" alt="Image 07"/>

图 7: 在 SIMD 单元、核心和节点之间并行化。

如图 [7](#page-6-1) 所示，ClickHouse 在数据元素、数据块和表分片的级别上并行化查询。多个数据元素可以在操作符内部同时使用 SIMD 指令进行处理。在单个节点上，查询引擎在多个线程中同时执行操作符。ClickHouse 使用与 MonetDB/X100 相同的向量化模型 [\[11\]](#page-12-0)，即操作符产生、传递和消耗多个行（数据块），而不是单行，以最小化虚拟函数调用的开销。如果源表被拆分成不相交的表分片，则多个节点可以同时扫描这些分片。因此，所有硬件资源得到充分利用，查询处理可以通过添加节点的方式水平扩展，或通过添加核心的方式垂直扩展。

本节的其余部分首先详细描述数据元素、数据块和分片粒度下的并行处理。然后，我们展示一些关键优化，以最大化查询性能。最后，我们讨论 ClickHouse 如何在存在并发查询时管理共享系统资源。

### 4.1 SIMD 并行化 {#4-1-simd-parallelization}

在操作符之间传递多个行创造了向量化的机会。向量化可以基于手动编写的内嵌函数 [\[64,](#page-13-18) [80\]](#page-13-19) 或编译器自动向量化 [\[25\]](#page-12-19)。受益于向量化的代码被编译成不同的计算内核。例如，一个查询操作符的内部热点循环可以通过非向量化内核、自动向量化的 AVX2 内核和手动向量化的 AVX-512 内核来实现。运行时 [选择](https://clickhou.se/cpu-dispatch) 最快的内核，基于 cpuid 指令。这种方法允许 ClickHouse 在至少 15 年前的系统上运行（要求最低 SSE 4.2），同时在较新的硬件上提供显著的加速。

### 4.2 多核并行化 {#4-2-multi-core-parallelization}

<Anchor id="page-7-1"/><Image img={image_08} size="lg" alt="Image 08"/>

图 8: 具有三个通道的物理操作符计划。

ClickHouse 遵循传统的方法 [\[31\]](#page-12-13)，将 SQL 查询转换为物理计划操作符的有向图。操作符计划的输入由特殊的源操作符表示，这些操作符以本地格式或任何支持的第三方格式读取数据（见第 [5 章](#page-9-0)）。同样，一个特殊的汇聚操作符将结果转换为所需的输出格式。物理操作符计划在查询编译时展开为基于可配置最大工作线程数（默认情况下为核心数）和源表大小的独立执行通道。通道将并行操作符需要处理的数据分解为不重叠的范围。为了最大化并行处理的机会，通道的合并尽可能延迟。

例如，在图 [8](#page-7-1) 中，节点 1 的框显示了针对表示页面展示统计信息的表的典型 OLAP 查询的操作符图。在第一阶段，源表的三个不重叠范围同时被过滤。Repartition exchange 操作符动态路由结果片段在第一和第二阶段之间，以保持处理线程的均匀利用。如果扫描的范围具有显著不同的选择性，通道在第一阶段后可能会变得不平衡。在第二阶段，存活下来的行按照 RegionID 分组。Aggregate 操作符维护以 RegionID 作为分组列的本地结果组以及每组的和与计数作为 avg() 的部分聚合状态。最终，本地聚合结果通过 GroupStateMerge 操作符合并为全局聚合结果。该操作符也是管道断点，即，仅在完全计算出聚合结果后，第三阶段才能开始。在第三阶段，结果组首先通过 Distribute exchange 操作符分成三个大小相等的不重叠分区，然后根据 AvgLatency 进行排序。排序分为三个步骤：首先， ChunkSort 操作符对每个分区的个别块进行排序。其次，StreamSort 操作符维护本地排序结果，将其与使用两路归并排序的传入排序块结合。最后，MergeSort 操作符使用 k 路排序将本地结果合并，以获得最终结果。

操作符是状态机，并通过输入和输出端口连接在一起。操作符的三种可能状态是 need-chunk、ready 和 done。要从 need-chunk 移动到 ready，必须将一个块放入操作符的输入端口。要从 ready 移动到 done，操作符处理输入块并生成输出块。要从 done 移动到 need-chunk，输出块从操作符的输出端口中移除。两个连接操作符中的第一个和第三个状态转换只能在一个组合步骤中执行。源操作符（汇聚操作符）仅具有 ready 和 done 状态（need-chunk 和 done）。

工作线程不断遍历物理操作符计划并执行状态转换。为了保持 CPU 缓存热，计划中包含提示，以便同一线程应在同一通道中处理连续的操作符。并行处理同时在一个阶段的不同输入之间水平发生（例如，在 [图 8](#page-7-1) 中，Aggregate 操作符被同时执行），并在不被管道断点分隔的阶段之间垂直发生（例如，在 [图 8](#page-7-1) 中，同一通道中的 Filter 和 Aggregate 操作符可以同时运行）。为了避免在新查询启动时或并发查询完成时的过度和不足的订阅，查询中的并行度可以在查询启动时在一和为该查询在查询开始时指定的最大工作线程数之间改变（见第 [4.5 章](#page-9-1)）。

操作符还可以通过两种方式在运行时进一步影响查询执行。首先，操作符可以动态创建并连接新的操作符。这主要用于切换到外部聚合、排序或连接算法，而不是在内存消耗超过可配置的阈值时取消查询。其次，操作符可以请求工作线程移动到异步队列中。这在等待远程数据时提供了更有效的工作线程使用。

ClickHouse 的查询执行引擎和 morsel-driven 并行性 [\[44\]](#page-12-20) 具有相似之处，因为通道通常在不同的核心 / NUMA 套接字上执行，工作线程可以从其他通道窃取任务。此外，没有中央调度组件；相反，工作线程通过不断遍历操作符计划单独选择任务。与 morsel-driven 并行性不同，ClickHouse 将最大并行度嵌入计划中，并使用比默认为约 100,000 行更大的范围来划分源表。尽管这在某些情况下可能导致停顿（例如，当不同通道中的过滤操作符运行时间差异很大时），我们发现自由使用诸如 Repartition 之类的交换操作符至少避免了这种不平衡在各阶段之间的累积。

### 4.3 多节点并行化 {#4-3-multi-node-parallelization}

如果查询的源表是分片的，那么接收查询的节点（发起节点）上的查询优化器尝试在其他节点上尽可能多地执行工作。来自其他节点的结果可以集成到查询计划的不同点。根据查询，远程节点可以 1. 将原始源表列流式传输到发起节点，2. 过滤源列并发送存活的行，3. 执行过滤和聚合步骤，并发送带部分聚合状态的本地结果组，或 4. 运行整个查询（包括过滤、聚合和排序）。

图 [8](#page-7-1) 中的节点 2 ... N 显示在持有命中表分片的其他节点上执行的计划片段。这些节点过滤和分组本地数据，并将结果发送到发起节点。节点 1 上的 GroupStateMerge 操作符在最终对结果组进行排序之前合并本地和远程结果。

### <Anchor id="page-7-0"/>4.4 整体性能优化 {#4-4-holistic-performance-optimization}

本节介绍了一些在查询执行不同阶段应用的关键性能优化。

**查询优化**。第一组优化是在从查询的 AST 获得的语义查询表示上应用的。此类优化的示例包括常量折叠（例如，concat(lower('a'),upper('b')) 变为 'aB'）、从某些聚合函数中提取标量（例如，sum(a*2) 变为 2 * sum(a)）、公用子表达式消除，以及将等式过滤器的析取转换为 IN 列表（例如，x=c OR x=d 变为 x IN (c,d)）。优化的语义查询表示随后被转换为逻辑操作符计划。逻辑计划上的优化包括过滤下推、重新排序函数评估和排序步骤，具体取决于哪个估计更昂贵。最后，逻辑查询计划被转换为物理操作符计划。此转换可以利用所涉及的表引擎的特性。例如，在 MergeTree* 表引擎的情况下，如果 ORDER BY 列形成主键的前缀，则数据可以按磁盘顺序读取，排序操作符可以从计划中删除。此外，如果聚合中的分组列形成主键的前缀，则 ClickHouse 可以使用排序聚合 [\[33\]](#page-12-21)，即直接聚合预排序输入中相同值的运行。与哈希聚合相比，排序聚合显著减少内存占用，并且聚合值在处理完运行后可以立即传递给下一个操作符。

**查询编译**。ClickHouse 采用基于 [LLVM](https://clickhou.se/jit) 的查询编译来动态融合相邻的计划操作符 [\[38,](#page-12-22) [53\]](#page-13-0)。例如，表达式 a * b + c + 1 可以合并为单个操作符，而不是三个操作符。除了表达式，ClickHouse 还利用编译同时评估多个聚合函数（即用于 GROUP BY）和当有多个排序键时的排序。查询编译减少了虚拟调用的次数，将数据保留在寄存器或 CPU 缓存中，并帮助分支预测器，因为需要执行的代码更少。此外，运行时编译启用丰富的优化集合，例如编译器中的逻辑优化和 Peephole 优化，并访问最快的本地可用 CPU 指令。仅当相同的常规、聚合或排序表达式被不同查询执行超过可配置的次数时，才会启动编译。编译的查询操作符被缓存，可以被未来的查询重用。

**主键索引评估**。如果条件的合取范式中的过滤子句的子集构成主键列的前缀，则 ClickHouse 使用主键索引评估 WHERE 条件。主键索引在按字典序排序的键值范围上从左到右分析。与主键列对应的过滤子句使用三元逻辑进行评估——它们都为真、都为假，或在范围内的值为真/假混合。在后一种情况下，范围被拆分为子范围，递归分析。过滤条件中存在函数的额外优化。首先，函数具有描述其单调性的特征，例如，toDayOfMonth(date) 在一个月内是分段单调的。单调性特征允许推断函数在有序输入键值范围上产生有序结果。其次，一些函数可以计算给定函数结果的前像。利用这一点，可以通过将键列的值与前像进行比较，替换对常量的比较。例如，toYear(k) = 2024 可以被替换为 k >= 2024-01-01 && k < 2025-01-01。

**数据跳过**。ClickHouse 尝试在查询运行时使用第 [3.2 章](#page-3-0) 中介绍的数据结构来避免数据读取。此外，基于启发式和（可选）列统计信息，按降序的估计选择性顺序依次评估不同列上的过滤器。仅将包含至少一行匹配结果的数据块传递给下一个谓词。这逐渐减少了从谓词到谓词的读取数据量和需要执行的计算数量。该优化仅在至少存在一个高度选择性的谓词时应用；否则，相较于并行评估所有谓词，查询的延迟将会恶化。

**哈希表**。哈希表是聚合和哈希连接的基本数据结构。选择合适类型的哈希表对于性能至关重要。ClickHouse 采用基于通用哈希表模板的多种哈希表 [实例化](https://clickhou.se/hashtables)（截至 2024 年 3 月，有 30 多种），其中哈希函数、分配器、单元类型和调整策略作为变更点。根据分组列的数据类型、估计的哈希表基数和其他因素，为每个查询操作符单独选择最快的哈希表。哈希表的进一步优化包括：

- 具有 256 个子表的两级布局（基于哈希的第一个字节）以支持巨大的键集，
- 适用于不同字符串长度的字符串哈希表 [\[79\]](#page-13-20)，具有四个子表和不同的哈希函数，
- 当键数量很少时，查找表直接使用键作为桶索引（即不进行哈希），
- 对于比较昂贵的值（如字符串、AST），嵌入哈希以加快冲突解决，
- 基于运行时统计信息的预测大小创建哈希表以避免不必要的调整，
- 在单个内存块上分配多个具有相同创建/销毁生命周期的小哈希表，
- 使用每个哈希图和每个单元版本计数器即时清除哈希表以供重用，
- 使用 CPU 预取（__builtin_prefetch）加速哈希键后值的检索。

**连接**。由于 ClickHouse 起初仅支持基本连接，许多用例在历史上 resorted to non-normalized 表。如今，数据库 [提供](https://clickhou.se/joins) 所有在 SQL 中可用的连接类型（内部、左/右/完全外部、交叉、实时），以及多种连接算法，如哈希连接（简单的、优雅的）、排序-合并连接和为快速键值查找（通常是字典）设计的索引连接。

由于连接是最昂贵的数据库操作之一，提供经典连接算法的并行变体非常重要，理想情况下具有可配置的空间/时间取舍。对于哈希连接，ClickHouse 实现了来自 [\[7\]](#page-12-23) 的非阻塞共享分区算法。例如，图 [9](#page-8-3) 中的查询计算用户如何通过对页面命中统计表的自连接在 URL 之间移动。连接的构建阶段被分为三个通道，覆盖源表的三个不重叠范围。使用分区哈希表而不是全局哈希表。工作线程（通常为三个）通过计算哈希函数的模来确定构建侧每个输入行的目标分区。对哈希表分区的访问使用 Gather exchange 操作符进行同步。探测阶段以类似方式查找输入元组的目标分区。虽然该算法对每个元组引入了两个额外的哈希计算，但根据哈希表分区的数量，它大大减少了构建阶段的锁竞争。

<Anchor id="page-8-3"/><Image img={image_09} size="lg" alt="Image 09"/>

图 9: 具有三个哈希表分区的并行哈希连接。
### <Anchor id="page-9-1"/>4.5 工作负载隔离 {#4-5-workload-isolation}

ClickHouse 提供并发控制、内存使用限制和 I/O 调度，使用户能够将查询隔离到工作负载类别中。通过为特定工作负载类别设置共享资源（CPU 核心、DRAM、磁盘和网络 I/O）的限制，确保这些查询不会影响其他重要的业务查询。

并发控制防止在并发查询数量高的情况下出现线程超订阅。更具体地说，每个查询的工作线程数量根据与可用 CPU 核心数量的指定比率动态调整。

ClickHouse 在服务器、用户和查询级别跟踪内存分配的字节大小，从而允许设置灵活的内存使用限制。内存超承诺允许查询使用超出保证内存的额外空闲内存，同时确保其他查询的内存限制。此外，可以限制聚合、排序和连接子句的内存使用，在超出限制时，退回到外部算法。

最后，I/O 调度允许用户根据最大带宽、正在进行的请求和策略（例如 FIFO，SFC [\[32\]](#page-12-24)）限制工作负载类别的本地和远程磁盘访问。
### <Anchor id="page-9-0"/>5 集成层 {#5-integration-layer}

实时决策应用通常依赖于对多个位置中数据的高效和低延迟访问。为了使外部数据在 OLAP 数据库中可用，有两种方法。通过推送式数据访问，第三方组件将数据库与外部数据存储连接起来。这方面的一个示例是将远程数据推送到目标系统的专用提取-转换-加载（ETL）工具。在拉取导向模型中，数据库本身连接到远程数据源，并将数据拉取到本地表中以进行查询或将数据导出到远程系统。虽然推送式方法更灵活且更常见，但它们通常需要更大的架构占用和可扩展性瓶颈。相比之下，数据库中直接的远程连接提供了有趣的功能，例如在本地和远程数据之间的连接，同时保持整体架构简单，并缩短获取洞察的时间。

本节其余部分探讨 ClickHouse 中的拉取式数据集成方法，旨在访问远程位置中的数据。我们指出，在 SQL 数据库中远程连接的概念并不新鲜。例如，SQL/MED 标准 [\[35\]](#page-12-25) 在 2001 年提出，自 2011 年起由 PostgreSQL 实现 [\[65\]](#page-13-21)，建议将外部数据包装器作为管理外部数据的统一接口。与其他数据存储和存储格式的最大互操作性是 ClickHouse 的设计目标之一。截至 2024 年 3 月，ClickHouse 提供了我们所知的所有分析数据库中最多的内置数据集成选项。

外部连接。ClickHouse 提供了 [50+](https://clickhou.se/query-integrations) 种集成表函数和引擎，以便与外部系统和存储位置连接，包括 ODBC、MySQL、PostgreSQL、SQLite、Kafka、Hive、MongoDB、Redis、S3/GCP/Azure 对象存储和各种数据湖。我们进一步将它们分为以下赠品图中显示的类别（不属于原始 vldb 论文的一部分）。

<Anchor id="bonus-figure"/><Image img={image_10} size="lg" alt="Image 10"/>

赠图：ClickBench 的互操作性选项。

临时访问与集成 **表函数**。可以在 SELECT 查询的 FROM 子句中调用表函数，以读取远程数据以进行探索性临时查询。或者，它们可用于使用 INSERT INTO TABLE FUNCTION 语句将数据写入远程存储。

持久访问。有三种方法可以与远程数据存储和处理系统创建永久连接。

首先，集成 **表引擎** 将远程数据源（例如 MySQL 表）表示为持久的本地表。用户使用 CREATE TABLE AS 语法存储表定义，结合 SELECT 查询和表函数。可以指定自定义架构，例如，仅引用远程列的子集，或使用架构推断自动确定列名和等效的 ClickHouse 类型。我们进一步区分被动和主动的运行时行为：被动表引擎将查询转发到远程系统，并将结果填充到本地代理表中。相反，主动表引擎定期从远程系统拉取数据或订阅远程更改，例如，通过 PostgreSQL 的逻辑复制协议。因此，本地表包含远程表的完整副本。

其次，集成 **数据库引擎** 将远程数据存储中的一个表模式的所有表映射到 ClickHouse 中。与前者不同，通常要求远程数据存储为关系数据库，并且额外提供对 DDL 语句的有限支持。

第三，**字典** 可以使用针对几乎所有可能的数据源的任意查询填充，使用相应的集成表函数或引擎。运行时行为是主动的，因为数据会在固定间隔内从远程存储中拉取。

数据格式。为了与第三方系统互动，现代分析数据库还必须能够处理任何格式的数据。除了其本地格式，ClickHouse 支持 [90+](https://clickhou.se/query-formats) 种格式，包括 CSV、JSON、Parquet、Avro、ORC、Arrow 和 Protobuf。每种格式可以是输入格式（ClickHouse 可以读取），输出格式（ClickHouse 可以导出），或者两者兼具。一些面向分析的格式（如 Parquet）也与查询处理相结合，即优化器可以利用嵌入的统计信息，过滤器直接在压缩数据上进行评估。

兼容性接口。除了其本地的二进制通信协议和 HTTP 外，客户端可以通过兼容 MySQL 或 PostgreSQL 的通信协议接口与 ClickHouse 互动。此兼容性特性对于启用来自特定应用程序（例如某些商业智能工具）的访问非常有用，尤其是这些供应商尚未实现本地的 ClickHouse 连接。

## 6 性能作为一种特性 {#6-performance-as-a-feature}

本节介绍用于性能分析的内置工具，并使用真实世界和基准查询评估性能。

### 6.1 内置性能分析工具 {#6-1-built-in-performance-analysis-tools}

有多种工具可用于研究单个查询或后台操作中的性能瓶颈。用户通过基于系统表的统一接口与所有工具进行交互。

**服务器和查询指标**。服务器级统计，例如活动部分计数、网络吞吐量和缓存命中率，辅以每个查询统计，例如读取的块数或索引使用统计。度量同步计算（按需）或在可配置间隔内异步计算。

**采样分析器**。可以使用采样分析器收集服务器线程的调用栈。结果可以选择性地导出到外部工具，例如火焰图可视化器。

**OpenTelemetry 集成**。OpenTelemetry 是一种开放标准，用于跨多个数据处理系统追踪数据行 [\[8\]](#page-12-26)。ClickHouse 可以为所有查询处理步骤生成具有可配置粒度的 OpenTelemetry 日志跨度，并收集和分析来自其他系统的 OpenTelemetry 日志跨度。

**解释查询**。与其他数据库一样，SELECT 查询可以在前面加上 EXPLAIN，以便于深入了解查询的 AST、逻辑和物理操作符计划以及执行时行为。

### 6.2 基准测试 {#6-2-benchmarks}

尽管基准测试因为不够现实而受到批评 [\[10\]](#page-12-27) [52,](#page-13-22) [66,](#page-13-23) [74\]](#page-13-24)，但它仍然有助于识别数据库的优缺点。在下面的内容中，我们讨论基准测试如何用于评估 ClickHouse 的性能。
#### 6.2.1 去规范化表 {#6-2-1-denormalized-tables}

对去规范化事实表的过滤和聚合查询历来代表 ClickHouse 的主要用例。我们报告 ClickBench 的运行时间，这是一个典型的工作负载，模拟了用于点击流和流量分析的临时和定期报告查询。基准测试包含 43 个查询，这些查询针对一个包含 1 亿个匿名页面访问的数据表，数据源来自世界上最大的分析平台之一。一个在线仪表板 [\[17\]](#page-12-28) 显示截至 2024 年 6 月的超过 45 个商业和研究数据库的测量数据（冷/热运行时间、数据导入时间、磁盘大小）。结果由独立的贡献者基于公开的数据集和查询提交 [\[16\]](#page-12-29)。查询测试顺序和索引扫描访问路径，定期暴露 CPU、IO 或内存绑定的关系操作符。

[图 10](#page-10-0) 显示了在用于分析的数据库中顺序执行所有 ClickBench 查询的总相对冷和热运行时间。这些测量是在单节点 AWS EC2 c6a.4xlarge 实例上进行的，该实例具有 16 个 vCPU、32 GB RAM 和 5000 IOPS / 1000 MiB/s 的磁盘。与之相似的系统用于 Redshift（[ra3.4xlarge](https://clickhou.se/redshift-sizes)，12 vCPU，96 GB RAM）和 Snowflake（[仓库大小 S](https://clickhou.se/snowflake-sizes)：2x8 vCPU，2x16 GB RAM）。物理数据库设计只是轻微调优，例如，我们指定主键，但不更改单个列的压缩方式，不创建投影，也不使用跳过索引。我们还在每次冷查询运行之前刷新 Linux 页缓存，但不调整数据库或操作系统的参数。对于每个查询，使用最快的运行时间作为基线。其他数据库的相对查询运行时间计算为 ( + 10)/(_ + 10)。一个数据库的总相对运行时间是每个查询比例的几何平均值。虽然研究数据库 Umbra [\[54\]](#page-13-25) 实现了最佳的热运行时间，但 ClickHouse 的热和冷运行时间均超越其他所有生产级数据库。

<Anchor id="page-10-0"/><Image img={image_11} size="lg" alt="Image 11"/>

图 10: ClickBench 的相对冷和热运行时间。

为了跟踪 SELECT 在更广泛工作负载中的性能变化，我们 [使用](https://clickhou.se/performance-over-years) 综合了四个基准测试，称为 VersionsBench [\[19\]](#page-12-30)。每当发布新版本时，都会每月执行一次此基准测试，以评估其性能 [\[20\]](#page-12-31) 并识别可能降低性能的代码更改：各个基准测试包括：1. ClickBench（如上所述），2. 15 个 MgBench [\[21\]](#page-12-32) 查询，3. 针对拥有 6 亿行的去规范化星型模式基准 [\[57\]](#page-13-26) 的 13 个查询。4. 针对 [纽约出租车乘车数据](https://clickhou.se/nyc-taxi-rides-benchmark) 的 4 个查询，该数据包含 34 亿行 [\[70\]](#page-13-27)。

[图 11](#page-10-5) 显示了 2018 年 3 月至 2024 年 3 月之间 77 个 ClickHouse 版本的 VersionsBench 运行时间发展情况。为了补偿单个查询相对运行时间的差异，我们使用几何平均值进行规范化，最低查询运行时间的比例作为权重。VersionBench 的性能在过去六年中提高了 1.72 倍。长期支持（LTS）版本的发布日期标记在 x 轴上。尽管在某些时期性能暂时下降，但 LTS 版本通常具有与之前的 LTS 版本相当或更好的性能。2022 年 8 月的显著改进是由于在第 [4.4节](#page-7-0) 中描述的逐列过滤评估技术所致。

<Anchor id="page-10-5"/><Image img={image_12} size="lg" alt="Image 12"/>

图 11: 2018-2024 年 VersionsBench 的相对热运行时间。
#### 6.2.2 规范化表 {#6-2-2-normalized-tables}

在经典仓库中，数据通常使用星型或雪花模式建模。我们展示了 TPC-H 查询（比例因子 100）的运行时间，但指出规范化表是 ClickHouse 持续出现的用例。[图 12](#page-10-6) 显示了基于第 [4.4节](#page-7-0) 中描述的并行哈希连接算法的 TPC-H 查询的热运行时间。测量是在单节点 AWS EC2 c6i.16xlarge 实例上进行的，该实例具有 64 个 vCPU、128 GB RAM 和 5000 IOPS / 1000 MiB/s 的磁盘。记录了五次运行中的最快一次。为了参考，我们在一个可比较大小的 Snowflake 系统上进行了相同的测量（仓库大小 L，8x8 vCPU，8x16 GB RAM）。表中排除了 11 个查询的结果：查询 Q2、Q4、Q13、Q17 和 Q20-22 包含 ClickHouse v24.6 版本不支持的相关子查询。查询 Q7-Q9 和 Q19 依赖于连接的扩展计划级优化，例如连接重排序和连接谓词下推（在 ClickHouse v24.6 中均缺失），以实现可行的运行时间。自动子查询去相关化和更好的连接优化器支持预计将在 2024 年实施 [\[18\]](#page-12-33)。在其余 11 个查询中，有 5 个（6 个）查询在 ClickHouse 中执行得更快（在 Snowflake 中）。如前所述，这些优化被认为对性能至关重要 [\[27\]](#page-12-34)，我们预计它们在实施后会进一步改善这些查询的运行时间。

<Anchor id="page-10-6"/><Image img={image_13} size="lg" alt="Image 13"/>

图 12: TPC-H 查询的热运行时间（以秒为单位）。
## 7 相关工作 {#7-related-work}

分析数据库在最近几十年受到广泛的学术和商业关注 [\[1\]](#page-12-35)。早期系统如 Sybase IQ [\[48\]](#page-13-28)、Teradata [\[72\]](#page-13-29)、Vertica [\[42\]](#page-12-36) 和 Greenplum [\[47\]](#page-13-30) 的特征是昂贵的批量 ETL 作业和由于其本地部署性质而受限的弹性。在 2010 年代初，云原生数据仓库和数据库即服务（DBaaS）提供（如 Snowflake [\[22\]](#page-12-37)、BigQuery [\[49\]](#page-13-31) 和 Redshift [\[4\]](#page-12-38)）的出现显著降低了组织进行分析的成本和复杂性，同时受益于高可用性和自动资源扩展。最近，分析执行内核（如 Photon [\[5\]](#page-12-39) 和 Velox [\[62\]](#page-13-32)）提供了用于不同分析、流处理和机器学习应用的协同修改数据处理。

在目标和设计原则方面，与 ClickHouse 最相似的数据库是 Druid [\[78\]](#page-13-33) 和 Pinot [\[34\]](#page-12-40)。这两个系统的目标是实现数据摄取速率高的实时分析。与 ClickHouse 一样，表被分割成称为段的水平部分。虽然 ClickHouse 持续合并较小的部分，并可选择使用第 [3.3节](#page-4-3) 中的技术减少数据量，但在 Druid 和 Pinot 中部分始终保持不变。此外，Druid 和 Pinot 需要专门的节点来创建、变更和搜索表，而 ClickHouse 则使用单一的二进制文件来执行这些任务。

Snowflake [\[22\]](#page-12-37) 是一种流行的基于共享磁盘架构的专有云数据仓库。其将表划分为微分区的方法与 ClickHouse 中的部分概念相似。Snowflake 使用混合 PAX 页面 [\[3\]](#page-12-41) 进行持久存储，而 ClickHouse 的存储格式严格采用列式存储。Snowflake 还强调使用自动创建的轻量级索引 [\[31,](#page-12-13) [51\]](#page-13-14) 进行本地缓存和数据修剪，以获得良好的性能。与 ClickHouse 的主键类似，用户可以选择创建聚簇索引以共同定位相同值的数据。

Photon [\[5\]](#page-12-39) 和 Velox [\[62\]](#page-13-32) 是旨在作为复杂数据管理系统中的组件使用的查询执行引擎。这两个系统将查询计划作为输入，然后在本地节点上针对 Parquet（Photon）或 Arrow（Velox）文件执行这些计划 [\[46\]](#page-13-34)。ClickHouse 能够处理和生成这些通用格式的数据，但更倾向于其本地文件格式进行存储。尽管 Velox 和 Photon 不优化查询计划（Velox 进行基本的表达式优化），但它们采用运行时自适应技术，例如根据数据特性动态切换计算内核。类似地，ClickHouse 中的计划操作符可以在运行时创建其他操作符，主要是根据查询内存消耗在外部汇总或连接操作符之间切换。Photon 论文指出，生成代码的设计 [\[38,](#page-12-22) [41,](#page-12-42) [53\]](#page-13-0) 比解释的向量化设计 [\[11\]](#page-12-0) 更难开发和调试。Velox 中对代码生成的（实验性）支持构建和链接一个从运行时生成的 C++ 代码生成的共享库，而 ClickHouse 则直接与 LLVM 的按需编译 API 进行交互。

DuckDB [\[67\]](#page-13-6) 也旨在被宿主进程嵌入，但额外提供查询优化和事务。它是为 OLAP 查询与偶尔发生的 OLTP 语句混合而设计的。因此，DuckDB 选择了 DataBlocks [\[43\]](#page-12-43) 存储格式，采用轻量压缩方法，如顺序保持字典或参考裁剪 [\[2\]](#page-12-10)，以在混合工作负载中实现良好的性能。相比之下，ClickHouse 针对追加操作的用例进行了优化，即没有或很少的更新和删除。数据块使用重压缩技术（如 LZ4）进行压缩，假设用户广泛使用数据修剪，以加快频繁查询的速度，并且 I/O 成本远超过剩余查询的解压缩成本。DuckDB 还基于 Hyper 的 MVCC 方案提供可序列化事务 [\[55\]](#page-13-35)，而 ClickHouse 仅提供快照隔离。
## 8 结论与展望 {#8-conclusion-and-outlook}

我们介绍了 ClickHouse 的架构，ClickHouse 是一个开源的高性能 OLAP 数据库。凭借优化写入的存储层和最先进的矢量化查询引擎，ClickHouse 使实时分析来自 PB 级数据集的高摄取率成为可能。通过在后台异步合并和转换数据，ClickHouse 高效地解耦了数据维护和并行插入。其存储层通过稀疏主索引、跳过索引和投影表实现了激进的数据修剪。我们描述了 ClickHouse 对更新和删除的实现、幂等插入和跨节点的数据复制，以确保高可用性。查询处理层利用丰富的技术优化查询，并在所有服务器和集群资源之间进行并行执行。集成表引擎和功能提供了一种方便的方式，与其他数据管理系统以及数据格式无缝交互。通过基准测试，我们展示了 ClickHouse 是市场上最快的分析数据库之一，并且我们展示了 ClickHouse 在实际部署中常见查询性能的显著提高。

所有计划于 2024 年实施的功能和改进将在公开路线图上找到 [\[18\]](#page-12-33)。计划中的改进包括支持用户事务、PromQL [\[69\]](#page-13-36) 作为替代查询语言、用于半结构化数据（如 JSON）的新数据类型、更好的连接计划级优化，以及轻量级更新的实现，以补充轻量级删除。
## 致谢 {#acknowledgements}

根据版本 24.6，SELECT * FROM system.contributors 返回 1994 名对 ClickHouse 做出贡献的个人。我们想感谢 ClickHouse Inc. 的整个工程团队以及 ClickHouse 令人惊叹的开源社区，他们在共同构建这个数据库方面所付出的辛勤工作和奉献精神。
## REFERENCES {#references}

- <Anchor id="page-12-35"/>[1] Daniel Abadi, Peter Boncz, Stavros Harizopoulos, Stratos Idreaos, and Samuel Madden. 2013. 现代列式数据库系统的设计与实现. https://doi.org/10.1561/9781601987556
- <Anchor id="page-12-10"/>[2] Daniel Abadi, Samuel Madden, and Miguel Ferreira. 2006. 在列式数据库系统中集成压缩与执行. 在2006年ACM SIGMOD国际数据管理大会论文集中（SIGMOD '06）。671–682.https://doi.org/10.1145/1142473.1142548
- <Anchor id="page-12-41"/>[3] Anastassia Ailamaki, David J. DeWitt, Mark D. Hill, and Marios Skounakis. 2001. 织造关系以提高缓存性能. 在第27届国际大型数据库会议论文集中（VLDB '01）。Morgan Kaufmann Publishers Inc.，旧金山，加利福尼亚州，美国，169–180.
- <Anchor id="page-12-38"/>[4] Nikos Armenatzoglou, Sanuj Basu, Naga Bhanoori, Mengchu Cai, Naresh Chainani, Kiran Chinta, Venkatraman Govindaraju, Todd J. Green, Monish Gupta, Sebastian Hillig, Eric Hotinger, Yan Leshinksy, Jintian Liang, Michael McCreedy, Fabian Nagel, Ippokratis Pandis, Panos Parchas, Rahul Pathak, Orestis Polychroniou, Foyzur Rahman, Gaurav Saxena, Gokul Soundararajan, Sriram Subramanian, and Doug Terry. 2022. 亚马逊Redshift的重塑. 在2022年国际数据管理会议论文集中（费城，PA，美国）（SIGMOD '22）。计算机协会，纽约，NY，美国，2205–2217. https://doi.org/10.1145/3514221.3526045
- <Anchor id="page-12-39"/>[5] Alexander Behm, Shoumik Palkar, Utkarsh Agarwal, Timothy Armstrong, David Cashman, Ankur Dave, Todd Greenstein, Shant Hovsepian, Ryan Johnson, Arvind Sai Krishnan, Paul Leventis, Ala Luszczak, Prashanth Menon, Mostafa Mokhtar, Gene Pang, Sameer Paranjpye, Greg Rahn, Bart Samwel, Tom van Bussel, Herman van Hovell, Maryann Xue, Reynold Xin, and Matei Zaharia. 2022. Photon: 适用于湖屋系统的快速查询引擎（SIGMOD '22）。计算机协会，纽约，NY，美国，2326–2339. [https://doi.org/10.1145/3514221.](https://doi.org/10.1145/3514221.3526054) [3526054](https://doi.org/10.1145/3514221.3526054)
- <Anchor id="page-12-18"/>[6] Philip A. Bernstein and Nathan Goodman. 1981. 分布式数据库系统中的并发控制. ACM计算机调查 13, 2 (1981), 185–221. https://doi.org/10.1145/356842.356846
- <Anchor id="page-12-23"/>[7] Spyros Blanas, Yinan Li, and Jignesh M. Patel. 2011. 为多核CPU设计和评估主内存哈希连接算法. 在2011年ACM SIGMOD国际数据管理会议论文集中（希腊雅典）（SIGMOD '11）。计算机协会，纽约，NY，美国，37–48. https://doi.org/10.1145/1989323.1989328
- <Anchor id="page-12-26"/><Anchor id="page-12-14"/>[8] Daniel Gomez Blanco. 2023. 实用的OpenTelemetry. Springer Nature.
- [9] Burton H. Bloom. 1970. 带可容错的哈希编码中的空间/时间权衡. Commun. ACM 13, 7 (1970), 422–426. [https://doi.org/10.1145/362686.](https://doi.org/10.1145/362686.362692) [362692](https://doi.org/10.1145/362686.362692)
- <Anchor id="page-12-27"/>[10] Peter Boncz, Thomas Neumann, and Orri Erling. 2014. TPC-H 分析：从一个影响力基准中学习的隐秘信息和经验教训. 在性能特征化和基准测试中. 61–76. [https://doi.org/10.1007/978-3-319-](https://doi.org/10.1007/978-3-319-04936-6_5) [04936-6_5](https://doi.org/10.1007/978-3-319-04936-6_5)
- <Anchor id="page-12-0"/>[11] Peter Boncz, Marcin Zukowski, and Niels Nes. 2005. MonetDB/X100：超流水线查询执行. 在CIDR.
- <Anchor id="page-12-8"/>[12] Martin Burtscher and Paruj Ratanaworabhan. 2007. 高吞吐量压缩双精度浮点数据. 在数据压缩会议（DCC）。293–302. https://doi.org/10.1109/DCC.2007.44
- <Anchor id="page-12-6"/>[13] Jef Carpenter and Eben Hewitt. 2016. Cassandra：权威指南（第2版）。O'Reilly Media, Inc.
- <Anchor id="page-12-17"/>[14] Bernadette Charron-Bost, Fernando Pedone, and André Schiper (Eds.). 2010. 复制：理论与实践. Springer-Verlag.
- <Anchor id="page-12-3"/>[15] chDB. 2024. chDB - 内嵌OLAP SQL引擎. 从 https://github.com/chdb-io/chdb 获取，2024-06-20
- <Anchor id="page-12-29"/>[16] ClickHouse. 2024. ClickBench：用于分析数据库的基准测试. 从 https://github.com/ClickHouse/ClickBench 获取，2024-06-20
- <Anchor id="page-12-28"/>[17] ClickHouse. 2024. ClickBench：比较测量. 从 https://benchmark.clickhouse.com 获取，2024-06-20
- <Anchor id="page-12-33"/>[18] ClickHouse. 2024. ClickHouse路线图2024（GitHub）。从 https://github.com/ClickHouse/ClickHouse/issues/58392 获取，2024-06-20
- <Anchor id="page-12-30"/>[19] ClickHouse. 2024. ClickHouse版本基准测试. 从 https://github.com/ClickHouse/ClickBench/tree/main/versions 获取，2024-06-20
- <Anchor id="page-12-31"/>[20] ClickHouse. 2024. ClickHouse版本基准测试结果. 从 https://benchmark.clickhouse.com/versions/ 获取，2024-06-20
- <Anchor id="page-12-32"/>[21] Andrew Crotty. 2022. MgBench. 从 [https://github.com/](https://github.com/andrewcrotty/mgbench) [andrewcrotty/mgbench](https://github.com/andrewcrotty/mgbench) 获取，2024-06-20
- <Anchor id="page-12-37"/>[22] Benoit Dageville, Thierry Cruanes, Marcin Zukowski, Vadim Antonov, Artin Avanes, Jon Bock, Jonathan Claybaugh, Daniel Engovatov, Martin Hentschel, Jiansheng Huang, Allison W. Lee, Ashish Motivala, Abdul Q. Munir, Steven Pelley, Peter Povinec, Greg Rahn, Spyridon Triantafyllis, and Philipp Unterbrunner. 2016. Snowflake弹性数据仓库. 在2016年国际数据管理会议论文集中（旧金山，加利福尼亚州，美国）（SIGMOD '16）。计算机协会，纽约，NY，美国，215–226. [https:](https://doi.org/10.1145/2882903.2903741) [//doi.org/10.1145/2882903.2903741](https://doi.org/10.1145/2882903.2903741)
- <Anchor id="page-12-9"/>[23] Patrick Damme, Annett Ungethüm, Juliana Hildebrandt, Dirk Habich, and Wolfgang Lehner. 2019. 从全面的实验调查到针对轻量级整数压缩算法的基于成本的选择策略. ACM Trans. 数据库系统 44, 3, Article 9 (2019), 46页. https://doi.org/10.1145/3323991
- <Anchor id="page-12-1"/>[24] Philippe Dobbelaere 和 Kyumars Sheykh Esmaili. 2017. Kafka与RabbitMQ：两个行业参考发布/订阅实现的比较研究：行业论文（DEBS '17）。计算机协会，纽约，NY，美国，227–238. https://doi.org/10.1145/3093742.3093908
- <Anchor id="page-12-19"/>[25] LLVM文档. 2024. LLVM中的自动向量化. 从 https://llvm.org/docs/Vectorizers.html 获取，2024-06-20
- <Anchor id="page-12-7"/>[26] Siying Dong, Andrew Kryczka, Yanqin Jin, and Michael Stumm. 2021. RocksDB：为大规模应用服务的键值存储开发优先级演变. ACM Transactions on Storage 17, 4, Article 26 (2021), 32页. https://doi.org/10.1145/3483840
- <Anchor id="page-12-34"/>[27] Markus Dreseler, Martin Boissier, Tilmann Rabl, and Matthias Ufacker. 2020. 定量TPC-H瓶颈及其优化措施. Proc. VLDB Endow. 13, 8 (2020), 1206–1220. https://doi.org/10.14778/3389133.3389138
- <Anchor id="page-12-12"/>[28] Ted Dunning. 2021. t-digest：高效的分布估计. 软件影响 7 (2021). https://doi.org/10.1016/j.simpa.2020.100049
- <Anchor id="page-12-16"/>[29] Martin Faust, Martin Boissier, Marvin Keller, David Schwalb, Holger Bischof, Katrin Eisenreich, Franz Färber, and Hasso Plattner. 2016. 通过哈希索引在SAP HANA中实现足迹减少和唯一性强制. 在数据库和专家系统应用中. 137–151. [https://doi.org/10.1007/978-3-319-44406-](https://doi.org/10.1007/978-3-319-44406-2_11) [2_11](https://doi.org/10.1007/978-3-319-44406-2_11)
- <Anchor id="page-12-11"/>[30] Philippe Flajolet, Eric Fusy, Olivier Gandouet, and Frederic Meunier. 2007. HyperLogLog：近似最优基数估计算法的分析. 在AofA：算法分析中，DMTCS Proceedings第AH卷，2007年算法分析会议（AofA 07）。离散数学与理论计算机科学，137–156. https://doi.org/10.46298/dmtcs.3545
- <Anchor id="page-12-13"/>[31] Hector Garcia-Molina, Jefrey D. Ullman, and Jennifer Widom. 2009. 数据库系统 - 完整书籍（第2版）。
- <Anchor id="page-12-24"/>[32] Pawan Goyal, Harrick M. Vin, and Haichen Chen. 1996. 启动时间公平排队：用于综合服务分组交换网络的调度算法. 26, 4 (1996), 157–168. https://doi.org/10.1145/248157.248171
- <Anchor id="page-12-21"/>[33] Goetz Graefe. 1993. 大型数据库的查询评估技术. ACM计算机调查 25, 2 (1993), 73–169. https://doi.org/10.1145/152610.152611
- <Anchor id="page-12-40"/>[34] Jean-François Im, Kishore Gopalakrishna, Subbu Subramaniam, Mayank Shrivastava, Adwait Tumbde, Xiaotian Jiang, Jennifer Dai, Seunghyun Lee, Neha Pawar, Jialiang Li, and Ravi Aringunram. 2018. Pinot：实时OLAP，服务530万用户. 在2018年国际数据管理会议论文集中（休斯顿，TX，美国）（SIGMOD '18）。计算机协会，纽约，NY，美国，583–594. https://doi.org/10.1145/3183713.3190661
- <Anchor id="page-12-25"/>[35] ISO/IEC 9075-9:2001 2001. 信息技术 — 数据库语言 — SQL — 第9部分：外部数据管理（SQL/MED）。标准。国际标准化组织。
- <Anchor id="page-12-2"/>[36] Paras Jain, Peter Kraft, Conor Power, Tathagata Das, Ion Stoica, and Matei Zaharia. 2023. 分析和比较湖屋存储系统。CIDR.
- <Anchor id="page-12-4"/>[37] Project Jupyter. 2024. Jupyter Notebooks. 从 [https:](https://jupyter.org/) [//jupyter.org/](https://jupyter.org/) 获取，2024-06-20
- <Anchor id="page-12-22"/>[38] Timo Kersten, Viktor Leis, Alfons Kemper, Thomas Neumann, Andrew Pavlo, and Peter Boncz. 2018. 关于编译和向量化查询的所有知识但你不敢问. Proc. VLDB Endow. 11, 13 (2018年9月)，2209–2222. https://doi.org/10.14778/3275366.3284966
- <Anchor id="page-12-15"/>[39] Changkyu Kim, Jatin Chhugani, Nadathur Satish, Eric Sedlar, Anthony D. Nguyen, Tim Kaldewey, Victor W. Lee, Scott A. Brandt, and Pradeep Dubey. 2010. FAST：在现代CPU和GPU上快速架构敏感树搜索. 在2010年ACM SIGMOD国际数据管理会议论文集中（印第安纳波利斯，印第安纳，美国）（SIGMOD '10）。计算机协会，纽约，NY，美国，339–350. https://doi.org/10.1145/1807167.1807206
- <Anchor id="page-12-5"/>[40] Donald E. Knuth. 1973. 计算机程序设计的艺术，第三卷：排序与搜索. Addison-Wesley.
- <Anchor id="page-12-42"/>[41] André Kohn, Viktor Leis, and Thomas Neumann. 2018. 编译查询的自适应执行. 在2018 IEEE第34届国际数据工程会议（ICDE）。197–208. https://doi.org/10.1109/ICDE.2018.00027
- <Anchor id="page-12-36"/>[42] Andrew Lamb, Matt Fuller, Ramakrishna Varadarajan, Nga Tran, Ben Vandiver, Lyric Doshi, and Chuck Bear. 2012. Vertica分析数据库：C-Store 7年后. Proc. VLDB Endow. 5, 12 (2012年8月)，1790–1801. [https://doi.org/10.](https://doi.org/10.14778/2367502.2367518) [14778/2367502.2367518](https://doi.org/10.14778/2367502.2367518)
- <Anchor id="page-12-43"/>[43] Harald Lang, Tobias Mühlbauer, Florian Funke, Peter A. Boncz, Thomas Neumann, and Alfons Kemper. 2016. 数据块：在压缩存储上使用向量化和编译进行混合OLTP和OLAP. 在2016年国际数据管理会议论文集中（旧金山，加利福尼亚州，美国）（SIGMOD '16）。计算机协会，纽约，NY，美国，311–326. https://doi.org/10.1145/2882903.2882925
- <Anchor id="page-12-20"/>[44] Viktor Leis, Peter Boncz, Alfons Kemper, and Thomas Neumann. 2014. 以块为驱动的并行：针对多核时代的NUMA感知查询评估框架. 在2014年ACM SIGMOD国际数据管理会议论文集中（雪鸟，犹他州，美国）（SIGMOD '14）。计算机协会，纽约，NY，美国，743–754. [https://doi.org/10.1145/2588555.](https://doi.org/10.1145/2588555.2610507) [2610507](https://doi.org/10.1145/2588555.2610507)
- <Anchor id="page-13-17"/>[45] Viktor Leis, Alfons Kemper, and Thomas Neumann. 2013. 自适应基数树：为主内存数据库提供精巧的索引. 在2013 IEEE第29届国际数据工程会议（ICDE）。38–49. [https://doi.org/10.1109/ICDE.](https://doi.org/10.1109/ICDE.2013.6544812) [2013.6544812](https://doi.org/10.1109/ICDE.2013.6544812)
- <Anchor id="page-13-34"/>[46] Chunwei Liu, Anna Pavlenko, Matteo Interlandi, and Brandon Haynes. 2023. 深入探讨分析型DBMS的常见开放格式. 16, 11 (2023年7月)，3044–3056. https://doi.org/10.14778/3611479.3611507
- <Anchor id="page-13-30"/>[47] Zhenghua Lyu, Huan Hubert Zhang, Gang Xiong, Gang Guo, Haozhou Wang, Jinbao Chen, Asim Praveen, Yu Yang, Xiaoming Gao, Alexandra Wang, Wen Lin, Ashwin Agrawal, Junfeng Yang, Hao Wu, Xiaoliang Li, Feng Guo, Jiang Wu, Jesse Zhang, and Venkatesh Raghavan. 2021. Greenplum：混合数据库，适合事务和分析工作负载（SIGMOD '21）。计算机协会，纽约，NY，美国，2530–2542. [https:](https://doi.org/10.1145/3448016.3457562) [//doi.org/10.1145/3448016.3457562](https://doi.org/10.1145/3448016.3457562)
- <Anchor id="page-13-28"/>[48] Roger MacNicol and Blaine French. 2004. Sybase IQ Multiplex - 为分析而设计. 在第三十届国际大型数据库会议的论文中 - 第30卷（多伦多，加拿大）（VLDB '04）。VLDB基金会，1227–1230.
- <Anchor id="page-13-31"/>[49] Sergey Melnik, Andrey Gubarev, Jing Jing Long, Geofrey Romer, Shiva Shivakumar, Matt Tolton, Theo Vassilakis, Hossein Ahmadi, Dan Delorey, Slava Min, Mosha Pasumansky, and Jef Shute. 2020. Dremel：十年交互式SQL分析在网络规模下. Proc. VLDB Endow. 13, 12 (2020年8月)，3461–3472. https://doi.org/10.14778/3415478.3415568
- <Anchor id="page-13-2"/>[50] Microsoft. 2024. Kusto查询语言. 从 [https:](https://github.com/microsoft/Kusto-Query-Language) [//github.com/microsoft/Kusto-Query-Language](https://github.com/microsoft/Kusto-Query-Language) 获取，2024-06-20
- <Anchor id="page-13-14"/>[51] Guido Moerkotte. 1998. 小型物化聚合：用于数据仓库的轻量级索引结构. 在第24届国际大型数据库会议论文集（VLDB '98）。476–487.
- <Anchor id="page-13-22"/>[52] Jalal Mostafa, Sara Wehbi, Suren Chilingaryan, and Andreas Kopmann. 2022. SciTS：用于科学实验和工业物联网的时间序列数据库基准. 在第34届国际科学和统计数据库管理会议（SSDBM '22）论文集中。文章12. [https:](https://doi.org/10.1145/3538712.3538723) [//doi.org/10.1145/3538712.3538723](https://doi.org/10.1145/3538712.3538723)
- <Anchor id="page-13-0"/>[53] Thomas Neumann. 2011. 高效地为现代硬件编译有效的查询计划. Proc. VLDB Endow. 4, 9 (2011年6月)，539–550. [https://doi.org/10.14778/](https://doi.org/10.14778/2002938.2002940) [2002938.2002940](https://doi.org/10.14778/2002938.2002940)
- <Anchor id="page-13-25"/>[54] Thomas Neumann 和 Michael J. Freitag. 2020. Umbra：一种磁盘基础系统，具备内存性能. 在第10届创新数据系统研究会议，CIDR 2020，荷兰阿姆斯特丹，2020年1月12-15日，在线论文集。www.cidrdb.org. [http://cidrdb.org/cidr2020/papers/p29-neumann](http://cidrdb.org/cidr2020/papers/p29-neumann-cidr20.pdf)[cidr20.pdf](http://cidrdb.org/cidr2020/papers/p29-neumann-cidr20.pdf)
- <Anchor id="page-13-35"/>[55] Thomas Neumann, Tobias Mühlbauer, and Alfons Kemper. 2015. 针对主内存数据库系统的快速可序列化多版本并发控制. 在2015年ACM SIGMOD国际数据管理会议论文集中（澳大利亚墨尔本）（SIGMOD '15）。计算机协会，纽约，NY，美国，677–689. [https://doi.org/10.1145/2723372.](https://doi.org/10.1145/2723372.2749436) [2749436](https://doi.org/10.1145/2723372.2749436)
- <Anchor id="page-13-8"/>[56] LevelDB on GitHub. 2024. LevelDB. 从 [https://github.](https://github.com/google/leveldb) [com/google/leveldb](https://github.com/google/leveldb) 获取，2024-06-20
- <Anchor id="page-13-26"/>[57] Patrick O'Neil, Elizabeth O'Neil, Xuedong Chen, and Stephen Revilak. 2009. 星型模式基准与增强的事实表索引. 在性能评估与基准测试中。Springer Berlin Heidelberg，237–252. [https:](https://doi.org/10.1007/978-3-642-10424-4_17) [//doi.org/10.1007/978-3-642-10424-4_17](https://doi.org/10.1007/978-3-642-10424-4_17)
- <Anchor id="page-13-7"/>[58] Patrick E. O'Neil, Edward Y. C. Cheng, Dieter Gawlick, and Elizabeth J. O'Neil. 1996. 日志结构合并树（LSM-tree）。计算机学报 33 (1996)，351–385. https://doi.org/10.1007/s002360050048
- <Anchor id="page-13-4"/>[59] Diego Ongaro 和 John Ousterhout. 2014. 在寻找易于理解的共识算法. 在2014年USENIX会议上的USENIX年度技术会议论文集中（USENIX ATC'14）。305–320. [https://doi.org/doi/10.](https://doi.org/doi/10.5555/2643634.2643666) [5555/2643634.2643666](https://doi.org/doi/10.5555/2643634.2643666)
- <Anchor id="page-13-3"/>[60] Patrick O'Neil, Edward Cheng, Dieter Gawlick, and Elizabeth O'Neil. 1996. 日志结构合并树（LSM-Tree）。计算机科学杂志 33, 4 (1996)，351–385. [https:](https://doi.org/10.1007/s002360050048) [//doi.org/10.1007/s002360050048](https://doi.org/10.1007/s002360050048)
- <Anchor id="page-13-5"/>[61] Pandas. 2024. Pandas数据框. 从 [https://pandas.](https://pandas.pydata.org/) [pydata.org/](https://pandas.pydata.org/) 获取，2024-06-20
- <Anchor id="page-13-32"/>[62] Pedro Pedreira, Orri Erling, Masha Basmanova, Kevin Wilfong, Laith Sakka, Krishna Pai, Wei He, and Biswapesh Chattopadhyay. 2022. Velox：Meta的统一执行引擎. Proc. VLDB Endow. 15, 12 (2022年8月)，3372–3384. [https:](https://doi.org/10.14778/3554821.3554829) [//doi.org/10.14778/3554821.3554829](https://doi.org/10.14778/3554821.3554829)
- <Anchor id="page-13-10"/>[63] Tuomas Pelkonen, Scott Franklin, Justin Teller, Paul Cavallaro, Qi Huang, Justin Meza, and Kaushik Veeraraghavan. 2015. Gorilla：快速，可扩展的内存时间序列数据库. VLDB基金会会议录 8, 12 (2015)，1816–1827. https://doi.org/10.14778/2824032.2824078
- <Anchor id="page-13-18"/>[64] Orestis Polychroniou, Arun Raghavan, and Kenneth A. Ross. 2015. 重新思考内存数据库的SIMD向量化. 在2015年ACM SIGMOD国际数据管理会议论文集中（SIGMOD '15）。1493–1508. https://doi.org/10.1145/2723372.2747645
- <Anchor id="page-13-21"/>[65] PostgreSQL. 2024. PostgreSQL - 外部数据包装器. 从 https://wiki.postgresql.org/wiki/Foreign_data_wrappers 获取，2024-06-20
- <Anchor id="page-13-23"/>[66] Mark Raasveldt, Pedro Holanda, Tim Gubner, and Hannes Mühleisen. 2018. 公平基准测试被认为很困难：数据库性能测试中的常见陷阱. 在数据库系统测试研讨会论文集中（休斯顿，TX，美国）（DBTest'18）。文章2，6页。https://doi.org/10.1145/3209950.3209955
- <Anchor id="page-13-6"/>[67] Mark Raasveldt 和 Hannes Mühleisen. 2019. DuckDB：可嵌入的分析数据库（SIGMOD '19）。计算机协会，纽约，NY，美国，1981–1984. https://doi.org/10.1145/3299869.3320212
- <Anchor id="page-13-16"/>[68] Jun Rao 和 Kenneth A. Ross. 1999. 面向决策支持的缓存意识索引. 在第25届国际大型数据库会议论文集中（VLDB '99）。旧金山，加利福尼亚州，美国，78–89.
- <Anchor id="page-13-36"/>[69] Navin C. Sabharwal 和 Piyush Kant Pandey. 2020. 使用Prometheus查询语言（PromQL）进行工作. 在监控微服务和容器化应用程序中. https://doi.org/10.1007/978-1-4842-6216-0_5
- <Anchor id="page-13-27"/>[70] Todd W. Schneider. 2022. 纽约市出租车和租赁车辆数据. 从 https://github.com/toddwschneider/nyc-taxi-data 获取，2024-06-20
- <Anchor id="page-13-13"/>[71] Mike Stonebraker, Daniel J. Abadi, Adam Batkin, Xuedong Chen, Mitch Cherniack, Miguel Ferreira, Edmond Lau, Amerson Lin, Sam Madden, Elizabeth O'Neil, Pat O'Neil, Alex Rasin, Nga Tran, 和 Stan Zdonik. 2005. C-Store：一种列式数据库管理系统. 在第31届国际大型数据库会议论文集中（VLDB '05）。553–564.
- <Anchor id="page-13-29"/>[72] Teradata. 2024. Teradata数据库. 从 [https://www.](https://www.teradata.com/resources/datasheets/teradata-database) [teradata.com/resources/datasheets/teradata-database](https://www.teradata.com/resources/datasheets/teradata-database) 获取，2024-06-20
- <Anchor id="page-13-15"/>[73] Frederik Transier. 2010. 面向内存文本搜索引擎的算法和数据结构. 博士论文. https://doi.org/10.5445/IR/1000015824
- <Anchor id="page-13-24"/>[74] Adrian Vogelsgesang, Michael Haubenschild, Jan Finis, Alfons Kemper, Viktor Leis, Tobias Muehlbauer, Thomas Neumann, and Manuel Then. 2018. 现实：基准测试如何未能代表现实世界. 在数据库系统测试研讨会论文集中（休斯顿，TX，美国）（DBTest'18）。文章1，6页。https://doi.org/10.1145/3209950.3209952
- <Anchor id="page-13-9"/>[75] LZ4官方网站. 2024. LZ4. 从 https://lz4.org/ 获取，2024-06-20
- <Anchor id="page-13-11"/><Anchor id="page-13-1"/>[76] PRQL官方网站. 2024. PRQL. 从 https://prql-lang.org 获取，2024-06-20 [77] Till Westmann, Donald Kossmann, Sven Helmer, 和 Guido Moerkotte. 2000. 压缩数据库的实现与性能. SIGMOD记录.
- <Anchor id="page-13-33"/>29, 3 (2000年9月)，55–67. https://doi.org/10.1145/362084.362137 [78] Fangjin Yang, Eric Tschetter, Xavier Léauté, Nelson Ray, Gian Merlino, 和 Deep Ganguli. 2014. Druid：一种实时分析数据存储. 在2014年ACM SIGMOD国际数据管理会议论文集中（雪鸟，犹他州，美国）（SIGMOD '14）。计算机协会，纽约，NY，美国，157–168. https://doi.org/10.1145/2588555.2595631
- <Anchor id="page-13-20"/>[79] Tianqi Zheng, Zhibin Zhang, 和 Xueqi Cheng. 2020. SAHA：适用于分析数据库的字符串自适应哈希表. 应用科学 10, 6 (2020). [https:](https://doi.org/10.3390/app10061915) [//doi.org/10.3390/app10061915](https://doi.org/10.3390/app10061915)
- <Anchor id="page-13-19"/>[80] Jingren Zhou 和 Kenneth A. Ross. 2002. 使用SIMD指令实现数据库操作. 在2002年ACM SIGMOD国际数据管理会议论文集中（SIGMOD '02）。145–156. [https://doi.org/10.](https://doi.org/10.1145/564691.564709) [1145/564691.564709](https://doi.org/10.1145/564691.564709)
- <Anchor id="page-13-12"/>[81] Marcin Zukowski, Sandor Heman, Niels Nes, 和 Peter Boncz. 2006. 超标量RAM-CPU缓存压缩. 在第22届国际数据工程会议（ICDE '06）论文集中。59. [https://doi.org/10.1109/ICDE.](https://doi.org/10.1109/ICDE.2006.150) [2006.150](https://doi.org/10.1109/ICDE.2006.150)
